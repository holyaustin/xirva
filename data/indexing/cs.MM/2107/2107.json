[{"id": "2107.00328", "submitter": "Shurun Wang", "authors": "Shurun Wang, Zhao Wang, Shiqi Wang, Yan Ye", "title": "End-to-end Compression Towards Machine Vision: Network Architecture\n  Design and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The research of visual signal compression has a long history. Fueled by deep\nlearning, exciting progress has been made recently. Despite achieving better\ncompression performance, existing end-to-end compression algorithms are still\ndesigned towards better signal quality in terms of rate-distortion\noptimization. In this paper, we show that the design and optimization of\nnetwork architecture could be further improved for compression towards machine\nvision. We propose an inverted bottleneck structure for end-to-end compression\ntowards machine vision, which specifically accounts for efficient\nrepresentation of the semantic information. Moreover, we quest the capability\nof optimization by incorporating the analytics accuracy into the optimization\nprocess, and the optimality is further explored with generalized rate-accuracy\noptimization in an iterative manner. We use object detection as a showcase for\nend-to-end compression towards machine vision, and extensive experiments show\nthat the proposed scheme achieves significant BD-rate savings in terms of\nanalysis performance. Moreover, the promise of the scheme is also demonstrated\nwith strong generalization capability towards other machine vision tasks, due\nto the enabling of signal-level reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:36:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wang", "Shurun", ""], ["Wang", "Zhao", ""], ["Wang", "Shiqi", ""], ["Ye", "Yan", ""]]}, {"id": "2107.00648", "submitter": "Nathaniel Braman", "authors": "Nathaniel Braman, Jacob W. H. Gordon, Emery T. Goossens, Caleb Willis,\n  Martin C. Stumpe, Jagadish Venkataraman", "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery\n  Integrating Radiology, Pathology, Genomic, and Clinical Data", "comments": "Accepted for presentation at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM q-bio.GN q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:59:01 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Braman", "Nathaniel", ""], ["Gordon", "Jacob W. H.", ""], ["Goossens", "Emery T.", ""], ["Willis", "Caleb", ""], ["Stumpe", "Martin C.", ""], ["Venkataraman", "Jagadish", ""]]}, {"id": "2107.00650", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Anna Rohrbach, Trevor Darrell", "title": "CLIP-It! Language-Guided Video Summarization", "comments": "Website at https://medhini.github.io/clip_it/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A generic video summary is an abridged version of a video that conveys the\nwhole story and features the most important scenes. Yet the importance of\nscenes in a video is often subjective, and users should have the option of\ncustomizing the summary by using natural language to specify what is important\nto them. Further, existing models for fully automatic generic summarization\nhave not exploited available language models, which can serve as an effective\nprior for saliency. This work introduces CLIP-It, a single framework for\naddressing both generic and query-focused video summarization, typically\napproached separately in the literature. We propose a language-guided\nmultimodal transformer that learns to score frames in a video based on their\nimportance relative to one another and their correlation with a user-defined\nquery (for query-focused summarization) or an automatically generated dense\nvideo caption (for generic video summarization). Our model can be extended to\nthe unsupervised setting by training without ground-truth supervision. We\noutperform baselines and prior work by a significant margin on both standard\nvideo summarization datasets (TVSum and SumMe) and a query-focused video\nsummarization dataset (QFVS). Particularly, we achieve large improvements in\nthe transfer setting, attesting to our method's strong generalization\ncapabilities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:59:27 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""]]}, {"id": "2107.00938", "submitter": "Mathias-Felipe De-Lima-Santos", "authors": "Mathias-Felipe de-Lima-Santos and Arwa Kooli", "title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ\n  Labs Instagram Page", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.GR cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News outlets are developing formats dedicated to social platforms that\ncapture audience attention, such as Instagram stories, Facebook Instant\narticles, and YouTube videos. In some cases, these formats are created in\ncollaboration with the tech companies themselves. At the same time, the use of\ndata-driven storytelling is becoming increasingly integrated into the\never-complex business models of news outlets, generating more impact and\nvisibility. Previous studies have focused on studying these two effects\nseparately. To address this gap in the literature, this paper identifies and\nanalyzes the use of data journalism on the Instagram content of AJ Labs, the\nteam dedicated to producing data-driven and interactive stories for the Al\nJazeera news network. Drawing upon a mixed-method approach, this study examines\nthe use and characteristics of data stories on social media platforms. Results\nsuggest that there is reliance on producing visual content that covers topics\nsuch as politics and violence. In general, AJ Labs relies on the use of\ninfographics and produces its own unique data. To conclude, this paper suggests\npotential ways to improve the use of Instagram to tell data stories.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:51:49 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["de-Lima-Santos", "Mathias-Felipe", ""], ["Kooli", "Arwa", ""]]}, {"id": "2107.01175", "submitter": "Su Zhang", "authors": "Su Zhang, Yi Ding, Ziquan Wei, Cuntai Guan", "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.469 for valence and 0.649 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 16:28:55 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 09:07:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Su", ""], ["Ding", "Yi", ""], ["Wei", "Ziquan", ""], ["Guan", "Cuntai", ""]]}, {"id": "2107.01461", "submitter": "C.-H. Huck Yang", "authors": "Chao-Han Huck Yang, Hu Hu, Sabato Marco Siniscalchi, Qing Wang, Yuyang\n  Wang, Xianjun Xia, Yuanjun Zhao, Yuzhong Wu, Yannan Wang, Jun Du, Chin-Hui\n  Lee", "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust\n  Neural Acoustic Scene Classification", "comments": "5 figures. DCASE 2021. The project started in November 2020", "journal-ref": "Detection and Classification of Acoustic Scenes and Events\n  (DCASE), 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled Acoustic Lottery. Acoustic Lottery could compress an ASC model over\n$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and\nLog loss of 0.76) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 16:25:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yang", "Chao-Han Huck", ""], ["Hu", "Hu", ""], ["Siniscalchi", "Sabato Marco", ""], ["Wang", "Qing", ""], ["Wang", "Yuyang", ""], ["Xia", "Xianjun", ""], ["Zhao", "Yuanjun", ""], ["Wu", "Yuzhong", ""], ["Wang", "Yannan", ""], ["Du", "Jun", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "2107.02112", "submitter": "Meng-Jiun Chiou", "authors": "Meng-Jiun Chiou, Henghui Ding, Hanshu Yan, Changhu Wang, Roger\n  Zimmermann, Jiashi Feng", "title": "Recovering the Unbiased Scene Graphs from the Biased Ones", "comments": "Accepted by ACMMM 2021. Source code will be available at\n  https://github.com/coldmanck/recovering-unbiased-scene-graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given input images, scene graph generation (SGG) aims to produce\ncomprehensive, graphical representations describing visual relationships among\nsalient objects. Recently, more efforts have been paid to the long tail problem\nin SGG; however, the imbalance in the fraction of missing labels of different\nclasses, or reporting bias, exacerbating the long tail is rarely considered and\ncannot be solved by the existing debiasing methods. In this paper we show that,\ndue to the missing labels, SGG can be viewed as a \"Learning from Positive and\nUnlabeled data\" (PU learning) problem, where the reporting bias can be removed\nby recovering the unbiased probabilities from the biased ones by utilizing\nlabel frequencies, i.e., the per-class fraction of labeled, positive examples\nin all the positive examples. To obtain accurate label frequency estimates, we\npropose Dynamic Label Frequency Estimation (DLFE) to take advantage of\ntraining-time data augmentation and average over multiple training iterations\nto introduce more valid examples. Extensive experiments show that DLFE is more\neffective in estimating label frequencies than a naive variant of the\ntraditional estimate, and DLFE significantly alleviates the long tail and\nachieves state-of-the-art debiasing performance on the VG dataset. We also show\nqualitatively that SGG models with DLFE produce prominently more balanced and\nunbiased scene graphs.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:10:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chiou", "Meng-Jiun", ""], ["Ding", "Henghui", ""], ["Yan", "Hanshu", ""], ["Wang", "Changhu", ""], ["Zimmermann", "Roger", ""], ["Feng", "Jiashi", ""]]}, {"id": "2107.02192", "submitter": "Wei Ping", "authors": "Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein,\n  Anima Anandkumar, Bryan Catanzaro", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:00:14 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 19:34:30 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhu", "Chen", ""], ["Ping", "Wei", ""], ["Xiao", "Chaowei", ""], ["Shoeybi", "Mohammad", ""], ["Goldstein", "Tom", ""], ["Anandkumar", "Anima", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2107.02408", "submitter": "Shahroz Tariq", "authors": "Minha Kim and Shahroz Tariq and Simon S. Woo", "title": "CoReD: Generalizing Fake Media Detection with Continual Representation\n  using Distillation", "comments": "10 pages, 2 Figures, 10 Tables, Accepted for publication in the 29th\n  ACM International Conference on Multimedia (ACMMM '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CoL), Representation Learning (ReL), and\nKnowledge Distillation (KD). We design CoReD to perform sequential domain\nadaptation tasks on new deepfake and GAN-generated synthetic face datasets,\nwhile effectively minimizing the catastrophic forgetting in a teacher-student\nmodel setting. Our extensive experimental results demonstrate that our method\nis efficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:07:17 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 05:27:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kim", "Minha", ""], ["Tariq", "Shahroz", ""], ["Woo", "Simon S.", ""]]}, {"id": "2107.02434", "submitter": "Shunquan Tan", "authors": "Long Zhuo and Shunquan Tan and Bin Li and Jiwu Huang", "title": "Self-Adversarial Training incorporating Forgery Attention for Image\n  Forgery Localization", "comments": "submitted to TIFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image editing techniques enable people to modify the content of an image\nwithout leaving visual traces and thus may cause serious security risks. Hence\nthe detection and localization of these forgeries become quite necessary and\nchallenging. Furthermore, unlike other tasks with extensive data, there is\nusually a lack of annotated forged images for training due to annotation\ndifficulties. In this paper, we propose a self-adversarial training strategy\nand a reliable coarse-to-fine network that utilizes a self-attention mechanism\nto localize forged regions in forgery images. The self-attention module is\nbased on a Channel-Wise High Pass Filter block (CW-HPF). CW-HPF leverages\ninter-channel relationships of features and extracts noise features by high\npass filters. Based on the CW-HPF, a self-attention mechanism, called forgery\nattention, is proposed to capture rich contextual dependencies of intrinsic\ninconsistency extracted from tampered regions. Specifically, we append two\ntypes of attention modules on top of CW-HPF respectively to model internal\ninterdependencies in spatial dimension and external dependencies among\nchannels. We exploit a coarse-to-fine network to enhance the noise\ninconsistency between original and tampered regions. More importantly, to\naddress the issue of insufficient training data, we design a self-adversarial\ntraining strategy that expands training data dynamically to achieve more robust\nperformance. Specifically, in each training iteration, we perform adversarial\nattacks against our network to generate adversarial examples and train our\nmodel on them. Extensive experimental results demonstrate that our proposed\nalgorithm steadily outperforms state-of-the-art methods by a clear margin in\ndifferent benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:20:08 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhuo", "Long", ""], ["Tan", "Shunquan", ""], ["Li", "Bin", ""], ["Huang", "Jiwu", ""]]}, {"id": "2107.03088", "submitter": "Peidong Liu", "authors": "Peidong Liu, Zibin He, Xiyu Yan, Yong Jiang, Shutao Xia, Feng Zheng,\n  Maowei Hu", "title": "WeClick: Weakly-Supervised Video Semantic Segmentation with Click\n  Annotations", "comments": "Accepted by ACM MM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with tedious per-pixel mask annotating, it is much easier to\nannotate data by clicks, which costs only several seconds for an image.\nHowever, applying clicks to learn video semantic segmentation model has not\nbeen explored before. In this work, we propose an effective weakly-supervised\nvideo semantic segmentation pipeline with click annotations, called WeClick,\nfor saving laborious annotating effort by segmenting an instance of the\nsemantic class with only a single click. Since detailed semantic information is\nnot captured by clicks, directly training with click labels leads to poor\nsegmentation predictions. To mitigate this problem, we design a novel memory\nflow knowledge distillation strategy to exploit temporal information (named\nmemory flow) in abundant unlabeled video frames, by distilling the neighboring\npredictions to the target frame via estimated motion. Moreover, we adopt\nvanilla knowledge distillation for model compression. In this case, WeClick\nlearns compact video semantic segmentation models with the low-cost click\nannotations during the training phase yet achieves real-time and accurate\nmodels during the inference period. Experimental results on Cityscapes and\nCamvid show that WeClick outperforms the state-of-the-art methods, increases\nperformance by 10.24% mIoU than baseline, and achieves real-time execution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:12:46 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liu", "Peidong", ""], ["He", "Zibin", ""], ["Yan", "Xiyu", ""], ["Jiang", "Yong", ""], ["Xia", "Shutao", ""], ["Zheng", "Feng", ""], ["Hu", "Maowei", ""]]}, {"id": "2107.03120", "submitter": "Hao Tang", "authors": "Gaowen Liu, Hao Tang, Hugo Latapie, Jason Corso, Yan Yan", "title": "Cross-View Exocentric to Egocentric Video Synthesis", "comments": "ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view video synthesis task seeks to generate video sequences of one view\nfrom another dramatically different view. In this paper, we investigate the\nexocentric (third-person) view to egocentric (first-person) view video\ngeneration task. This is challenging because egocentric view sometimes is\nremarkably different from the exocentric view. Thus, transforming the\nappearances across the two different views is a non-trivial task. Particularly,\nwe propose a novel Bi-directional Spatial Temporal Attention Fusion Generative\nAdversarial Network (STA-GAN) to learn both spatial and temporal information to\ngenerate egocentric video sequences from the exocentric view. The proposed\nSTA-GAN consists of three parts: temporal branch, spatial branch, and attention\nfusion. First, the temporal and spatial branches generate a sequence of fake\nframes and their corresponding features. The fake frames are generated in both\ndownstream and upstream directions for both temporal and spatial branches.\nNext, the generated four different fake frames and their corresponding features\n(spatial and temporal branches in two directions) are fed into a novel\nmulti-generation attention fusion module to produce the final video sequence.\nMeanwhile, we also propose a novel temporal and spatial dual-discriminator for\nmore robust network optimization. Extensive experiments on the Side2Ego and\nTop2Ego datasets show that the proposed STA-GAN significantly outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 10:00:52 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liu", "Gaowen", ""], ["Tang", "Hao", ""], ["Latapie", "Hugo", ""], ["Corso", "Jason", ""], ["Yan", "Yan", ""]]}, {"id": "2107.03298", "submitter": "Hui Lu", "authors": "Hui Lu, Zhiyong Wu, Xixin Wu, Xu Li, Shiyin Kang, Xunying Liu, Helen\n  Meng", "title": "VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive\n  Text-to-Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a variational auto-encoder based non-autoregressive\ntext-to-speech (VAENAR-TTS) model. The autoregressive TTS (AR-TTS) models based\non the sequence-to-sequence architecture can generate high-quality speech, but\ntheir sequential decoding process can be time-consuming. Recently,\nnon-autoregressive TTS (NAR-TTS) models have been shown to be more efficient\nwith the parallel decoding process. However, these NAR-TTS models rely on\nphoneme-level durations to generate a hard alignment between the text and the\nspectrogram. Obtaining duration labels, either through forced alignment or\nknowledge distillation, is cumbersome. Furthermore, hard alignment based on\nphoneme expansion can degrade the naturalness of the synthesized speech. In\ncontrast, the proposed model of VAENAR-TTS is an end-to-end approach that does\nnot require phoneme-level durations. The VAENAR-TTS model does not contain\nrecurrent structures and is completely non-autoregressive in both the training\nand inference phases. Based on the VAE architecture, the alignment information\nis encoded in the latent variable, and attention-based soft alignment between\nthe text and the latent variable is used in the decoder to reconstruct the\nspectrogram. Experiments show that VAENAR-TTS achieves state-of-the-art\nsynthesis quality, while the synthesis speed is comparable with other NAR-TTS\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:32:36 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lu", "Hui", ""], ["Wu", "Zhiyong", ""], ["Wu", "Xixin", ""], ["Li", "Xu", ""], ["Kang", "Shiyin", ""], ["Liu", "Xunying", ""], ["Meng", "Helen", ""]]}, {"id": "2107.04510", "submitter": "Anastasia Zvezdakova", "authors": "Maksim Siniukov, Anastasia Antsiferova, Dmitriy Kulikov, Dmitriy\n  Vatolin", "title": "Hacking VMAF and VMAF NEG: metrics vulnerability to different\n  preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality measurement plays a critical role in the development of video\nprocessing applications. In this paper, we show how popular quality metrics\nVMAF and its tuning-resistant version VMAF NEG can be artificially increased by\nvideo preprocessing. We propose a pipeline for tuning parameters of processing\nalgorithms that allows increasing VMAF by up to 218.8%. A subjective comparison\nof preprocessed videos showed that with the majority of methods visual quality\ndrops down or stays unchanged. We show that VMAF NEG scores can also be\nincreased by some preprocessing methods by up to 23.6%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:53:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Siniukov", "Maksim", ""], ["Antsiferova", "Anastasia", ""], ["Kulikov", "Dmitriy", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "2107.04768", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Bing-Kun Bao, Changsheng Xu", "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering", "comments": "12 pages, 12 figures", "journal-ref": "IEEE Transactions on Multimedia 2021", "doi": "10.1109/TMM.2021.3097171", "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video question answering is a challenging task, which requires agents to be\nable to understand rich video contents and perform spatial-temporal reasoning.\nHowever, existing graph-based methods fail to perform multi-step reasoning\nwell, neglecting two properties of VideoQA: (1) Even for the same video,\ndifferent questions may require different amount of video clips or objects to\ninfer the answer with relational reasoning; (2) During reasoning, appearance\nand motion features have complicated interdependence which are correlated and\ncomplementary to each other. Based on these observations, we propose a\nDual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an\nend-to-end fashion. The first contribution of our DualVGR is the design of an\nexplainable Query Punishment Module, which can filter out irrelevant visual\nfeatures through multiple cycles of reasoning. The second contribution is the\nproposed Video-based Multi-view Graph Attention Network, which captures the\nrelations between appearance and motion features. Our DualVGR network achieves\nstate-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and\ndemonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is\navailable at https://github.com/MMIR/DualVGR-VideoQA.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 06:08:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Jianyu", ""], ["Bao", "Bing-Kun", ""], ["Xu", "Changsheng", ""]]}, {"id": "2107.04770", "submitter": "Sohei Itahara", "authors": "Tomoya Sunami, Sohei Itahara, Yusuke Koda, Takayuki Nishio, and Koji\n  Yamamoto", "title": "Computer Vision-assisted Decimeter-level Single-antenna RSSI\n  Localization Harnessing Dynamic Blockage Events", "comments": "Submitted to IEEE transactions on mobile computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the feasibility of received power strength indicator\n(RSSI)-based single-antenna localization (R-SAL) with decimeter-level\nlocalization accuracy. To achieve decimeter-level accuracy, either fine-grained\nradio frequency (RF) information (e.g., channel state information) or\ncoarse-grained RF information (e.g., RSSI) from more than multiple antennas is\nrequired. Meanwhile, owing to deficiency of single-antenna RSSI which only\nindicates a distance between a receiver and a transmitter, realizing\nfine-grained localization accuracy with single coarse-grained RF information is\nchallenging. Our key idea to address this challenge is to leverage computer\nvision (CV) and to estimate the most likely Fresnel zone between the receiver\nand transmitter, where the role of RSSI is to detect blockage timings.\nSpecifically, historical positions of an obstacle that dynamically blocks the\nFresnel zone are detected by the CV technique, and we estimate positions at\nwhich a blockage starts and ends via a time series of RSSI. These estimated\nobstacle positions, in principle, coincide with points on the Fresnel zone\nboundaries, enabling the estimation of the Fresnel zone and localization of the\ntransmitter. The experimental evaluation revealed that the proposed R-SAL\nachieved decimeter-level localization in an indoor environment, which is\ncomparable to that of a simple previous RSSI-based localization with three\nreceivers.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 06:23:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sunami", "Tomoya", ""], ["Itahara", "Sohei", ""], ["Koda", "Yusuke", ""], ["Nishio", "Takayuki", ""], ["Yamamoto", "Koji", ""]]}, {"id": "2107.04878", "submitter": "Marcos V. Conde", "authors": "Marcos V. Conde, Kumar Shubham, Prateek Agnihotri, Nitin D. Movva,\n  Szilard Bessenyei", "title": "Weakly-Supervised Classification and Detection of Bird Sounds in the\n  Wild. A BirdCLEF 2021 Solution", "comments": "Proceedings Working Notes CEURWS @ CLEF 2021 - BirdCLEF 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is easier to hear birds than see them, however, they still play an\nessential role in nature and they are excellent indicators of deteriorating\nenvironmental quality and pollution. Recent advances in Machine Learning and\nConvolutional Neural Networks allow us to detect and classify bird sounds, by\ndoing this, we can assist researchers in monitoring the status and trends of\nbird populations and biodiversity in ecosystems. We propose a sound detection\nand classification pipeline for analyzing complex soundscape recordings and\nidentify birdcalls in the background. Our pipeline learns from weak labels,\nclassifies fine-grained bird vocalizations in the wild, and is robust against\nbackground sounds (e.g., airplanes, rain, etc). Our solution achieved 10th\nplace of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 17:11:44 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Conde", "Marcos V.", ""], ["Shubham", "Kumar", ""], ["Agnihotri", "Prateek", ""], ["Movva", "Nitin D.", ""], ["Bessenyei", "Szilard", ""]]}, {"id": "2107.04954", "submitter": "Kin Wai Cheuk", "authors": "Kin Wai Cheuk, Dorien Herremans, Li Su", "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for\n  Low-Resource Real-World Data", "comments": "Accepted in ACMMM 21. Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 03:25:58 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 04:49:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Cheuk", "Kin Wai", ""], ["Herremans", "Dorien", ""], ["Su", "Li", ""]]}, {"id": "2107.05223", "submitter": "Yi-Hui Chou", "authors": "Yi-Hui Chou, I-Chun Chen, Chin-Jui Chang, Joann Ching, and Yi-Hsuan\n  Yang", "title": "MidiBERT-Piano: Large-scale Pre-training for Symbolic Music\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an attempt to employ the mask language modeling approach\nof BERT to pre-train a 12-layer Transformer model over 4,166 pieces of\npolyphonic piano MIDI files for tackling a number of symbolic-domain\ndiscriminative music understanding tasks. These include two note-level\nclassification tasks, i.e., melody extraction and velocity prediction, as well\nas two sequence-level classification tasks, i.e., composer classification and\nemotion classification. We find that, given a pre-trained Transformer, our\nmodels outperform recurrent neural network based baselines with less than 10\nepochs of fine-tuning. Ablation studies show that the pre-training remains\neffective even if none of the MIDI data of the downstream tasks are seen at the\npre-training stage, and that freezing the self-attention layers of the\nTransformer at the fine-tuning stage slightly degrades performance. All the\nfive datasets employed in this work are publicly available, as well as\ncheckpoints of our pre-trained and fine-tuned models. As such, our research can\nbe taken as a benchmark for symbolic-domain music understanding.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 07:03:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chou", "Yi-Hui", ""], ["Chen", "I-Chun", ""], ["Chang", "Chin-Jui", ""], ["Ching", "Joann", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "2107.05297", "submitter": "Cise Midoglu M.Sc.", "authors": "Shivangi Aneja, Cise Midoglu, Duc-Tien Dang-Nguyen, Michael Alexander\n  Riegler, Paal Halvorsen, Matthias Niessner, Balu Adsumilli, Chris Bregler", "title": "MMSys'21 Grand Challenge on Detecting Cheapfakes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse} of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 10:14:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Aneja", "Shivangi", ""], ["Midoglu", "Cise", ""], ["Dang-Nguyen", "Duc-Tien", ""], ["Riegler", "Michael Alexander", ""], ["Halvorsen", "Paal", ""], ["Niessner", "Matthias", ""], ["Adsumilli", "Balu", ""], ["Bregler", "Chris", ""]]}, {"id": "2107.05315", "submitter": "Yinwei Wei", "authors": "Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li,\n  Tat-Seng Chua", "title": "Contrastive Learning for Cold-Start Recommendation", "comments": "Accepted by ACM Multimedia 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending cold-start items is a long-standing and fundamental challenge in\nrecommender systems. Without any historical interaction on cold-start items, CF\nscheme fails to use collaborative signals to infer user preference on these\nitems. To solve this problem, extensive studies have been conducted to\nincorporate side information into the CF scheme. Specifically, they employ\nmodern neural network techniques (e.g., dropout, consistency constraint) to\ndiscover and exploit the coalition effect of content features and collaborative\nrepresentations. However, we argue that these works less explore the mutual\ndependencies between content features and collaborative representations and\nlack sufficient theoretical supports, thus resulting in unsatisfactory\nperformance. In this work, we reformulate the cold-start item representation\nlearning from an information-theoretic standpoint. It aims to maximize the\nmutual dependencies between item content and collaborative signals.\nSpecifically, the representation learning is theoretically lower-bounded by the\nintegration of two terms: mutual information between collaborative embeddings\nof users and items, and mutual information between collaborative embeddings and\nfeature representations of items. To model such a learning process, we devise a\nnew objective function founded upon contrastive learning and develop a simple\nyet effective Contrastive Learning-based Cold-start Recommendation\nframework(CLCRec). In particular, CLCRec consists of three components:\ncontrastive pair organization, contrastive embedding, and contrastive\noptimization modules. It allows us to preserve collaborative signals in the\ncontent representations for both warm and cold-start items. Through extensive\nexperiments on four publicly accessible datasets, we observe that CLCRec\nachieves significant improvements over state-of-the-art approaches in both\nwarm- and cold-start scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 11:00:20 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 01:41:24 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 07:29:52 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wei", "Yinwei", ""], ["Wang", "Xiang", ""], ["Li", "Qi", ""], ["Nie", "Liqiang", ""], ["Li", "Yan", ""], ["Li", "Xuanping", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2107.05465", "submitter": "Yixin Liu", "authors": "Yixin Liu, Jiaxin Guo, Jieyang Dong, Luoqian Jiang and Haoyuan Ouyang", "title": "Priority prediction of Asian Hornet sighting report using machine\n  learning methods", "comments": "2021 IEEE International Conference on Software Engineering and\n  Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As infamous invaders to the North American ecosystem, the Asian giant hornet\n(Vespa mandarinia) is devastating not only to native bee colonies, but also to\nlocal apiculture. One of the most effective way to combat the harmful species\nis to locate and destroy their nests. By mobilizing the public to actively\nreport possible sightings of the Asian giant hornet, the governmentcould timely\nsend inspectors to confirm and possibly destroy the nests. However, such\nconfirmation requires lab expertise, where manually checking the reports one by\none is extremely consuming of human resources. Further given the limited\nknowledge of the public about the Asian giant hornet and the randomness of\nreport submission, only few of the numerous reports proved positive, i.e.\nexisting nests. How to classify or prioritize the reports efficiently and\nautomatically, so as to determine the dispatch of personnel, is of great\nsignificance to the control of the Asian giant hornet. In this paper, we\npropose a method to predict the priority of sighting reports based on machine\nlearning. We model the problem of optimal prioritization of sighting reports as\na problem of classification and prediction. We extracted a variety of rich\nfeatures in the report: location, time, image(s), and textual description.\nBased on these characteristics, we propose a classification model based on\nlogistic regression to predict the credibility of a certain report.\nFurthermore, our model quantifies the impact between reports to get the\npriority ranking of the reports. Extensive experiments on the public dataset\nfrom the WSDA (the Washington State Department of Agriculture) have proved the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:33:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Liu", "Yixin", ""], ["Guo", "Jiaxin", ""], ["Dong", "Jieyang", ""], ["Jiang", "Luoqian", ""], ["Ouyang", "Haoyuan", ""]]}, {"id": "2107.05617", "submitter": "Alina Roitberg", "authors": "Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold,\n  Simon Rei{\\ss}, Rainer Stiefelhagen", "title": "Let's Play for Action: Recognizing Activities of Daily Living by\n  Learning from Life Simulation Video Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Activities of Daily Living (ADL) is a vital process for\nintelligent assistive robots, but collecting large annotated datasets requires\ntime-consuming temporal labeling and raises privacy concerns, e.g., if the data\nis collected in a real household. In this work, we explore the concept of\nconstructing training examples for ADL recognition by playing life simulation\nvideo games and introduce the SIMS4ACTION dataset created with the popular\ncommercial game THE SIMS 4. We build Sims4Action by specifically executing\nactions-of-interest in a \"top-down\" manner, while the gaming circumstances\nallow us to freely switch between environments, camera angles and subject\nappearances. While ADL recognition on gaming data is interesting from the\ntheoretical perspective, the key challenge arises from transferring it to the\nreal-world applications, such as smart-homes or assistive robotics. To meet\nthis requirement, Sims4Action is accompanied with a GamingToReal benchmark,\nwhere the models are evaluated on real videos derived from an existing ADL\ndataset. We integrate two modern algorithms for video-based activity\nrecognition in our framework, revealing the value of life simulation video\ngames as an inexpensive and far less intrusive source of training data.\nHowever, our results also indicate that tasks involving a mixture of gaming and\nreal data are challenging, opening a new research direction. We will make our\ndataset publicly available at https://github.com/aroitberg/sims4action.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:53:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Roitberg", "Alina", ""], ["Schneider", "David", ""], ["Djamal", "Aulia", ""], ["Seibold", "Constantin", ""], ["Rei\u00df", "Simon", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2107.05677", "submitter": "Chris Donahue", "authors": "Rodrigo Castellon and Chris Donahue and Percy Liang", "title": "Codified audio language modeling learns useful representations for music\n  information retrieval", "comments": "To appear in the proceedings of ISMIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:28:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Castellon", "Rodrigo", ""], ["Donahue", "Chris", ""], ["Liang", "Percy", ""]]}, {"id": "2107.06219", "submitter": "Xingxuan Zhang", "authors": "Xingxuan Zhang, Linjun Zhou, Renzhe Xu, Peng Cui, Zheyan Shen, Haoxin\n  Liu", "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain\n  Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:20:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhang", "Xingxuan", ""], ["Zhou", "Linjun", ""], ["Xu", "Renzhe", ""], ["Cui", "Peng", ""], ["Shen", "Zheyan", ""], ["Liu", "Haoxin", ""]]}, {"id": "2107.06252", "submitter": "Gunjan Aggarwal", "authors": "Gunjan Aggarwal, Devi Parikh", "title": "Dance2Music: Automatic Dance-driven Music Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dance and music typically go hand in hand. The complexities in dance, music,\nand their synchronisation make them fascinating to study from a computational\ncreativity perspective. While several works have looked at generating dance for\na given music, automatically generating music for a given dance remains\nunder-explored. This capability could have several creative expression and\nentertainment applications. We present some early explorations in this\ndirection. We present a search-based offline approach that generates music\nafter processing the entire dance video and an online approach that uses a deep\nneural network to generate music on-the-fly as the video proceeds. We compare\nthese approaches to a strong heuristic baseline via human studies and present\nour findings. We have integrated our online approach in a live demo! A video of\nthe demo can be found here:\nhttps://sites.google.com/view/dance2music/live-demo.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:22:42 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:20:29 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Aggarwal", "Gunjan", ""], ["Parikh", "Devi", ""]]}, {"id": "2107.06262", "submitter": "Qingyuan Zheng", "authors": "Qingyuan Zheng, Zhuoru Li, Adam Bargteil", "title": "Learning Aesthetic Layouts via Visual Guidance", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We explore computational approaches for visual guidance to aid in creating\naesthetically pleasing art and graphic design. Our work complements and builds\non previous work that developed models for how humans look at images. Our\napproach comprises three steps. First, we collected a dataset of art\nmasterpieces and labeled the visual fixations with state-of-art vision models.\nSecond, we clustered the visual guidance templates of the art masterpieces with\nunsupervised learning. Third, we developed a pipeline using generative\nadversarial networks to learn the principles of visual guidance and that can\nproduce aesthetically pleasing layouts. We show that the aesthetic visual\nguidance principles can be learned and integrated into a high-dimensional model\nand can be queried by the features of graphic elements. We evaluate our\napproach by generating layouts on various drawings and graphic designs.\nMoreover, our model considers the color and structure of graphic elements when\ngenerating layouts. Consequently, we believe our tool, which generates multiple\naesthetic layout options in seconds, can help artists create beautiful art and\ngraphic designs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:46:42 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zheng", "Qingyuan", ""], ["Li", "Zhuoru", ""], ["Bargteil", "Adam", ""]]}, {"id": "2107.06492", "submitter": "Hoang Trinh Man", "authors": "Trinh Man Hoang, Jinjia Zhou", "title": "RCLC: ROI-based joint conventional and learning video compression", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 leads to the high demand for remote interactive systems ever seen.\nOne of the key elements of these systems is video streaming, which requires a\nvery high network bandwidth due to its specific real-time demand, especially\nwith high-resolution video. Existing video compression methods are struggling\nin the trade-off between video quality and the speed requirement. Addressed\nthat the background information rarely changes in most remote meeting cases, we\nintroduce a Region-Of-Interests (ROI) based video compression framework (named\nRCLC) that leverages the cutting-edge learning-based and conventional\ntechnologies. In RCLC, each coming frame is marked as a background-updating\n(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the\nBU frame is compressed with low-quality and high-compression, while the ROI\nfrom RU-frame is compressed with high-quality and low-compression. The\nlearning-based methods are applied to detect the ROI, blend background-ROI, and\nenhance video quality. The experimental results show that our RCLC can reduce\nup to 32.55\\% BD-rate for the ROI region compared to H.265 video codec under a\nsimilar compression time with 1080p resolution.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 05:38:37 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hoang", "Trinh Man", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2107.06538", "submitter": "Xinda Liu", "authors": "Xinda Liu, Lili Wang, Xiaoguang Han", "title": "Transformer with Peak Suppression and Knowledge Guidance for\n  Fine-grained Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image recognition is challenging because discriminative clues\nare usually fragmented, whether from a single image or multiple images. Despite\ntheir significant improvements, most existing methods still focus on the most\ndiscriminative parts from a single image, ignoring informative details in other\nregions and lacking consideration of clues from other associated images. In\nthis paper, we analyze the difficulties of fine-grained image recognition from\na new perspective and propose a transformer architecture with the peak\nsuppression module and knowledge guidance module, which respects the\ndiversification of discriminative features in a single image and the\naggregation of discriminative clues among multiple images. Specifically, the\npeak suppression module first utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention\nresponse generated by the transformer encoder. This module penalizes the\nattention to the most discriminative parts in the feature learning process,\ntherefore, enhancing the information exploitation of the neglected regions. The\nknowledge guidance module compares the image-based representation generated\nfrom the peak suppression module with the learnable knowledge embedding set to\nobtain the knowledge response coefficients. Afterwards, it formalizes the\nknowledge learning as a classification problem using response coefficients as\nthe classification scores. Knowledge embeddings and image-based representations\nare updated during training so that the knowledge embedding includes\ndiscriminative clues for different images. Finally, we incorporate the acquired\nknowledge embeddings into the image-based representations as comprehensive\nrepresentations, leading to significantly higher performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:07:58 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Xinda", ""], ["Wang", "Lili", ""], ["Han", "Xiaoguang", ""]]}, {"id": "2107.06831", "submitter": "Jinglin Liu", "authors": "Jinglin Liu, Zhiying Zhu, Yi Ren and Zhou Zhao", "title": "High-Speed and High-Quality Text-to-Lip Generation", "comments": "Author draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a novel parallel decoding model for\nhigh-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we\npredict the duration of the encoded linguistic features and model the target\nlip frames conditioned on the encoded linguistic features with their duration\nin a non-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L\ngenerates lip movements with competitive quality compared with the\nstate-of-the-art AR T2L model DualLip and exceeds the baseline AR model\nTransformerT2L by a notable margin benefiting from the mitigation of the error\npropagation problem; and 2) exhibits distinct superiority in inference speed\n(an average speedup of 19$\\times$ than DualLip on TCD-TIMIT).\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:44:04 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Jinglin", ""], ["Zhu", "Zhiying", ""], ["Ren", "Yi", ""], ["Zhao", "Zhou", ""]]}, {"id": "2107.07127", "submitter": "Kyoungjun Park", "authors": "Kyoungjun Park, Myungchul Kim, Laihyuk Park", "title": "NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile\n  Video Streaming", "comments": "13 pages, 8 figures, 3 tables, This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video streaming services strive to support high-quality videos at higher\nresolutions and frame rates to improve the quality of experience (QoE).\nHowever, high-quality videos consume considerable amounts of energy on mobile\ndevices. This paper proposes NeuSaver, which reduces the power consumption of\nmobile devices when streaming videos by applying an adaptive frame rate to each\nvideo chunk without compromising user experience. NeuSaver generates an optimal\npolicy that determines the appropriate frame rate for each video chunk using\nreinforcement learning (RL). The RL model automatically learns the policy that\nmaximizes the QoE goals based on previous observations. NeuSaver also uses an\nasynchronous advantage actor-critic algorithm to reinforce the RL model quickly\nand robustly. Streaming servers that support NeuSaver preprocesses videos into\nsegments with various frame rates, which is similar to the process of creating\nvideos with multiple bit rates in dynamic adaptive streaming over HTTP.\nNeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in\nvarious experiments and a user study through four video categories along with\nthe state-of-the-art model. Our experiments showed that NeuSaver effectively\nreduces the power consumption of mobile devices when streaming video by an\naverage of 16.14% and up to 23.12% while achieving high QoE.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 05:17:17 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Park", "Kyoungjun", ""], ["Kim", "Myungchul", ""], ["Park", "Laihyuk", ""]]}, {"id": "2107.07268", "submitter": "Jing Yi", "authors": "Jing Yi and Yaochen Zhu and Jiayi Xie and Zhenzhong Chen", "title": "Cross-modal Variational Auto-encoder for Content-based Micro-video\n  Background Music Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:47:43 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yi", "Jing", ""], ["Zhu", "Yaochen", ""], ["Xie", "Jiayi", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2107.07360", "submitter": "Sebastian L\\\"obbers", "authors": "Sebastian L\\\"obbers, Mathieu Barthet, Gy\\\"orgy Fazekas", "title": "Sketching sounds: an exploratory study on sound-shape associations", "comments": "accepted for International Computer Music Conference (ICMC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sound synthesiser controls typically correspond to technical parameters of\nsignal processing algorithms rather than intuitive sound descriptors that\nrelate to human perception of sound. This makes it difficult to realise sound\nideas in a straightforward way. Cross-modal mappings, for example between\ngestures and sound, have been suggested as a more intuitive control mechanism.\nA large body of research shows consistency in human associations between sounds\nand shapes. However, the use of drawings to drive sound synthesis has not been\nexplored to its full extent. This paper presents an exploratory study that\nasked participants to sketch visual imagery of sounds with a monochromatic\ndigital drawing interface, with the aim to identify different representational\napproaches and determine whether timbral sound characteristics can be\ncommunicated reliably through visual sketches. Results imply that the\ndevelopment of a synthesiser exploiting sound-shape associations is feasible,\nbut a larger and more focused dataset is needed in followup studies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:37:53 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["L\u00f6bbers", "Sebastian", ""], ["Barthet", "Mathieu", ""], ["Fazekas", "Gy\u00f6rgy", ""]]}, {"id": "2107.07502", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu,\n  Leslie Chen, Peter Wu, Michelle A. Lee, Yuke Zhu, Ruslan Salakhutdinov,\n  Louis-Philippe Morency", "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning", "comments": "Code: https://github.com/pliang279/MultiBench and Website:\n  https://cmu-multicomp-lab.github.io/multibench/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:54:36 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Liang", "Paul Pu", ""], ["Lyu", "Yiwei", ""], ["Fan", "Xiang", ""], ["Wu", "Zetian", ""], ["Cheng", "Yun", ""], ["Wu", "Jason", ""], ["Chen", "Leslie", ""], ["Wu", "Peter", ""], ["Lee", "Michelle A.", ""], ["Zhu", "Yuke", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2107.07786", "submitter": "Elona Shatri Miss", "authors": "Elona Shatri and Gy\\\"orgy Fazekas", "title": "DoReMi: First glance at a universal OMR dataset", "comments": "7 pages, including 2 pages appendix. Accepted for publishing at the\n  3rd International Workshop on Reading Music Systems 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:24:58 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Shatri", "Elona", ""], ["Fazekas", "Gy\u00f6rgy", ""]]}, {"id": "2107.07907", "submitter": "Kanglin Liu", "authors": "Kanglin Liu, Gaofeng Cao, Jiang Duan, Guoping Qiu", "title": "Lightness Modulated Deep Inverse Tone Mapping", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 13:56:20 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Liu", "Kanglin", ""], ["Cao", "Gaofeng", ""], ["Duan", "Jiang", ""], ["Qiu", "Guoping", ""]]}, {"id": "2107.08264", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Jianben He, Zhihua Jin, Muqiao Yang, Yong Wang, Huamin Qu", "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment\n  Analysis", "comments": "11 pages, 7 figures. This paper is accepted by IEEE VIS, 2021. To\n  appear in IEEE Transactions on Visualization and Computer Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 15:54:27 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 02:20:19 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Xingbo", ""], ["He", "Jianben", ""], ["Jin", "Zhihua", ""], ["Yang", "Muqiao", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2107.08356", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Yao Ming, Tongshuang Wu, Haipeng Zeng, Yong Wang, Huamin\n  Qu", "title": "DeHumor: Visual Analytics for Decomposing Humor", "comments": "15 pages. A preprint version of a publication at IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), 2021", "journal-ref": null, "doi": "10.1109/TVCG.2021.3097709", "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 04:01:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Xingbo", ""], ["Ming", "Yao", ""], ["Wu", "Tongshuang", ""], ["Zeng", "Haipeng", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2107.08621", "submitter": "Qingzhong Wang", "authors": "Qingzhong Wang, Pengfei Zhang, Haoyi Xiong and Jian Zhao", "title": "Face.evoLVe: A High-Performance Face Recognition Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 05:38:50 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:24:48 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Qingzhong", ""], ["Zhang", "Pengfei", ""], ["Xiong", "Haoyi", ""], ["Zhao", "Jian", ""]]}, {"id": "2107.08688", "submitter": "Hanzhou Wu", "authors": "Xiangyu Zhao, Yinzhe Yao, Hanzhou Wu and Xinpeng Zhang", "title": "Structural Watermarking to Deep Neural Networks via Network Channel\n  Pruning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to protect the intellectual property (IP) of deep neural networks\n(DNNs), many existing DNN watermarking techniques either embed watermarks\ndirectly into the DNN parameters or insert backdoor watermarks by fine-tuning\nthe DNN parameters, which, however, cannot resist against various attack\nmethods that remove watermarks by altering DNN parameters. In this paper, we\nbypass such attacks by introducing a structural watermarking scheme that\nutilizes channel pruning to embed the watermark into the host DNN architecture\ninstead of crafting the DNN parameters. To be specific, during watermark\nembedding, we prune the internal channels of the host DNN with the channel\npruning rates controlled by the watermark. During watermark extraction, the\nwatermark is retrieved by identifying the channel pruning rates from the\narchitecture of the target DNN model. Due to the superiority of pruning\nmechanism, the performance of the DNN model on its original task is reserved\nduring watermark embedding. Experimental results have shown that, the proposed\nwork enables the embedded watermark to be reliably recovered and provides a\nhigh watermark capacity, without sacrificing the usability of the DNN model. It\nis also demonstrated that the work is robust against common transforms and\nattacks designed for conventional watermarking approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 08:46:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Yao", "Yinzhe", ""], ["Wu", "Hanzhou", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2107.08939", "submitter": "Seung-Hun Nam", "authors": "Seung-Hun Nam, Wonhyuk Ahn, Myung-Joon Kwon, In-Jae Yu", "title": "Detection of Double Compression in MPEG-4 Videos Using Refined\n  Features-based CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double compression is accompanied by various types of video manipulation and\nits traces can be exploited to determine whether a video is a forgery. This\nLetter presents a convolutional neural network for detecting double compression\nin MPEG-4 videos. Through analysis of the intra-coding process, we utilize two\nrefined features for capturing the subtle artifacts caused by double\ncompression. The discrete cosine transform (DCT) histogram feature effectively\ndetects the change of statistical characteristics in DCT coefficients and the\nparameter-based feature is utilized as auxiliary information to help the\nnetwork learn double compression artifacts. When compared with state-of-the-art\nnetworks and forensic method, the results show that the proposed approach\nachieves a higher performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:53:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nam", "Seung-Hun", ""], ["Ahn", "Wonhyuk", ""], ["Kwon", "Myung-Joon", ""], ["Yu", "In-Jae", ""]]}, {"id": "2107.09262", "submitter": "Sanchita Ghose", "authors": "Sanchita Ghose and John J. Prevost", "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based\n  Synchronous Sound Generation in Silent Videos", "comments": "This article is under review in IEEE Transaction on Multimedia. It\n  contains total 12 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 04:59:26 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ghose", "Sanchita", ""], ["Prevost", "John J.", ""]]}, {"id": "2107.09491", "submitter": "Ying Cui", "authors": "Lingzhi Zhao, Ying Cui, Zhi Liu, Yunfei Zhang, Sheng Yang", "title": "Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown\n  FoV Viewing Probabilities in Wireless Networks", "comments": "18 pages, 20 figures, to appear in Trans. Image Process. arXiv admin\n  note: text overlap with arXiv:2009.01753", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates adaptive streaming of one or multiple tiled 360\nvideos from a multi-antenna base station (BS) to one or multiple single-antenna\nusers, respectively, in a multi-carrier wireless system. We aim to maximize the\nvideo quality while keeping rebuffering time small via encoding rate adaptation\nat each group of pictures (GOP) and transmission adaptation at each\n(transmission) slot. To capture the impact of field-of-view (FoV) prediction,\nwe consider three cases of FoV viewing probability distributions, i.e.,\nperfect, imperfect, and unknown FoV viewing probability distributions, and use\nthe average total utility, worst average total utility, and worst total utility\nas the respective performance metrics. In the single-user scenario, we optimize\nthe encoding rates of the tiles, encoding rates of the FoVs, and transmission\nbeamforming vectors for all subcarriers to maximize the total utility in each\ncase. In the multi-user scenario, we adopt rate splitting with successive\ndecoding and optimize the encoding rates of the tiles, encoding rates of the\nFoVs, rates of the common and private messages, and transmission beamforming\nvectors for all subcarriers to maximize the total utility in each case. Then,\nwe separate the challenging optimization problem into multiple tractable\nproblems in each scenario. In the single-user scenario, we obtain a globally\noptimal solution of each problem using transformation techniques and the\nKarush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a\nKKT point of each problem using the concave-convex procedure (CCCP). Finally,\nnumerical results demonstrate that the proposed solutions achieve notable gains\nover existing schemes in all three cases. To the best of our knowledge, this is\nthe first work revealing the impact of FoV prediction on the performance of\nadaptive streaming of tiled 360 videos.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:42:11 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhao", "Lingzhi", ""], ["Cui", "Ying", ""], ["Liu", "Zhi", ""], ["Zhang", "Yunfei", ""], ["Yang", "Sheng", ""]]}, {"id": "2107.09877", "submitter": "Ning Zhang", "authors": "Ning Zhang and Junchi Yan", "title": "Melody Structure Transfer Network: Generating Music with Separable\n  Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Symbolic music generation has attracted increasing attention, while most\nmethods focus on generating short piece (mostly less than 8 bars, and up to 32\nbars). Generating long music calls for effective expression of the coherent\nmusic structure. Despite their success on long sequences, self-attention\narchitectures still have challenge in dealing with long-term music as it\nrequires additional care on the subtle music structure. In this paper, we\npropose to transfer the structure of training samples for new music generation,\nand develop a novel separable self-attention based model which enable the\nlearning and transferring of the structure embedding. We show that our transfer\nmodel can generate music sequences (up to 100 bars) with interpretable\nstructures, which bears similar structures and composition techniques with the\ntemplate music from training set. Extensive experiments show its ability of\ngenerating music with target structure and well diversity. The generated 3,000\nsets of music is uploaded as supplemental material.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 04:38:15 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhang", "Ning", ""], ["Yan", "Junchi", ""]]}, {"id": "2107.09889", "submitter": "Wenxuan Liu", "authors": "Tianyao He, Wenxuan Liu, Chen Gong, Junchi Yan, Ning Zhang", "title": "Music Plagiarism Detection via Bipartite Graph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, with the prevalence of social media and music creation tools,\nmusical pieces are spreading much quickly, and music creation is getting much\neasier. The increasing number of musical pieces have made the problem of music\nplagiarism prominent. There is an urgent need for a tool that can detect music\nplagiarism automatically. Researchers have proposed various methods to extract\nlow-level and high-level features of music and compute their similarities.\nHowever, low-level features such as cepstrum coefficients have weak relation\nwith the copyright protection of musical pieces. Existing algorithms\nconsidering high-level features fail to detect the case in which two musical\npieces are not quite similar overall, but have some highly similar regions.\nThis paper proposes a new method named MESMF, which innovatively converts the\nmusic plagiarism detection problem into the bipartite graph matching task. It\ncan be solved via the maximum weight matching and edit distances model. We\ndesign several kinds of melody representations and the similarity computation\nmethods according to the music theory. The proposed method can deal with the\nshift, swapping, transposition, and tempo variance problems in music\nplagiarism. It can also effectively pick out the local similar regions from two\nmusical pieces with relatively low global similarity. We collect a new music\nplagiarism dataset from real legally-judged music plagiarism cases and conduct\ndetailed ablation studies. Experimental results prove the excellent performance\nof the proposed algorithm. The source code and our dataset are available at\nhttps://anonymous.4open.science/r/a41b8fb4-64cf-4190-a1e1-09b7499a15f5/\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 06:04:47 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["He", "Tianyao", ""], ["Liu", "Wenxuan", ""], ["Gong", "Chen", ""], ["Yan", "Junchi", ""], ["Zhang", "Ning", ""]]}, {"id": "2107.10220", "submitter": "Anastasia Antsiferova", "authors": "Anastasia Antsiferova, Alexander Yakovenko, Nickolay Safonov, Dmitriy\n  Kulikov, Alexander Gushin, and Dmitriy Vatolin", "title": "Objective video quality metrics application to video codecs comparisons:\n  choosing the best for subjective quality estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment plays a key role in creating and comparing video\ncompression algorithms. Despite the development of a large number of new\nmethods for assessing quality, generally accepted and well-known codecs\ncomparisons mainly use the classical methods like PSNR, SSIM and new method\nVMAF. These methods can be calculated following different rules: they can use\ndifferent frame-by-frame averaging techniques or different summation of color\ncomponents. In this paper, a fundamental comparison of various versions of\ngenerally accepted metrics is carried out to find the most relevant and\nrecommended versions of video quality metrics to be used in codecs comparisons.\nFor comparison, we used a set of videos encoded with video codecs of different\nstandards, and visual quality scores collected for the resulting set of streams\nsince 2018 until 2021\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:18:11 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Antsiferova", "Anastasia", ""], ["Yakovenko", "Alexander", ""], ["Safonov", "Nickolay", ""], ["Kulikov", "Dmitriy", ""], ["Gushin", "Alexander", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "2107.10300", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha and Vinija Jain", "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural\n  Language with Interpretability", "comments": "12 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:56:34 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chadha", "Aman", ""], ["Jain", "Vinija", ""]]}, {"id": "2107.11412", "submitter": "Arun Kumar Singh", "authors": "Arun Kumar Singh (1), Priyanka Singh (2), Karan Nathwani (1) ((1)\n  Indian Institute of Technology Jammu, (2) Dhirubhai Ambani Institute of\n  Information and Communication Technology)", "title": "Using Deep Learning Techniques and Inferential Speech Statistics for AI\n  Synthesised Speech Recognition", "comments": "13 Pages, 13 Figures, 6 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2009.01934", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:43:10 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Singh", "Arun Kumar", ""], ["Singh", "Priyanka", ""], ["Nathwani", "Karan", ""]]}, {"id": "2107.11576", "submitter": "Jingjing Jiang", "authors": "Jingjing Jiang, Ziyi Liu, Yifan Liu, Zhixiong Nan, and Nanning Zheng", "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization\n  in Visual Question Answering", "comments": "Accepted by ACM MM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Encouraging progress has been made towards Visual Question Answering (VQA) in\nrecent years, but it is still challenging to enable VQA models to adaptively\ngeneralize to out-of-distribution (OOD) samples. Intuitively, recompositions of\nexisting visual concepts (i.e., attributes and objects) can generate unseen\ncompositions in the training set, which will promote VQA models to generalize\nto OOD samples. In this paper, we formulate OOD generalization in VQA as a\ncompositional generalization problem and propose a graph generative\nmodeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM\nleverages graph generative modeling to iteratively generate a relation matrix\nand node representations for the predefined graph that utilizes\nattribute-object pairs as nodes. Furthermore, to alleviate the unstable\ntraining issue in graph generative modeling, we propose a gradient distribution\nconsistency loss to constrain the data distribution with adversarial\nperturbations and the generated distribution. The baseline VQA model (LXMERT)\ntrained with the X-GGM scheme achieves state-of-the-art OOD performance on two\nstandard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation\nstudies demonstrate the effectiveness of X-GGM components.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 10:17:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jiang", "Jingjing", ""], ["Liu", "Ziyi", ""], ["Liu", "Yifan", ""], ["Nan", "Zhixiong", ""], ["Zheng", "Nanning", ""]]}, {"id": "2107.11756", "submitter": "Yuqian Fu", "authors": "Yuqian Fu, Yanwei Fu, Yu-Gang Jiang", "title": "Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics\n  from Videos", "comments": "accpected by ICMR2021", "journal-ref": null, "doi": "10.1145/3460426.3463609", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video demonstration, can we imitate the action contained in this\nvideo? In this paper, we introduce a novel task, dubbed mesh-based action\nimitation. The goal of this task is to enable an arbitrary target human mesh to\nperform the same action shown on the video demonstration. To achieve this, a\nnovel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI\nfirst learns to reconstruct the meshes from the given source image frames, then\nthe initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence\nsmooth module proposed by us, to improve the temporal consistency. Finally, we\nimitate the actions by transferring the pose from the constructed human body to\nour target identity mesh. High-quality and detailed human body meshes can be\ngenerated by using our M-VAI. Extensive experiments demonstrate the feasibility\nof our task and the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 08:42:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fu", "Yuqian", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2107.11803", "submitter": "Yashar Deldjoo", "authors": "Yashar Deldjoo, Markus Schedl, Peter Knees", "title": "Content-driven Music Recommendation: Evolution, State of the Art, and\n  Challenges", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The music domain is among the most important ones for adopting recommender\nsystems technology. In contrast to most other recommendation domains, which\npredominantly rely on collaborative filtering (CF) techniques, music\nrecommenders have traditionally embraced content-based (CB) approaches. In the\npast years, music recommendation models that leverage collaborative and content\ndata -- which we refer to as content-driven models -- have been replacing pure\nCF or CB models.\n  In this survey, we review 47 articles on content-driven music recommendation.\nBased on a thorough literature analysis, we first propose an onion model\ncomprising five layers, each of which corresponds to a category of music\ncontent we identified: signal, embedded metadata, expert-generated content,\nuser-generated content, and derivative content. We provide a detailed\ncharacterization of each category along several dimensions. Second, we identify\nsix overarching challenges, according to which we organize our main discussion:\nincreasing recommendation diversity and novelty, providing transparency and\nexplanations, accomplishing context-awareness, recommending sequences of music,\nimproving scalability and efficiency, and alleviating cold start. Each article\naddressing one or more of these challenges is categorized according to the\ncontent layers of our onion model, the article's goal(s), and main\nmethodological choices. Furthermore, articles are discussed in temporal order\nto shed light on the evolution of content-driven music recommendation\nstrategies. Finally, we provide our personal selection of the persisting grand\nchallenges, which are still waiting to be solved in future research endeavors.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 13:41:47 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Schedl", "Markus", ""], ["Knees", "Peter", ""]]}, {"id": "2107.12168", "submitter": "Hanzhou Wu", "authors": "Biao Yi, Hanzhou Wu, Guorui Feng and Xinpeng Zhang", "title": "Exploiting Language Model for Efficient Linguistic Steganalysis: An\n  Empirical Study", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in linguistic steganalysis have successively applied CNNs,\nRNNs, GNNs and other deep learning models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated steganographic texts and carrier texts in terms of the conditional\nprobability distribution of individual words. Such kind of statistical\ndifference can be naturally captured by the language model used for generating\nsteganographic texts, which drives us to give the classifier a priori knowledge\nof the language model to enhance the steganalysis ability. To this end, we\npresent two methods to efficient linguistic steganalysis in this paper. One is\nto pre-train a language model based on RNN, and the other is to pre-train a\nsequence autoencoder. Experimental results show that the two methods have\ndifferent degrees of performance improvement when compared to the randomly\ninitialized RNN classifier, and the convergence speed is significantly\naccelerated. Moreover, our methods have achieved the best detection results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:37:18 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yi", "Biao", ""], ["Wu", "Hanzhou", ""], ["Feng", "Guorui", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2107.12292", "submitter": "Ting Yao", "authors": "Yehao Li and Ting Yao and Yingwei Pan and Tao Mei", "title": "Contextual Transformer Networks for Visual Recognition", "comments": "Rank 1 in open-set image classification task of Open World Vision\n  Challenge @ CVPR 2021; The source code and models are publicly available at:\n  \\url{https://github.com/JDAI-CV/CoTNet}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer with self-attention has led to the revolutionizing of natural\nlanguage processing field, and recently inspires the emergence of\nTransformer-style architecture design with competitive results in numerous\ncomputer vision tasks. Nevertheless, most of existing designs directly employ\nself-attention over a 2D feature map to obtain the attention matrix based on\npairs of isolated queries and keys at each spatial location, but leave the rich\ncontexts among neighbor keys under-exploited. In this work, we design a novel\nTransformer-style module, i.e., Contextual Transformer (CoT) block, for visual\nrecognition. Such design fully capitalizes on the contextual information among\ninput keys to guide the learning of dynamic attention matrix and thus\nstrengthens the capacity of visual representation. Technically, CoT block first\ncontextually encodes input keys via a $3\\times3$ convolution, leading to a\nstatic contextual representation of inputs. We further concatenate the encoded\nkeys with input queries to learn the dynamic multi-head attention matrix\nthrough two consecutive $1\\times1$ convolutions. The learnt attention matrix is\nmultiplied by input values to achieve the dynamic contextual representation of\ninputs. The fusion of the static and dynamic contextual representations are\nfinally taken as outputs. Our CoT block is appealing in the view that it can\nreadily replace each $3\\times3$ convolution in ResNet architectures, yielding a\nTransformer-style backbone named as Contextual Transformer Networks (CoTNet).\nThrough extensive experiments over a wide range of applications (e.g., image\nrecognition, object detection and instance segmentation), we validate the\nsuperiority of CoTNet as a stronger backbone. Source code is available at\n\\url{https://github.com/JDAI-CV/CoTNet}.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:00:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Mei", "Tao", ""]]}, {"id": "2107.12719", "submitter": "Alessio Xompero", "authors": "Alessio Xompero, Santiago Donaher, Vladimir Iashin, Francesca Palermo,\n  G\\\"okhan Solak, Claudio Coppola, Reina Ishikawa, Yuichi Nagao, Ryo Hachiuma,\n  Qi Liu, Fan Feng, Chuanlin Lan, Rosa H. M. Chan, Guilherme Christmann,\n  Jyun-Ting Song, Gonuguntla Neeharika, Chinnakotla Krishna Teja Reddy, Dinesh\n  Jain, Bakhtawar Ur Rehman, Andrea Cavallaro", "title": "Multi-modal estimation of the properties of containers and their\n  content: survey and evaluation", "comments": "13 pages, 9 tables, 5 figures, submitted to IEEE Transactions on\n  Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:36:19 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Xompero", "Alessio", ""], ["Donaher", "Santiago", ""], ["Iashin", "Vladimir", ""], ["Palermo", "Francesca", ""], ["Solak", "G\u00f6khan", ""], ["Coppola", "Claudio", ""], ["Ishikawa", "Reina", ""], ["Nagao", "Yuichi", ""], ["Hachiuma", "Ryo", ""], ["Liu", "Qi", ""], ["Feng", "Fan", ""], ["Lan", "Chuanlin", ""], ["Chan", "Rosa H. M.", ""], ["Christmann", "Guilherme", ""], ["Song", "Jyun-Ting", ""], ["Neeharika", "Gonuguntla", ""], ["Reddy", "Chinnakotla Krishna Teja", ""], ["Jain", "Dinesh", ""], ["Rehman", "Bakhtawar Ur", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2107.12854", "submitter": "Federico Simonetta", "authors": "Federico Simonetta, Stavros Ntalampiras, Federico Avanzini", "title": "Audio-to-Score Alignment Using Deep Automatic Music Transcription", "comments": "IEEE MMSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Audio-to-score alignment (A2SA) is a multimodal task consisting in the\nalignment of audio signals to music scores. Recent literature confirms the\nbenefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In\nthis work, we aim to elaborate on the exploitation of AMT Deep Learning (DL)\nmodels for achieving alignment at the note-level. We propose a method which\nbenefits from HMM-based score-to-score alignment and AMT, showing a remarkable\nadvancement beyond the state-of-the-art. We design a systematic procedure to\ntake advantage of large datasets which do not offer an aligned score. Finally,\nwe perform a thorough comparison and extensive tests on multiple datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:41:41 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Simonetta", "Federico", ""], ["Ntalampiras", "Stavros", ""], ["Avanzini", "Federico", ""]]}, {"id": "2107.12858", "submitter": "Zhikang Zou", "authors": "Zhikang Zou, Xiaoye Qu, Pan Zhou, Shuangjie Xu, Xiaoqing Ye, Wenhao\n  Wu, Jin Ye", "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring\n  Network", "comments": "Accepted by ACMMM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:47:24 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zou", "Zhikang", ""], ["Qu", "Xiaoye", ""], ["Zhou", "Pan", ""], ["Xu", "Shuangjie", ""], ["Ye", "Xiaoqing", ""], ["Wu", "Wenhao", ""], ["Ye", "Jin", ""]]}, {"id": "2107.12921", "submitter": "Menghan Hu", "authors": "Hang Liu, Menghan Hu, Yuzhen Chen, Qingli Li, Guangtao Zhai, Simon X.\n  Yang, Xiao-Ping Zhang, Xiaokang Yang", "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System\n  Validated by Multimodal Evaluation Approach", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 16:23:47 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Liu", "Hang", ""], ["Hu", "Menghan", ""], ["Chen", "Yuzhen", ""], ["Li", "Qingli", ""], ["Zhai", "Guangtao", ""], ["Yang", "Simon X.", ""], ["Zhang", "Xiao-Ping", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2107.13151", "submitter": "Jianhua Yang", "authors": "Jianhua Yang, Yi Liao, Fei Shang, Xiangui Kang, Yun-Qing Shi", "title": "JPEG Steganography with Embedding Cost Learning and Side-Information\n  Estimation", "comments": "11 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A great challenge to steganography has arisen with the wide application of\nsteganalysis methods based on convolutional neural networks (CNNs). To this\nend, embedding cost learning frameworks based on generative adversarial\nnetworks (GANs) have been proposed and achieved success for spatial\nsteganography. However, the application of GAN to JPEG steganography is still\nin the prototype stage; its anti-detectability and training efficiency should\nbe improved. In conventional steganography, research has shown that the\nside-information calculated from the precover can be used to enhance security.\nHowever, it is hard to calculate the side-information without the spatial\ndomain image. In this work, an embedding cost learning framework for JPEG\nSteganography via a Generative Adversarial Network (JS-GAN) has been proposed,\nthe learned embedding cost can be further adjusted asymmetrically according to\nthe estimated side-information. Experimental results have demonstrated that the\nproposed method can automatically learn a content-adaptive embedding cost\nfunction, and use the estimated side-information properly can effectively\nimprove the security performance. For example, under the attack of a classic\nsteganalyzer GFR with quality factor 75 and 0.4 bpnzAC, the proposed JS-GAN can\nincrease the detection error 2.58% over J-UNIWARD, and the estimated\nside-information aided version JS-GAN(ESI) can further increase the security\nperformance by 11.25% over JS-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 03:36:55 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Yang", "Jianhua", ""], ["Liao", "Yi", ""], ["Shang", "Fei", ""], ["Kang", "Xiangui", ""], ["Shi", "Yun-Qing", ""]]}, {"id": "2107.13180", "submitter": "Javier Naranjo-Alcazar", "authors": "Javier Naranjo-Alcazar, Sergi Perez-Castanos, Aaron Lopez-Garcia,\n  Pedro Zuccarello, Maximo Cobos, Francesc J. Ferri", "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for\n  Audio-Visual Scene Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 06:10:10 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Naranjo-Alcazar", "Javier", ""], ["Perez-Castanos", "Sergi", ""], ["Lopez-Garcia", "Aaron", ""], ["Zuccarello", "Pedro", ""], ["Cobos", "Maximo", ""], ["Ferri", "Francesc J.", ""]]}, {"id": "2107.13385", "submitter": "Adam Wieckowski", "authors": "Adam Wieckowski, Christian Lehmann, Benjamin Bross, Detlev Marpe,\n  Thibaud Biatek, Mickael Raulet, Jean Le Feuvre", "title": "A Complete End-To-End Open Source Toolchain for the Versatile Video\n  Coding (VVC) Standard", "comments": "4 pages, 2 figures, accepted to ACM International Conference on\n  Multimedia (MM'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Versatile Video Coding (VVC) is the most recent international video coding\nstandard jointly developed by ITU-T and ISO/IEC, which has been finalized in\nJuly 2020. VVC allows for significant bit-rate reductions around 50% for the\nsame subjective video quality compared to its predecessor, High Efficiency\nVideo Coding (HEVC). One year after finalization, VVC support in devices and\nchipsets is still under development, which is aligned with the typical\ndevelopment cycles of new video coding standards. This paper presents\nopen-source software packages that allow building a complete VVC end-to-end\ntoolchain already one year after its finalization. This includes the Fraunhofer\nHHI VVenC library for fast and efficient VVC encoding as well as HHI's VVdeC\nlibrary for live decoding. An experimental integration of VVC in the GPAC\nsoftware tools and FFmpeg media framework allows packaging VVC bitstreams, e.g.\nencoded with VVenC, in MP4 file format and using DASH for content creation and\nstreaming. The integration of VVdeC allows playback on the receiver. Given\nthese packages, step-by-step tutorials are provided for two possible\napplication scenarios: VVC file encoding plus playback and adaptive streaming\nwith DASH.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:25:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wieckowski", "Adam", ""], ["Lehmann", "Christian", ""], ["Bross", "Benjamin", ""], ["Marpe", "Detlev", ""], ["Biatek", "Thibaud", ""], ["Raulet", "Mickael", ""], ["Feuvre", "Jean Le", ""]]}, {"id": "2107.13469", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Zhenhua Guo, Site Li, Fangxu Xing, Jane You, C.-C. Jay\n  Kuo, Georges El Fakhri, Jonghye Woo", "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label\n  Shift: Infer, Align and Iterate", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:28:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Guo", "Zhenhua", ""], ["Li", "Site", ""], ["Xing", "Fangxu", ""], ["You", "Jane", ""], ["Kuo", "C. -C. Jay", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2107.13637", "submitter": "Peter Van Der Putten", "authors": "Manolis Fragkiadakis and Peter van der Putten", "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica", "comments": "Accepted for the 1st International Workshop on Automatic Translation\n  for Signed and Spoken Languages (ATS4SSL), August 20, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 20:48:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fragkiadakis", "Manolis", ""], ["van der Putten", "Peter", ""]]}, {"id": "2107.13720", "submitter": "Xinyang Feng", "authors": "Xinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao\n  Ni, Haifeng Chen", "title": "Convolutional Transformer based Dual Discriminator Generative\n  Adversarial Networks for Video Anomaly Detection", "comments": "Accepted for publication in the 29th ACM International Conference on\n  Multimedia (ACMMM '21)", "journal-ref": null, "doi": "10.1145/3474085.3475693", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 03:07:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Feng", "Xinyang", ""], ["Song", "Dongjin", ""], ["Chen", "Yuncong", ""], ["Chen", "Zhengzhang", ""], ["Ni", "Jingchao", ""], ["Chen", "Haifeng", ""]]}, {"id": "2107.14179", "submitter": "Anique Akhtar", "authors": "Anique Akhtar, Wen Gao, Li Li, Zhu Li, Wei Jia, Shan Liu", "title": "Video-based Point Cloud Compression Artifact Removal", "comments": "Accepted in IEEE Transactions on Multimedia, 2021", "journal-ref": "IEEE Transactions on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-realistic point cloud capture and transmission are the fundamental\nenablers for immersive visual communication. The coding process of dynamic\npoint clouds, especially video-based point cloud compression (V-PCC) developed\nby the MPEG standardization group, is now delivering state-of-the-art\nperformance in compression efficiency. V-PCC is based on the projection of the\npoint cloud patches to 2D planes and encoding the sequence as 2D texture and\ngeometry patch sequences. However, the resulting quantization errors from\ncoding can introduce compression artifacts, which can be very unpleasant for\nthe quality of experience (QoE). In this work, we developed a novel\nout-of-the-loop point cloud geometry artifact removal solution that can\nsignificantly improve reconstruction quality without additional bandwidth cost.\nOur novel framework consists of a point cloud sampling scheme, an artifact\nremoval network, and an aggregation scheme. The point cloud sampling scheme\nemploys a cube-based neighborhood patch extraction to divide the point cloud\ninto patches. The geometry artifact removal network then processes these\npatches to obtain artifact-removed patches. The artifact-removed patches are\nthen merged together using an aggregation scheme to obtain the final\nartifact-removed point cloud. We employ 3D deep convolutional feature learning\nfor geometry artifact removal that jointly recovers both the quantization\ndirection and the quantization noise level by exploiting projection and\nquantization prior. The simulation results demonstrate that the proposed method\nis highly effective and can considerably improve the quality of the\nreconstructed point cloud.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:04:42 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Akhtar", "Anique", ""], ["Gao", "Wen", ""], ["Li", "Li", ""], ["Li", "Zhu", ""], ["Jia", "Wei", ""], ["Liu", "Shan", ""]]}]