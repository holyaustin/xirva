[{"id": "2002.00251", "submitter": "Alexander Schindler", "authors": "Alexander Schindler", "title": "Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with\n  Visual Computing for Improved Music Video Analysis", "comments": "Dissertation at TU Wien", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis combines audio-analysis with computer vision to approach Music\nInformation Retrieval (MIR) tasks from a multi-modal perspective. This thesis\nfocuses on the information provided by the visual layer of music videos and how\nit can be harnessed to augment and improve tasks of the MIR research domain.\nThe main hypothesis of this work is based on the observation that certain\nexpressive categories such as genre or theme can be recognized on the basis of\nthe visual content alone, without the sound being heard. This leads to the\nhypothesis that there exists a visual language that is used to express mood or\ngenre. In a further consequence it can be concluded that this visual\ninformation is music related and thus should be beneficial for the\ncorresponding MIR tasks such as music genre classification or mood recognition.\nA series of comprehensive experiments and evaluations are conducted which are\nfocused on the extraction of visual information and its application in\ndifferent MIR tasks. A custom dataset is created, suitable to develop and test\nvisual features which are able to represent music related information.\nEvaluations range from low-level visual features to high-level concepts\nretrieved by means of Deep Convolutional Neural Networks. Additionally, new\nvisual features are introduced capturing rhythmic visual patterns. In all of\nthese experiments the audio-based results serve as benchmark for the visual and\naudio-visual approaches. The experiments are conducted for three MIR tasks\nArtist Identification, Music Genre Classification and Cross-Genre\nClassification. Experiments show that an audio-visual approach harnessing\nhigh-level semantic information gained from visual concept detection,\noutperforms audio-only genre-classification accuracy by 16.43%.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 17:57:14 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Schindler", "Alexander", ""]]}, {"id": "2002.01425", "submitter": "Anmol Biswas", "authors": "Anmol Biswas, Green Rosh K S, and Sachin Deepak Lomte", "title": "Spatially Variant Laplacian Pyramids for Multi-Frame Exposure Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laplacian Pyramid Blending is a commonly used method for several seamless\nimage blending tasks. While the method works well for images with comparable\nintensity levels, it is often unable to produce artifact free images for\napplications which handle images with large intensity variation such as\nexposure fusion. This paper proposes a spatially varying Laplacian Pyramid\nBlending to blend images with large intensity differences. The proposed method\ndynamically alters the blending levels during the final stage of Pyramid\nReconstruction based on the amount of local intensity variation. The proposed\nalgorithm out performs state-of-the-art methods for image blending both\nqualitatively as well as quantitatively on publicly available High Dynamic\nRange (HDR) imaging dataset. Qualitative improvements are demonstrated in terms\nof details, halos and dark halos. For quantitative comparison, the no-reference\nperceptual metric MEF-SSIM was used.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 17:47:36 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Biswas", "Anmol", ""], ["S", "Green Rosh K", ""], ["Lomte", "Sachin Deepak", ""]]}, {"id": "2002.01553", "submitter": "Suzan Bayhan", "authors": "Suzan Bayhan and Setareh Maghsudi and Anatolij Zubow", "title": "EdgeDASH: Exploiting Network-Assisted Adaptive Video Streaming for Edge\n  Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While edge video caching has great potential to decrease the core network\ntraffic as well as the users' experienced latency, it is often challenging to\nexploit the caches in current client-driven video streaming solutions due to\ntwo key reasons. First, even those clients interested in the same content might\nrequest different quality levels as a video content is encoded into multiple\nqualities to match a wide range of network conditions and device capabilities.\nSecond, the clients, who select the quality of the next chunk to request, are\nunaware of the cached content at the network edge. Hence, it becomes imperative\nto develop network-side solutions to exploit caching. This can also mitigate\nsome performance issues, in particular for the scenarios in which multiple\nvideo clients compete for some bottleneck capacity. In this paper, we propose a\nnetwork-side control logic running at a WiFi AP to facilitate the use of cached\nvideo content. In particular, an AP can assign a client station a different\nvideo quality than its request, in case the alternative quality provides a\nbetter utility. We formulate the quality assignment problem as an optimization\nproblem and develop several heuristics with polynomial complexity. Compared to\nthe baseline where the clients determine the quality adaptation, our proposals,\nreferred to as EdgeDASH, offer higher video quality, higher cache hits, and\nlower stalling ratio which are essential for user's satisfaction. Our\nsimulations show that EdgeDASH facilitates significant cache hits and decreases\nthe buffer stalls only by changing the client's request by one quality level.\nMoreover, from our analysis, we conclude that the network assistance provides\nsignificant performance improvement, especially when the clients with identical\ninterests compete for a bottleneck link's capacity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 21:45:27 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Bayhan", "Suzan", ""], ["Maghsudi", "Setareh", ""], ["Zubow", "Anatolij", ""]]}, {"id": "2002.02370", "submitter": "Bharath K P", "authors": "Hanisha Chowdary N, Karan K, Bharath K P, Rajesh Kumar M", "title": "Data hiding in speech signal using steganography and encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Data privacy and data security are always on highest priority in the world.\nWe need a reliable method to encrypt the data so that it reaches the\ndestination safely. Encryption is a simple yet effective way to protect our\ndata while transmitting it to a destination. The proposed method has state of\nart technology of steganography and encryption. This paper puts forward a\ndifferent approach for data hiding in speech signals. A ten-digit number within\nspeech signal using audio steganography and encrypting it with a unique key for\nbetter security. At the receiver end the same unique key is used to decrypt the\nreceived signal and then hidden numbers are extracted. The proposed approach\nperformance can be evaluated by PSNR, MSE, SSIM and bit-error rate. The\nsimulation results give better performance compared to existing approach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:59:18 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["N", "Hanisha Chowdary", ""], ["K", "Karan", ""], ["P", "Bharath K", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "2002.02609", "submitter": "Zheng Hui", "authors": "Zheng Hui, Jie Li, Xiumei Wang, Xinbo Gao", "title": "Image Fine-grained Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting techniques have shown promising improvement with the\nassistance of generative adversarial networks (GANs) recently. However, most of\nthem often suffered from completed results with unreasonable structure or\nblurriness. To mitigate this problem, in this paper, we present a one-stage\nmodel that utilizes dense combinations of dilated convolutions to obtain larger\nand more effective receptive fields. Benefited from the property of this\nnetwork, we can more easily recover large regions in an incomplete image. To\nbetter train this efficient generator, except for frequently-used VGG feature\nmatching loss, we design a novel self-guided regression loss for concentrating\non uncertain areas and enhancing the semantic details. Besides, we devise a\ngeometrical alignment constraint item to compensate for the pixel-based\ndistance between prediction features and ground-truth ones. We also employ a\ndiscriminator with local and global branches to ensure local-global contents\nconsistency. To further improve the quality of generated images, discriminator\nfeature matching on the local branch is introduced, which dynamically minimizes\nthe similarity of intermediate features between synthetic and ground-truth\npatches. Extensive experiments on several public datasets demonstrate that our\napproach outperforms current state-of-the-art methods. Code is available at\nhttps://github.com/Zheng222/DMFN.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 03:45:25 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 03:52:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hui", "Zheng", ""], ["Li", "Jie", ""], ["Wang", "Xiumei", ""], ["Gao", "Xinbo", ""]]}, {"id": "2002.02927", "submitter": "Matthias Kirchner", "authors": "Matthias Kirchner and Cameron Johnson", "title": "SPN-CNN: Boosting Sensor-Based Source Camera Attribution With Deep\n  Learning", "comments": "Presented at the IEEE International Workshop on Information Forensics\n  and Security (WIFS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore means to advance source camera identification based on sensor\nnoise in a data-driven framework. Our focus is on improving the sensor pattern\nnoise (SPN) extraction from a single image at test time. Where existing works\nsuppress nuisance content with denoising filters that are largely agnostic to\nthe specific SPN signal of interest, we demonstrate that a~deep learning\napproach can yield a more suitable extractor that leads to improved source\nattribution. A series of extensive experiments on various public datasets\nconfirms the feasibility of our approach and its applicability to image\nmanipulation localization and video source attribution. A critical discussion\nof potential pitfalls completes the text.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:55:28 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Kirchner", "Matthias", ""], ["Johnson", "Cameron", ""]]}, {"id": "2002.03156", "submitter": "Haijian Zhang", "authors": "Haijian Zhang", "title": "A Time-Frequency Perspective on Audio Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing audio watermarking methods usually treat the host audio signals of a\nfunction of time or frequency individually, while considering them in the joint\ntime-frequency (TF) domain has received less attention. This paper proposes an\naudio watermarking framework from the perspective of TF analysis. The proposed\nframework treats the host audio signal in the 2-dimensional (2D) TF plane, and\nselects a series of patches within the 2D TF image. These patches correspond to\nthe TF clusters with minimum averaged energy, and are used to form the feature\nvectors for watermark embedding. Classical spread spectrum embedding schemes\nare incorporated in the framework. The feature patches that carry the\nwatermarks only occupy a few TF regions of the host audio signal, thus leading\nto improved imperceptibility property. In addition, since the feature patches\ncontain a neighborhood area of TF representation of audio samples, the\ncorrelations among the samples within a single patch could be exploited for\nimproved robustness against a series of processing attacks. Extensive\nexperiments are carried out to illustrate the effectiveness of the proposed\nsystem, as compared to its counterpart systems. The aim of this work is to shed\nsome light on the notion of audio watermarking in TF feature domain, which may\npotentially lead us to more robust watermarking solutions against malicious\nattacks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 13:02:24 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Haijian", ""]]}, {"id": "2002.03322", "submitter": "Xingchen Zhang", "authors": "Xingchen Zhang, Ping Ye, Gang Xiao", "title": "VIFB: A Visible and Infrared Image Fusion Benchmark", "comments": "11 pages, 5 figures, 5 tables. Accepted to CVPRW2020. Compared to the\n  CVPRW2020 version, this version corrects minor mistakes in Table 4 and the\n  first paragraph of Section 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible and infrared image fusion is one of the most important areas in image\nprocessing due to its numerous applications. While much progress has been made\nin recent years with efforts on developing fusion algorithms, there is a lack\nof code library and benchmark which can gauge the state-of-the-art. In this\npaper, after briefly reviewing recent advances of visible and infrared image\nfusion, we present a visible and infrared image fusion benchmark (VIFB) which\nconsists of 21 image pairs, a code library of 20 fusion algorithms and 13\nevaluation metrics. We also carry out large scale experiments within the\nbenchmark to understand the performance of these algorithms. By analyzing\nqualitative and quantitative results, we identify effective algorithms for\nrobust image fusion and give some observations on the status and future\nprospects of this field.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 09:10:00 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 14:01:15 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 08:30:51 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 15:44:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Xingchen", ""], ["Ye", "Ping", ""], ["Xiao", "Gang", ""]]}, {"id": "2002.03557", "submitter": "Didan Deng", "authors": "Didan Deng, Zhaokang Chen, Bertram E. Shi", "title": "Multitask Emotion Recognition with Incomplete Labels", "comments": "Accepted by FG2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train a unified model to perform three tasks: facial action unit\ndetection, expression classification, and valence-arousal estimation. We\naddress two main challenges of learning the three tasks. First, most existing\ndatasets are highly imbalanced. Second, most existing datasets do not contain\nlabels for all three tasks. To tackle the first challenge, we apply data\nbalancing techniques to experimental datasets. To tackle the second challenge,\nwe propose an algorithm for the multitask model to learn from missing\n(incomplete) labels. This algorithm has two steps. We first train a teacher\nmodel to perform all three tasks, where each instance is trained by the ground\ntruth label of its corresponding task. Secondly, we refer to the outputs of the\nteacher model as the soft labels. We use the soft labels and the ground truth\nto train the student model. We find that most of the student models outperform\ntheir teacher model on all the three tasks. Finally, we use model ensembling to\nboost performance further on the three tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 05:32:12 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:52:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Deng", "Didan", ""], ["Chen", "Zhaokang", ""], ["Shi", "Bertram E.", ""]]}, {"id": "2002.03773", "submitter": "Kashif Ahmad", "authors": "Kashif Ahmad, Syed Zohaib, Nicola Conci and Ala Al-Fuqaha", "title": "Deriving Emotions and Sentiments from Visual Content: A Disaster\n  Analysis Use Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis aims to extract and express a person's perception,\nopinions and emotions towards an entity, object, product and a service,\nenabling businesses to obtain feedback from the consumers. The increasing\npopularity of the social networks and users' tendency towards sharing their\nfeelings, expressions and opinions in text, visual and audio content has opened\nnew opportunities and challenges in sentiment analysis. While sentiment\nanalysis of text streams has been widely explored in the literature, sentiment\nanalysis of images and videos is relatively new. This article introduces visual\nsentiment analysis and contrasts it with textual sentiment analysis with\nemphasis on the opportunities and challenges in this nascent research area. We\nalso propose a deep visual sentiment analyzer for disaster-related images as a\nuse-case, covering different aspects of visual sentiment analysis starting from\ndata collection, annotation, model selection, implementation and evaluations.\nWe believe such rigorous analysis will provide a baseline for future research\nin the domain.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 08:48:52 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ahmad", "Kashif", ""], ["Zohaib", "Syed", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "2002.03977", "submitter": "Ross Cutler", "authors": "Ross Cutler, Ramin Mehran, Sam Johnson, Cha Zhang, Adam Kirk, Oliver\n  Whyte, Adarsh Kowdle", "title": "Multimodal active speaker detection and virtual cinematography for video\n  conferencing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active speaker detection (ASD) and virtual cinematography (VC) can\nsignificantly improve the remote user experience of a video conference by\nautomatically panning, tilting and zooming of a video conferencing camera:\nusers subjectively rate an expert video cinematographer's video significantly\nhigher than unedited video. We describe a new automated ASD and VC that\nperforms within 0.3 MOS of an expert cinematographer based on subjective\nratings with a 1-5 scale. This system uses a 4K wide-FOV camera, a depth\ncamera, and a microphone array; it extracts features from each modality and\ntrains an ASD using an AdaBoost machine learning system that is very efficient\nand runs in real-time. A VC is similarly trained using machine learning to\noptimize the subjective quality of the overall experience. To avoid distracting\nthe room participants and reduce switching latency the system has no moving\nparts -- the VC works by cropping and zooming the 4K wide-FOV video stream. The\nsystem was tuned and evaluated using extensive crowdsourcing techniques and\nevaluated on a dataset with N=100 meetings, each 2-5 minutes in length.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 17:41:51 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 06:09:28 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Cutler", "Ross", ""], ["Mehran", "Ramin", ""], ["Johnson", "Sam", ""], ["Zhang", "Cha", ""], ["Kirk", "Adam", ""], ["Whyte", "Oliver", ""], ["Kowdle", "Adarsh", ""]]}, {"id": "2002.04537", "submitter": "Xue Zhang", "authors": "Xue Zhang, Gene Cheung, Jiahao Pang, Dong Tian", "title": "3D Point Cloud Enhancement using Graph-Modelled Multiview Depth\n  Measurements", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 3D point cloud is often synthesized from depth measurements collected by\nsensors at different viewpoints. The acquired measurements are typically both\ncoarse in precision and corrupted by noise. To improve quality, previous works\ndenoise a synthesized 3D point cloud a posteriori after projecting the\nimperfect depth data onto 3D space. Instead, we enhance depth measurements on\nthe sensed images a priori, exploiting inherent 3D geometric correlation across\nviews, before synthesizing a 3D point cloud from the improved measurements. By\nenhancing closer to the actual sensing process, we benefit from optimization\ntargeting specifically the depth image formation model, before subsequent\nprocessing steps that can further obscure measurement errors. Mathematically,\nfor each pixel row in a pair of rectified viewpoint depth images, we first\nconstruct a graph reflecting inter-pixel similarities via metric learning using\ndata in previous enhanced rows. To optimize left and right viewpoint images\nsimultaneously, we write a non-linear mapping function from left pixel row to\nthe right based on 3D geometry relations. We formulate a MAP optimization\nproblem, which, after suitable linear approximations, results in an\nunconstrained convex and differentiable objective, solvable using fast gradient\nmethod (FGM). Experimental results show that our method noticeably outperforms\nrecent denoising algorithms that enhance after 3D point clouds are synthesized.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 16:44:22 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zhang", "Xue", ""], ["Cheung", "Gene", ""], ["Pang", "Jiahao", ""], ["Tian", "Dong", ""]]}, {"id": "2002.04780", "submitter": "Shuang Xu", "authors": "Shuang Xu and Xiaoli Wei and Chunxia Zhang and Junmin Liu and Jiangshe\n  Zhang", "title": "MFFW: A new dataset for multi-focus image fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-focus image fusion (MFF) is a fundamental task in the field of\ncomputational photography. Current methods have achieved significant\nperformance improvement. It is found that current methods are evaluated on\nsimulated image sets or Lytro dataset. Recently, a growing number of\nresearchers pay attention to defocus spread effect, a phenomenon of real-world\nmulti-focus images. Nonetheless, defocus spread effect is not obvious in\nsimulated or Lytro datasets, where popular methods perform very similar. To\ncompare their performance on images with defocus spread effect, this paper\nconstructs a new dataset called MFF in the wild (MFFW). It contains 19 pairs of\nmulti-focus images collected on the Internet. We register all pairs of source\nimages, and provide focus maps and reference images for part of pairs. Compared\nwith Lytro dataset, images in MFFW significantly suffer from defocus spread\neffect. In addition, the scenes of MFFW are more complex. The experiments\ndemonstrate that most state-of-the-art methods on MFFW dataset cannot robustly\ngenerate satisfactory fusion images. MFFW can be a new baseline dataset to test\nwhether an MMF algorithm is able to deal with defocus spread effect.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 03:35:37 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Xu", "Shuang", ""], ["Wei", "Xiaoli", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2002.05070", "submitter": "Jianren Wang", "authors": "Jianren Wang, Zhaoyuan Fang, Hang Zhao", "title": "AlignNet: A Unifying Approach to Audio-Visual Alignment", "comments": "WACV2020. Project video and code are available at\n  https://jianrenw.github.io/AlignNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AlignNet, a model that synchronizes videos with reference audios\nunder non-uniform and irregular misalignments. AlignNet learns the end-to-end\ndense correspondence between each frame of a video and an audio. Our method is\ndesigned according to simple and well-established principles: attention,\npyramidal processing, warping, and affinity function. Together with the model,\nwe release a dancing dataset Dance50 for training and evaluation. Qualitative,\nquantitative and subjective evaluation results on dance-music alignment and\nspeech-lip alignment demonstrate that our method far outperforms the\nstate-of-the-art methods. Project video and code are available at\nhttps://jianrenw.github.io/AlignNet.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 16:19:28 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Wang", "Jianren", ""], ["Fang", "Zhaoyuan", ""], ["Zhao", "Hang", ""]]}, {"id": "2002.05305", "submitter": "Wanze Xie", "authors": "Wanze Xie, Yining Liang, Janet Johnson, Andrea Mower, Samuel Burns,\n  Colleen Chelini, Paul D Alessandro, Nadir Weibel, J\\\"urgen P. Schulze", "title": "Interactive Multi-User 3D Visual Analytics in Augmented Reality", "comments": "In Proceedings of IS&T The Engineering Reality of Virtual Reality\n  2020", "journal-ref": "Electronic Imaging, The Engineering Reality of Virtual Reality\n  2020, pp. 363-1-363-6(6)", "doi": "10.2352/ISSN.2470-1173.2020.13.ERVR-363", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This publication reports on a research project in which we set out to explore\nthe advantages and disadvantages augmented reality (AR) technology has for\nvisual data analytics. We developed a prototype of an AR data analytics\napplication, which provides users with an interactive 3D interface, hand\ngesture-based controls and multi-user support for a shared experience, enabling\nmultiple people to collaboratively visualize, analyze and manipulate data with\nhigh dimensional features in 3D space. Our software prototype, called DataCube,\nruns on the Microsoft HoloLens - one of the first true stand-alone AR headsets,\nthrough which users can see computer-generated images overlaid onto real-world\nobjects in the user's physical environment. Using hand gestures, the users can\nselect menu options, control the 3D data visualization with various filtering\nand visualization functions, and freely arrange the various menus and virtual\ndisplays in their environment. The shared multi-user experience allows all\nparticipating users to see and interact with the virtual environment, changes\none user makes will become visible to the other users instantly. As users\nengage together they are not restricted from observing the physical world\nsimultaneously and therefore they can also see non-verbal cues such as\ngesturing or facial reactions of other users in the physical environment. The\nmain objective of this research project was to find out if AR interfaces and\ncollaborative analysis can provide an effective solution for data analysis\ntasks, and our experience with our prototype system confirms this.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 01:35:56 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Xie", "Wanze", ""], ["Liang", "Yining", ""], ["Johnson", "Janet", ""], ["Mower", "Andrea", ""], ["Burns", "Samuel", ""], ["Chelini", "Colleen", ""], ["Alessandro", "Paul D", ""], ["Weibel", "Nadir", ""], ["Schulze", "J\u00fcrgen P.", ""]]}, {"id": "2002.05314", "submitter": "Yifan Ding", "authors": "Yifan Ding, Yong Xu, Shi-Xiong Zhang, Yahuan Cong and Liqiang Wang", "title": "Self-supervised learning for audio-visual speaker diarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization, which is to find the speech segments of specific\nspeakers, has been widely used in human-centered applications such as video\nconferences or human-computer interaction systems. In this paper, we propose a\nself-supervised audio-video synchronization learning method to address the\nproblem of speaker diarization without massive labeling effort. We improve the\nprevious approaches by introducing two new loss functions: the dynamic triplet\nloss and the multinomial loss. We test them on a real-world human-computer\ninteraction system and the results show our best model yields a remarkable gain\nof +8%F1-scoresas well as diarization error rate reduction. Finally, we\nintroduce a new large scale audio-video corpus designed to fill the vacancy of\naudio-video datasets in Chinese.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 02:36:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Ding", "Yifan", ""], ["Xu", "Yong", ""], ["Zhang", "Shi-Xiong", ""], ["Cong", "Yahuan", ""], ["Wang", "Liqiang", ""]]}, {"id": "2002.05604", "submitter": "Kai Zhen", "authors": "Kai Zhen, Mi Suk Lee, Jongmo Sung, Seungkwon Beack, Minje Kim", "title": "Efficient And Scalable Neural Residual Waveform Coding With\n  Collaborative Quantization", "comments": "Accepted in Proceedings of the IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP) , Barcelona, Spain, May\n  4-8, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability and efficiency are desired in neural speech codecs, which\nsupports a wide range of bitrates for applications on various devices. We\npropose a collaborative quantization (CQ) scheme to jointly learn the codebook\nof LPC coefficients and the corresponding residuals. CQ does not simply\nshoehorn LPC to a neural network, but bridges the computational capacity of\nadvanced neural network models and traditional, yet efficient and\ndomain-specific digital signal processing methods in an integrated manner. We\ndemonstrate that CQ achieves much higher quality than its predecessor at 9 kbps\nwith even lower model complexity. We also show that CQ can scale up to 24 kbps\nwhere it outperforms AMR-WB and Opus. As a neural waveform codec, CQ models are\nwith less than 1 million parameters, significantly less than many other\ngenerative models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:28:09 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Zhen", "Kai", ""], ["Lee", "Mi Suk", ""], ["Sung", "Jongmo", ""], ["Beack", "Seungkwon", ""], ["Kim", "Minje", ""]]}, {"id": "2002.05639", "submitter": "Tejas Srinivasan", "authors": "Tejas Srinivasan, Ramon Sanabria, Florian Metze", "title": "Looking Enhances Listening: Recovering Missing Speech Using Images", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is understood better by using visual context; for this reason, there\nhave been many attempts to use images to adapt automatic speech recognition\n(ASR) systems. Current work, however, has shown that visually adapted ASR\nmodels only use images as a regularization signal, while completely ignoring\ntheir semantic content. In this paper, we present a set of experiments where we\nshow the utility of the visual modality under noisy conditions. Our results\nshow that multimodal ASR models can recover words which are masked in the input\nacoustic signal, by grounding its transcriptions using the visual\nrepresentations. We observe that integrating visual context can result in up to\n35% relative improvement in masked word recovery. These results demonstrate\nthat end-to-end multimodal ASR systems can become more robust to noise by\nleveraging the visual context.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:12:51 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Srinivasan", "Tejas", ""], ["Sanabria", "Ramon", ""], ["Metze", "Florian", ""]]}, {"id": "2002.06652", "submitter": "Bin Wang", "authors": "Bin Wang, C.-C. Jay Kuo", "title": "SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence embedding is an important research topic in natural language\nprocessing (NLP) since it can transfer knowledge to downstream tasks.\nMeanwhile, a contextualized word representation, called BERT, achieves the\nstate-of-the-art performance in quite a few NLP tasks. Yet, it is an open\nproblem to generate a high quality sentence representation from BERT-based word\nmodels. It was shown in previous study that different layers of BERT capture\ndifferent linguistic properties. This allows us to fusion information across\nlayers to find better sentence representation. In this work, we study the\nlayer-wise pattern of the word representation of deep contextualized models.\nThen, we propose a new sentence embedding method by dissecting BERT-based word\nmodels through geometric analysis of the space spanned by the word\nrepresentation. It is called the SBERT-WK method. No further training is\nrequired in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and\ndownstream supervised tasks. Furthermore, ten sentence-level probing tasks are\npresented for detailed linguistic analysis. Experiments show that SBERT-WK\nachieves the state-of-the-art performance. Our codes are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:02:52 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:39:09 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wang", "Bin", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2002.06794", "submitter": "Zichi Wang", "authors": "Zhenxing Qian, Zichi Wang, Xinpeng Zhang", "title": "Computing in Covert Domain Using Data Hiding", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an idea of data computing in the covert domain (DCCD). We\nshow that with information hiding some data computing tasks can be executed\nbeneath the covers like images, audios, random data, etc. In the proposed\nframework, a sender hides his source data into two covers and uploads them onto\na server. The server executes computation within the stego and returns the\ncovert computing result to a receiver. With the covert result, the receiver can\nextract the computing result of the source data. During the process, it is\nimperceptible for the server and the adversaries to obtain the source data as\nthey are hidden in the cover. The transmission can be done over public\nchannels. Meanwhile, since the computation is realized in the covert domain,\nthe cloud cannot obtain the knowledge of the computing result. Therefore, the\nproposed idea is useful for cloud computing.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:30:17 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Qian", "Zhenxing", ""], ["Wang", "Zichi", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2002.06817", "submitter": "Kaihsiang Cheng", "authors": "Tsung-Han Hsieh, Kai-Hsiang Cheng, Zhe-Cheng Fan, Yu-Ching Yang,\n  Yi-Hsuan Yang", "title": "Addressing the confounds of accompaniments in singer identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying singers is an important task with many applications. However, the\ntask remains challenging due to many issues. One major issue is related to the\nconfounding factors from the background instrumental music that is mixed with\nthe vocals in music production. A singer identification model may learn to\nextract non-vocal related features from the instrumental part of the songs, if\na singer only sings in certain musical contexts (e.g., genres). The model\ncannot therefore generalize well when the singer sings in unseen contexts. In\nthis paper, we attempt to address this issue. Specifically, we employ\nopen-unmix, an open source tool with state-of-the-art performance in source\nseparation, to separate the vocal and instrumental tracks of music. We then\ninvestigate two means to train a singer identification model: by learning from\nthe separated vocal only, or from an augmented set of data where we\n\"shuffle-and-remix\" the separated vocal tracks and instrumental tracks of\ndifferent songs to artificially make the singers sing in different contexts. We\nalso incorporate melodic features learned from the vocal melody contour for\nbetter performance. Evaluation results on a benchmark dataset called the\nartist20 shows that this data augmentation method greatly improves the accuracy\nof singer identification.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:49:21 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Hsieh", "Tsung-Han", ""], ["Cheng", "Kai-Hsiang", ""], ["Fan", "Zhe-Cheng", ""], ["Yang", "Yu-Ching", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "2002.06923", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Vincent Labatut (LIA), Georges Linares (LIA)", "title": "Serial Speakers: a Dataset of TV Series", "comments": null, "journal-ref": "12th International Conference on Language Resources and Evaluation\n  (LREC 2020), p.4256-4264, May 2020, Marseille, France", "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For over a decade, TV series have been drawing increasing interest, both from\nthe audience and from various academic fields. But while most viewers are\nhooked on the continuous plots of TV serials, the few annotated datasets\navailable to researchers focus on standalone episodes of classical TV series.\nWe aim at filling this gap by providing the multimedia/speech processing\ncommunities with Serial Speakers, an annotated dataset of 161 episodes from\nthree popular American TV serials: Breaking Bad, Game of Thrones and House of\nCards. Serial Speakers is suitable both for investigating multimedia retrieval\nin realistic use case scenarios, and for addressing lower level speech related\ntasks in especially challenging conditions. We publicly release annotations for\nevery speech turn (boundaries, speaker) and scene boundary, along with\nannotations for shot boundaries, recurring shots, and interacting speakers in a\nsubset of episodes. Because of copyright restrictions, the textual content of\nthe speech turns is encrypted in the public version of the dataset, but we\nprovide the users with a simple online tool to recover the plain text from\ntheir own subtitle files.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 12:51:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "2002.07048", "submitter": "Saeed  Ranjbar Alvar", "authors": "Saeed Ranjbar Alvar and Ivan V. Baji\\'c", "title": "Bit Allocation for Multi-Task Collaborative Intelligence", "comments": "Accepted for publication ICASSP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that collaborative intelligence (CI) is a promising\nframework for deployment of Artificial Intelligence (AI)-based services on\nmobile devices. In CI, a deep neural network is split between the mobile device\nand the cloud. Deep features obtained at the mobile are compressed and\ntransferred to the cloud to complete the inference. So far, the methods in the\nliterature focused on transferring a single deep feature tensor from the mobile\nto the cloud. Such methods are not applicable to some recent, high-performance\nnetworks with multiple branches and skip connections. In this paper, we propose\nthe first bit allocation method for multi-stream, multi-task CI. We first\nestablish a model for the joint distortion of the multiple tasks as a function\nof the bit rates assigned to different deep feature tensors. Then, using the\nproposed model, we solve the rate-distortion optimization problem under a total\nrate constraint to obtain the best rate allocation among the tensors to be\ntransferred. Experimental results illustrate the efficacy of the proposed\nscheme compared to several alternative bit allocation methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 02:02:39 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Alvar", "Saeed Ranjbar", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2002.07082", "submitter": "Shiv Ram Dubey", "authors": "Kancharagunta Kishan Babu and Shiv Ram Dubey", "title": "PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks\n  for Thermal and NIR to Visible Image Transformation", "comments": "Published in Neurocomputing Journal, Elsevier", "journal-ref": "Neurocomputing, 413:41-50, Nov 2020", "doi": "10.1016/j.neucom.2020.06.104", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world scenarios, it is difficult to capture the images in the\nvisible light spectrum (VIS) due to bad lighting conditions. However, the\nimages can be captured in such scenarios using Near-Infrared (NIR) and Thermal\n(THM) cameras. The NIR and THM images contain the limited details. Thus, there\nis a need to transform the images from THM/NIR to VIS for better understanding.\nHowever, it is non-trivial task due to the large domain discrepancies and lack\nof abundant datasets. Nowadays, Generative Adversarial Network (GAN) is able to\ntransform the images from one domain to another domain. Most of the available\nGAN based methods use the combination of the adversarial and the pixel-wise\nlosses (like $L_1$ or $L_2$) as the objective function for training. The\nquality of transformed images in case of THM/NIR to VIS transformation is still\nnot up to the mark using such objective function. Thus, better objective\nfunctions are needed to improve the quality, fine details and realism of the\ntransformed images. A new model for THM/NIR to VIS image transformation called\nPerceptual Cyclic-Synthesized Generative Adversarial Network (PCSGAN) is\nintroduced to address these issues. The PCSGAN uses the combination of the\nperceptual (i.e., feature based) losses along with the pixel-wise and the\nadversarial losses. Both the quantitative and qualitative measures are used to\njudge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR\nscene datasets. The proposed PCSGAN outperforms the state-of-the-art image\ntransformation models, including Pix2pix, DualGAN, CycleGAN, PS2GAN, and PAN in\nterms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is\navailable at https://github.com/KishanKancharagunta/PCSGAN.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 11:55:03 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 11:50:33 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Babu", "Kancharagunta Kishan", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2002.07677", "submitter": "Bharath K P", "authors": "Pratibha Balaji, Shruthi Narayan, Durga Sraddha, Bharath K P, Karthik\n  R, Rajesh Kumar Muthu", "title": "Performance Analysis of Adaptive Noise Cancellation for Speech Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper gives a broader insight on the application of adaptive filter in\nnoise cancellation during various processes where signal is transmitted.\nAdaptive filtering techniques like RLS, LMS and normalized LMS are used to\nfilter the input signal using the concept of negative feedback to predict its\nnature and remove it effectively from the input. In this paper a comparative\nstudy between the effectiveness of RLS, LMS and normalized LMS is done based on\nparameters like SNR (Signal to Noise ratio), MSE (Mean squared error) and cross\ncorrelation. Implementation and analysis of the filters are done by taking\ndifferent step sizes on different orders of the filters.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 10:01:14 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Balaji", "Pratibha", ""], ["Narayan", "Shruthi", ""], ["Sraddha", "Durga", ""], ["P", "Bharath K", ""], ["R", "Karthik", ""], ["Muthu", "Rajesh Kumar", ""]]}, {"id": "2002.09140", "submitter": "Jiahua Xu", "authors": "Jiahua Xu, Wei Zhou and Zhibo Chen", "title": "Blind Omnidirectional Image Quality Assessment with Viewport Oriented\n  Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment of omnidirectional images has become increasingly urgent\ndue to the rapid growth of virtual reality applications. Different from\ntraditional 2D images and videos, omnidirectional contents can provide\nconsumers with freely changeable viewports and a larger field of view covering\nthe $360^{\\circ}\\times180^{\\circ}$ spherical surface, which makes the objective\nquality assessment of omnidirectional images more challenging. In this paper,\nmotivated by the characteristics of the human vision system (HVS) and the\nviewing process of omnidirectional contents, we propose a novel Viewport\noriented Graph Convolution Network (VGCN) for blind omnidirectional image\nquality assessment (IQA). Generally, observers tend to give the subjective\nrating of a 360-degree image after passing and aggregating different viewports\ninformation when browsing the spherical scenery. Therefore, in order to model\nthe mutual dependency of viewports in the omnidirectional image, we build a\nspatial viewport graph. Specifically, the graph nodes are first defined with\nselected viewports with higher probabilities to be seen, which is inspired by\nthe HVS that human beings are more sensitive to structural information. Then,\nthese nodes are connected by spatial relations to capture interactions among\nthem. Finally, reasoning on the proposed graph is performed via graph\nconvolutional networks. Moreover, we simultaneously obtain global quality using\nthe entire omnidirectional image without viewport sampling to boost the\nperformance according to the viewing experience. Experimental results\ndemonstrate that our proposed model outperforms state-of-the-art full-reference\nand no-reference IQA metrics on two public omnidirectional IQA databases.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 05:54:20 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 07:41:31 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Xu", "Jiahua", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2002.09461", "submitter": "Peng Xu", "authors": "Peng Xu, Kun Liu, Tao Xiang, Timothy M. Hospedales, Zhanyu Ma, Jun\n  Guo, Yi-Zhe Song", "title": "Fine-Grained Instance-Level Sketch-Based Video Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing sketch-analysis work studies sketches depicting static objects or\nscenes. In this work, we propose a novel cross-modal retrieval problem of\nfine-grained instance-level sketch-based video retrieval (FG-SBVR), where a\nsketch sequence is used as a query to retrieve a specific target video\ninstance. Compared with sketch-based still image retrieval, and coarse-grained\ncategory-level video retrieval, this is more challenging as both visual\nappearance and motion need to be simultaneously matched at a fine-grained\nlevel. We contribute the first FG-SBVR dataset with rich annotations. We then\nintroduce a novel multi-stream multi-modality deep network to perform FG-SBVR\nunder both strong and weakly supervised settings. The key component of the\nnetwork is a relation module, designed to prevent model over-fitting given\nscarce training data. We show that this model significantly outperforms a\nnumber of existing state-of-the-art models designed for video analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:28:35 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Xu", "Peng", ""], ["Liu", "Kun", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2002.09607", "submitter": "Kele Xu", "authors": "Liang Gao and Kele Xu and Huaimin Wang and Yuxing Peng", "title": "Multi-Representation Knowledge Distillation For Audio Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important component of multimedia analysis tasks, audio classification\naims to discriminate between different audio signal types and has received\nintensive attention due to its wide applications. Generally speaking, the raw\nsignal can be transformed into various representations (such as Short Time\nFourier Transform and Mel Frequency Cepstral Coefficients), and information\nimplied in different representations can be complementary. Ensembling the\nmodels trained on different representations can greatly boost the\nclassification performance, however, making inference using a large number of\nmodels is cumbersome and computationally expensive. In this paper, we propose a\nnovel end-to-end collaborative learning framework for the audio classification\ntask. The framework takes multiple representations as the input to train the\nmodels in parallel. The complementary information provided by different\nrepresentations is shared by knowledge distillation. Consequently, the\nperformance of each model can be significantly promoted without increasing the\ncomputational overhead in the inference stage. Extensive experimental results\ndemonstrate that the proposed approach can improve the classification\nperformance and achieve state-of-the-art results on both acoustic scene\nclassification tasks and general audio tagging tasks.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 02:53:39 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gao", "Liang", ""], ["Xu", "Kele", ""], ["Wang", "Huaimin", ""], ["Peng", "Yuxing", ""]]}, {"id": "2002.09748", "submitter": "Daphne Odekerken", "authors": "Daphne Odekerken, Hendrik Vincent Koops, Anja Volk", "title": "DECIBEL: Improving Audio Chord Estimation for Popular Music by Alignment\n  and Integration of Crowd-Sourced Symbolic Representations", "comments": "81 pages, 47 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Chord Estimation (ACE) is a fundamental task in Music Information\nRetrieval (MIR) and has applications in both music performance and MIR\nresearch. The task consists of segmenting a music recording or score and\nassigning a chord label to each segment. Although it has been a task in the\nannual benchmarking evaluation MIREX for over 10 years, ACE is not yet a solved\nproblem, since performance has stagnated and modern systems have started to\ntune themselves to subjective training data. We propose DECIBEL, a new ACE\nsystem that exploits widely available MIDI and tab representations to improve\nACE from audio only. From an audio file and a set of MIDI and tab files\ncorresponding to the same popular music song, DECIBEL first estimates chord\nsequences. For audio, state-of-the-art audio ACE methods are used. MIDI files\nare aligned to the audio, followed by a MIDI chord estimation step. Tab files\nare transformed into untimed chord sequences and then aligned to the audio.\nNext, DECIBEL uses data fusion to integrate all estimated chord sequences into\none final output sequence. DECIBEL improves all tested state-of-the-art ACE\nmethods by over 3 percent on average. This result shows that the integration of\nmusical knowledge from heterogeneous symbolic music representations is a\nsuitable strategy for addressing challenging MIR tasks such as ACE.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 18:42:00 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Odekerken", "Daphne", ""], ["Koops", "Hendrik Vincent", ""], ["Volk", "Anja", ""]]}, {"id": "2002.10651", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Chia-Ju Chen, Li-Heng Chen, Neil Birkbeck, Balu\n  Adsumilli, and Alan C. Bovik", "title": "A Comparative Evaluation of Temporal Pooling Methods for Blind Video\n  Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many objective video quality assessment (VQA) algorithms include a key step\nof temporal pooling of frame-level quality scores. However, less attention has\nbeen paid to studying the relative efficiencies of different pooling methods on\nno-reference (blind) VQA. Here we conduct a large-scale comparative evaluation\nto assess the capabilities and limitations of multiple temporal pooling\nstrategies on blind VQA of user-generated videos. The study yields insights and\ngeneral guidance regarding the application and selection of temporal pooling\nmodels. In addition, we also propose an ensemble pooling model built on top of\nhigh-performing temporal pooling models. Our experimental results demonstrate\nthe relative efficacies of the evaluated temporal pooling models, using several\npopular VQA algorithms, and evaluated on two recent large-scale natural video\nquality databases. In addition to the new ensemble model, we provide a general\nrecipe for applying temporal pooling of frame-based quality predictions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:35:06 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Chen", "Chia-Ju", ""], ["Chen", "Li-Heng", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2002.10696", "submitter": "Markku Suomalainen", "authors": "Israel Becerra, Markku Suomalainen, Eliezer Lozano, Katherine J.\n  Mimnaugh, Rafael Murrieta-Cid and Steven M. LaValle", "title": "Human Perception-Optimized Planning for Comfortable VR-Based\n  Telepresence", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "journal-ref": null, "doi": "10.1109/LRA.2020.3015191", "report-no": null, "categories": "cs.RO cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an emerging motion planning problem by considering a\nhuman that is immersed into the viewing perspective of a remote robot. The\nchallenge is to make the experience both effective (such as delivering a sense\nof presence) and comfortable (such as avoiding adverse sickness symptoms,\nincluding nausea). We refer to this challenging new area as human\nperception-optimized planning and propose a general multiobjective optimization\nframework that can be instantiated in many envisioned scenarios. We then\nconsider a specific VR telepresence task as a case of human\nperception-optimized planning, in which we simulate a robot that sends 360\nvideo to a remote user to be viewed through a head-mounted display. In this\nparticular task, we plan trajectories that minimize VR sickness (and thereby\nmaximize comfort). An A* type method is used to create a Pareto-optimal\ncollection of piecewise linear trajectories while taking into account criteria\nthat improve comfort. We conducted a study with human subjects touring a\nvirtual museum, in which paths computed by our algorithm are compared against a\nreference RRT-based trajectory. Generally, users suffered less from VR sickness\nand preferred the paths created by the presented algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 06:48:43 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 06:09:48 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Becerra", "Israel", ""], ["Suomalainen", "Markku", ""], ["Lozano", "Eliezer", ""], ["Mimnaugh", "Katherine J.", ""], ["Murrieta-Cid", "Rafael", ""], ["LaValle", "Steven M.", ""]]}, {"id": "2002.10798", "submitter": "Hui Yuan", "authors": "Qi Liu, Hui Yuan, Junhui Hou, Raouf Hamzaoui, and Honglei Su", "title": "Model-based Joint Bit Allocation between Geometry and Color for\n  Video-based 3D Point Cloud Compression", "comments": "13pages, 10 figures, submitted to IEEE Transactions on Multimedia", "journal-ref": "IEEE Transactions on Multimedia, 2020", "doi": "10.1109/TMM.2020.3023294", "report-no": null, "categories": "eess.IV cs.MM cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate distortion optimization plays a very important role in image/video\ncoding. But for 3D point cloud, this problem has not been investigated. In this\npaper, the rate and distortion characteristics of 3D point cloud are\ninvestigated in detail, and a typical and challenging rate distortion\noptimization problem is solved for 3D point cloud. Specifically, since the\nquality of the reconstructed 3D point cloud depends on both the geometry and\ncolor distortions, we first propose analytical rate and distortion models for\nthe geometry and color information in video-based 3D point cloud compression\nplatform, and then solve the joint bit allocation problem for geometry and\ncolor based on the derived models. To maximize the reconstructed quality of 3D\npoint cloud, the bit allocation problem is formulated as a constrained\noptimization problem and solved by an interior point method. Experimental\nresults show that the rate-distortion performance of the proposed solution is\nclose to that obtained with exhaustive search but at only 0.68% of its time\ncomplexity. Moreover, the proposed rate and distortion models can also be used\nfor the other rate-distortion optimization problems (such as prediction mode\ndecision) and rate control technologies for 3D point cloud coding in the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 11:35:32 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:00:47 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Liu", "Qi", ""], ["Yuan", "Hui", ""], ["Hou", "Junhui", ""], ["Hamzaoui", "Raouf", ""], ["Su", "Honglei", ""]]}, {"id": "2002.11079", "submitter": "Yukai Shi", "authors": "Yukai Shi, Haoyu Zhong, Zhijing Yang, Xiaojun Yang, Liang Lin", "title": "DDet: Dual-path Dynamic Enhancement Network for Real-World Image\n  Super-Resolution", "comments": "Code address: https://github.com/ykshi/DDet", "journal-ref": null, "doi": "10.1109/LSP.2020.2978410", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from traditional image super-resolution task, real image\nsuper-resolution(Real-SR) focus on the relationship between real-world\nhigh-resolution(HR) and low-resolution(LR) image. Most of the traditional image\nSR obtains the LR sample by applying a fixed down-sampling operator. Real-SR\nobtains the LR and HR image pair by incorporating different quality optical\nsensors. Generally, Real-SR has more challenges as well as broader application\nscenarios. Previous image SR methods fail to exhibit similar performance on\nReal-SR as the image data is not aligned inherently. In this article, we\npropose a Dual-path Dynamic Enhancement Network(DDet) for Real-SR, which\naddresses the cross-camera image mapping by realizing a dual-way dynamic\nsub-pixel weighted aggregation and refinement. Unlike conventional methods\nwhich stack up massive convolutional blocks for feature representation, we\nintroduce a content-aware framework to study non-inherently aligned image pair\nin image SR issue. First, we use a content-adaptive component to exhibit the\nMulti-scale Dynamic Attention(MDA). Second, we incorporate a long-term skip\nconnection with a Coupled Detail Manipulation(CDM) to perform collaborative\ncompensation and manipulation. The above dual-path model is joint into a\nunified model and works collaboratively. Extensive experiments on the\nchallenging benchmarks demonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:24:51 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Shi", "Yukai", ""], ["Zhong", "Haoyu", ""], ["Yang", "Zhijing", ""], ["Yang", "Xiaojun", ""], ["Lin", "Liang", ""]]}, {"id": "2002.11088", "submitter": "Jie Zhang", "authors": "Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming Zhang, Wenbo\n  Zhou, Hao Cui, Nenghai Yu", "title": "Model Watermarking for Image Processing Networks", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved tremendous success in numerous industrial\napplications. As training a good model often needs massive high-quality data\nand computation resources, the learned models often have significant business\nvalues. However, these valuable deep models are exposed to a huge risk of\ninfringements. For example, if the attacker has the full information of one\ntarget model including the network structure and weights, the model can be\neasily finetuned on new datasets. Even if the attacker can only access the\noutput of the target model, he/she can still train another similar surrogate\nmodel by generating a large scale of input-output training pairs. How to\nprotect the intellectual property of deep models is a very important but\nseriously under-researched problem. There are a few recent attempts at\nclassification network protection only. In this paper, we propose the first\nmodel watermarking framework for protecting image processing models. To achieve\nthis goal, we leverage the spatial invisible watermarking mechanism.\nSpecifically, given a black-box target model, a unified and invisible watermark\nis hidden into its outputs, which can be regarded as a special task-agnostic\nbarrier. In this way, when the attacker trains one surrogate model by using the\ninput-output pairs of the target model, the hidden watermark will be learned\nand extracted afterward. To enable watermarks from binary bits to\nhigh-resolution images, both traditional and deep spatial invisible\nwatermarking mechanism are considered. Experiments demonstrate the robustness\nof the proposed watermarking mechanism, which can resist surrogate models\nlearned with different network structures and objective functions. Besides deep\nmodels, the proposed method is also easy to be extended to protect data and\ntraditional image processing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:36:18 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Zhang", "Jie", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Fang", "Han", ""], ["Zhang", "Weiming", ""], ["Zhou", "Wenbo", ""], ["Cui", "Hao", ""], ["Yu", "Nenghai", ""]]}, {"id": "2002.11616", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach,\n  Chenliang Xu", "title": "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video\n  Super-Resolution", "comments": "This work is accepted in CVPR 2020. The source code and pre-trained\n  model are available on https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.\n  12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the space-time video super-resolution task, which\naims to generate a high-resolution (HR) slow-motion video from a low frame rate\n(LFR), low-resolution (LR) video. A simple solution is to split it into two\nsub-tasks: video frame interpolation (VFI) and video super-resolution (VSR).\nHowever, temporal interpolation and spatial super-resolution are intra-related\nin this task. Two-stage methods cannot fully take advantage of the natural\nproperty. In addition, state-of-the-art VFI or VSR networks require a large\nframe-synthesis or reconstruction module for predicting high-quality video\nframes, which makes the two-stage methods have large model sizes and thus be\ntime-consuming. To overcome the problems, we propose a one-stage space-time\nvideo super-resolution framework, which directly synthesizes an HR slow-motion\nvideo from an LFR, LR video. Rather than synthesizing missing LR video frames\nas VFI networks do, we firstly temporally interpolate LR frame features in\nmissing LR video frames capturing local temporal contexts by the proposed\nfeature temporal interpolation network. Then, we propose a deformable ConvLSTM\nto align and aggregate temporal information simultaneously for better\nleveraging global temporal contexts. Finally, a deep reconstruction network is\nadopted to predict HR slow-motion video frames. Extensive experiments on\nbenchmark datasets demonstrate that the proposed method not only achieves\nbetter quantitative and qualitative performance but also is more than three\ntimes faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR\nand DAIN+RBPN.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:59:48 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Tian", "Yapeng", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""], ["Allebach", "Jan P.", ""], ["Xu", "Chenliang", ""]]}, {"id": "2002.11812", "submitter": "Qingyuan Zheng", "authors": "Qingyuan Zheng, Zhuoru Li and Adam Bargteil", "title": "Learning to Shadow Hand-drawn Sketches", "comments": "To appear in CVPR 2020 (Oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic method to generate detailed and accurate\nartistic shadows from pairs of line drawing sketches and lighting directions.\nWe also contribute a new dataset of one thousand examples of pairs of line\ndrawings and shadows that are tagged with lighting directions. Remarkably, the\ngenerated shadows quickly communicate the underlying 3D structure of the\nsketched scene. Consequently, the shadows generated by our approach can be used\ndirectly or as an excellent starting point for artists. We demonstrate that the\ndeep learning network we propose takes a hand-drawn sketch, builds a 3D model\nin latent space, and renders the resulting shadows. The generated shadows\nrespect the hand-drawn lines and underlying 3D space and contain sophisticated\nand accurate details, such as self-shadowing effects. Moreover, the generated\nshadows contain artistic effects, such as rim lighting or halos appearing from\nback lighting, that would be achievable with traditional 3D rendering methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 21:57:17 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 23:12:21 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Zheng", "Qingyuan", ""], ["Li", "Zhuoru", ""], ["Bargteil", "Adam", ""]]}, {"id": "2002.11891", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Jessie Lin, Yilin Wang, Balu Adsumilli, and Alan C.\n  Bovik", "title": "BBAND Index: A No-Reference Banding Artifact Predictor", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Banding artifact, or false contouring, is a common video compression\nimpairment that tends to appear on large flat regions in encoded videos. These\nstaircase-shaped color bands can be very noticeable in high-definition videos.\nHere we study this artifact, and propose a new distortion-specific no-reference\nvideo quality model for predicting banding artifacts, called the Blind BANding\nDetector (BBAND index). BBAND is inspired by human visual models. The proposed\ndetector can generate a pixel-wise banding visibility map and output a banding\nseverity score at both the frame and video levels. Experimental results show\nthat our proposed method outperforms state-of-the-art banding detection\nalgorithms and delivers better consistency with subjective evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:05:26 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Lin", "Jessie", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2002.12275", "submitter": "Yilin Wang", "authors": "Joong Gon Yim, Yilin Wang, Neil Birkbeck, Balu Adsumilli", "title": "Subjective Quality Assessment for YouTube UGC Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the scale of social video sharing, User Generated Content (UGC) is\ngetting more attention from academia and industry. To facilitate\ncompression-related research on UGC, YouTube has released a large-scale\ndataset. The initial dataset only provided videos, limiting its use in quality\nassessment. We used a crowd-sourcing platform to collect subjective quality\nscores for this dataset. We analyzed the distribution of Mean Opinion Score\n(MOS) in various dimensions, and investigated some fundamental questions in\nvideo quality assessment, like the correlation between full video MOS and\ncorresponding chunk MOS, and the influence of chunk variation in quality score\naggregation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 17:34:34 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Yim", "Joong Gon", ""], ["Wang", "Yilin", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""]]}, {"id": "2002.12521", "submitter": "Licheng Xiao", "authors": "Licheng Xiao, Hairong Wang, Nam Ling", "title": "Improved Image Coding Autoencoder With Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build autoencoder based pipelines for extreme end-to-end\nimage compression based on Ball\\'e's approach, which is the state-of-the-art\nopen source implementation in image compression using deep learning. We\ndeepened the network by adding one more hidden layer before each strided\nconvolutional layer with exactly the same number of down-samplings and\nup-samplings. Our approach outperformed Ball\\'e's approach, and achieved around\n4.0% reduction in bits per pixel (bpp), 0.03% increase in multi-scale\nstructural similarity (MS-SSIM), and only 0.47% decrease in peak\nsignal-to-noise ratio (PSNR), It also outperforms all traditional image\ncompression methods including JPEG2000 and HEIC by at least 20% in terms of\ncompression efficiency at similar reconstruction image quality. Regarding\nencoding and decoding time, our approach takes similar amount of time compared\nwith traditional methods with the support of GPU, which means it's almost ready\nfor industrial applications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 03:21:47 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Xiao", "Licheng", ""], ["Wang", "Hairong", ""], ["Ling", "Nam", ""]]}]