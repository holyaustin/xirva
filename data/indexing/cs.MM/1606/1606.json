[{"id": "1606.00264", "submitter": "Christian Timmerer", "authors": "Christian Timmerer and Alan Bertoni", "title": "Advanced Transport Options for the Dynamic Adaptive Streaming over HTTP", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia streaming over HTTP is no longer a niche research topic as it has\nentered our daily live. The common assumption is that it is deployed on top of\nthe existing infrastructure utilizing application (HTTP) and transport (TCP)\nlayer protocols as is. Interestingly, standards like MPEG's Dynamic Adaptive\nStreaming over HTTP (DASH) do not mandate the usage of any specific transport\nprotocol allowing for sufficient deployment flexibility which is further\nsupported by emerging developments within both protocol layers. This paper\ninvestigates and evaluates the usage of advanced transport options for the\ndynamic adaptive streaming over HTTP. We utilize a common test setup to\nevaluate HTTP/2.0 and Google's Quick UDP Internet Connections (QUIC) protocol\nin the context of DASH-based services.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:49:09 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Timmerer", "Christian", ""], ["Bertoni", "Alan", ""]]}, {"id": "1606.00341", "submitter": "Christian Timmerer", "authors": "Christian Timmerer, Matteo Maiero, and Benjamin Rainer", "title": "Which Adaptation Logic? An Objective and Subjective Performance\n  Evaluation of HTTP-based Adaptive Media Streaming Systems", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia content delivery over the Internet is predominantly using the\nHypertext Transfer Protocol (HTTP) as its primary protocol and multiple\nproprietary solutions exits. The MPEG standard Dynamic Adaptive Streaming over\nHTTP (DASH) provides an interoperable solution and in recent years various\nadaptation logics/algorithms have been proposed. However, to the best of our\nknowledge, there is no comprehensive evaluation of the various\nlogics/algorithms. Therefore, this paper provides a comprehensive evaluation of\nten different adaptation logics/algorithms, which have been proposed in the\npast years. The evaluation is done both objectively and subjectively. The\nformer is using a predefined bandwidth trajectory within a controlled\nenvironment and the latter is done in a real-world environment adopting\ncrowdsourcing. The results shall provide insights about which strategy can be\nadopted in actual deployment scenarios. Additionally, the evaluation\nmethodology described in this paper can be used to evaluate any other/new\nadaptation logic and to compare it directly with the results reported here.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 16:05:14 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Timmerer", "Christian", ""], ["Maiero", "Matteo", ""], ["Rainer", "Benjamin", ""]]}, {"id": "1606.01021", "submitter": "Mario Taschwer", "authors": "Mario Taschwer and Oge Marques", "title": "Automatic Separation of Compound Figures in Scientific Articles", "comments": "accepted for Multimedia Tools and Applications with minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based analysis and retrieval of digital images found in scientific\narticles is often hindered by images consisting of multiple subfigures\n(compound figures). We address this problem by proposing a method to\nautomatically classify and separate compound figures, which consists of two\nmain steps: (i) a supervised compound figure classifier (CFC) discriminates\nbetween compound and non-compound figures using task-specific image features;\nand (ii) an image processing algorithm is applied to predicted compound images\nto perform compound figure separation (CFS). Our CFC approach is shown to\nachieve state-of-the-art classification performance on a published dataset. Our\nCFS algorithm shows superior separation accuracy on two different datasets\ncompared to other known automatic approaches. Finally, we propose a method to\nevaluate the effectiveness of the CFC-CFS process chain and use it to optimize\nthe misclassification loss of CFC for maximal effectiveness in the process\nchain.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 09:53:01 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 06:26:58 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Taschwer", "Mario", ""], ["Marques", "Oge", ""]]}, {"id": "1606.01621", "submitter": "Shu Kong", "authors": "Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, Charless Fowlkes", "title": "Photo Aesthetics Ranking Network with Attributes and Content Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications could benefit from the ability to automatically\ngenerate a fine-grained ranking of photo aesthetics. However, previous methods\nfor image aesthetics analysis have primarily focused on the coarse, binary\ncategorization of images into high- or low-aesthetic categories. In this work,\nwe propose to learn a deep convolutional neural network to rank photo\naesthetics in which the relative ranking of photo aesthetics are directly\nmodeled in the loss function. Our model incorporates joint learning of\nmeaningful photographic attributes and image content information which can help\nregularize the complicated photo aesthetics rating problem.\n  To train and analyze this model, we have assembled a new aesthetics and\nattributes database (AADB) which contains aesthetic scores and meaningful\nattributes assigned to each image by multiple human raters. Anonymized rater\nidentities are recorded across images allowing us to exploit intra-rater\nconsistency using a novel sampling strategy when computing the ranking loss of\ntraining image pairs. We show the proposed sampling strategy is very effective\nand robust in face of subjective judgement of image aesthetics by individuals\nwith different aesthetic tastes. Experiments demonstrate that our unified model\ncan generate aesthetic rankings that are more consistent with human ratings. To\nfurther validate our model, we show that by simply thresholding the estimated\naesthetic scores, we are able to achieve state-or-the-art classification\nperformance on the existing AVA dataset benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 06:14:00 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 00:20:07 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kong", "Shu", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Mech", "Radomir", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1606.02096", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, and George Fazekas, Mark Sandler", "title": "Towards Playlist Generation Algorithms Using RNNs Trained on\n  Within-Track Transitions", "comments": "4 pages, 2 figures, accepted to Workshop on Surprise, Opposition, and\n  Obstruction in Adaptive and Personalized Systems (SOAP) 2016, Halifax, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel playlist generation algorithm that focuses on the\nquality of transitions using a recurrent neural network (RNN). The proposed\nmodel assumes that optimal transitions between tracks can be modelled and\npredicted by internal transitions within music tracks. We introduce modelling\nsequences of high-level music descriptors using RNNs and discuss an experiment\ninvolving different similarity functions, where the sequences are provided by a\nmusical structural analysis algorithm. Qualitative observations show that the\nproposed approach can effectively model transitions of music tracks in\nplaylists.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 11:07:56 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""]]}, {"id": "1606.02276", "submitter": "Mercan Topkara", "authors": "Nikolaos Pappas, Miriam Redi, Mercan Topkara, Brendan Jou, Hongyi Liu,\n  Tao Chen, Shih-Fu Chang", "title": "Multilingual Visual Sentiment Concept Matching", "comments": null, "journal-ref": "Proceedings ICMR '16 Proceedings of the 2016 ACM on International\n  Conference on Multimedia Retrieval Pages 151-158", "doi": "10.1145/2911996.2912016", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of culture in visual emotion perception has recently captured the\nattention of multimedia research. In this study, we pro- vide powerful\ncomputational linguistics tools to explore, retrieve and browse a dataset of\n16K multilingual affective visual concepts and 7.3M Flickr images. First, we\ndesign an effective crowdsourc- ing experiment to collect human judgements of\nsentiment connected to the visual concepts. We then use word embeddings to\nrepre- sent these concepts in a low dimensional vector space, allowing us to\nexpand the meaning around concepts, and thus enabling insight about\ncommonalities and differences among different languages. We compare a variety\nof concept representations through a novel evaluation task based on the notion\nof visual semantic relatedness. Based on these representations, we design\nclustering schemes to group multilingual visual concepts, and evaluate them\nwith novel metrics based on the crowdsourced sentiment annotations as well as\nvisual semantic relatedness. The proposed clustering framework enables us to\nanalyze the full multilingual dataset in-depth and also show an application on\na facial data subset, exploring cultural in- sights of portrait-related\naffective visual concepts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:40:00 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Jou", "Brendan", ""], ["Liu", "Hongyi", ""], ["Chen", "Tao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1606.02312", "submitter": "James Collins", "authors": "James Collins, Sos Agaian", "title": "High Capacity Image Steganography using Adjunctive Numerical\n  Representations with Multiple Bit-Plane Decomposition Methods", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSB steganography is a one of the most widely used methods for implementing\ncovert data channels in image file exchanges [1][2]. The low computational\ncomplexity and implementation simplicity of the algorithm are significant\nfactors for its popularity with the primary reason being low image distortion.\nMany attempts have been made to increase the embedding capacity of LSB\nalgorithms by expanding into the second or third binary layers of the image\nwhile maintaining a low probability of detection with minimal distortive\neffects [2][3][4]. In this paper, we introduce an advanced technique for\ncovertly embedding data within images using redundant number system\ndecomposition over non-standard digital bit planes. Both grayscale and\nbit-mapped images are equally effective as cover files. It will be shown that\nthis unique steganography method has minimal visual distortive affects while\nalso preserving the cover file statistics, making it less susceptible to most\ngeneral steganography detection algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 20:00:41 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Collins", "James", ""], ["Agaian", "Sos", ""]]}, {"id": "1606.02424", "submitter": "Yassine Hachaichi", "authors": "Imen Ben Saad, Younes Lahbib, Yassine Hacha\\\"ichi (LAMSIN), Sonia\n  Mami, Abdelkader Mami", "title": "Generic-Precision algorithm for DCT-Cordic architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DM cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a generic algorithm to calculate the rotation\nparameters of CORDIC angles required for the Discrete Cosine Transform\nalgorithm (DCT). This leads us to increase the precision of calculation meeting\nany accuracy.Our contribution is to use this decomposition in CORDIC based DCT\nwhich is appropriate for domains which require high quality and top precision.\nWe then propose a hardware implementation of the novel transformation, and as\nexpected, a substantial improvement in PSNR quality is found.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 07:08:10 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Saad", "Imen Ben", "", "LAMSIN"], ["Lahbib", "Younes", "", "LAMSIN"], ["Hacha\u00efchi", "Yassine", "", "LAMSIN"], ["Mami", "Sonia", ""], ["Mami", "Abdelkader", ""]]}, {"id": "1606.02546", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler", "title": "Estimation of solar irradiance using ground-based whole sky imagers", "comments": "Accepted in Proc. IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS), July 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-based whole sky imagers (WSIs) can provide localized images of the sky\nof high temporal and spatial resolution, which permits fine-grained cloud\nobservation. In this paper, we show how images taken by WSIs can be used to\nestimate solar radiation. Sky cameras are useful here because they provide\nadditional information about cloud movement and coverage, which are otherwise\nnot available from weather station data. Our setup includes ground-based\nweather stations at the same location as the imagers. We use their measurements\nto validate our methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 13:21:30 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 09:38:37 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1606.02816", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Benjamin Elizalde, Bhiksha Raj", "title": "Audio Content based Geotagging in Multimedia", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose methods to extract geographically relevant\ninformation in a multimedia recording using its audio. Our method primarily is\nbased on the fact that urban acoustic environment consists of a variety of\nsounds. Hence, location information can be inferred from the composition of\nsound events/classes present in the audio. More specifically, we adopt matrix\nfactorization techniques to obtain semantic content of recording in terms of\ndifferent sound classes. These semantic information are then combined to\nidentify the location of recording.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 04:01:36 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 02:45:44 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Kumar", "Anurag", ""], ["Elizalde", "Benjamin", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1606.03333", "submitter": "Mortaza Doulaty", "authors": "Mortaza Doulaty, Oscar Saz, Raymond W. M. Ng, Thomas Hain", "title": "Automatic Genre and Show Identification of Broadcast Media", "comments": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge amounts of digital videos are being produced and broadcast every day,\nleading to giant media archives. Effective techniques are needed to make such\ndata accessible further. Automatic meta-data labelling of broadcast media is an\nessential task for multimedia indexing, where it is standard to use multi-modal\ninput for such purposes. This paper describes a novel method for automatic\ndetection of media genre and show identities using acoustic features, textual\nfeatures or a combination thereof. Furthermore the inclusion of available\nmeta-data, such as time of broadcast, is shown to lead to very high\nperformance. Latent Dirichlet Allocation is used to model both acoustics and\ntext, yielding fixed dimensional representations of media recordings that can\nthen be used in Support Vector Machines based classification. Experiments are\nconducted on more than 1200 hours of TV broadcasts from the British\nBroadcasting Corporation (BBC), where the task is to categorise the broadcasts\ninto 8 genres or 133 show identities. On a 200-hour test set, accuracies of\n98.6% and 85.7% were achieved for genre and show identification respectively,\nusing a combination of acoustic and textual features with meta-data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 14:09:32 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Doulaty", "Mortaza", ""], ["Saz", "Oscar", ""], ["Ng", "Raymond W. M.", ""], ["Hain", "Thomas", ""]]}, {"id": "1606.03664", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Weakly Supervised Scalable Audio Content Analysis", "comments": "ICME 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Event Detection is an important task for content analysis of multimedia\ndata. Most of the current works on detection of audio events is driven through\nsupervised learning approaches. We propose a weakly supervised learning\nframework which can make use of the tremendous amount of web multimedia data\nwith significantly reduced annotation effort and expense. Specifically, we use\nseveral multiple instance learning algorithms to show that audio event\ndetection through weak labels is feasible. We also propose a novel scalable\nmultiple instance learning algorithm and show that its competitive with other\nmultiple instance learning algorithms for audio event detection tasks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 04:07:45 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1606.04195", "submitter": "Zhi Wang Dr.", "authors": "Zhi Wang, Lifeng Sun, Miao Zhang, Haitian Pang, Erfang Tian, Wenwu Zhu", "title": "Social- and Mobility-Aware Device-to-Device Content Delivery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile online social network services have seen a rapid increase, in which\nthe huge amount of user-generated social media contents propagating between\nusers via social connections has significantly challenged the traditional\ncontent delivery paradigm: First, replicating all of the contents generated by\nusers to edge servers that well \"fit\" the receivers becomes difficult due to\nthe limited bandwidth and storage capacities. Motivated by device-to-device\n(D2D) communication that allows users with smart devices to transfer content\ndirectly, we propose replicating bandwidth-intensive social contents in a\ndevice-to-device manner. Based on large-scale measurement studies on social\ncontent propagation and user mobility patterns in edge-network regions, we\nobserve that (1) Device-to-device replication can significantly help users\ndownload social contents from nearby neighboring peers; (2) Both social\npropagation and mobility patterns affect how contents should be replicated; (3)\nThe replication strategies depend on regional characteristics ({\\em e.g.}, how\nusers move across regions).\n  Using these measurement insights, we propose a joint \\emph{propagation- and\nmobility-aware} content replication strategy for edge-network regions, in which\nsocial contents are assigned to users in edge-network regions according to a\njoint consideration of social graph, content propagation and user mobility. We\nformulate the replication scheduling as an optimization problem and design\ndistributed algorithm only using historical, local and partial information to\nsolve it. Trace-driven experiments further verify the superiority of our\nproposal: compared with conventional pure movement-based and popularity-based\napproach, our design can significantly ($2-4$ times) improve the amount of\nsocial contents successfully delivered by device-to-device replication.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 02:52:45 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Wang", "Zhi", ""], ["Sun", "Lifeng", ""], ["Zhang", "Miao", ""], ["Pang", "Haitian", ""], ["Tian", "Erfang", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1606.04631", "submitter": "Fumin Shen Dr.", "authors": "Yi Bin, Yang Yang, Zi Huang, Fumin Shen, Xing Xu, Heng Tao Shen", "title": "Bidirectional Long-Short Term Memory for Video Description", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning has been attracting broad research attention in multimedia\ncommunity. However, most existing approaches either ignore temporal information\namong video frames or just employ local contextual temporal knowledge. In this\nwork, we propose a novel video captioning framework, termed as\n\\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures\nbidirectional global temporal structure in video. Specifically, we first devise\na joint visual modelling approach to encode video data by combining a forward\nLSTM pass, a backward LSTM pass, together with visual features from\nConvolutional Neural Networks (CNNs). Then, we inject the derived video\nrepresentation into the subsequent language model for initialization. The\nbenefits are in two folds: 1) comprehensively preserving sequential and visual\ninformation; and 2) adaptively learning dense visual features and sparse\nsemantic representations for videos and sentences, respectively. We verify the\neffectiveness of our proposed video captioning framework on a commonly-used\nbenchmark, i.e., Microsoft Video Description (MSVD) corpus, and the\nexperimental results demonstrate that the superiority of the proposed approach\nas compared to several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 03:26:53 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Bin", "Yi", ""], ["Yang", "Yang", ""], ["Huang", "Zi", ""], ["Shen", "Fumin", ""], ["Xu", "Xing", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1606.05294", "submitter": "Hanzhou Wu", "authors": "Han-Zhou Wu, Hong-Xia Wang and Yun-Qing Shi", "title": "Can Machine Learn Steganography? - Implementing LSB Substitution and\n  Matrix Coding Steganography with Feed-Forward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, due to the powerful abilities to deal with highly complex\ntasks, the artificial neural networks (ANNs) have been studied in the hope of\nachieving human-like performance in many applications. Since the ANNs have the\nability to approximate complex functions from observations, it is\nstraightforward to consider the ANNs for steganography. In this paper, we aim\nto implement the well-known LSB substitution and matrix coding steganography\nwith the feed-forward neural networks (FNNs). Our experimental results have\nshown that, the used FNNs can achieve the data embedding operation of the LSB\nsubstitution and matrix coding steganography. For steganography with the ANNs,\nthough there may be some challenges to us, it would be very promising and\nvaluable to pay attention to the ANNs for steganography, which may be a new\ndirection for steganography.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:52:59 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Wu", "Han-Zhou", ""], ["Wang", "Hong-Xia", ""], ["Shi", "Yun-Qing", ""]]}, {"id": "1606.05705", "submitter": "Shoou-I Yu", "authors": "Shoou-I Yu, Yi Yang, Zhongwen Xu, Shicheng Xu, Deyu Meng, Zexi Mao,\n  Zhigang Ma, Ming Lin, Xuanchong Li, Huan Li, Zhenzhong Lan, Lu Jiang,\n  Alexander G. Hauptmann, Chuang Gan, Xingzhong Du, Xiaojun Chang", "title": "Strategies for Searching Video Content with Text Queries or Video\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large number of user-generated videos uploaded on to the Internet\neveryday has led to many commercial video search engines, which mainly rely on\ntext metadata for search. However, metadata is often lacking for user-generated\nvideos, thus these videos are unsearchable by current search engines.\nTherefore, content-based video retrieval (CBVR) tackles this metadata-scarcity\nproblem by directly analyzing the visual and audio streams of each video. CBVR\nencompasses multiple research topics, including low-level feature design,\nfeature fusion, semantic detector training and video search/reranking. We\npresent novel strategies in these topics to enhance CBVR in both accuracy and\nspeed under different query inputs, including pure textual queries and query by\nvideo examples. Our proposed strategies have been incorporated into our\nsubmission for the TRECVID 2014 Multimedia Event Detection evaluation, where\nour system outperformed other submissions in both text queries and video\nexample queries, thus demonstrating the effectiveness of our proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:27:06 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Yu", "Shoou-I", ""], ["Yang", "Yi", ""], ["Xu", "Zhongwen", ""], ["Xu", "Shicheng", ""], ["Meng", "Deyu", ""], ["Mao", "Zexi", ""], ["Ma", "Zhigang", ""], ["Lin", "Ming", ""], ["Li", "Xuanchong", ""], ["Li", "Huan", ""], ["Lan", "Zhenzhong", ""], ["Jiang", "Lu", ""], ["Hauptmann", "Alexander G.", ""], ["Gan", "Chuang", ""], ["Du", "Xingzhong", ""], ["Chang", "Xiaojun", ""]]}, {"id": "1606.06152", "submitter": "Hossein Ziaei Nafchi", "authors": "Hossein Ziaei Nafchi and Mohamed Cheriet", "title": "A Note on Efficiency of Downsampling and Color Transformation in Image\n  Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several existing and successful full reference image quality assessment (IQA)\nmodels use linear color transformation and downsampling before measuring\nsimilarity or quality of images. This paper indicates to the right order of\nthese two procedures and that the existing models have not chosen the more\nefficient approach. In addition, efficiency of these metrics is not compared in\na fair basis in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:52:47 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Nafchi", "Hossein Ziaei", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1606.06197", "submitter": "Oiver Weede Prof. Dr.-Ing.", "authors": "Oliver Weede", "title": "Polymetric Rhythmic Feel for a Cognitive Drum Computer", "comments": null, "journal-ref": "Proc. 14th Int Conf on Culture and Computer Science, Berlin,\n  Germany, May 26-27, 2016, pp. 281-295", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a question about music cognition: how do we derive\npolymetric structures. A preference rule system is presented which is\nimplemented into a drum computer. The preference rule system allows inferring\nlocal polymetric structures, like two-over-three and three-over-two. By\nanalyzing the micro-timing of West African percussion music a timing pattern\nconsisting of six pulses was discovered. It integrates binary and ternary\nrhythmic feels. The presented drum computer integrates the discovered\nsuperimposed polymetric swing (timing and velocity) appropriate to the rhythmic\nsequence the user inputs. For binary sequences, the amount of binary swing is\nincreased and for ternary sequences, the ternary swing is increased.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 16:27:12 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 11:43:11 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Weede", "Oliver", ""]]}, {"id": "1606.06259", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Rowan Zellers, Eli Pincus, Louis-Philippe Morency", "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis\n  in Online Opinion Videos", "comments": "Accepted as Journal Publication in IEEE Intelligent Systems", "journal-ref": "IEEE Intelligent Systems 31.6 (2016): 82-88", "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are sharing their opinions, stories and reviews through online video\nsharing websites every day. Studying sentiment and subjectivity in these\nopinion videos is experiencing a growing attention from academia and industry.\nWhile sentiment analysis has been successful for text, it is an understudied\nresearch question for videos and multimedia content. The biggest setbacks for\nstudies in this direction are lack of a proper dataset, methodology, baselines\nand statistical analysis of how information from different modality sources\nrelate to each other. This paper introduces to the scientific community the\nfirst opinion-level annotated corpus of sentiment and subjectivity analysis in\nonline videos called Multimodal Opinion-level Sentiment Intensity dataset\n(MOSI). The dataset is rigorously annotated with labels for subjectivity,\nsentiment intensity, per-frame and per-opinion annotated visual features, and\nper-milliseconds annotated audio features. Furthermore, we present baselines\nfor future studies in this direction as well as a new multimodal fusion\napproach that jointly models spoken words and visual gestures.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 19:23:53 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 02:39:40 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zadeh", "Amir", ""], ["Zellers", "Rowan", ""], ["Pincus", "Eli", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1606.06873", "submitter": "Sharath Chandra Guntuku", "authors": "Sharath Chandra Guntuku, Michael James Scott, Gheorghita Ghinea, Weisi\n  Lin", "title": "Personality, Culture, and System Factors - Impact on Affective Response\n  to Multimedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst affective responses to various forms and genres of multimedia content\nhave been well researched, precious few studies have investigated the combined\nimpact that multimedia system parameters and human factors have on affect.\nConsequently, in this paper we explore the role that two primordial dimensions\nof human factors - personality and culture - in conjunction with system factors\n- frame rate, resolution, and bit rate - have on user affect and enjoyment of\nmultimedia presentations. To this end, a two-site, cross-cultural study was\nundertaken, the results of which produced three predictve models. Personality\nand Culture traits were shown statistically to represent 5.6% of the variance\nin positive affect, 13.6% in negative affect and 9.3% in enjoyment. The\ncorrelation between affect and enjoyment, was significant. Predictive modeling\nincorporating human factors showed about 8%, 7% and 9% improvement in\npredicting positive affect, negative affect and enjoyment respectively when\ncompared to models trained only on system factors. Results and analysis\nindicate the significant role played by human factors in influencing affect\nthat users experience while watching multimedia.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 10:02:39 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 08:45:14 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Guntuku", "Sharath Chandra", ""], ["Scott", "Michael James", ""], ["Ghinea", "Gheorghita", ""], ["Lin", "Weisi", ""]]}, {"id": "1606.07414", "submitter": "Renato J Cintra", "authors": "T. L. T. Silveira, R. S. Oliveira, F. M. Bayer, R. J. Cintra, A.\n  Madanayake", "title": "Multiplierless 16-point DCT Approximation for Low-complexity Image and\n  Video Coding", "comments": "12 pages, 5 figures, 3 tables", "journal-ref": null, "doi": "10.1007/s11760-016-0923-4", "report-no": null, "categories": "cs.CV cs.MM cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orthogonal 16-point approximate discrete cosine transform (DCT) is\nintroduced. The proposed transform requires neither multiplications nor\nbit-shifting operations. A fast algorithm based on matrix factorization is\nintroduced, requiring only 44 additions---the lowest arithmetic cost in\nliterature. To assess the introduced transform, computational complexity,\nsimilarity with the exact DCT, and coding performance measures are computed.\nClassical and state-of-the-art 16-point low-complexity transforms were used in\na comparative analysis. In the context of image compression, the proposed\napproximation was evaluated via PSNR and SSIM measurements, attaining the best\ncost-benefit ratio among the competitors. For video encoding, the proposed\napproximation was embedded into a HEVC reference software for direct comparison\nwith the original HEVC standard. Physically realized and tested using FPGA\nhardware, the proposed transform showed 35% and 37% improvements of area-time\nand area-time-squared VLSI metrics when compared to the best competing\ntransform in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 19:26:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Silveira", "T. L. T.", ""], ["Oliveira", "R. S.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Madanayake", "A.", ""]]}, {"id": "1606.07583", "submitter": "Biljana Risteska Stojkoska Dr", "authors": "Biljana Stojkoska, Danco Davcev and Vladimir Trajkovik", "title": "N-queens-based algorithm for moving object detection in distributed\n  wireless sensor networks", "comments": "6 pages", "journal-ref": "Proceedings of the ITI 2008 30th Int. Conf. on Information\n  Technology Interfaces, June 23-26, 2008, Cavtat, Croatia, pp.899-904", "doi": "10.1109/ITI.2008.4588530", "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main constraint of wireless sensor networks (WSN) in enabling wireless\nimage communication is the high energy requirement, which may exceed even the\nfuture capabilities of battery technologies. In this paper we have shown that\nthis bottleneck can be overcome by developing local in-network image processing\nalgorithm that offers optimal energy consumption. Our algorithm is very\nsuitable for intruder detection applications. Each node is responsible for\nprocessing the image captured by the video sensor, which consists of NxN\nblocks. If an intruder is detected in the monitoring region, the node will\ntransmit the image for further processing. Otherwise, the node takes no action.\nResults provided from our experiments show that our algorithm is better than\nthe traditional moving object detection techniques by a factor of (N/2) in\nterms of energy savings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 07:18:42 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Stojkoska", "Biljana", ""], ["Davcev", "Danco", ""], ["Trajkovik", "Vladimir", ""]]}, {"id": "1606.07908", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "Label Tree Embeddings for Acoustic Scene Classification", "comments": "to appear in the Proceedings of ACM Multimedia 2016 (ACMMM 2016)", "journal-ref": null, "doi": "10.1145/2964284.2967268", "report-no": null, "categories": "cs.MM cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper an efficient approach for acoustic scene\nclassification by exploring the structure of class labels. Given a set of class\nlabels, a category taxonomy is automatically learned by collectively optimizing\na clustering of the labels into multiple meta-classes in a tree structure. An\nacoustic scene instance is then embedded into a low-dimensional feature\nrepresentation which consists of the likelihoods that it belongs to the\nmeta-classes. We demonstrate state-of-the-art results on two different datasets\nfor the acoustic scene classification task, including the DCASE 2013 and LITIS\nRouen datasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 12:57:44 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 11:42:20 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1606.07921", "submitter": "Gonzalo Vaca-Castano", "authors": "Gonzalo Vaca-Castano", "title": "Finding the Topic of a Set of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the problem of determining the topic that a set of\nimages is describing, where every topic is represented as a set of words.\nDifferent from other problems like tag assignment or similar, a) we assume\nmultiple images are used as input instead of single image, b) Input images are\ntypically not visually related, c) Input images are not necessarily\nsemantically close, and d) Output word space is unconstrained. In our proposed\nsolution, visual information of each query image is used to retrieve similar\nimages with text labels (tags) from an image database. We consider a scenario\nwhere the tags are very noisy and diverse, given that they were obtained by\nimplicit crowd-sourcing in a database of 1 million images and over seventy\nseven thousand tags. The words or tags associated to each query are processed\njointly in a word selection algorithm using random walks that allows to refine\nthe search topic, rejecting words that are not part of the topic and produce a\nset of words that fairly describe the topic. Experiments on a dataset of 300\ntopics, with up to twenty images per topic, show that our algorithm performs\nbetter than the proposed baseline for any number of query images. We also\npresent a new Conditional Random Field (CRF) word mapping algorithm that\npreserves the semantic similarity of the mapped words, increasing the\nperformance of the results over the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 15:06:27 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Vaca-Castano", "Gonzalo", ""]]}, {"id": "1606.08955", "submitter": "Vinay Bettadapura", "authors": "Vinay Bettadapura, Caroline Pantofaru, Irfan Essa", "title": "Leveraging Contextual Cues for Generating Basketball Highlights", "comments": "Proceedings of ACM Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive growth of sports videos has resulted in a need for automatic\ngeneration of sports highlights that are comparable in quality to the\nhand-edited highlights produced by broadcasters such as ESPN. Unlike previous\nworks that mostly use audio-visual cues derived from the video, we propose an\napproach that additionally leverages contextual cues derived from the\nenvironment that the game is being played in. The contextual cues provide\ninformation about the excitement levels in the game, which can be ranked and\nselected to automatically produce high-quality basketball highlights. We\nintroduce a new dataset of 25 NCAA games along with their play-by-play stats\nand the ground-truth excitement data for each basket. We explore the\ninformativeness of five different cues derived from the video and from the\nenvironment through user studies. Our experiments show that for our study\nparticipants, the highlights produced by our system are comparable to the ones\nproduced by ESPN for the same games.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 05:04:27 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Bettadapura", "Vinay", ""], ["Pantofaru", "Caroline", ""], ["Essa", "Irfan", ""]]}, {"id": "1606.08999", "submitter": "Yin-Hsi Kuo", "authors": "Yin-Hsi Kuo and Winston H. Hsu", "title": "De-Hashing: Server-Side Context-Aware Feature Reconstruction for Mobile\n  Visual Search", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the prevalence of mobile devices, mobile search becomes a more\nconvenient way than desktop search. Different from the traditional desktop\nsearch, mobile visual search needs more consideration for the limited resources\non mobile devices (e.g., bandwidth, computing power, and memory consumption).\nThe state-of-the-art approaches show that bag-of-words (BoW) model is robust\nfor image and video retrieval; however, the large vocabulary tree might not be\nable to be loaded on the mobile device. We observe that recent works mainly\nfocus on designing compact feature representations on mobile devices for\nbandwidth-limited network (e.g., 3G) and directly adopt feature matching on\nremote servers (cloud). However, the compact (binary) representation might fail\nto retrieve target objects (images, videos). Based on the hashed binary codes,\nwe propose a de-hashing process that reconstructs BoW by leveraging the\ncomputing power of remote servers. To mitigate the information loss from binary\ncodes, we further utilize contextual information (e.g., GPS) to reconstruct a\ncontext-aware BoW for better retrieval results. Experiment results show that\nthe proposed method can achieve competitive retrieval accuracy as BoW while\nonly transmitting few bits from mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 08:33:28 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Kuo", "Yin-Hsi", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1606.09047", "submitter": "Li Su", "authors": "Li Su and Hau-tieng Wu", "title": "Minimum-latency Time-frequency Analysis Using Asymmetric Window\n  Functions", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the real-time dynamics retrieval from a time series via the\ntime-frequency (TF) analysis with the minimal latency guarantee. While\ndifferent from the well-known intrinsic latency definition in the filter\ndesign, a rigorous definition of intrinsic latency for different time-frequency\nrepresentations (TFR) is provided, including the short time Fourier transform\n(STFT), synchrosqeezing transform (SST) and reassignment method (RM). To\nachieve the minimal latency, a systematic method is proposed to construct an\nasymmetric window from a well-designed symmetric one based on the concept of\nminimum-phase, if the window satisfies some weak conditions. We theoretically\nshow that the TFR determined by SST with the constructed asymmetric window does\nhave a smaller intrinsic latency. Finally, the music onset detection problem is\nstudied to show the strength of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 11:18:41 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Su", "Li", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1606.09264", "submitter": "Xingjie Wei", "authors": "Xingjie Wei and David Stillwell", "title": "How smart does your profile image look? Estimating intelligence from\n  social network profile images", "comments": null, "journal-ref": null, "doi": "10.1145/3018661.3018663", "report-no": null, "categories": "cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Profile images on social networks are users' opportunity to present\nthemselves and to affect how others judge them. We examine what Facebook images\nsay about users' perceived and measured intelligence. 1,122 Facebook users\ncompleted a matrices intelligence test and shared their current Facebook\nprofile image. Strangers also rated the images for perceived intelligence. We\nuse automatically extracted image features to predict both measured and\nperceived intelligence. Intelligence estimation from images is a difficult task\neven for humans, but experimental results show that human accuracy can be\nequalled using computing methods. We report the image features that predict\nboth measured and perceived intelligence, and highlight misleading features\nsuch as \"smiling\" and \"wearing glasses\" that are correlated with perceived but\nnot measured intelligence. Our results give insights into inaccurate\nstereotyping from profile images and also have implications for privacy,\nespecially since in most social networks profile images are public by default.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:03:55 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 18:01:13 GMT"}, {"version": "v3", "created": "Sun, 11 Dec 2016 21:16:17 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Wei", "Xingjie", ""], ["Stillwell", "David", ""]]}]