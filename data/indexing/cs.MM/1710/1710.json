[{"id": "1710.00421", "submitter": "Yitong Li", "authors": "Yitong Li, Martin Renqiang Min, Dinghan Shen, David Carlson, Lawrence\n  Carin", "title": "Video Generation From Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating videos from text has proven to be a significant challenge for\nexisting generative models. We tackle this problem by training a conditional\ngenerative model to extract both static and dynamic information from text. This\nis manifested in a hybrid framework, employing a Variational Autoencoder (VAE)\nand a Generative Adversarial Network (GAN). The static features, called \"gist,\"\nare used to sketch text-conditioned background color and object layout\nstructure. Dynamic features are considered by transforming input text into an\nimage filter. To obtain a large amount of data for training the deep-learning\nmodel, we develop a method to automatically create a matched text-video corpus\nfrom publicly available online videos. Experimental results show that the\nproposed framework generates plausible and diverse videos, while accurately\nreflecting the input text information. It significantly outperforms baseline\nmodels that directly adapt text-to-image generation procedures to produce\nvideos. Performance is evaluated both visually and by adapting the inception\nscore used to evaluate image generation in GANs.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 21:51:52 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Li", "Yitong", ""], ["Min", "Martin Renqiang", ""], ["Shen", "Dinghan", ""], ["Carlson", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1710.01230", "submitter": "Hamzeh Ghasemzadeh", "authors": "Hamzeh Ghasemzadeh", "title": "Multi-layer architecture for efficient steganalysis of Undermp3cover in\n  multi-encoder scenario", "comments": "9 pages, 7 tables, 6 figures, journal paper", "journal-ref": "IEEE Transactions on Information Forensics and Security, 2018", "doi": "10.1109/TIFS.2018.2847678", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mp3 is a very popular audio format and hence it can be a good host for\ncarrying hidden messages. Therefore, different steganography methods have been\nproposed for mp3 hosts. But, current literature has only focused on\nsteganalysis of mp3stego. In this paper we mention some of the limitations of\nmp3stego and argue that UnderMp3Cover (Ump3c) does not have those limitations.\nUmp3c makes subtle changes only to the global gain of bitstream and keeps the\nrest of bitstream intact. Therefore, its detection is much harder than\nmp3stego. To address this, joint distributions between global gain and other\nfields of mp3 bit stream are used. The changes are detected by measuring the\nmutual information from those joint distributions. Furthermore, we show that\ndifferent mp3 encoders have dissimilar performances. Consequently, a novel\nmulti-layer architecture for steganalysis of Ump3c is proposed. In this manner,\nthe first layer detects the encoder and the second layer performs the\nsteganalysis job. One of advantages of this architecture is that feature\nextraction and feature selection can be optimized for each encoder separately.\nWe show this multi-layer architecture outperforms the conventional single-layer\nmethods. Comparing results of the proposed method with other works shows an\nimprovement of 20.4% in the accuracy of steganalysis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 15:57:19 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 03:10:25 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ghasemzadeh", "Hamzeh", ""]]}, {"id": "1710.02459", "submitter": "Anatoliy Zabrovskiy", "authors": "Anatoliy Zabrovskiy, Evgeny Petrov, Evgeny Kuzmin, Christian Timmerer", "title": "Evaluation of the Performance of Adaptive HTTP Streaming Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive video streaming over HTTP is becoming omnipresent in our daily life.\nIn the past, dozens of research papers have proposed novel approaches to\naddress different aspects of adaptive streaming and a decent amount of player\nimplementations (commercial and open source) are available. However, state of\nthe art evaluations are sometimes superficial as many proposals only\ninvestigate a certain aspect of the problem or focus on a specific platform -\nplayer implementations used in actual services are rarely considered. HTML5 is\nnow available on many platforms and foster the deployment of adaptive media\nstreaming applications. We propose a common evaluation framework for adaptive\nHTML5 players and demonstrate its applicability by evaluating eight different\nplayers which are actually deployed in real-world services.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 15:47:10 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Zabrovskiy", "Anatoliy", ""], ["Petrov", "Evgeny", ""], ["Kuzmin", "Evgeny", ""], ["Timmerer", "Christian", ""]]}, {"id": "1710.03532", "submitter": "Yiting Shao", "authors": "Yiting Shao, Zhaobin Zhang, Zhu Li, Kui Fan, Ge Li", "title": "Attribute Compression of 3D Point Clouds Using Laplacian Sparsity\n  Optimized Graph Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D sensing and content capture have made significant progress in recent years\nand the MPEG standardization organization is launching a new project on\nimmersive media with point cloud compression (PCC) as one key corner stone. In\nthis work, we introduce a new binary tree based point cloud content partition\nand explore the graph signal processing tools, especially the graph transform\nwith optimized Laplacian sparsity, to achieve better energy compaction and\ncompression efficiency. The resulting rate-distortion operating points are\nconvex-hull optimized over the existing Lagrangian solutions. Simulation\nresults with the latest high quality point cloud content captured from the MPEG\nPCC demonstrated the transform efficiency and rate-distortion (R-D) optimal\npotential of the proposed solutions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 12:25:25 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Shao", "Yiting", ""], ["Zhang", "Zhaobin", ""], ["Li", "Zhu", ""], ["Fan", "Kui", ""], ["Li", "Ge", ""]]}, {"id": "1710.04837", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, and\n  Shaogang Gong", "title": "Recent Advances in Zero-shot Recognition", "comments": "accepted by IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent renaissance of deep convolution neural networks, encouraging\nbreakthroughs have been achieved on the supervised recognition tasks, where\neach class has sufficient training data and fully annotated training data.\nHowever, to scale the recognition to a large number of classes with few or now\ntraining samples for each class remains an unsolved problem. One approach to\nscaling up the recognition is to develop models capable of recognizing unseen\ncategories without any training instances, or zero-shot recognition/ learning.\nThis article provides a comprehensive review of existing zero-shot recognition\ntechniques covering various aspects ranging from representations of models, and\nfrom datasets and evaluation settings. We also overview related recognition\ntasks including one-shot and open set recognition which can be used as natural\nextensions of zero-shot recognition when limited number of class samples become\navailable or when zero-shot recognition is implemented in a real-world setting.\nImportantly, we highlight the limitations of existing approaches and point out\nfuture research directions in this existing new research area.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:29:29 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Fu", "Yanwei", ""], ["Xiang", "Tao", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""], ["Sigal", "Leonid", ""], ["Gong", "Shaogang", ""]]}, {"id": "1710.05106", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Jinwei Qi and Yuxin Yuan", "title": "CM-GANs: Cross-modal Generative Adversarial Networks for Common\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the inconsistent distribution and representation of\ndifferent modalities, such as image and text, cause the heterogeneity gap that\nmakes it challenging to correlate such heterogeneous data. Generative\nadversarial networks (GANs) have shown its strong ability of modeling data\ndistribution and learning discriminative representation, existing GANs-based\nworks mainly focus on generative problem to generate new data. We have\ndifferent goal, aim to correlate heterogeneous data, by utilizing the power of\nGANs to model cross-modal joint distribution. Thus, we propose Cross-modal GANs\nto learn discriminative common representation for bridging heterogeneity gap.\nThe main contributions are: (1) Cross-modal GANs architecture is proposed to\nmodel joint distribution over data of different modalities. The inter-modality\nand intra-modality correlation can be explored simultaneously in generative and\ndiscriminative models. Both of them beat each other to promote cross-modal\ncorrelation learning. (2) Cross-modal convolutional autoencoders with\nweight-sharing constraint are proposed to form generative model. They can not\nonly exploit cross-modal correlation for learning common representation, but\nalso preserve reconstruction information for capturing semantic consistency\nwithin each modality. (3) Cross-modal adversarial mechanism is proposed, which\nutilizes two kinds of discriminative models to simultaneously conduct\nintra-modality and inter-modality discrimination. They can mutually boost to\nmake common representation more discriminative by adversarial training process.\nTo the best of our knowledge, our proposed CM-GANs approach is the first to\nutilize GANs to perform cross-modal common representation learning. Experiments\nare conducted to verify the performance of our proposed approach on cross-modal\nretrieval paradigm, compared with 10 methods on 3 cross-modal datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 00:15:56 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 16:38:56 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Peng", "Yuxin", ""], ["Qi", "Jinwei", ""], ["Yuan", "Yuxin", ""]]}, {"id": "1710.05311", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Vector Quantization using the Improved Differential Evolution Algorithm\n  for Image Compression", "comments": "11 pages", "journal-ref": null, "doi": "10.17605/OSF.IO/M9RNZ", "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector Quantization, VQ is a popular image compression technique with a\nsimple decoding architecture and high compression ratio. Codebook designing is\nthe most essential part in Vector Quantization. LindeBuzoGray, LBG is a\ntraditional method of generation of VQ Codebook which results in lower PSNR\nvalue. A Codebook affects the quality of image compression, so the choice of an\nappropriate codebook is a must. Several optimization techniques have been\nproposed for global codebook generation to enhance the quality of image\ncompression. In this paper, a novel algorithm called IDE-LBG is proposed which\nuses Improved Differential Evolution Algorithm coupled with LBG for generating\noptimum VQ Codebooks. The proposed IDE works better than the traditional DE\nwith modifications in the scaling factor and the boundary control mechanism.\nThe IDE generates better solutions by efficient exploration and exploitation of\nthe search space. Then the best optimal solution obtained by the IDE is\nprovided as the initial Codebook for the LBG. This approach produces an\nefficient Codebook with less computational time and the consequences include\nexcellent PSNR values and superior quality reconstructed images. It is observed\nthat the proposed IDE-LBG find better VQ Codebooks as compared to IPSO-LBG,\nBA-LBG and FA-LBG.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 10:31:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "1710.05477", "submitter": "Bin Li", "authors": "Bin Li, Hu Luo, Haoxin Zhang, Shunquan Tan, Zhongzhou Ji", "title": "A multi-branch convolutional neural network for detecting double JPEG\n  compression", "comments": "This paper was accepted by the 3rd International Workshop on Digital\n  Crime and Forensics (IWDCF2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of double JPEG compression is important to forensics analysis. A\nfew methods were proposed based on convolutional neural networks (CNNs). These\nmethods only accept inputs from pre-processed data, such as histogram features\nand/or decompressed images. In this paper, we present a CNN solution by using\nraw DCT (discrete cosine transformation) coefficients from JPEG images as\ninput. Considering the DCT sub-band nature in JPEG, a multiple-branch CNN\nstructure has been designed to reveal whether a JPEG format image has been\ndoubly compressed. Comparing with previous methods, the proposed method\nprovides end-to-end detection capability. Extensive experiments have been\ncarried out to demonstrate the effectiveness of the proposed network.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 02:54:57 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Li", "Bin", ""], ["Luo", "Hu", ""], ["Zhang", "Haoxin", ""], ["Tan", "Shunquan", ""], ["Ji", "Zhongzhou", ""]]}, {"id": "1710.06582", "submitter": "Xiaoming Zhang", "authors": "Feiran Huang, Xiaoming Zhang, Zhoujun Li, Tao Mei, Yueying He,\n  Zhonghua Zhao", "title": "Learning Social Image Embedding with Deep Multimodal Attention Networks", "comments": null, "journal-ref": "Proceedings of Thematic Workshops of the 25th ACM Multimedia 2017", "doi": "10.1145/3126686.3126720", "report-no": null, "categories": "cs.MM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning social media data embedding by deep models has attracted extensive\nresearch interest as well as boomed a lot of applications, such as link\nprediction, classification, and cross-modal search. However, for social images\nwhich contain both link information and multimodal contents (e.g., text\ndescription, and visual content), simply employing the embedding learnt from\nnetwork structure or data content results in sub-optimal social image\nrepresentation. In this paper, we propose a novel social image embedding\napproach called Deep Multimodal Attention Networks (DMAN), which employs a deep\nmodel to jointly embed multimodal contents and link information. Specifically,\nto effectively capture the correlations between multimodal contents, we propose\na multimodal attention network to encode the fine-granularity relation between\nimage regions and textual words. To leverage the network structure for\nembedding learning, a novel Siamese-Triplet neural network is proposed to model\nthe links among images. With the joint deep model, the learnt embedding can\ncapture both the multimodal contents and the nonlinear network information.\nExtensive experiments are conducted to investigate the effectiveness of our\napproach in the applications of multi-label classification and cross-modal\nsearch. Compared to state-of-the-art image embeddings, our proposed DMAN\nachieves significant improvement in the tasks of multi-label classification and\ncross-modal search.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 04:28:20 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Huang", "Feiran", ""], ["Zhang", "Xiaoming", ""], ["Li", "Zhoujun", ""], ["Mei", "Tao", ""], ["He", "Yueying", ""], ["Zhao", "Zhonghua", ""]]}, {"id": "1710.09160", "submitter": "Huynh Van Luong", "authors": "Srivatsa Prativadibhayankaram, Huynh Van Luong, Thanh-Ha Le, Andre\n  Kaup", "title": "Compressive Online Robust Principal Component Analysis with Optical Flow\n  for Video Foreground-Background Separation", "comments": "preprint accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of online Robust Principle Component Analysis (RPCA) for the\nvideo foreground-background separation, we propose a compressive online RPCA\nwith optical flow that separates recursively a sequence of frames into sparse\n(foreground) and low-rank (background) components. Our method considers a small\nset of measurements taken per data vector (frame), which is different from\nconventional batch RPCA, processing all the data directly. The proposed method\nalso incorporates multiple prior information, namely previous foreground and\nbackground frames, to improve the separation and then updates the prior\ninformation for the next frame. Moreover, the foreground prior frames are\nimproved by estimating motions between the previous foreground frames using\noptical flow and compensating the motions to achieve higher quality foreground\nprior. The proposed method is applied to online video foreground and background\nseparation from compressive measurements. The visual and quantitative results\nshow that our method outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 10:39:47 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Prativadibhayankaram", "Srivatsa", ""], ["Van Luong", "Huynh", ""], ["Le", "Thanh-Ha", ""], ["Kaup", "Andre", ""]]}, {"id": "1710.09919", "submitter": "Lee Prangnell", "authors": "Lee Prangnell and Victor Sanchez", "title": "JND-Based Perceptual Video Coding for 4:4:4 Screen Content Data in HEVC", "comments": "Preprint: 2018 IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The JCT-VC standardized Screen Content Coding (SCC) extension in the HEVC HM\nRExt + SCM reference codec offers an impressive coding efficiency performance\nwhen compared with HM RExt alone; however, it is not significantly perceptually\noptimized. For instance, it does not include advanced HVS-based perceptual\ncoding methods, such as JND-based spatiotemporal masking schemes. In this\npaper, we propose a novel JND-based perceptual video coding technique for HM\nRExt + SCM. The proposed method is designed to further improve the compression\nperformance of HM RExt + SCM when applied to YCbCr 4:4:4 SC video data. In the\nproposed technique, luminance masking and chrominance masking are exploited to\nperceptually adjust the Quantization Step Size (QStep) at the Coding Block (CB)\nlevel. Compared with HM RExt 16.10 + SCM 8.0, the proposed method considerably\nreduces bitrates (Kbps), with a maximum reduction of 48.3%. In addition to\nthis, the subjective evaluations reveal that SC-PAQ achieves visually lossless\ncoding at very low bitrates.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 21:28:57 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 18:47:58 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Prangnell", "Lee", ""], ["Sanchez", "Victor", ""]]}, {"id": "1710.09975", "submitter": "Renato J Cintra", "authors": "A. Edirisuriya, A. Madanayake, R. J. Cintra, V. S. Dimitrov", "title": "A Single-Channel Architecture for Algebraic Integer Based 8$\\times$8 2-D\n  DCT Computation", "comments": "8 pages, 6 figures, 5 tables", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  volume 23, number 12, pages 2083-2089, Dec. 2013", "doi": "10.1109/TCSVT.2013.2270397", "report-no": null, "categories": "cs.AR cs.MM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An area efficient row-parallel architecture is proposed for the real-time\nimplementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine\ntransform (DCT) for image and video processing. The proposed architecture\ncomputes 8$\\times$8 2-D DCT transform based on the Arai DCT algorithm. An\nimproved fast algorithm for AI based 1-D DCT computation is proposed along with\na single channel 2-D DCT architecture. The design improves on the 4-channel AI\nDCT architecture that was published recently by reducing the number of integer\nchannels to one and the number of 8-point 1-D DCT cores from 5 down to 2. The\narchitecture offers exact computation of 8$\\times$8 blocks of the 2-D DCT\ncoefficients up to the FRS, which converts the coefficients from the AI\nrepresentation to fixed-point format using the method of expansion factors.\nPrototype circuits corresponding to FRS blocks based on two expansion factors\nare realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6\nXC6VLX240T device. Post place-and-route results show a 20% reduction in terms\nof area compared to the 2-D DCT architecture requiring five 1-D AI cores. The\narea-time and area-time${}^2$ complexity metrics are also reduced by 23% and\n22% respectively for designs with 8-bit input word length. The digital\nrealizations are simulated up to place and route for ASICs using 45 nm CMOS\nstandard cells. The maximum estimated clock rate is 951 MHz for the CMOS\nrealizations indicating 7.608$\\cdot$10$^9$ pixels/seconds and a 8$\\times$8\nblock rate of 118.875 MHz.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 03:32:48 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Edirisuriya", "A.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1710.09980", "submitter": "Yuhang Song", "authors": "Yuhang Song, Mai Xu, Shengxi Li", "title": "Watching Videos with Certain and Constant Quality: PID-based Quality\n  Control Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video coding, compressed videos with certain and constant quality can\nensure quality of experience (QoE). To this end, we propose in this paper a\nnovel PID-based quality control (PQC) method for video coding. Specifically, a\nformulation is modelled to control quality of video coding with two objectives:\nminimizing control error and quality fluctuation. Then, we apply the Laplace\ndomain analysis to model the relationship between quantization parameter (QP)\nand control error in this formulation. Given the relationship between QP and\ncontrol error, we propose a solution to the PQC formulation, such that videos\ncan be compressed at certain and constant quality. Finally, experimental\nresults show that our PQC method is effective in both control accuracy and\nquality fluctuation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 03:54:38 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Song", "Yuhang", ""], ["Xu", "Mai", ""], ["Li", "Shengxi", ""]]}, {"id": "1710.10451", "submitter": "Taejun Kim", "authors": "Taejun Kim, Jongpil Lee, Juhan Nam", "title": "Sample-level CNN Architectures for Music Auto-tagging Using Raw\n  Waveforms", "comments": "Accepted for publication at ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that the end-to-end approach using convolutional neural\nnetwork (CNN) is effective in various types of machine learning tasks. For\naudio signals, the approach takes raw waveforms as input using an 1-D\nconvolution layer. In this paper, we improve the 1-D CNN architecture for music\nauto-tagging by adopting building blocks from state-of-the-art image\nclassification models, ResNets and SENets, and adding multi-level feature\naggregation to it. We compare different combinations of the modules in building\nCNN architectures. The results show that they achieve significant improvements\nover previous state-of-the-art models on the MagnaTagATune dataset and\ncomparable results on Million Song Dataset. Furthermore, we analyze and\nvisualize our model to show how the 1-D CNN operates.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 11:55:50 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 04:39:50 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Kim", "Taejun", ""], ["Lee", "Jongpil", ""], ["Nam", "Juhan", ""]]}, {"id": "1710.11090", "submitter": "Haiqiang Wang", "authors": "Haiqiang Wang, Ioannis Katsavounidis, Qin Huang, Xin Zhou, and C.-C.\n  Jay Kuo", "title": "Prediction of Satisfied User Ratio for Compressed Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large-scale video quality dataset called the VideoSet has been constructed\nrecently to measure human subjective experience of H.264 coded video in terms\nof the just-noticeable-difference (JND). It measures the first three JND points\nof 5-second video of resolution 1080p, 720p, 540p and 360p. Based on the\nVideoSet, we propose a method to predict the satisfied-user-ratio (SUR) curves\nusing a machine learning framework. First, we partition a video clip into local\nspatial-temporal segments and evaluate the quality of each segment using the\nVMAF quality index. Then, we aggregate these local VMAF measures to derive a\nglobal one. Finally, the masking effect is incorporated and the support vector\nregression (SVR) is used to predict the SUR curves, from which the JND points\ncan be derived. Experimental results are given to demonstrate the performance\nof the proposed SUR prediction method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 17:37:29 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Wang", "Haiqiang", ""], ["Katsavounidis", "Ioannis", ""], ["Huang", "Qin", ""], ["Zhou", "Xin", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1710.11200", "submitter": "Renato J Cintra", "authors": "N. Rajapaksha, A. Madanayake, R. J. Cintra, J. Adikari, V. S. Dimitrov", "title": "VLSI Computational Architectures for the Arithmetic Cosine Transform", "comments": "8 pages, 2 figures, 6 tables", "journal-ref": "IEEE Transactions on Computers, vol. 64, no. 9, Sep 2015", "doi": "10.1109/TC.2014.2366732", "report-no": null, "categories": "cs.AR cs.DS cs.MM math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete cosine transform (DCT) is a widely-used and important signal\nprocessing tool employed in a plethora of applications. Typical fast algorithms\nfor nearly-exact computation of DCT require floating point arithmetic, are\nmultiplier intensive, and accumulate round-off errors. Recently proposed fast\nalgorithm arithmetic cosine transform (ACT) calculates the DCT exactly using\nonly additions and integer constant multiplications, with very low area\ncomplexity, for null mean input sequences. The ACT can also be computed\nnon-exactly for any input sequence, with low area complexity and low power\nconsumption, utilizing the novel architecture described. However, as a\ntrade-off, the ACT algorithm requires 10 non-uniformly sampled data points to\ncalculate the 8-point DCT. This requirement can easily be satisfied for\napplications dealing with spatial signals such as image sensors and biomedical\nsensor arrays, by placing sensor elements in a non-uniform grid. In this work,\na hardware architecture for the computation of the null mean ACT is proposed,\nfollowed by a novel architectures that extend the ACT for non-null mean\nsignals. All circuits are physically implemented and tested using the Xilinx\nXC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for\nperformance assessment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:06:19 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Rajapaksha", "N.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Adikari", "J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1710.11549", "submitter": "Andrew Shin", "authors": "Andrew Shin, Leopold Crestel, Hiroharu Kato, Kuniaki Saito, Katsunori\n  Ohnishi, Masataka Yamaguchi, Masahiro Nakawaki, Yoshitaka Ushiku, Tatsuya\n  Harada", "title": "Melody Generation for Pop Music via Word Representation of Musical\n  Properties", "comments": "submitted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic melody generation for pop music has been a long-time aspiration for\nboth AI researchers and musicians. However, learning to generate euphonious\nmelody has turned out to be highly challenging due to a number of factors.\nRepresentation of multivariate property of notes has been one of the primary\nchallenges. It is also difficult to remain in the permissible spectrum of\nmusical variety, outside of which would be perceived as a plain random play\nwithout auditory pleasantness. Observing the conventional structure of pop\nmusic poses further challenges. In this paper, we propose to represent each\nnote and its properties as a unique `word,' thus lessening the prospect of\nmisalignments between the properties, as well as reducing the complexity of\nlearning. We also enforce regularization policies on the range of notes, thus\nencouraging the generated melody to stay close to what humans would find easy\nto follow. Furthermore, we generate melody conditioned on song part\ninformation, thus replicating the overall structure of a full song.\nExperimental results demonstrate that our model can generate auditorily\npleasant songs that are more indistinguishable from human-written ones than\nprevious models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 16:04:23 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Shin", "Andrew", ""], ["Crestel", "Leopold", ""], ["Kato", "Hiroharu", ""], ["Saito", "Kuniaki", ""], ["Ohnishi", "Katsunori", ""], ["Yamaguchi", "Masataka", ""], ["Nakawaki", "Masahiro", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}]