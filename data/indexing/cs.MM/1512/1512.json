[{"id": "1512.02736", "submitter": "Xingyu Zeng", "authors": "Xingyu Zeng, Wanli Ouyang, Xiaogang Wang", "title": "Window-Object Relationship Guided Representation Learning for Generic\n  Object Detections", "comments": "9 pages, including 1 reference page, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing works that learn representation for object detection, the\nrelationship between a candidate window and the ground truth bounding box of an\nobject is simplified by thresholding their overlap. This paper shows\ninformation loss in this simplification and picks up the relative location/size\ninformation discarded by thresholding. We propose a representation learning\npipeline to use the relationship as supervision for improving the learned\nrepresentation in object detection. Such relationship is not limited to object\nof the target category, but also includes surrounding objects of other\ncategories. We show that image regions with multiple contexts and multiple\nrotations are effective in capturing such relationship during the\nrepresentation learning process and in handling the semantic and visual\nvariation caused by different window-object configurations. Experimental\nresults show that the representation learned by our approach can improve the\nobject detection accuracy by 6.4% in mean average precision (mAP) on\nILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved\nby our single model and it is the best among published results. On PASCAL VOC,\nit outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute\nmAP.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 03:32:21 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Zeng", "Xingyu", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1512.04354", "submitter": "Stefane Paris", "authors": "St\\'efane Paris (QGAR)", "title": "A proposal project for a blind image quality assessment by learning\n  distortions from the full reference image quality assessments", "comments": "International Workshop on Quality of Multimedia Experience, 2012,\n  Melbourne, Australia", "journal-ref": null, "doi": "10.1109/QoMEX.2012.6263876", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper presents a perspective plan to build a null reference image\nquality assessment. Its main goal is to deliver both the objective score and\nthe distortion map for a given distorted image without the knowledge of its\nreference image.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 12:21:04 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Paris", "St\u00e9fane", "", "QGAR"]]}, {"id": "1512.04785", "submitter": "Phong Vo", "authors": "Phong D. Vo, Alexandru Ginsca, Herv\\'e Le Borgne, Adrian Popescu", "title": "On Deep Representation Learning from Noisy Web Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The keep-growing content of Web images may be the next important data source\nto scale up deep neural networks, which recently obtained a great success in\nthe ImageNet classification challenge and related tasks. This prospect,\nhowever, has not been validated on convolutional networks (convnet) -- one of\nbest performing deep models -- because of their supervised regime. While\nunsupervised alternatives are not so good as convnet in generalizing the\nlearned model to new domains, we use convnet to leverage semi-supervised\nrepresentation learning. Our approach is to use massive amounts of unlabeled\nand noisy Web images to train convnets as general feature detectors despite\nchallenges coming from data such as high level of mislabeled data, outliers,\nand data biases. Extensive experiments are conducted at several data scales,\ndifferent network architectures, and data reranking techniques. The learned\nrepresentations are evaluated on nine public datasets of various topics. The\nbest results obtained by our convnets, trained on 3.14 million Web images,\noutperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is\nclosing the gap with VGG-16. These prominent results suggest a budget solution\nto use deep learning in practice and motivate more research in semi-supervised\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 13:57:39 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 20:50:54 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Vo", "Phong D.", ""], ["Ginsca", "Alexandru", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Popescu", "Adrian", ""]]}, {"id": "1512.05705", "submitter": "Majed Haddad", "authors": "Imen Triki, Rachid El-Azouzi and Majed Haddad", "title": "NEWCAST: Anticipating Resource Management and QoE Provisioning for\n  Mobile Video Streaming", "comments": "14 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge of future throughput variations in mobile networks becomes more\nand more possible today thanks to the rich contextual information provided by\nmobile applications and services and smartphone sensors. It is even likely that\nsuch contextual information, which may include traffic, mobility and radio\nconditions will lead to a novel agile resource management not yet thought of.\nIn this paper, we propose an framework (called NEWCAST) that anticipates the\nthroughput variations to deliver video streaming content. We develop an\noptimization problem that realizes a fundamental trade-off among critical\nmetrics that impact the user's perceptual quality of experience (QoE) and the\ncost of system utilization. Both simulated and real-world throughput traces\ncollected from [1], were carried out to evaluate the performance of NEWCAST. In\nparticular, we show from our numerical results that NEWCAST provides the\nefficiency that the new 5G architectures require in terms of computational\ncomplexity and robustness. We also implement a prototype system of NEWCAST and\nevaluate it in a real environment with a real player to show its efficiency and\nscalability compared to baseline adaptive bitrate algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 18:16:25 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 21:07:48 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 22:59:36 GMT"}, {"version": "v4", "created": "Thu, 25 Jan 2018 12:38:19 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Triki", "Imen", ""], ["El-Azouzi", "Rachid", ""], ["Haddad", "Majed", ""]]}, {"id": "1512.07748", "submitter": "Tomohiko Nakamura", "authors": "Tomohiko Nakamura, Eita Nakamura and Shigeki Sagayama", "title": "Real-Time Audio-to-Score Alignment of Music Performances Containing\n  Errors and Arbitrary Repeats and Skips", "comments": "12 pages, 8 figures, version accepted in IEEE/ACM Transactions on\n  Audio, Speech, and Language Processing", "journal-ref": null, "doi": "10.1109/TASLP.2015.2507862", "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses real-time alignment of audio signals of music\nperformance to the corresponding score (a.k.a. score following) which can\nhandle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)\nin performances. This type of score following is particularly useful in\nautomatic accompaniment for practices and rehearsals, where errors and\nrepeats/skips are often made. Simple extensions of the algorithms previously\nproposed in the literature are not applicable in these situations for scores of\npractical length due to the problem of large computational complexity. To cope\nwith this problem, we present two hidden Markov models of monophonic\nperformance with errors and arbitrary repeats/skips, and derive efficient\nscore-following algorithms with an assumption that the prior probability\ndistributions of score positions before and after repeats/skips are independent\nfrom each other. We confirmed real-time operation of the algorithms with music\nscores of practical length (around 10000 notes) on a modern laptop and their\ntracking ability to the input performance within 0.7 s on average after\nrepeats/skips in clarinet performance data. Further improvements and extension\nfor polyphonic signals are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 08:21:48 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Nakamura", "Tomohiko", ""], ["Nakamura", "Eita", ""], ["Sagayama", "Shigeki", ""]]}, {"id": "1512.08854", "submitter": "Qifei Wang", "authors": "Qifei Wang", "title": "An Overview of Emerging Technologies for High Efficiency 3D Video Coding", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D video coding is one of the most popular research area in multimedia. This\npaper reviews the recent progress of the coding technologies for multiview\nvideo (MVV) and free view-point video (FVV) which is represented by MVV and\ndepth maps. We first discuss the traditional multiview video coding (MVC)\nframework with different prediction structures. The rate-distortion performance\nand the view switching delay of the three main coding prediction structures are\nanalyzed. We further introduce the joint coding technologies for MVV and depth\nmaps and evaluate the rate-distortion performance of them. The scalable 3D\nvideo coding technologies are reviewed by the quality and view scalability,\nrespectively. Finally, we summarize the bit allocation work of 3D video coding.\nThis paper also points out some future research problems in high efficiency 3D\nvideo coding such as the view switching latency optimization in coding\nstructure and bit allocation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 05:41:21 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Wang", "Qifei", ""]]}]