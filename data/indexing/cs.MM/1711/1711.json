[{"id": "1711.00536", "submitter": "Rossano Schifanella", "authors": "Luca M. Aiello, Rossano Schifanella, Miriam Redi, Stacey Svetlichnaya,\n  Frank Liu, Simon Osindero", "title": "Beautiful and damned. Combined effect of content quality and social ties\n  on user engagement", "comments": "13 pages, 12 figures, final version published in IEEE Transactions on\n  Knowledge and Data Engineering (Volume: PP, Issue: 99)", "journal-ref": null, "doi": "10.1109/TKDE.2017.2747552", "report-no": null, "categories": "cs.SI cs.AI cs.CV cs.MM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User participation in online communities is driven by the intertwinement of\nthe social network structure with the crowd-generated content that flows along\nits links. These aspects are rarely explored jointly and at scale. By looking\nat how users generate and access pictures of varying beauty on Flickr, we\ninvestigate how the production of quality impacts the dynamics of online social\nsystems. We develop a deep learning computer vision model to score images\naccording to their aesthetic value and we validate its output through\ncrowdsourcing. By applying it to over 15B Flickr photos, we study for the first\ntime how image beauty is distributed over a large-scale social system.\nBeautiful images are evenly distributed in the network, although only a small\ncore of people get social recognition for them. To study the impact of exposure\nto quality on user engagement, we set up matching experiments aimed at\ndetecting causality from observational data. Exposure to beauty is\ndouble-edged: following people who produce high-quality content increases one's\nprobability of uploading better photos; however, an excessive imbalance between\nthe quality generated by a user and the user's neighbors leads to a decline in\nengagement. Our analysis has practical implications for improving link\nrecommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:48:30 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Aiello", "Luca M.", ""], ["Schifanella", "Rossano", ""], ["Redi", "Miriam", ""], ["Svetlichnaya", "Stacey", ""], ["Liu", "Frank", ""], ["Osindero", "Simon", ""]]}, {"id": "1711.00541", "submitter": "Yi Luo", "authors": "Yi Luo, Nima Mesgarani", "title": "TasNet: time-domain audio separation network for real-time,\n  single-channel speech separation", "comments": "Camera ready version for ICASSP 2018, Calgary, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robust speech processing in multi-talker environments requires effective\nspeech separation. Recent deep learning systems have made significant progress\ntoward solving this problem, yet it remains challenging particularly in\nreal-time, short latency applications. Most methods attempt to construct a mask\nfor each source in time-frequency representation of the mixture signal which is\nnot necessarily an optimal representation for speech separation. In addition,\ntime-frequency decomposition results in inherent problems such as\nphase/magnitude decoupling and long time window which is required to achieve\nsufficient frequency resolution. We propose Time-domain Audio Separation\nNetwork (TasNet) to overcome these limitations. We directly model the signal in\nthe time-domain using an encoder-decoder framework and perform the source\nseparation on nonnegative encoder outputs. This method removes the frequency\ndecomposition step and reduces the separation problem to estimation of source\nmasks on encoder outputs which is then synthesized by the decoder. Our system\noutperforms the current state-of-the-art causal and noncausal speech separation\nalgorithms, reduces the computational cost of speech separation, and\nsignificantly reduces the minimum required latency of the output. This makes\nTasNet suitable for applications where low-power, real-time implementation is\ndesirable such as in hearable and telecommunication devices.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 21:19:22 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 02:25:29 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Luo", "Yi", ""], ["Mesgarani", "Nima", ""]]}, {"id": "1711.00888", "submitter": "I-Hong Jhuo", "authors": "I-Hong Jhuo, Jun Wang", "title": "Set-to-Set Hashing with Applications in Visual Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data, such as an image or a sequence of video frames, is often\nnaturally represented as a point set. In this paper, we consider the\nfundamental problem of finding a nearest set from a collection of sets, to a\nquery set. This problem has obvious applications in large-scale visual\nretrieval and recognition, and also in applied fields beyond computer vision.\nOne challenge stands out in solving the problem---set representation and\nmeasure of similarity. Particularly, the query set and the sets in dataset\ncollection can have varying cardinalities. The training collection is large\nenough such that linear scan is impractical. We propose a simple representation\nscheme that encodes both statistical and structural information of the sets.\nThe derived representations are integrated in a kernel framework for flexible\nsimilarity measurement. For the query set process, we adopt a learning-to-hash\npipeline that turns the kernel representations into hash bits based on simple\nlearners, using multiple kernel learning. Experiments on two visual retrieval\ndatasets show unambiguously that our set-to-set hashing framework outperforms\nprior methods that do not take the set-to-set search setting.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 18:57:43 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 02:16:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Jhuo", "I-Hong", ""], ["Wang", "Jun", ""]]}, {"id": "1711.01306", "submitter": "Aidin Ferdowsi", "authors": "Aidin Ferdowsi and Walid Saad", "title": "Deep Learning-Based Dynamic Watermarking for Secure Signal\n  Authentication in the Internet of Things", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Securing the Internet of Things (IoT) is a necessary milestone toward\nexpediting the deployment of its applications and services. In particular, the\nfunctionality of the IoT devices is extremely dependent on the reliability of\ntheir message transmission. Cyber attacks such as data injection,\neavesdropping, and man-in-the-middle threats can lead to security challenges.\nSecuring IoT devices against such attacks requires accounting for their\nstringent computational power and need for low-latency operations. In this\npaper, a novel deep learning method is proposed for dynamic watermarking of IoT\nsignals to detect cyber attacks. The proposed learning framework, based on a\nlong short-term memory (LSTM) structure, enables the IoT devices to extract a\nset of stochastic features from their generated signal and dynamically\nwatermark these features into the signal. This method enables the IoT's cloud\ncenter, which collects signals from the IoT devices, to effectively\nauthenticate the reliability of the signals. Furthermore, the proposed method\nprevents complicated attack scenarios such as eavesdropping in which the cyber\nattacker collects the data from the IoT devices and aims to break the\nwatermarking algorithm. Simulation results show that, with an attack detection\ndelay of under 1 second the messages can be transmitted from IoT devices with\nan almost 100% reliability.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 19:12:23 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ferdowsi", "Aidin", ""], ["Saad", "Walid", ""]]}, {"id": "1711.01369", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Maksim Khadkevich, Christian Fugen", "title": "Knowledge Transfer from Weakly Labeled Audio using Convolutional Neural\n  Network for Sound Events and Scenes", "comments": "ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose approaches to effectively transfer knowledge from\nweakly labeled web audio data. We first describe a convolutional neural network\n(CNN) based framework for sound event detection and classification using weakly\nlabeled audio data. Our model trains efficiently from audios of variable\nlengths; hence, it is well suited for transfer learning. We then propose\nmethods to learn representations using this model which can be effectively used\nfor solving the target task. We study both transductive and inductive transfer\nlearning tasks, showing the effectiveness of our methods for both domain and\ntask adaptation. We show that the learned representations using the proposed\nCNN model generalizes well enough to reach human level accuracy on ESC-50 sound\nevents dataset and set state of art results on this dataset. We further use\nthem for acoustic scene classification task and once again show that our\nproposed approaches suit well for this task as well. We also show that our\nmethods are helpful in capturing semantic meanings and relations as well.\nMoreover, in this process we also set state-of-art results on Audioset dataset,\nrelying on balanced training set.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 00:22:23 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 19:20:00 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 19:41:44 GMT"}, {"version": "v4", "created": "Fri, 7 Sep 2018 05:22:27 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Kumar", "Anurag", ""], ["Khadkevich", "Maksim", ""], ["Fugen", "Christian", ""]]}, {"id": "1711.01775", "submitter": "Athanasia Zlatintsi", "authors": "A. Zlatintsi, I. Rodomagoulakis, P. Koutras, A. C. Dometios, V.\n  Pitsikalis, C. S. Tzafestas, and P. Maragos", "title": "Multimodal Signal Processing and Learning Aspects of Human-Robot\n  Interaction for an Assistive Bathing Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore new aspects of assistive living on smart human-robot interaction\n(HRI) that involve automatic recognition and online validation of speech and\ngestures in a natural interface, providing social features for HRI. We\nintroduce a whole framework and resources of a real-life scenario for elderly\nsubjects supported by an assistive bathing robot, addressing health and hygiene\ncare issues. We contribute a new dataset and a suite of tools used for data\nacquisition and a state-of-the-art pipeline for multimodal learning within the\nframework of the I-Support bathing robot, with emphasis on audio and RGB-D\nvisual streams. We consider privacy issues by evaluating the depth visual\nstream along with the RGB, using Kinect sensors. The audio-gestural recognition\ntask on this new dataset yields up to 84.5%, while the online validation of the\nI-Support system on elderly users accomplishes up to 84% when the two\nmodalities are fused together. The results are promising enough to support\nfurther research in the area of multimodal recognition for assistive social\nHRI, considering the difficulties of the specific task. Upon acceptance of the\npaper part of the data will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 08:16:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zlatintsi", "A.", ""], ["Rodomagoulakis", "I.", ""], ["Koutras", "P.", ""], ["Dometios", "A. C.", ""], ["Pitsikalis", "V.", ""], ["Tzafestas", "C. S.", ""], ["Maragos", "P.", ""]]}, {"id": "1711.01889", "submitter": "Jianshu Zhang", "authors": "Jianshu Zhang, Yixing Zhu, Jun Du and Lirong Dai", "title": "Radical analysis network for zero-shot learning in printed Chinese\n  character recognition", "comments": "Accepted by ICME2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese characters have a huge set of character categories, more than 20,000\nand the number is still increasing as more and more novel characters continue\nbeing created. However, the enormous characters can be decomposed into a\ncompact set of about 500 fundamental and structural radicals. This paper\nintroduces a novel radical analysis network (RAN) to recognize printed Chinese\ncharacters by identifying radicals and analyzing two-dimensional spatial\nstructures among them. The proposed RAN first extracts visual features from\ninput by employing convolutional neural networks as an encoder. Then a decoder\nbased on recurrent neural networks is employed, aiming at generating captions\nof Chinese characters by detecting radicals and two-dimensional structures\nthrough a spatial attention mechanism. The manner of treating a Chinese\ncharacter as a composition of radicals rather than a single character class\nlargely reduces the size of vocabulary and enables RAN to possess the ability\nof recognizing unseen Chinese character classes, namely zero-shot learning.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 14:40:28 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 05:25:29 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zhang", "Jianshu", ""], ["Zhu", "Yixing", ""], ["Du", "Jun", ""], ["Dai", "Lirong", ""]]}, {"id": "1711.02150", "submitter": "Abbas Soltanian", "authors": "Abbas Soltanian, Diala Naboulsi, Mohammad A. Salahuddin, Roch Glitho,\n  Halima Elbiaze, Constant Wette", "title": "ADS: Adaptive and Dynamic Scaling Mechanism for Multimedia Conferencing\n  Services in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia conferencing is used extensively in a wide range of applications,\nsuch as online games and distance learning. These applications need to\nefficiently scale the conference size as the number of participants fluctuates.\nCloud is a technology that addresses the scalability issue. However, the\nproposed cloud-based solutions have several shortcomings in considering the\nfuture demand of applications while meeting both Quality of Service (QoS)\nrequirements and efficiency in resource usage. In this paper, we propose an\nAdaptive and Dynamic Scaling mechanism (ADS) for multimedia conferencing\nservices in the cloud. This mechanism enables scalable and elastic resource\nallocation with respect to the number of participants. ADS produces a\ncost-efficient scaling schedule while considering the QoS requirements and the\nfuture demand of the conferencing service. We formulate the problem using\nInteger Linear Programming (ILP) and design a heuristic for it. Simulation\nresults show that ADS mechanism elastically scales conferencing services.\nMoreover, the ADS heuristic is shown to outperform a greedy algorithm from a\nresource-efficiency perspective.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 20:13:58 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Soltanian", "Abbas", ""], ["Naboulsi", "Diala", ""], ["Salahuddin", "Mohammad A.", ""], ["Glitho", "Roch", ""], ["Elbiaze", "Halima", ""], ["Wette", "Constant", ""]]}, {"id": "1711.02231", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley", "title": "Visually-Aware Fashion Recommendation and Design with Generative Image\n  Models", "comments": "10 pages, 6 figures. Accepted by ICDM'17 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building effective recommender systems for domains like fashion is\nchallenging due to the high level of subjectivity and the semantic complexity\nof the features involved (i.e., fashion styles). Recent work has shown that\napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made\nmore accurate by incorporating visual signals directly into the recommendation\nobjective, using `off-the-shelf' feature representations derived from deep\nnetworks. Here, we seek to extend this contribution by showing that\nrecommendation performance can be significantly improved by learning `fashion\naware' image representations directly, i.e., by training the image\nrepresentation (from the pixel level) and the recommender system jointly; this\ncontribution is related to recent work using Siamese CNNs, though we are able\nto show improvements over state-of-the-art recommendation techniques such as\nBPR and variants that make use of pre-trained visual features. Furthermore, we\nshow that our model can be used \\emph{generatively}, i.e., given a user and a\nproduct category, we can generate new images (i.e., clothing items) that are\nmost consistent with their personal taste. This represents a first step towards\nbuilding systems that go beyond recommending existing items from a product\ncorpus, but which can be used to suggest styles and aid the design of new\nproducts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 00:17:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["McAuley", "Julian", ""]]}, {"id": "1711.02386", "submitter": "Cagri Ozcinar", "authors": "Cagri Ozcinar, Ana De Abreu, Aljosa Smolic", "title": "Viewport-aware adaptive 360{\\deg} video streaming using tiles for\n  virtual reality", "comments": "IEEE International Conference on Image Processing (ICIP) 2017", "journal-ref": "2017 IEEE International Conference on Image Processing (ICIP),\n  Beijing, China, 2017", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  360{\\deg} video is attracting an increasing amount of attention in the\ncontext of Virtual Reality (VR). Owing to its very high-resolution\nrequirements, existing professional streaming services for 360{\\deg} video\nsuffer from severe drawbacks. This paper introduces a novel end-to-end\nstreaming system from encoding to displaying, to transmit 8K resolution\n360{\\deg} video and to provide an enhanced VR experience using Head Mounted\nDisplays (HMDs). The main contributions of the proposed system are about\ntiling, integration of the MPEG-Dynamic Adaptive Streaming over HTTP (DASH)\nstandard, and viewport-aware bitrate level selection. Tiling and adaptive\nstreaming enable the proposed system to deliver very high-resolution 360{\\deg}\nvideo at good visual quality. Further, the proposed viewport-aware bitrate\nassignment selects an optimum DASH representation for each tile in a\nviewport-aware manner. The quality performance of the proposed system is\nverified in simulations with varying network bandwidth using realistic view\ntrajectories recorded from user experiments. Our results show that the proposed\nstreaming system compares favorably compared to existing methods in terms of\nPSNR and SSIM inside the viewport.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 10:50:13 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Ozcinar", "Cagri", ""], ["De Abreu", "Ana", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1711.02581", "submitter": "Mehdi Sharifzadeh", "authors": "Mehdi Sharifzadeh, Chirag Agarwal, Mohammed Aloraini, Dan Schonfeld", "title": "Convolutional Neural Network Steganalysis's Application to Steganography", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.08616", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to increase the performance bounds of\nimage steganography under the criteria of minimizing distortion. The proposed\napproach utilizes a steganalysis convolutional neural network (CNN) framework\nto understand an image's model and embed in less detectable regions to preserve\nthe model. In other word, the trained steganalysis CNN is used to calculate\nderivatives of the statistical model of an image with respect to embedding\nchanges. The experimental results show that the proposed algorithm outperforms\nprevious state-of-the-art methods in a wide range of low relative payloads when\ncompared with HUGO, S-UNIWARD, and HILL by the state-of-the-art steganalysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:51:07 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Sharifzadeh", "Mehdi", ""], ["Agarwal", "Chirag", ""], ["Aloraini", "Mohammed", ""], ["Schonfeld", "Dan", ""]]}, {"id": "1711.03362", "submitter": "Cagri Ozcinar", "authors": "Cagri Ozcinar, Ana De Abreu, Sebastian Knorr, Aljosa Smolic", "title": "Estimation of optimal encoding ladders for tiled 360{\\deg} VR video in\n  adaptive streaming systems", "comments": "The 19th IEEE International Symposium on Multimedia (ISM 2017),\n  Taichung, Taiwan", "journal-ref": "The 19th IEEE International Symposium on Multimedia (ISM 2017),\n  Taichung, Taiwan", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the significant industrial growth of demand for virtual reality (VR),\n360{\\deg} video streaming is one of the most important VR applications that\nrequire cost-optimal solutions to achieve widespread proliferation of VR\ntechnology. Because of its inherent variability of data-intensive content types\nand its tiled-based encoding and streaming, 360{\\deg} video requires new\nencoding ladders in adaptive streaming systems to achieve cost-optimal and\nimmersive streaming experiences. In this context, this paper targets both the\nprovider's and client's perspectives and introduces a new content-aware\nencoding ladder estimation method for tiled 360{\\deg} VR video in adaptive\nstreaming systems. The proposed method first categories a given 360{\\deg} video\nusing its features of encoding complexity and estimates the visual distortion\nand resource cost of each bitrate level based on the proposed distortion and\nresource cost models. An optimal encoding ladder is then formed using the\nproposed integer linear programming (ILP) algorithm by considering practical\nconstraints. Experimental results of the proposed method are compared with the\nrecommended encoding ladders of professional streaming service providers.\nEvaluations show that the proposed encoding ladders deliver better results\ncompared to the recommended encoding ladders in terms of objective quality for\n360{\\deg} video, providing optimal encoding ladders using a set of service\nprovider's constraint parameters.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 13:07:45 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Ozcinar", "Cagri", ""], ["De Abreu", "Ana", ""], ["Knorr", "Sebastian", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1711.03418", "submitter": "Jean-Louis Rougier", "authors": "Roman Sorokin, Jean-Louis Rougier", "title": "IP Video Conferencing: A Tutorial", "comments": "Video Conferencing, codec, SVC, MCU, SIP, RTP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video conferencing is a well-established area of communications, which have\nbeen studied for decades. Recently this area has received a new impulse due to\nsignificantly increased bandwidth of Local and Wide area networks, appearance\nof low-priced video equipment and development of web based media technologies.\nThis paper presents the main techniques behind the modern IP-based\nvideoconferencing services, with a particular focus on codecs, network\nprotocols, architectures and standardization efforts. Questions of security and\ntopologies are also tackled. A description of a typical video conference\nscenario is provided, demonstrating how the technologies, responsible for\ndifferent conference aspects, are working together. Traditional industrial\ndisposition as well as modern innovative approaches are both addressed. Current\nindustry trends are highlighted in respect to the topics, described in the\ntutorial. Legacy analog/digital technologies, together with the gateways\nbetween the traditional and the IP videoconferencing systems, are not\nconsidered.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:22:15 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Sorokin", "Roman", ""], ["Rougier", "Jean-Louis", ""]]}, {"id": "1711.03951", "submitter": "Luc Trudeau", "authors": "Luc N. Trudeau and Nathan E. Egge and David Barr", "title": "Predicting Chroma from Luma in AV1", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chroma from luma (CfL) prediction is a new and promising chroma-only intra\npredictor that models chroma pixels as a linear function of the coincident\nreconstructed luma pixels. In this paper, we present the CfL predictor adopted\nin Alliance Video 1 (AV1), a royalty-free video codec developed by the Alliance\nfor Open Media (AOM). The proposed CfL distinguishes itself from prior art not\nonly by reducing decoder complexity, but also by producing more accurate\npredictions. On average, CfL reduces the BD-rate, when measured with CIEDE2000,\nby 5% for still images and 2% for video sequences.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:26:04 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 15:51:46 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Trudeau", "Luc N.", ""], ["Egge", "Nathan E.", ""], ["Barr", "David", ""]]}, {"id": "1711.04258", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng, Zenglin Xu", "title": "Unified Spectral Clustering with Optimal Graph", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has found extensive use in many areas. Most traditional\nspectral clustering algorithms work in three separate steps: similarity graph\nconstruction; continuous labels learning; discretizing the learned labels by\nk-means clustering. Such common practice has two potential flaws, which may\nlead to severe information loss and performance degradation. First, predefined\nsimilarity graph might not be optimal for subsequent clustering. It is\nwell-accepted that similarity graph highly affects the clustering results. To\nthis end, we propose to automatically learn similarity information from data\nand simultaneously consider the constraint that the similarity matrix has exact\nc connected components if there are c clusters. Second, the discrete solution\nmay deviate from the spectral solution since k-means method is well-known as\nsensitive to the initialization of cluster centers. In this work, we transform\nthe candidate solution into a new one that better approximates the discrete\none. Finally, those three subtasks are integrated into a unified framework,\nwith each subtask iteratively boosted by using the results of the others\ntowards an overall optimal solution. It is known that the performance of a\nkernel method is largely determined by the choice of kernels. To tackle this\npractical problem of how to select the most suitable kernel for a particular\ndata set, we further extend our model to incorporate multiple kernel learning\nability. Extensive experiments demonstrate the superiority of our proposed\nmethod as compared to existing clustering approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 09:20:25 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""], ["Xu", "Zenglin", ""]]}, {"id": "1711.04916", "submitter": "Yan Ke", "authors": "Yan Ke, Minqing Zhang, Jia Liu, Tingting Su, Xiaoyuan Yang", "title": "Generative Steganography with Kerckhoffs' Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distortion in steganography that usually comes from the modification or\nrecoding on the cover image during the embedding process leaves the\nsteganalyzer with possibility of discriminating. Faced with such a risk, we\npropose generative steganography with Kerckhoffs' principle (GSK) in this\nletter. In GSK, the secret messages are generated by a cover image using a\ngenerator rather than embedded into the cover, thus resulting in no\nmodifications in the cover. To ensure the security, the generators are trained\nto meet Kerckhoffs' principle based on generative adversarial networks (GAN).\nEverything about the GSK system, except the extraction key, is public knowledge\nfor the receivers. The secret messages can be outputted by the generator if and\nonly if the extraction key and the cover image are both inputted. In the\ngenerator training procedures, there are two GANs, Message- GAN and Cover-GAN,\ndesigned to work jointly making the generated results under the control of the\nextraction key and the cover image. We provide experimental results on the\ntraining process and give an example of the working process by adopting a\ngenerator trained on MNIST, which demonstrate that GSK can use a cover image\nwithout any modification to generate messages, and without the extraction key\nor the cover image, only meaningless results would be obtained.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 02:50:06 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 13:39:33 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 14:40:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ke", "Yan", ""], ["Zhang", "Minqing", ""], ["Liu", "Jia", ""], ["Su", "Tingting", ""], ["Yang", "Xiaoyuan", ""]]}, {"id": "1711.05480", "submitter": "Balasubramanyam Appina Mr", "authors": "Appina Balasubramanyam, Jalli Akshith, Battula Shanmukh Srinivas,\n  Channappayya S Sumohana", "title": "No Reference Stereoscopic Video Quality Assessment Using Joint Motion\n  and Depth Statistics", "comments": "13 PAGES, 7 FIGURES, 7 TABLES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a no reference (NR) quality assessment algorithm for assessing the\nperceptual quality of natural stereoscopic 3D (S3D) videos. This work is\ninspired by our finding that the joint statistics of the subband coefficients\nof motion (optical flow or motion vector magnitude) and depth (disparity map)\nof natural S3D videos possess a unique signature. Specifically, we empirically\nshow that the joint statistics of the motion and depth subband coefficients of\nS3D video frames can be modeled accurately using a Bivariate Generalized\nGaussian Distribution (BGGD). We then demonstrate that the parameters of the\nBGGD model possess the ability to discern quality variations in S3D videos.\nTherefore, the BGGD model parameters are employed as motion and depth quality\nfeatures. In addition to these features, we rely on a frame level spatial\nquality feature that is computed using a robust off the shelf NR image quality\nassessment (IQA) algorithm. These frame level motion, depth and spatial\nfeatures are consolidated and used with the corresponding S3D video's\ndifference mean opinion score (DMOS) labels for supervised learning using\nsupport vector regression (SVR). The overall quality of an S3D video is\ncomputed by averaging the frame level quality predictions of the constituent\nvideo frames. The proposed algorithm, dubbed Video QUality Evaluation using\nMOtion and DEpth Statistics (VQUEMODES) is shown to outperform the state of the\nart methods when evaluated over the IRCCYN and LFOVIA S3D subjective quality\nassessment databases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 09:52:23 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Balasubramanyam", "Appina", ""], ["Akshith", "Jalli", ""], ["Srinivas", "Battula Shanmukh", ""], ["Sumohana", "Channappayya S", ""]]}, {"id": "1711.05535", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu,\n  Yi-Dong Shen", "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss", "comments": "15pages, 15 figures, 8 tables", "journal-ref": null, "doi": "10.1145/3383184", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:40:11 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 02:40:11 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 17:38:54 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 07:45:26 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Garrett", "Michael", ""], ["Yang", "Yi", ""], ["Xu", "Mingliang", ""], ["Shen", "Yi-Dong", ""]]}, {"id": "1711.06434", "submitter": "Ziqiang Shi", "authors": "Ziqiang Shi and Mengjiao Wang and Liu Liu and Huibin Lin and Rujie Liu", "title": "A Double Joint Bayesian Approach for J-Vector Based Text-dependent\n  Speaker Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  J-vector has been proved to be very effective in text-dependent speaker\nverification with short-duration speech. However, the current state-of-the-art\nback-end classifiers, e.g. joint Bayesian model, cannot make full use of such\ndeep features. In this paper, we generalize the standard joint Bayesian\napproach to model the multi-faceted information in the j-vector explicitly and\njointly. In our generalization, the j-vector was modeled as a result derived by\na generative Double Joint Bayesian (DoJoBa) model, which contains several kinds\nof latent variables. With DoJoBa, we are able to explicitly build a model that\ncan combine multiple heterogeneous information from the j-vectors. In\nverification step, we calculated the likelihood to describe whether the two\nj-vectors having consistent labels or not. On the public RSR2015 data corpus,\nthe experimental results showed that our approach can achieve 0.02\\% EER and\n0.02\\% EER for impostor wrong and impostor correct cases respectively.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 07:19:03 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Shi", "Ziqiang", ""], ["Wang", "Mengjiao", ""], ["Liu", "Liu", ""], ["Lin", "Huibin", ""], ["Liu", "Rujie", ""]]}, {"id": "1711.07201", "submitter": "Atique Rehman", "authors": "Atique ur Rehman, Rafia Rahim, M Shahroz Nadeem, Sibt ul Hussain", "title": "End-to-end Trained CNN Encode-Decoder Networks for Image Steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All the existing image steganography methods use manually crafted features to\nhide binary payloads into cover images. This leads to small payload capacity\nand image distortion. Here we propose a convolutional neural network based\nencoder-decoder architecture for embedding of images as payload. To this end,\nwe make following three major contributions: (i) we propose a deep learning\nbased generic encoder-decoder architecture for image steganography; (ii) we\nintroduce a new loss function that ensures joint end-to-end training of\nencoder-decoder networks; (iii) we perform extensive empirical evaluation of\nproposed architecture on a range of challenging publicly available datasets\n(MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art\npayload capacity at high PSNR and SSIM values.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:49:32 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Rehman", "Atique ur", ""], ["Rahim", "Rafia", ""], ["Nadeem", "M Shahroz", ""], ["Hussain", "Sibt ul", ""]]}, {"id": "1711.07306", "submitter": "Songtao Wu", "authors": "Songtao Wu, Sheng-hua Zhong, and Yan Liu", "title": "A Novel Convolutional Neural Network for Image Steganalysis with Shared\n  Normalization", "comments": "submitted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based image steganalysis has attracted increasing attentions in\nrecent years. Several Convolutional Neural Network (CNN) models have been\nproposed and achieved state-of-the-art performances on detecting steganography.\nIn this paper, we explore an important technique in deep learning, the batch\nnormalization, for the task of image steganalysis. Different from natural image\nclassification, steganalysis is to discriminate cover images and stego images\nwhich are the result of adding weak stego signals into covers. This\ncharacteristic makes a cover image is more statistically similar to its stego\nthan other cover images, requiring steganalytic methods to use paired learning\nto extract effective features for image steganalysis. Our theoretical analysis\nshows that a CNN model with multiple normalization layers is hard to be\ngeneralized to new data in the test set when it is well trained with paired\nlearning. To hand this difficulty, we propose a novel normalization technique\ncalled Shared Normalization (SN) in this paper. Unlike the batch normalization\nlayer utilizing the mini-batch mean and standard deviation to normalize each\ninput batch, SN shares same statistics for all training and test batches. Based\non the proposed SN layer, we further propose a novel neural network model for\nimage steganalysis. Extensive experiments demonstrate that the proposed network\nwith SN layers is stable and can detect the state of the art steganography with\nbetter performances than previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 13:44:29 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:56:43 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wu", "Songtao", ""], ["Zhong", "Sheng-hua", ""], ["Liu", "Yan", ""]]}, {"id": "1711.07430", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Mi, Jianxin Wu, Ke Lu, Hongkai Xiong", "title": "Action Recognition with Coarse-to-Fine Deep Feature Integration and\n  Asynchronous Fusion", "comments": "accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is an important yet challenging task in computer vision.\nIn this paper, we propose a novel deep-based framework for action recognition,\nwhich improves the recognition accuracy by: 1) deriving more precise features\nfor representing actions, and 2) reducing the asynchrony between different\ninformation streams. We first introduce a coarse-to-fine network which extracts\nshared deep features at different action class granularities and progressively\nintegrates them to obtain a more accurate feature representation for input\nactions. We further introduce an asynchronous fusion network. It fuses\ninformation from different streams by asynchronously integrating stream-wise\nfeatures at different time points, hence better leveraging the complementary\ninformation in different streams. Experimental results on action recognition\nbenchmarks demonstrate that our approach achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:35:46 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Lin", "Weiyao", ""], ["Mi", "Yang", ""], ["Wu", "Jianxin", ""], ["Lu", "Ke", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1711.07901", "submitter": "Yehuda Dar", "authors": "Yehuda Dar, Michael Elad, and Alfred M. Bruckstein", "title": "Optimized Pre-Compensating Compression", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2845125", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In imaging systems, following acquisition, an image/video is transmitted or\nstored and eventually presented to human observers using different and often\nimperfect display devices. While the resulting quality of the output image may\nseverely be affected by the display, this degradation is usually ignored in the\npreceding compression. In this paper we model the sub-optimality of the display\ndevice as a known degradation operator applied on the decompressed image/video.\nWe assume the use of a standard compression path, and augment it with a\nsuitable pre-processing procedure, providing a compressed signal intended to\ncompensate the degradation without any post-filtering. Our approach originates\nfrom an intricate rate-distortion problem, optimizing the modifications to the\ninput image/video for reaching best end-to-end performance. We address this\nseemingly computationally intractable problem using the alternating direction\nmethod of multipliers (ADMM) approach, leading to a procedure in which a\nstandard compression technique is iteratively applied. We demonstrate the\nproposed method for adjusting HEVC image/video compression to compensate\npost-decompression visual effects due to a common type of displays.\nParticularly, we use our method to reduce motion-blur perceived while viewing\nvideo on LCD devices. The experiments establish our method as a leading\napproach for preprocessing high bit-rate compression to counterbalance a\npost-decompression degradation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:47:11 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 16:01:30 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Dar", "Yehuda", ""], ["Elad", "Michael", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1711.08118", "submitter": "Mohammad Saidur Rahman", "authors": "Mohammad Saidur Rahman, Ashfaqur Rahman", "title": "Channel Transition Invariant Fast Broadcasting Scheme", "comments": "2014 9th International Forum on Strategic Technology (IFOST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast broadcasting (FB) is a popular near video-on-demand system where a video\nis divided into equal size segments those are repeatedly transmitted over a\nnumber of channels following a pattern. For user satisfaction, it is required\nto reduce the initial user waiting time and client side buffer requirement at\nstreaming. Use of additional channels can achieve the objective. However, some\naugmentation is required to the basic FB scheme as it lacks any mechanism to\nrealise a well defined relationship among the segment sizes at channel\ntransition. Lack of correspondence between the segments causes intermediate\nwaiting for the clients while watching videos. Use of additional channel\nrequires additional bandwidth. In this paper, we propose a modified FB scheme\nthat achieves zero initial clients waiting time and provides a mechanism to\ncontrol client side buffer requirement at streaming without requiring\nadditional channels. We present several results to demonstrate the\neffectiveness of the proposed FB scheme over the existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 03:02:28 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Rahman", "Mohammad Saidur", ""], ["Rahman", "Ashfaqur", ""]]}, {"id": "1711.08571", "submitter": "Hamzeh Ghasemzadeh", "authors": "Hamzeh Ghasemzadeh, Mohammad H. Kayvanrad", "title": "Calibrated Audio Steganalysis", "comments": "7 pages, 4 figures, 3 tables, 2016 1st International Conference on\n  New Research Achievements in Electrical and Computer Engineering,\n  https://en.civilica.com/Paper-CBCONF01-CBCONF01_0107=Calibrated-Audio-Steganalysis.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration is a common practice in image steganalysis for extracting\nprominent features. Based on the idea of reembedding, a new set of calibrated\nfeatures for audio steganalysis applications are proposed. These features are\nextracted from a model that has maximum deviation from human auditory system\nand had been specifically designed for audio steganalysis. Ability of the\nproposed system is tested extensively. Simulations demonstrate that the\nproposed method can accurately detect the presence of hidden messages even in\nvery low embedding rates. Proposed method achieves an accuracy of 99.3%\n(StegHide@0.76% BPB) which is 9.5% higher than the previous R-MFCC based\nsteganalysis method.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 04:05:56 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 01:51:31 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ghasemzadeh", "Hamzeh", ""], ["Kayvanrad", "Mohammad H.", ""]]}, {"id": "1711.08690", "submitter": "Wenjie Pei", "authors": "Wenjie Pei and Hamdi Dibeklio\\u{g}lu and Tadas Baltru\\v{s}aitis and\n  David M.J. Tax", "title": "Attended End-to-end Architecture for Age Estimation from Facial\n  Expression Videos", "comments": "Accepted by Transactions on Image Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenges of age estimation from facial expression videos lie not\nonly in the modeling of the static facial appearance, but also in the capturing\nof the temporal facial dynamics. Traditional techniques to this problem focus\non constructing handcrafted features to explore the discriminative information\ncontained in facial appearance and dynamics separately. This relies on\nsophisticated feature-refinement and framework-design. In this paper, we\npresent an end-to-end architecture for age estimation, called Spatially-Indexed\nAttention Model (SIAM), which is able to simultaneously learn both the\nappearance and dynamics of age from raw videos of facial expressions.\nSpecifically, we employ convolutional neural networks to extract effective\nlatent appearance representations and feed them into recurrent networks to\nmodel the temporal dynamics. More importantly, we propose to leverage attention\nmodels for salience detection in both the spatial domain for each single image\nand the temporal domain for the whole video as well. We design a specific\nspatially-indexed attention mechanism among the convolutional layers to extract\nthe salient facial regions in each individual image, and a temporal attention\nlayer to assign attention weights to each frame. This two-pronged approach not\nonly improves the performance by allowing the model to focus on informative\nframes and facial areas, but it also offers an interpretable correspondence\nbetween the spatial facial regions as well as temporal frames, and the task of\nage estimation. We demonstrate the strong performance of our model in\nexperiments on a large, gender-balanced database with 400 subjects with ages\nspanning from 8 to 76 years. Experiments reveal that our model exhibits\nsignificant superiority over the state-of-the-art methods given sufficient\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:43:49 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 15:46:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pei", "Wenjie", ""], ["Dibeklio\u011flu", "Hamdi", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Tax", "David M. J.", ""]]}, {"id": "1711.08925", "submitter": "Alessandro Artusi PhD", "authors": "E. Sikudova, T. Pouli, A. Artusi, A. O. Akyuz, F. Banterle, Z. M.\n  Mazlumoglu and E. Reinhard", "title": "A Gamut-Mapping Framework for Color-Accurate Reproduction of HDR Images", "comments": null, "journal-ref": "IEEE Computer Graphics and Applications, Volume 36, Issue 4,\n  p.78-90, ISSN 0272-1716, July-August 2016", "doi": "10.1109/MCG.2015.116", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few tone mapping operators (TMOs) take color management into consideration,\nlimiting compression to luminance values only. This may lead to changes in\nimage chroma and hues which are typically managed with a post-processing step.\nHowever, current post-processing techniques for tone reproduction do not\nexplicitly consider the target display gamut. Gamut mapping on the other hand,\ndeals with mapping images from one color gamut to another, usually smaller,\ngamut but has traditionally focused on smaller scale, chromatic changes. In\nthis context, we present a novel gamut and tone management framework for\ncolor-accurate reproduction of high dynamic range (HDR) images, which is\nconceptually and computationally simple, parameter-free, and compatible with\nexisting TMOs. In the CIE LCh color space, we compress chroma to fit the gamut\nof the output color space. This prevents hue and luminance shifts while taking\ngamut boundaries into consideration. We also propose a compatible lightness\ncompression scheme that minimizes the number of color space conversions. Our\nresults show that our gamut management method effectively compresses the chroma\nof tone mapped images, respecting the target gamut and without reducing image\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 11:06:07 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sikudova", "E.", ""], ["Pouli", "T.", ""], ["Artusi", "A.", ""], ["Akyuz", "A. O.", ""], ["Banterle", "F.", ""], ["Mazlumoglu", "Z. M.", ""], ["Reinhard", "E.", ""]]}, {"id": "1711.09335", "submitter": "Jianhua Yang", "authors": "Jianhua Yang, Yun-Qing Shi, Edward K.Wong, Xiangui Kang", "title": "JPEG Steganalysis Based on DenseNet", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from the conventional deep learning work based on an images content\nin computer vision, deep steganalysis is an art to detect the secret\ninformation embedded in an image via deep learning, pose challenge of detection\nweak information invisible hidden in a host image thus learning in a very low\nsignal-to-noise (SNR) case. In this paper, we propose a 32- layer convolutional\nneural Networks (CNNs) in to improve the efficiency of preprocess and reuse the\nfeatures by concatenating all features from the previous layers with the same\nfeature- map size, thus improve the flow of information and gradient. The\nshared features and bottleneck layers further improve the feature propagation\nand reduce the CNN model parameters dramatically. Experimental results on the\nBOSSbase, BOWS2 and ImageNet datasets have showed that the proposed CNN\narchitecture can improve the performance and enhance the robustness. To further\nboost the detection accuracy, an ensemble architecture called as CNN-SCA-GFR is\nproposed, CNN-SCA- GFR is also the first work to combine the CNN architecture\nand conventional method in the JPEG domain. Experiments show that it can\nfurther lower detection errors. Compared with the state-of-the-art method XuNet\n[1] on BOSSbase, the proposed CNN-SCA-GFR architecture can reduce detection\nerror rate by 5.67% for 0.1 bpnzAC and by 4.41% for 0.4 bpnzAC while the number\nof training parameters in CNN is only 17% of what used by XuNet. It also\ndecreases the detection errors from the conventional method SCA-GFR by 7.89%\nfor 0.1 bpnzAC and 8.06% for 0.4 bpnzAC, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 05:10:48 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 14:14:09 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 01:35:48 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Yang", "Jianhua", ""], ["Shi", "Yun-Qing", ""], ["Wong", "Edward K.", ""], ["Kang", "Xiangui", ""]]}, {"id": "1711.11115", "submitter": "Naimul Khan", "authors": "Randy Tan, Naimul Khan, Ling Guan", "title": "Real-Time System for Human Activity Analysis", "comments": "Accepted at IEEE Symposium on Multimedia (ISM) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time human activity analysis system, where a user's\nactivity can be quantiatively evaluated with respect to a ground truth\nrecording. We use two Kinects to solve the ptorblem of self-occlusion through\nextraction optimal joint positions using Singular Value Decomposition (SVD) and\nSequential Quadratic Programming (SQP). Incremental Dynamic Time Warping (IDTW)\nis used to compare the user and expert (ground truth) to quantiatively score\nthe user's performance. Furthermore, the user's performance is displayed\nthrough a visual feedback system, where colors on the skeleton represent the\nuser's score. Our experiements use a motion capture suit as ground truth to\ncompare our dual Kinect setup to a single Kinect. We also show that with out\nvisual feedback method, users gain statistically significant boost to learning\nas opposed to watching a simple video.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 21:35:56 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Tan", "Randy", ""], ["Khan", "Naimul", ""], ["Guan", "Ling", ""]]}, {"id": "1711.11326", "submitter": "Alessandro Artusi PhD", "authors": "Alessandro Artusi, Thomas Richter, Touradj Ebrahimi, Rafal K. Mantiuk", "title": "High Dynamic Range Imaging Technology", "comments": "Lecture Notes", "journal-ref": "IEEE Signal Processing Magazine ( Volume: 34, Issue: 5, Sept. 2017\n  )", "doi": "10.1109/MSP.2017.2716957", "report-no": null, "categories": "cs.GR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this lecture note, we describe high dynamic range (HDR) imaging systems;\nsuch systems are able to represent luminances of much larger brightness and,\ntypically, also a larger range of colors than conventional standard dynamic\nrange (SDR) imaging systems. The larger luminance range greatly improve the\noverall quality of visual content, making it appears much more realistic and\nappealing to observers. HDR is one of the key technologies of the future\nimaging pipeline, which will change the way the digital visual content is\nrepresented and manipulated today.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 11:38:11 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Artusi", "Alessandro", ""], ["Richter", "Thomas", ""], ["Ebrahimi", "Touradj", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "1711.11368", "submitter": "Paul Vickers", "authors": "Paul Vickers and Robert H\\\"oldrich", "title": "Direct Segmented Sonification of Characteristic Features of the Data\n  Domain", "comments": "10 pages, 12 figures, pre-print of submitted journal article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sonification and audification create auditory displays of datasets.\nAudification translates data points into digital audio samples and the auditory\ndisplay's duration is determined by the playback rate. Like audification,\nauditory graphs maintain the temporal relationships of data while using\nparameter mappings (typically data-to-frequency) to represent the ordinate\nvalues. Such direct approaches have the advantage of presenting the data stream\n`as is' without the imposed interpretations or accentuation of particular\nfeatures found in indirect approaches. However, datasets can often be\nsubdivided into short non-overlapping variable length segments that each\nencapsulate a discrete unit of domain-specific significant information and\ncurrent direct approaches cannot represent these. We present Direct Segmented\nSonification (DSSon) for highlighting the segments' data distributions as\nindividual sonic events. Using domain knowledge to segment data, DSSon presents\nsegments as discrete auditory gestalts while retaining the overall temporal\nregime and relationships of the dataset. The method's structural decoupling\nfrom the sound stream's formation means playback speed is independent of the\nindividual sonic event durations, thereby offering highly flexible time\ncompression/stretching to allow zooming into or out of the data. Demonstrated\nby three models applied to biomechanical data, DSSon displays high directness,\nletting the data `speak' for themselves.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:13:14 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 13:36:13 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Vickers", "Paul", ""], ["H\u00f6ldrich", "Robert", ""]]}, {"id": "1711.11565", "submitter": "Weipeng He", "authors": "Weipeng He, Petr Motlicek and Jean-Marc Odobez", "title": "Deep Neural Networks for Multiple Speaker Detection and Localization", "comments": "Accepted for ICRA 2018", "journal-ref": "2018 IEEE International Conference on Robotics and Automation\n  (ICRA), Brisbane, Australia, 2018, pp. 74-79", "doi": "10.1109/ICRA.2018.8461267", "report-no": null, "categories": "cs.SD cs.AI cs.MM cs.RO eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use neural networks for simultaneous detection and localization\nof multiple sound sources in human-robot interaction. In contrast to\nconventional signal processing techniques, neural network-based sound source\nlocalization methods require fewer strong assumptions about the environment.\nPrevious neural network-based methods have been focusing on localizing a single\nsound source, which do not extend to multiple sources in terms of detection and\nlocalization. In this paper, we thus propose a likelihood-based encoding of the\nnetwork output, which naturally allows the detection of an arbitrary number of\nsources. In addition, we investigate the use of sub-band cross-correlation\ninformation as features for better localization in sound mixtures, as well as\nthree different network architectures based on different motivations.\nExperiments on real data recorded from a robot show that our proposed methods\nsignificantly outperform the popular spatial spectrum-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:35:22 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 17:27:05 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 09:04:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["He", "Weipeng", ""], ["Motlicek", "Petr", ""], ["Odobez", "Jean-Marc", ""]]}]