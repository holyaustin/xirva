[{"id": "1907.00042", "submitter": "Wei Cai", "authors": "Tengfei Wang and Shuyi Zhang and Xiao Wu and Wei Cai", "title": "Rhythm Dungeon: A Blockchain-based Music Roguelike Game", "comments": null, "journal-ref": "2019 Foundation of Digital Games Demos (FDG 2019 DEMO), San Luis\n  Obispo, California, USA, August 26-30, 2019", "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rhythm Dungeon is a rhythm game which leverages the blockchain as a shared\nopen database. During the gaming session, the player explores a roguelike\ndungeon by inputting specific sequences in time to music rhythm. By integrating\nsmart contract to the game program, the enemies through the venture are\ngenerated from other games which share the identical blockchain. On the other\nhand, the player may upload their characters at the end of their journey, so\nthat their own character may appear in other games and make an influence.\nRhythm Dungeon is designed and implemented to show the potential of\ndecentralized gaming experience, which utilizes the blockchain to provide\nasynchronous interactions among massive players.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 19:05:53 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Tengfei", ""], ["Zhang", "Shuyi", ""], ["Wu", "Xiao", ""], ["Cai", "Wei", ""]]}, {"id": "1907.00178", "submitter": "Alexander Lerch", "authors": "Alexander Lerch and Claire Arthur and Ashis Pati and Siddharth\n  Gururani", "title": "Music Performance Analysis: A Survey", "comments": "To be published in: Proceedings of the International Society for\n  Music Information Retrieval Conference (ISMIR), Delft, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music Information Retrieval (MIR) tends to focus on the analysis of audio\nsignals. Often, a single music recording is used as representative of a \"song\"\neven though different performances of the same song may reveal different\nproperties. A performance is distinct in many ways from a (arguably more\nabstract) representation of a \"song,\" \"piece,\" or musical score. The\ncharacteristics of the (recorded) performance -- as opposed to the score or\nmusical idea -- can have a major impact on how a listener perceives music. The\nanalysis of music performance, however, has been traditionally only a\nperipheral topic for the MIR research community. This paper surveys the field\nof Music Performance Analysis (MPA) from various perspectives, discusses its\nsignificance to the field of MIR, and points out opportunities for future\nresearch in this field.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 10:43:27 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lerch", "Alexander", ""], ["Arthur", "Claire", ""], ["Pati", "Ashis", ""], ["Gururani", "Siddharth", ""]]}, {"id": "1907.00193", "submitter": "Debin Meng", "authors": "Debin Meng, Xiaojiang Peng, Kai Wang, Yu Qiao", "title": "Frame attention networks for facial expression recognition in videos", "comments": "Accepted by ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video-based facial expression recognition aims to classify a given video\ninto several basic emotions. How to integrate facial features of individual\nframes is crucial for this task. In this paper, we propose the Frame Attention\nNetworks (FAN), to automatically highlight some discriminative frames in an\nend-to-end framework. The network takes a video with a variable number of face\nimages as its input and produces a fixed-dimension representation. The whole\nnetwork is composed of two modules. The feature embedding module is a deep\nConvolutional Neural Network (CNN) which embeds face images into feature\nvectors. The frame attention module learns multiple attention weights which are\nused to adaptively aggregate the feature vectors to form a single\ndiscriminative video representation. We conduct extensive experiments on CK+\nand AFEW8.0 datasets. Our proposed FAN shows superior performance compared to\nother CNN based methods and achieves state-of-the-art performance on CK+.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 12:11:44 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 07:21:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Meng", "Debin", ""], ["Peng", "Xiaojiang", ""], ["Wang", "Kai", ""], ["Qiao", "Yu", ""]]}, {"id": "1907.00483", "submitter": "Amit Kumar Jaiswal", "authors": "Amit Kumar Jaiswal, Haiming Liu and Ingo Frommholz", "title": "Effects of Foraging in Personalized Content-based Image Recommendation", "comments": "Accepted in Proceedings of the the 2nd International Workshop on\n  Explainable Recommendation and Search (EARS) at SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge of recommender systems is to help users locating\ninteresting items. Personalized recommender systems have become very popular as\nthey attempt to predetermine the needs of users and provide them with\nrecommendations to personalize their navigation. However, few studies have\naddressed the question of what drives the users' attention to specific content\nwithin the collection and what influences the selection of interesting items.\nTo this end, we employ the lens of Information Foraging Theory (IFT) to image\nrecommendation to demonstrate how the user could utilize visual bookmarks to\nlocate interesting images. We investigate a personalized content-based image\nrecommendation system to understand what affects user attention by reinforcing\nvisual attention cues based on IFT. We further find that visual bookmarks\n(cues) lead to a stronger scent of the recommended image collection. Our\nevaluation is based on the Pinterest image collection.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:16:32 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 12:43:53 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Jaiswal", "Amit Kumar", ""], ["Liu", "Haiming", ""], ["Frommholz", "Ingo", ""]]}, {"id": "1907.00971", "submitter": "Philippe Esling", "authors": "Philippe Esling, Naotake Masuda, Adrien Bardet, Romeo Despres, Axel\n  Chemla--Romeu-Santos", "title": "Universal audio synthesizer control with normalizing flows", "comments": "DaFX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ubiquity of sound synthesizers has reshaped music production and even\nentirely defined new music genres. However, the increasing complexity and\nnumber of parameters in modern synthesizers make them harder to master. Hence,\nthe development of methods allowing to easily create and explore with\nsynthesizers is a crucial need. Here, we introduce a novel formulation of audio\nsynthesizer control. We formalize it as finding an organized latent audio space\nthat represents the capabilities of a synthesizer, while constructing an\ninvertible mapping to the space of its parameters. By using this formulation,\nwe show that we can address simultaneously automatic parameter inference,\nmacro-control learning and audio-based preset exploration within a single\nmodel. To solve this new formulation, we rely on Variational Auto-Encoders\n(VAE) and Normalizing Flows (NF) to organize and map the respective auditory\nand parameter spaces. We introduce the disentangling flows, which allow to\nperform the invertible mapping between separate latent spaces, while steering\nthe organization of some latent dimensions to match target variation factors by\nsplitting the objective as partial density evaluation. We evaluate our proposal\nagainst a large set of baseline models and show its superiority in both\nparameter inference and audio reconstruction. We also show that the model\ndisentangles the major factors of audio variations as latent dimensions, that\ncan be directly used as macro-parameters. We also show that our model is able\nto learn semantic controls of a synthesizer by smoothly mapping to its\nparameters. Finally, we discuss the use of our model in creative applications\nand its real-time implementation in Ableton Live\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:49:07 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Esling", "Philippe", ""], ["Masuda", "Naotake", ""], ["Bardet", "Adrien", ""], ["Despres", "Romeo", ""], ["Chemla--Romeu-Santos", "Axel", ""]]}, {"id": "1907.01081", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u, Rafael F. Schaefer, and Gerhard Kramer", "title": "Private Authentication with Physical Identifiers Through Broadcast\n  Channel Measurements", "comments": "Longer version of the published paper", "journal-ref": "IEEE Information Theory Workshop (2019) 1-5", "doi": "10.1109/ITW44776.2019.8989327", "report-no": null, "categories": "cs.IT cs.CR cs.MM eess.SP math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic model for key agreement with biometric or physical identifiers is\nextended to include measurements of a hidden source through a general broadcast\nchannel (BC). An inner bound for strong secrecy, maximum key rate, and minimum\nprivacy-leakage and database-storage rates is proposed. The inner bound is\nshown to be tight for physically-degraded and less-noisy BCs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 21:28:49 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 13:11:00 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Schaefer", "Rafael F.", ""], ["Kramer", "Gerhard", ""]]}, {"id": "1907.01154", "submitter": "Jon McCormack", "authors": "Patrick Hutchings and Jon McCormack", "title": "Adaptive Music Composition for Games", "comments": "Preprint. Accepted for publication in IEEE Transactions on Games,\n  2019", "journal-ref": null, "doi": "10.1109/TG.2019.2921979", "report-no": null, "categories": "cs.MM cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of music that adapts dynamically to content and actions has an\nimportant role in building more immersive, memorable and emotive game\nexperiences. To date, the development of adaptive music systems for video games\nis limited by both the nature of algorithms used for real-time music generation\nand the limited modelling of player action, game world context and emotion in\ncurrent games. We propose that these issues must be addressed in tandem for the\nquality and flexibility of adaptive game music to significantly improve.\nCognitive models of knowledge organisation and emotional affect are integrated\nwith multi-modal, multi-agent composition techniques to produce a novel\nAdaptive Music System (AMS). The system is integrated into two stylistically\ndistinct games. Gamers reported an overall higher immersion and correlation of\nmusic with game-world concepts with the AMS than with the original game\nsoundtracks in both games.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 04:10:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Hutchings", "Patrick", ""], ["McCormack", "Jon", ""]]}, {"id": "1907.01607", "submitter": "Hsia Liang", "authors": "Xia Liang and Junmin Wu and Yan Yin", "title": "MIDI-Sandwich: Multi-model Multi-task Hierarchical Conditional VAE-GAN\n  networks for Symbolic Single-track Music Generation", "comments": "cast KSEM2019 on May 3, 2019 (weak rejected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing neural network models for music generation explore how to\ngenerate music bars, then directly splice the music bars into a song. However,\nthese methods do not explore the relationship between the bars, and the\nconnected song as a whole has no musical form structure and sense of musical\ndirection. To address this issue, we propose a Multi-model Multi-task\nHierarchical Conditional VAE-GAN (Variational Autoencoder-Generative\nadversarial networks) networks, named MIDI-Sandwich, which combines musical\nknowledge, such as musical form, tonic, and melodic motion. The MIDI-Sandwich\nhas two submodels: Hierarchical Conditional Variational Autoencoder (HCVAE) and\nHierarchical Conditional Generative Adversarial Network (HCGAN). The HCVAE uses\nhierarchical structure. The underlying layer of HCVAE uses Local Conditional\nVariational Autoencoder (L-CVAE) to generate a music bar which is pre-specified\nby the First and Last Notes (FLN). The upper layer of HCVAE uses Global\nVariational Autoencoder(G-VAE) to analyze the latent vector sequence generated\nby the L-CVAE encoder, to explore the musical relationship between the bars,\nand to produce the song pieced together by multiple music bars generated by the\nL-CVAE decoder, which makes the song both have musical structure and sense of\ndirection. At the same time, the HCVAE shares a part of itself with the HCGAN\nto further improve the performance of the generated music. The MIDI-Sandwich is\nvalidated on the Nottingham dataset and is able to generate a single-track\nmelody sequence (17x8 beats), which is superior to the length of most of the\ngenerated models (8 to 32 beats). Meanwhile, by referring to the experimental\nmethods of many classical kinds of literature, the quality evaluation of the\ngenerated music is performed. The above experiments prove the validity of the\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 19:55:33 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 06:59:33 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Liang", "Xia", ""], ["Wu", "Junmin", ""], ["Yin", "Yan", ""]]}, {"id": "1907.01886", "submitter": "Jia Liu", "authors": "Jia Liu, Yan Ke, Yu Lei, Zhuo Zhang, Jun Li, Peng Luo, Minqing Zhang\n  and Xiaoyuan Yang", "title": "Recent Advances of Image Steganography with Generative Adversarial\n  Networks", "comments": "39 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the Generative Adversarial Network (GAN) which\nproposed in 2014 has achieved great success. GAN has achieved many research\nresults in the field of computer vision and natural language processing. Image\nsteganography is dedicated to hiding secret messages in digital images, and has\nachieved the purpose of covert communication. Recently, research on image\nsteganography has demonstrated great potential for using GAN and neural\nnetworks. In this paper we review different strategies for steganography such\nas cover modification, cover selection and cover synthesis by GANs, and discuss\nthe characteristics of these methods as well as evaluation metrics and provide\nsome possible future research directions in image steganography.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:57:46 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Liu", "Jia", ""], ["Ke", "Yan", ""], ["Lei", "Yu", ""], ["Zhang", "Zhuo", ""], ["Li", "Jun", ""], ["Luo", "Peng", ""], ["Zhang", "Minqing", ""], ["Yang", "Xiaoyuan", ""]]}, {"id": "1907.01985", "submitter": "Keyan Ding", "authors": "Keyan Ding, Kede Ma, Shiqi Wang", "title": "Intrinsic Image Popularity Assessment", "comments": "Accepted by ACM Multimedia 2019", "journal-ref": "Proceedings of the 27th ACM International Conference on\n  Multimedia, 2019", "doi": "10.1145/3343031.3351007", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of research in automatic image popularity assessment (IPA) is to\ndevelop computational models that can accurately predict the potential of a\nsocial image to go viral on the Internet. Here, we aim to single out the\ncontribution of visual content to image popularity, i.e., intrinsic image\npopularity. Specifically, we first describe a probabilistic method to generate\nmassive popularity-discriminable image pairs, based on which the first\nlarge-scale image database for intrinsic IPA (I$^2$PA) is established. We then\ndevelop computational models for I$^2$PA based on deep neural networks,\noptimizing for ranking consistency with millions of popularity-discriminable\nimage pairs. Experiments on Instagram and other social platforms demonstrate\nthat the optimized model performs favorably against existing methods, exhibits\nreasonable generalizability on different databases, and even surpasses\nhuman-level performance on Instagram. In addition, we conduct a psychophysical\nexperiment to analyze various aspects of human behavior in I$^2$PA.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:15:21 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 15:38:50 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ding", "Keyan", ""], ["Ma", "Kede", ""], ["Wang", "Shiqi", ""]]}, {"id": "1907.02663", "submitter": "Weicheng Cai", "authors": "Weicheng Cai, Haiwei Wu, Danwei Cai, and Ming Li", "title": "The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data\n  Augmentation, Feature Representation, Classification, and Fusion", "comments": "Accepted for INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CR cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our DKU replay detection system for the ASVspoof 2019\nchallenge. The goal is to develop spoofing countermeasure for automatic speaker\nrecognition in physical access scenario. We leverage the countermeasure system\npipeline from four aspects, including the data augmentation, feature\nrepresentation, classification, and fusion. First, we introduce an\nutterance-level deep learning framework for anti-spoofing. It receives the\nvariable-length feature sequence and outputs the utterance-level scores\ndirectly. Based on the framework, we try out various kinds of input feature\nrepresentations extracted from either the magnitude spectrum or phase spectrum.\nBesides, we also perform the data augmentation strategy by applying the speed\nperturbation on the raw waveform. Our best single system employs a residual\nneural network trained by the speed-perturbed group delay gram. It achieves EER\nof 1.04% on the development set, as well as EER of 1.08% on the evaluation set.\nFinally, using the simple average score from several single systems can further\nimprove the performance. EER of 0.24% on the development set and 0.66% on the\nevaluation set is obtained for our primary system.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 03:00:05 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Cai", "Weicheng", ""], ["Wu", "Haiwei", ""], ["Cai", "Danwei", ""], ["Li", "Ming", ""]]}, {"id": "1907.02665", "submitter": "Weixia Zhang", "authors": "Weixia Zhang and Kede Ma and Jia Yan and Dexiang Deng and Zhou Wang", "title": "Blind Image Quality Assessment Using A Deep Bilinear Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2018.2886771", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep bilinear model for blind image quality assessment (BIQA)\nthat handles both synthetic and authentic distortions. Our model consists of\ntwo convolutional neural networks (CNN), each of which specializes in one\ndistortion scenario. For synthetic distortions, we pre-train a CNN to classify\nimage distortion type and level, where we enjoy large-scale training data. For\nauthentic distortions, we adopt a pre-trained CNN for image classification. The\nfeatures from the two CNNs are pooled bilinearly into a unified representation\nfor final quality prediction. We then fine-tune the entire model on target\nsubject-rated databases using a variant of stochastic gradient descent.\nExtensive experiments demonstrate that the proposed model achieves superior\nperformance on both synthetic and authentic databases. Furthermore, we verify\nthe generalizability of our method on the Waterloo Exploration Database using\nthe group maximum differentiation competition.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 03:35:35 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Kede", ""], ["Yan", "Jia", ""], ["Deng", "Dexiang", ""], ["Wang", "Zhou", ""]]}, {"id": "1907.02670", "submitter": "Jeong Choi", "authors": "Jeong Choi, Jongpil Lee, Jiyoung Park, and Juhan Nam", "title": "Zero-shot Learning for Audio-based Music Classification and Tagging", "comments": "20th International Society for Music Information Retrieval Conference\n  (ISMIR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-based music classification and tagging is typically based on\ncategorical supervised learning with a fixed set of labels. This intrinsically\ncannot handle unseen labels such as newly added music genres or semantic words\nthat users arbitrarily choose for music retrieval. Zero-shot learning can\naddress this problem by leveraging an additional semantic space of labels where\nside information about the labels is used to unveil the relationship between\neach other. In this work, we investigate the zero-shot learning in the music\ndomain and organize two different setups of side information. One is using\nhuman-labeled attribute information based on Free Music Archive and\nOpenMIC-2018 datasets. The other is using general word semantic information\nbased on Million Song Dataset and Last.fm tag annotations. Considering a music\ntrack is usually multi-labeled in music classification and tagging datasets, we\nalso propose a data split scheme and associated evaluation settings for the\nmulti-label zero-shot learning. Finally, we report experimental results and\ndiscuss the effectiveness and new possibilities of zero-shot learning in the\nmusic domain.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 04:19:37 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 11:02:36 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Choi", "Jeong", ""], ["Lee", "Jongpil", ""], ["Park", "Jiyoung", ""], ["Nam", "Juhan", ""]]}, {"id": "1907.02704", "submitter": "Vincent Labatut", "authors": "Vincent Labatut (LIA), Xavier Bost (LIA)", "title": "Extraction and Analysis of Fictional Character Networks: A Survey", "comments": null, "journal-ref": "ACM Computing Surveys, Association for Computing Machinery, 2019,\n  52 (5), pp.89", "doi": "10.1145/3344548", "report-no": null, "categories": "cs.SI cs.CL cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:27:31 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 09:00:13 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 12:51:45 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 13:01:30 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Labatut", "Vincent", "", "LIA"], ["Bost", "Xavier", "", "LIA"]]}, {"id": "1907.02821", "submitter": "Lia Morra", "authors": "Lia Morra and Fabrizio Lamberti", "title": "Benchmarking unsupervised near-duplicate image detection", "comments": "Accepted for publication in Expert Systems with Applications", "journal-ref": "Expert Systems with Applications, online first, 2019", "doi": "10.1016/j.eswa.2019.05.002", "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised near-duplicate detection has many practical applications ranging\nfrom social media analysis and web-scale retrieval, to digital image forensics.\nIt entails running a threshold-limited query on a set of descriptors extracted\nfrom the images, with the goal of identifying all possible near-duplicates,\nwhile limiting the false positives due to visually similar images. Since the\nrate of false alarms grows with the dataset size, a very high specificity is\nthus required, up to $1 - 10^{-9}$ for realistic use cases; this important\nrequirement, however, is often overlooked in literature. In recent years,\ndescriptors based on deep convolutional neural networks have matched or\nsurpassed traditional feature extraction methods in content-based image\nretrieval tasks. To the best of our knowledge, ours is the first attempt to\nestablish the performance range of deep learning-based descriptors for\nunsupervised near-duplicate detection on a range of datasets, encompassing a\nbroad spectrum of near-duplicate definitions. We leverage both established and\nnew benchmarks, such as the Mir-Flick Near-Duplicate (MFND) dataset, in which a\nknown ground truth is provided for all possible pairs over a general, large\nscale image collection. To compare the specificity of different descriptors, we\nreduce the problem of unsupervised detection to that of binary classification\nof near-duplicate vs. not-near-duplicate images. The latter can be conveniently\ncharacterized using Receiver Operating Curve (ROC). Our findings in general\nfavor the choice of fine-tuning deep convolutional networks, as opposed to\nusing off-the-shelf features, but differences at high specificity settings\ndepend on the dataset and are often small. The best performance was observed on\nthe MFND benchmark, achieving 96\\% sensitivity at a false positive rate of\n$1.43 \\times 10^{-6}$.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 09:08:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Morra", "Lia", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "1907.03030", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, B.V.K Vijaya Kumar, Chao Yang, Qingming Tang, Jane You", "title": "Dependency-aware Attention Control for Unconstrained Face Recognition\n  with Image Sets", "comments": "Fixed the unreadable code in CVF version. arXiv admin note: text\n  overlap with arXiv:1707.00130 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets the problem of image set-based face verification and\nidentification. Unlike traditional single media (an image or video) setting, we\nencounter a set of heterogeneous contents containing orderless images and\nvideos. The importance of each image is usually considered either equal or\nbased on their independent quality assessment. How to model the relationship of\norderless images within a set remains a challenge. We address this problem by\nformulating it as a Markov Decision Process (MDP) in the latent space.\nSpecifically, we first present a dependency-aware attention control (DAC)\nnetwork, which resorts to actor-critic reinforcement learning for sequential\nattention decision of each image embedding to fully exploit the rich\ncorrelation cues among the unordered images. Moreover, we introduce its\nsample-efficient variant with off-policy experience replay to speed up the\nlearning process. The pose-guided representation scheme can further boost the\nperformance at the extremes of the pose variation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 21:40:56 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Kumar", "B. V. K Vijaya", ""], ["Yang", "Chao", ""], ["Tang", "Qingming", ""], ["You", "Jane", ""]]}, {"id": "1907.03240", "submitter": "Li Jiacheng", "authors": "Jiacheng Li, Haizhou Shi, Siliang Tang, Fei Wu, Yueting Zhuang", "title": "Informative Visual Storytelling with Cross-modal Rules", "comments": "9 pages, to appear in ACM Multimedia 2019", "journal-ref": null, "doi": "10.1145/3343031.3350918", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods in the Visual Storytelling field often suffer from the\nproblem of generating general descriptions, while the image contains a lot of\nmeaningful contents remaining unnoticed. The failure of informative story\ngeneration can be concluded to the model's incompetence of capturing enough\nmeaningful concepts. The categories of these concepts include entities,\nattributes, actions, and events, which are in some cases crucial to grounded\nstorytelling. To solve this problem, we propose a method to mine the\ncross-modal rules to help the model infer these informative concepts given\ncertain visual input. We first build the multimodal transactions by\nconcatenating the CNN activations and the word indices. Then we use the\nassociation rule mining algorithm to mine the cross-modal rules, which will be\nused for the concept inference. With the help of the cross-modal rules, the\ngenerated stories are more grounded and informative. Besides, our proposed\nmethod holds the advantages of interpretation, expandability, and\ntransferability, indicating potential for wider application. Finally, we\nleverage these concepts in our encoder-decoder framework with the attention\nmechanism. We conduct several experiments on the VIsual StoryTelling~(VIST)\ndataset, the results of which demonstrate the effectiveness of our approach in\nterms of both automatic metrics and human evaluation. Additional experiments\nare also conducted showing that our mined cross-modal rules as additional\nknowledge helps the model gain better performance when trained on a small\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 07:41:59 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 05:24:15 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Li", "Jiacheng", ""], ["Shi", "Haizhou", ""], ["Tang", "Siliang", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1907.03246", "submitter": "Wei Song", "authors": "Yan Wang, Wei Song, Giancarlo Fortino, Lizhe Qi, Wenqiang Zhang,\n  Antonio Liotta", "title": "An Experimental-based Review of Image Enhancement and Image Restoration\n  Methods for Underwater Imaging", "comments": "19", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater images play a key role in ocean exploration, but often suffer from\nsevere quality degradation due to light absorption and scattering in water\nmedium. Although major breakthroughs have been made recently in the general\narea of image enhancement and restoration, the applicability of new methods for\nimproving the quality of underwater images has not specifically been captured.\nIn this paper, we review the image enhancement and restoration methods that\ntackle typical underwater image impairments, including some extreme\ndegradations and distortions. Firstly, we introduce the key causes of quality\nreduction in underwater images, in terms of the underwater image formation\nmodel (IFM). Then, we review underwater restoration methods, considering both\nthe IFM-free and the IFM-based approaches. Next, we present an\nexperimental-based comparative evaluation of state-of-the-art IFM-free and\nIFM-based methods, considering also the prior-based parameter estimation\nalgorithms of the IFM-based methods, using both subjective and objective\nanalysis (the used code is freely available at\nhttps://github.com/wangyanckxx/Single-Underwater-Image-Enhancement-and-Color-Restoration).\nStarting from this study, we pinpoint the key shortcomings of existing methods,\ndrawing recommendations for future research in this area. Our review of\nunderwater image enhancement and restoration provides researchers with the\nnecessary background to appreciate challenges and opportunities in this\nimportant field.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 08:22:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Wang", "Yan", ""], ["Song", "Wei", ""], ["Fortino", "Giancarlo", ""], ["Qi", "Lizhe", ""], ["Zhang", "Wenqiang", ""], ["Liotta", "Antonio", ""]]}, {"id": "1907.03698", "submitter": "Ts\\`i-U\\'i \\.Ik", "authors": "Yu-Chuan Huang, I-No Liao, Ching-Hsuan Chen, Ts\\`i-U\\'i \\.Ik, Wen-Chih\n  Peng", "title": "TrackNet: A Deep Learning Network for Tracking High-speed and Tiny\n  Objects in Sports Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ball trajectory data are one of the most fundamental and useful information\nin the evaluation of players' performance and analysis of game strategies.\nAlthough vision-based object tracking techniques have been developed to analyze\nsport competition videos, it is still challenging to recognize and position a\nhigh-speed and tiny ball accurately. In this paper, we develop a deep learning\nnetwork, called TrackNet, to track the tennis ball from broadcast videos in\nwhich the ball images are small, blurry, and sometimes with afterimage tracks\nor even invisible. The proposed heatmap-based deep learning network is trained\nto not only recognize the ball image from a single frame but also learn flying\npatterns from consecutive frames. TrackNet takes images with a size of\n$640\\times360$ to generate a detection heatmap from either a single frame or\nseveral consecutive frames to position the ball and can achieve high precision\neven on public domain videos. The network is evaluated on the video of the\nmen's singles final at the 2017 Summer Universiade, which is available on\nYouTube. The precision, recall, and F1-measure of TrackNet reach $99.7\\%$,\n$97.3\\%$, and $98.5\\%$, respectively. To prevent overfitting, 9 additional\nvideos are partially labeled together with a subset from the previous dataset\nto implement 10-fold cross-validation, and the precision, recall, and\nF1-measure are $95.3\\%$, $75.7\\%$, and $84.3\\%$, respectively. A conventional\nimage processing algorithm is also implemented to compare with TrackNet. Our\nexperiments indicate that TrackNet outperforms conventional method by a big\nmargin and achieves exceptional ball tracking performance. The dataset and demo\nvideo are available at https://nol.cs.nctu.edu.tw/ndo3je6av9/.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 16:08:43 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Huang", "Yu-Chuan", ""], ["Liao", "I-No", ""], ["Chen", "Ching-Hsuan", ""], ["\u0130k", "Ts\u00ec-U\u00ed", ""], ["Peng", "Wen-Chih", ""]]}, {"id": "1907.03842", "submitter": "Anastasia Zvezdakova", "authors": "Anastasia Zvezdakova, Dmitriy Kulikov, Denis Kondranin, Dmitriy\n  Vatolin", "title": "Barriers towards no-reference metrics application to compressed video\n  quality analysis: on the example of no-reference metric NIQE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyses the application of no-reference metric NIQE to the task\nof video-codec comparison. A number of issues in the metric behaviour on videos\nwas detected and described. The metric has outlying scores on black and\nsolid-coloured frames. The proposed averaging technique for metric quality\nscores helped to improve the results in some cases. Also, NIQE has low-quality\nscores for videos with detailed textures and higher scores for videos of lower\nbitrates due to the blurring of these textures after compression. Although NIQE\nshowed natural results for many tested videos, it is not universal and\ncurrently can not be used for video-codec comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:07:16 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 14:30:22 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zvezdakova", "Anastasia", ""], ["Kulikov", "Dmitriy", ""], ["Kondranin", "Denis", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1907.04025", "submitter": "Erwin Quiring", "authors": "Erwin Quiring, Matthias Kirchner, Konrad Rieck", "title": "On the Security and Applicability of Fragile Camera Fingerprints", "comments": "ESORICS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera sensor noise is one of the most reliable device characteristics in\ndigital image forensics, enabling the unique linkage of images to digital\ncameras. This so-called camera fingerprint gives rise to different\napplications, such as image forensics and authentication. However, if images\nare publicly available, an adversary can estimate the fingerprint from her\nvictim and plant it into spurious images. The concept of fragile camera\nfingerprints addresses this attack by exploiting asymmetries in data access:\nWhile the camera owner will always have access to a full fingerprint from\nuncompressed images, the adversary has typically access to compressed images\nand thus only to a truncated fingerprint. The security of this defense,\nhowever, has not been systematically explored yet. This paper provides the\nfirst comprehensive analysis of fragile camera fingerprints under attack. A\nseries of theoretical and practical tests demonstrate that fragile camera\nfingerprints allow a reliable device identification for common compression\nlevels in an adversarial environment.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 07:31:23 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Quiring", "Erwin", ""], ["Kirchner", "Matthias", ""], ["Rieck", "Konrad", ""]]}, {"id": "1907.04362", "submitter": "Yang Yang", "authors": "Yang Yang", "title": "BASN -- Learning Steganography with Binary Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secret information sharing through image carrier has aroused much research\nattention in recent years with images' growing domination on the Internet and\nmobile applications. However, with the booming trend of convolutional neural\nnetworks, image steganography is facing a more significant challenge from\nneural-network-automated tasks. To improve the security of image steganography\nand minimize task result distortion, models must maintain the feature maps\ngenerated by task-specific networks being irrelative to any hidden information\nembedded in the carrier. This paper introduces a binary attention mechanism\ninto image steganography to help alleviate the security issue, and in the\nmeanwhile, increase embedding payload capacity. The experimental results show\nthat our method has the advantage of high payload capacity with little feature\nmap distortion and still resist detection by state-of-the-art image\nsteganalysis algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:33:51 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Yang", "Yang", ""]]}, {"id": "1907.04470", "submitter": "Benjamin Genchel", "authors": "Richard Savery, Benjamin Genchel, Jason Smith, Anthony Caulkins, Molly\n  Jones, Anna Savery", "title": "Learning from History: Recreating and Repurposing Sister Harriet\n  Padberg's Computer Composed Canon and Free Fugue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Harriet Padberg wrote Computer-Composed Canon and Free Fugue as part of her\n1964 dissertation in Mathematics and Music at Saint Louis University. This\nprogram is one of the earliest examples of text-to-music software and\nalgorithmic composition, which are areas of great interest in the present-day\nfield of music technology. This paper aims to analyze the technological\ninnovation, aesthetic design process, and impact of Harriet Padberg's original\n1964 thesis as well as the design of a modern recreation and utilization, in\norder to gain insight to the nature of revisiting older works. Here, we present\nour open source recreation of Padberg's program with a modern interface and,\nthrough its use as an artistic tool by three composers, show how historical\nworks can be effectively used for new creative purposes in contemporary\ncontexts. Not Even One by Molly Jones draws on the historical and social\nsignificance of Harriet Padberg through using her program in a piece about the\nlack of representation of women judges in composition competitions. Brevity by\nAnna Savery utilizes the original software design as a composition tool, and\nThe Padberg Piano by Anthony Caulkins uses the melodic generation of the\noriginal to create a software instrument.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 00:44:27 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Savery", "Richard", ""], ["Genchel", "Benjamin", ""], ["Smith", "Jason", ""], ["Caulkins", "Anthony", ""], ["Jones", "Molly", ""], ["Savery", "Anna", ""]]}, {"id": "1907.04476", "submitter": "Yuxin Peng", "authors": "Xiangteng He, Yuxin Peng and Liu Xie", "title": "A New Benchmark and Approach for Fine-grained Cross-media Retrieval", "comments": "9 pages, ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3350974", "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-media retrieval is to return the results of various media types\ncorresponding to the query of any media type. Existing researches generally\nfocus on coarse-grained cross-media retrieval. When users submit an image of\n\"Slaty-backed Gull\" as a query, coarse-grained cross-media retrieval treats it\nas \"Bird\", so that users can only get the results of \"Bird\", which may include\nother bird species with similar appearance (image and video), descriptions\n(text) or sounds (audio), such as \"Herring Gull\". Such coarse-grained\ncross-media retrieval is not consistent with human lifestyle, where we\ngenerally have the fine-grained requirement of returning the exactly relevant\nresults of \"Slaty-backed Gull\" instead of \"Herring Gull\". However, few\nresearches focus on fine-grained cross-media retrieval, which is a highly\nchallenging and practical task. Therefore, in this paper, we first construct a\nnew benchmark for fine-grained cross-media retrieval, which consists of 200\nfine-grained subcategories of the \"Bird\", and contains 4 media types, including\nimage, text, video and audio. To the best of our knowledge, it is the first\nbenchmark with 4 media types for fine-grained cross-media retrieval. Then, we\npropose a uniform deep model, namely FGCrossNet, which simultaneously learns 4\ntypes of media without discriminative treatments. We jointly consider three\nconstraints for better common representation learning: classification\nconstraint ensures the learning of discriminative features, center constraint\nensures the compactness characteristic of the features of the same subcategory,\nand ranking constraint ensures the sparsity characteristic of the features of\ndifferent subcategories. Extensive experiments verify the usefulness of the new\nbenchmark and the effectiveness of our FGCrossNet. They will be made available\nat https://github.com/PKU-ICST-MIPL/FGCrossNet_ACMMM2019.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 01:15:22 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 06:37:53 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["He", "Xiangteng", ""], ["Peng", "Yuxin", ""], ["Xie", "Liu", ""]]}, {"id": "1907.04807", "submitter": "Anastasia Antsiferova", "authors": "Anastasia Zvezdakova, Sergey Zvezdakov, Dmitriy Kulikov, Dmitriy\n  Vatolin", "title": "Hacking VMAF with Video Color and Contrast Distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality measurement takes an important role in many applications.\nFull-reference quality metrics which are usually used in video codecs\ncomparisons are expected to reflect any changes in videos. In this article, we\nconsider different color corrections of compressed videos which increase the\nvalues of full-reference metric VMAF and almost don't decrease other\nwidely-used metric SSIM. The proposed video contrast enhancement approach shows\nthe metric inapplicability in some cases for video codecs comparisons, as it\nmay be used for cheating in the comparisons via tuning to improve this metric\nvalues.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:56:33 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 13:35:31 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zvezdakova", "Anastasia", ""], ["Zvezdakov", "Sergey", ""], ["Kulikov", "Dmitriy", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1907.04868", "submitter": "Chris Donahue", "authors": "Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W.\n  Cottrell, Julian McAuley", "title": "LakhNES: Improving multi-instrumental music generation with cross-domain\n  pre-training", "comments": "Published as a conference paper at ISMIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are interested in the task of generating multi-instrumental music scores.\nThe Transformer architecture has recently shown great promise for the task of\npiano score generation; here we adapt it to the multi-instrumental setting.\nTransformers are complex, high-dimensional language models which are capable of\ncapturing long-term structure in sequence data, but require large amounts of\ndata to fit. Their success on piano score generation is partially explained by\nthe large volumes of symbolic data readily available for that domain. We\nleverage the recently-introduced NES-MDB dataset of four-instrument scores from\nan early video game sound synthesis chip (the NES), which we find to be\nwell-suited to training with the Transformer architecture. To further improve\nthe performance of our model, we propose a pre-training technique to leverage\nthe information in a large collection of heterogeneous music, namely the Lakh\nMIDI dataset. Despite differences between the two corpora, we find that this\ntransfer learning procedure improves both quantitative and qualitative\nperformance for our primary task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 18:00:04 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Donahue", "Chris", ""], ["Mao", "Huanru Henry", ""], ["Li", "Yiting Ethan", ""], ["Cottrell", "Garrison W.", ""], ["McAuley", "Julian", ""]]}, {"id": "1907.04926", "submitter": "Carlos Aguilar-Paredes", "authors": "Javier Sanz, Andreas Wulff-Abramsson, Carlos Aguilar-Paredes, Luis\n  Emilio Bruni and Lydia Sanchez", "title": "Synchronizing Audio-Visual Film Stimuli in Unity (version 5.5.1f1): Game\n  Engines as a Tool for Research", "comments": "13 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unity is a software specifically designed for the development of video games.\nHowever, due to its programming possibilities and the polyvalence of its\narchitecture, it can prove to be a versatile tool for stimuli presentation in\nresearch experiments. Nevertheless, it also has some limitations and conditions\nthat need to be taken into account to ensure optimal performance in particular\nexperimental situations. Such is the case if we want to use it in an\nexperimental design that includes the acquisition of biometric signals\nsynchronized with the broadcasting of video and audio in real time. In the\npresent paper, we analyse how Unity (version 5.5.1f1) reacts in one such\nexperimental design that requires the execution of audio-visual material. From\nthe analysis of an experimental procedure in which the video was executed\nfollowing the standard software specifications, we have detected the following\nproblems desynchronization between the emission of the video and the audio;\ndesynchronization between the temporary counter and the video; a delay in the\nexecution of the screenshot; and depending on the encoding of the video a bad\nfluency in the video playback, which even though it maintains the total\nplayback time, it causes Unity to freeze frames and proceed to compensate with\nlittle temporary jumps in the video. Finally, having detected all the problems,\na compensation and verification process is designed to be able to work with\naudio-visual material in Unity (version 5.5.1f1) in an accurate way. We present\na protocol for checks and compensations that allows solving these problems to\nensure the execution of robust experiments in terms of reliability.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 09:05:37 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Sanz", "Javier", ""], ["Wulff-Abramsson", "Andreas", ""], ["Aguilar-Paredes", "Carlos", ""], ["Bruni", "Luis Emilio", ""], ["Sanchez", "Lydia", ""]]}, {"id": "1907.04986", "submitter": "Sz Jiang", "authors": "Dengpan Ye, Shunzhi Jiang, and Jiaqin Huang", "title": "Heard More Than Heard: An Audio Steganography Method Based on GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio steganography is a collection of techniques for concealing the\nexistence of information by embedding it within a non-secret audio, which is\nreferred to as carrier. Distinct from cryptography, the steganography put\nemphasis on the hiding of the secret existence. The existing audio\nsteganography methods mainly depend on human handcraft, while we proposed an\naudio steganography algorithm which automatically generated from adversarial\ntraining. The method consists of three neural networks: encoder which embeds\nthe secret message in the carrier, decoder which extracts the message, and\ndiscriminator which determine the carriers contain secret messages. All the\nnetworks are simultaneously trained to create embedding, extracting and\ndiscriminating process. The system is trained with different training settings\non two datasets. Competed the majority of audio steganographic schemes, the\nproposed scheme could produce high fidelity steganographic audio which contains\nsecret audio. Besides, the additional experiments verify the robustness and\nsecurity of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 03:46:21 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ye", "Dengpan", ""], ["Jiang", "Shunzhi", ""], ["Huang", "Jiaqin", ""]]}, {"id": "1907.05123", "submitter": "Ammar Mohammadi", "authors": "Ammar Mohammadi, and Mansor Nakhkash", "title": "Reversible Data Hiding in Encrypted Images using Local Difference of\n  Neighboring Pixels", "comments": null, "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  2020", "doi": "10.1109/TCSVT.2020.2990952", "report-no": null, "categories": "eess.IV cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a reversible data hiding in encrypted image (RDHEI),\nwhich divides image into non-overlapping blocks. In each block, central pixel\nof the block is considered as leader pixel and others as follower ones. The\nprediction errors between the intensity of follower pixels and leader ones are\ncalculated and analyzed to determine a feature for block embedding capacity.\nThis feature indicates the amount of data that can be embedded in a block.\nUsing this pre-process for whole blocks, we vacate rooms before the encryption\nof the original image to achieve high embedding capacity. Also, using the\nfeatures of all blocks, embedded data is extracted and the original image is\nperfectly reconstructed at the decoding phase. In effect, comparing to existent\nRDHEI algorithms, embedding capacity is significantly increased in the proposed\nalgorithm. Experimental results confirm that the proposed algorithm outperforms\nstate of the art ones.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 11:42:10 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 12:22:37 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mohammadi", "Ammar", ""], ["Nakhkash", "Mansor", ""]]}, {"id": "1907.05129", "submitter": "Ammar Mohammadi", "authors": "Ammar Mohammadi, Mansour Nakhkash", "title": "Sorting Methods and Adaptive Thresholding for Histogram Based Reversible\n  Data Hiding", "comments": null, "journal-ref": "Multimedia Tools and Applications, 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a histogram based reversible data hiding (RDH) scheme,\nwhich divides image pixels into different cell frequency bands to sort them for\ndata embedding. Data hiding is more efficient in lower cell frequency bands\nbecause it provides more accurate prediction. Using pixel existence probability\nin some pixels of ultra-low cell frequency band, another sorting is performed.\nEmploying these two novel sorting methods in combination with the hiding\nintensity analysis that determines optimum prediction error, we improve the\nquality of the marked image especially for low embedding capacities. In effect,\ncomparing to existent RDH algorithms, the hiding capacity is increased for a\nspecific level of the distortion for the marked image. Experimental results\nconfirm that the proposed algorithm outperforms state of the art ones.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 12:00:12 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 12:02:44 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Mohammadi", "Ammar", ""], ["Nakhkash", "Mansour", ""]]}, {"id": "1907.05297", "submitter": "Mariel Pettee", "authors": "Mariel Pettee, Chase Shimmin, Douglas Duhaime, Ilya Vidrin", "title": "Beyond Imitation: Generative and Variational Choreography via Machine\n  Learning", "comments": "8 pages, 11 figures, presented at the 10th International Conference\n  on Computational Creativity (ICCC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our team of dance artists, physicists, and machine learning researchers has\ncollectively developed several original, configurable machine-learning tools to\ngenerate novel sequences of choreography as well as tunable variations on input\nchoreographic sequences. We use recurrent neural network and autoencoder\narchitectures from a training dataset of movements captured as 53\nthree-dimensional points at each timestep. Sample animations of generated\nsequences and an interactive version of our model can be found at http:\n//www.beyondimitation.com.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:12:10 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Pettee", "Mariel", ""], ["Shimmin", "Chase", ""], ["Duhaime", "Douglas", ""], ["Vidrin", "Ilya", ""]]}, {"id": "1907.05916", "submitter": "Yahui Liu", "authors": "Yahui Liu, Marco De Nadai, Gloria Zen, Nicu Sebe, Bruno Lepri", "title": "Gesture-to-Gesture Translation in the Wild via Category-Independent\n  Conditional Maps", "comments": "15 pages, 12 figures", "journal-ref": "27th ACM International Conference on Multimedia, 2019", "doi": "10.1145/3343031.3351020", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown Generative Adversarial Networks (GANs) to be\nparticularly effective in image-to-image translations. However, in tasks such\nas body pose and hand gesture translation, existing methods usually require\nprecise annotations, e.g. key-points or skeletons, which are time-consuming to\ndraw. In this work, we propose a novel GAN architecture that decouples the\nrequired annotations into a category label - that specifies the gesture type -\nand a simple-to-draw category-independent conditional map - that expresses the\nlocation, rotation and size of the hand gesture. Our architecture synthesizes\nthe target gesture while preserving the background context, thus effectively\ndealing with gesture translation in the wild. To this aim, we use an attention\nmodule and a rolling guidance approach, which loops the generated images back\ninto the network and produces higher quality images compared to competing\nworks. Thus, our GAN learns to generate new images from simple annotations\nwithout requiring key-points or skeleton labels. Results on two public datasets\nshow that our method outperforms state of the art approaches both\nquantitatively and qualitatively. To the best of our knowledge, no work so far\nhas addressed the gesture-to-gesture translation in the wild by requiring\nuser-friendly annotations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 18:39:27 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 10:24:21 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 08:55:11 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Liu", "Yahui", ""], ["De Nadai", "Marco", ""], ["Zen", "Gloria", ""], ["Sebe", "Nicu", ""], ["Lepri", "Bruno", ""]]}, {"id": "1907.06392", "submitter": "Pavlos Sermpezis", "authors": "Pavlos Sermpezis, Savvas Kastanakis, Jo\\~ao Ismael Pinheiro, Felipe\n  Assis, Mateus Nogueira, Daniel Menasch\\'e, Thrasyvoulos Spyropoulos", "title": "Towards QoS-Aware Recommendations", "comments": null, "journal-ref": "ACM RecSys 2020 workshops (CARS: workshop on Context-Aware\n  Recommender Systems)", "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose that recommendation systems (RSs) for multimedia\nservices should be \"QoS-aware\", i.e., take into account the expected QoS with\nwhich a content can be delivered, to increase the user satisfaction.\nNetwork-aware recommendations have been very recently proposed as a promising\nsolution to improve network performance. However, the idea of QoS-aware RSs has\nbeen studied from the network perspective. Its feasibility and performance\nperformance advantages for the content-provider or user perspective have only\nbeen speculated. Hence, in this paper we aim to provide initial answers for the\nfeasibility of the concept of QoS-aware RS, by investigating its impact on real\nuser experience. To this end, we conduct experiments with real users on a\ntestbed, and present initial experimental results. Our analysis demonstrates\nthe potential of the idea: QoS-aware RSs could be beneficial for both the users\n(better experience) and content providers (higher user engagement). Moreover,\nbased on the collected dataset, we build statistical models to (i) predict the\nuser experience as a function of QoS, relevance of recommendations (QoR) and\nuser interest, and (ii) provide useful insights for the design of QoS-aware\nRSs. We believe that our study is an important first step towards QoS-aware\nrecommendations, by providing experimental evidence for their feasibility and\nbenefits, and can help open a future research direction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 09:42:11 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 08:26:44 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sermpezis", "Pavlos", ""], ["Kastanakis", "Savvas", ""], ["Pinheiro", "Jo\u00e3o Ismael", ""], ["Assis", "Felipe", ""], ["Nogueira", "Mateus", ""], ["Menasch\u00e9", "Daniel", ""], ["Spyropoulos", "Thrasyvoulos", ""]]}, {"id": "1907.06956", "submitter": "Marc Chaumont", "authors": "Mehdi Yedroudj and Fr\\'ed\\'eric Comby and Marc Chaumont", "title": "Steganography using a 3 player game", "comments": "The first version of this paper has been written in March 2019 and\n  submitted to IH&MMSec'2019 but rejected. The first ArXiv version (July 2019)\n  is a second version with additional explanations compared to the version of\n  March 2019. This last version is an extended version and has been accepted to\n  Journal of Visual Communication and Image Representation, Elsevier, 2019 -\n  version of September 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image steganography aims to securely embed secret information into cover\nimages. Until now, adaptive embedding algorithms such as S-UNIWARD or Mi-POD,\nare among the most secure and most used methods for image steganography. With\nthe arrival of deep learning and more specifically the Generative Adversarial\nNetworks (GAN), new techniques have appeared. Among these techniques, there is\nthe 3 player game approaches, where three networks compete against each\nother.In this paper, we propose three different architectures based on the 3\nplayer game. The first-architecture is proposed as a rigorous alternative to\ntwo recent publications. The second takes into account stego noise power.\nFinally, our third architecture enriches the second one with a better\ninteraction between the embedding and extracting networks. Our method achieves\nbetter results compared to the existing works GSIVAT, HiDDeN, and paves the way\nfor future research on this topic.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 15:59:39 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 17:30:35 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Yedroudj", "Mehdi", ""], ["Comby", "Fr\u00e9d\u00e9ric", ""], ["Chaumont", "Marc", ""]]}, {"id": "1907.07345", "submitter": "Sergey Podlesnyy", "authors": "Sergey Podlesnyy", "title": "Towards Data-Driven Automatic Video Editing", "comments": "2019 15th International Conference on Natural Computation, Fuzzy\n  Systems and Knowledge Discovery, Kunming, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video editing involving at least the steps of selecting the most\nvaluable footage from points of view of visual quality and the importance of\naction filmed; and cutting the footage into a brief and coherent visual story\nthat would be interesting to watch is implemented in a purely data-driven\nmanner. Visual semantic and aesthetic features are extracted by the\nImageNet-trained convolutional neural network, and the editing controller is\ntrained by an imitation learning algorithm. As a result, at test time the\ncontroller shows the signs of observing basic cinematography editing rules\nlearned from the corpus of motion pictures masterpieces.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 05:54:37 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Podlesnyy", "Sergey", ""]]}, {"id": "1907.07645", "submitter": "Iskandar Aripov", "authors": "Iskandar Aripov", "title": "The Statistical Analysis of the Live TV Bit Rate", "comments": null, "journal-ref": "Dhinaharan Nagamalai et al. (Eds) : SAI, NCO, SOFT, ICAITA, CDKP,\n  CMC, SIGNAL - 2019 pp. 213-223, 2019. CS & IT-CSCP 2019", "doi": "10.5121/csit.2019.90716", "report-no": null, "categories": "cs.NI cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the statistical nature of TV channels streaming variable\nbit rate distribution and allocation. The goal of the paper is to derive the\nbest-fit rate distribution to describe TV streaming bandwidth allocation, which\ncan reveal traffic demands of users. Our analysis uses multiplexers channel\nbandwidth allocation (PID) data of 13 TV live channels. We apply 17 continuous\nand 3 discrete distributions to determine the best-fit distribution function\nfor each individual channel and for the whole set of channels. We found that\nthe generalized extreme distribution fitting most of our channels most\nprecisely according to the Bayesian information criterion. By the same\ncriterion tlocationscale distribution matches best for the whole system. We use\nthese results to propose parameters for streaming server queuing model. Results\nare useful for streaming servers scheduling policy design process targeting to\nimprove limited infrastructural resources, traffic engineering through dynamic\nrouting at CDN, SDN.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 17:07:29 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Aripov", "Iskandar", ""]]}, {"id": "1907.08448", "submitter": "Diego Valsesia", "authors": "Diego Valsesia, Giulia Fracastoro, Enrico Magli", "title": "Deep Graph-Convolutional Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local self-similarity is well-known to be an effective prior for the\nimage denoising problem. However, little work has been done to incorporate it\nin convolutional neural networks, which surpass non-local model-based methods\ndespite only exploiting local information. In this paper, we propose a novel\nend-to-end trainable neural network architecture employing layers based on\ngraph convolution operations, thereby creating neurons with non-local receptive\nfields. The graph convolution operation generalizes the classic convolution to\narbitrary graphs. In this work, the graph is dynamically computed from\nsimilarities among the hidden features of the network, so that the powerful\nrepresentation learning capabilities of the network are exploited to uncover\nself-similar patterns. We introduce a lightweight Edge-Conditioned Convolution\nwhich addresses vanishing gradient and over-parameterization issues of this\nparticular graph convolution. Extensive experiments show state-of-the-art\nperformance with improved qualitative and quantitative results on both\nsynthetic Gaussian noise and real noise.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 10:27:41 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""]]}, {"id": "1907.08769", "submitter": "Lin Zhu", "authors": "Lin Zhu and Siwei Dong and Tiejun Huang and Yonghong Tian", "title": "A Retina-inspired Sampling Method for Visual Texture Reconstruction", "comments": "Published in ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional frame-based camera is not able to meet the demand of rapid\nreaction for real-time applications, while the emerging dynamic vision sensor\n(DVS) can realize high speed capturing for moving objects. However, to achieve\nvisual texture reconstruction, DVS need extra information apart from the output\nspikes. This paper introduces a fovea-like sampling method inspired by the\nneuron signal processing in retina, which aims at visual texture reconstruction\nonly taking advantage of the properties of spikes. In the proposed method, the\npixels independently respond to the luminance changes with temporal\nasynchronous spikes. Analyzing the arrivals of spikes makes it possible to\nrestore the luminance information, enabling reconstructing the natural scene\nfor visualization. Three decoding methods of spike stream for texture\nreconstruction are proposed for high-speed motion and stationary scenes.\nCompared to conventional frame-based camera and DVS, our model can achieve\nbetter image quality and higher flexibility, which is capable of changing the\nway that demanding machine vision applications are built.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 06:54:14 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhu", "Lin", ""], ["Dong", "Siwei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "1907.09085", "submitter": "Jianbo Yuan", "authors": "Jianbo Yuan, Haofu Liao, Rui Luo, Jiebo Luo", "title": "Automatic Radiology Report Generation based on Multi-view Image Fusion\n  and Medical Concept Enrichment", "comments": null, "journal-ref": "MICCAI 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating radiology reports is time-consuming and requires extensive\nexpertise in practice. Therefore, reliable automatic radiology report\ngeneration is highly desired to alleviate the workload. Although deep learning\ntechniques have been successfully applied to image classification and image\ncaptioning tasks, radiology report generation remains challenging in regards to\nunderstanding and linking complicated medical visual contents with accurate\nnatural language descriptions. In addition, the data scales of open-access\ndatasets that contain paired medical images and reports remain very limited. To\ncope with these practical challenges, we propose a generative encoder-decoder\nmodel and focus on chest x-ray images and reports with the following\nimprovements. First, we pretrain the encoder with a large number of chest x-ray\nimages to accurately recognize 14 common radiographic observations, while\ntaking advantage of the multi-view images by enforcing the cross-view\nconsistency. Second, we synthesize multi-view visual features based on a\nsentence-level attention mechanism in a late fusion fashion. In addition, in\norder to enrich the decoder with descriptive semantics and enforce the\ncorrectness of the deterministic medical-related contents such as mentions of\norgans or diagnoses, we extract medical concepts based on the radiology reports\nin the training data and fine-tune the encoder to extract the most frequent\nmedical concepts from the x-ray images. Such concepts are fused with each\ndecoding step by a word-level attention model. The experimental results\nconducted on the Indiana University Chest X-Ray dataset demonstrate that the\nproposed model achieves the state-of-the-art performance compared with other\nbaseline approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 02:25:33 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 00:45:21 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Yuan", "Jianbo", ""], ["Liao", "Haofu", ""], ["Luo", "Rui", ""], ["Luo", "Jiebo", ""]]}, {"id": "1907.09211", "submitter": "Quang-Trung Luu", "authors": "Quang-Trung Luu, Sylvaine Kerboeuf, Alexandre Mouradian, and Michel\n  Kieffer", "title": "A Coverage-Aware Resource Provisioning Method for Network Slicing", "comments": "15 pages, 13 figures. Parts of this work have been presented at IEEE\n  GLOBECOM 2018 (Abu Dhabi, UAE,\n  https://ieeexplore.ieee.org/abstract/document/8648039) and IEEE ICC 2020\n  (Dublin, Ireland, https://ieeexplore.ieee.org/document/9148897)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With network slicing in 5G networks, Mobile Network Operators can create\nvarious slices for Service Providers (SPs) to accommodate customized services.\nUsually, the various Service Function Chains (SFCs) belonging to a slice are\ndeployed on a best-effort basis. Nothing ensures that the Infrastructure\nProvider (InP) will be able to allocate enough resources to cope with the\nincreasing demands of some SP. Moreover, in many situations, slices have to be\ndeployed over some geographical area: coverage as well as minimum per-user rate\nconstraints have then to be taken into account. This paper takes the InP\nperspective and proposes a slice resource provisioning approach to cope with\nmultiple slice demands in terms of computing, storage, coverage, and rate\nconstraints. The resource requirements of the various SFCs within a slice are\naggregated within a graph of Slice Resource Demands (SRD). Infrastructure nodes\nand links have then to be provisioned so as to satisfy all SRDs. This problem\nleads to a Mixed Integer Linear Programming formulation. A two-step approach is\nconsidered, with several variants, depending on whether the constraints of each\nslice to be provisioned are taken into account sequentially or jointly. Once\nprovisioning has been performed, any slice deployment strategy may be\nconsidered on the reduced-size infrastructure graph on which resources have\nbeen provisioned. Simulation results demonstrate the effectiveness of the\nproposed approach compared to a more classical direct slice embedding approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 10:07:10 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 13:51:44 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 10:00:59 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Luu", "Quang-Trung", ""], ["Kerboeuf", "Sylvaine", ""], ["Mouradian", "Alexandre", ""], ["Kieffer", "Michel", ""]]}, {"id": "1907.09293", "submitter": "David Powers", "authors": "David M W Powers", "title": "DREAMT -- Embodied Motivational Conversational Storytelling", "comments": "12 pages; to be presented as lightning talk plus poster at StoryNLP\n  on 1 August 2019 at ACL in Florence - poster pdf and powerpoint available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storytelling is fundamental to language, including culture, conversation and\ncommunication in their broadest senses. It thus emerges as an essential\ncomponent of intelligent systems, including systems where natural language is\nnot a primary focus or where we do not usually think of a story being involved.\nIn this paper we explore the emergence of storytelling as a requirement in\nembodied conversational agents, including its role in educational and health\ninterventions, as well as in a general-purpose computer interface for people\nwith disabilities or other constraints that prevent the use of traditional\nkeyboard and speech interfaces. We further present a characterization of\nstorytelling as an inventive fleshing out of detail according to a particular\npersonal perspective, and propose the DREAMT model to focus attention on the\ndifferent layers that need to be present in a character-driven storytelling\nsystem. Most if not all aspects of the DREAMT model have arisen from or been\nexplored in some aspect of our implemented research systems, but currently only\nat a primitive and relatively unintegrated level. However, this experience\nleads us to formalize and elaborate the DREAMT model mnemonically as follows: -\nDescription/Dialogue/Definition/Denotation - Realization/Representation/Role -\nExplanation/Education/Entertainment - Actualization/Activation -\nMotivation/Modelling - Topicalization/Transformation\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 01:49:37 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Powers", "David M W", ""]]}, {"id": "1907.09404", "submitter": "Alessandro Lameiras Koerich", "authors": "Kelly Lais Wiggers, Alceu de Souza Britto Junior, Alessandro Lameiras\n  Koerich, Laurent Heutte, Luiz Eduardo Soares de Oliveira", "title": "Deep Learning Approaches for Image Retrieval and Pattern Spotting in\n  Ancient Documents", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes two approaches for content-based image retrieval and\npattern spotting in document images using deep learning. The first approach\nuses a pre-trained CNN model to cope with the lack of training data, which is\nfine-tuned to achieve a compact yet discriminant representation of queries and\nimage candidates. The second approach uses a Siamese Convolution Neural Network\ntrained on a previously prepared subset of image pairs from the ImageNet\ndataset to provide the similarity-based feature maps. In both methods, the\nlearned representation scheme considers feature maps of different sizes which\nare evaluated in terms of retrieval performance. A robust experimental protocol\nusing two public datasets (Tobacoo-800 and DocExplore) has shown that the\nproposed methods compare favorably against state-of-the-art document image\nretrieval and pattern spotting methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:27:19 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Wiggers", "Kelly Lais", ""], ["Junior", "Alceu de Souza Britto", ""], ["Koerich", "Alessandro Lameiras", ""], ["Heutte", "Laurent", ""], ["de Oliveira", "Luiz Eduardo Soares", ""]]}, {"id": "1907.09594", "submitter": "Jungseock Joo", "authors": "Nan Xi, Di Ma, Marcus Liou, Zachary C. Steinert-Threlkeld, Jason\n  Anastasopoulos, Jungseock Joo", "title": "Understanding the Political Ideology of Legislators from Social Media\n  Images", "comments": "To appear in the Proceedings of International AAAI Conference on Web\n  and Social Media (ICWSM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.HC cs.MM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we seek to understand how politicians use images to express\nideological rhetoric through Facebook images posted by members of the U.S.\nHouse and Senate. In the era of social media, politics has become saturated\nwith imagery, a potent and emotionally salient form of political rhetoric which\nhas been used by politicians and political organizations to influence public\nsentiment and voting behavior for well over a century. To date, however, little\nis known about how images are used as political rhetoric. Using deep learning\ntechniques to automatically predict Republican or Democratic party affiliation\nsolely from the Facebook photographs of the members of the 114th U.S. Congress,\nwe demonstrate that predicted class probabilities from our model function as an\naccurate proxy of the political ideology of images along a left-right\n(liberal-conservative) dimension. After controlling for the gender and race of\npoliticians, our method achieves an accuracy of 59.28% from single photographs\nand 82.35% when aggregating scores from multiple photographs (up to 150) of the\nsame person. To better understand image content distinguishing liberal from\nconservative images, we also perform in-depth content analyses of the\nphotographs. Our findings suggest that conservatives tend to use more images\nsupporting status quo political institutions and hierarchy maintenance,\nfeaturing individuals from dominant social groups, and displaying greater\nhappiness than liberals.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:43:49 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Xi", "Nan", ""], ["Ma", "Di", ""], ["Liou", "Marcus", ""], ["Steinert-Threlkeld", "Zachary C.", ""], ["Anastasopoulos", "Jason", ""], ["Joo", "Jungseock", ""]]}, {"id": "1907.10450", "submitter": "Kader Pustu-Iren", "authors": "Kader Pustu-Iren and Markus M\\\"uhling and Nikolaus Korfhage and Joanna\n  Bars and Sabrina Bernh\\\"oft and Angelika H\\\"orth and Bernd Freisleben and\n  Ralph Ewerth", "title": "Investigating Correlations of Inter-coder Agreement and Machine\n  Annotation Performance for Historical Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video indexing approaches such as visual concept classification and person\nrecognition are essential to enable fine-grained semantic search in large-scale\nvideo archives such as the historical video collection of former German\nDemocratic Republic (GDR) maintained by the German Broadcasting Archive (DRA).\nTypically, a lexicon of visual concepts has to be defined for semantic search.\nHowever, the definition of visual concepts can be more or less subjective due\nto individually differing judgments of annotators, which may have an impact on\nannotation quality and subsequently training of supervised machine learning\nmethods. In this paper, we analyze the inter-coder agreement for historical TV\ndata of the former GDR for visual concept classification and person\nrecognition. The inter-coder agreement is evaluated for a group of expert as\nwell as non-expert annotators in order to determine differences in annotation\nhomogeneity. Furthermore, correlations between visual recognition performance\nand inter-annotator agreement are measured. In this context, information about\nimage quantity and agreement are used to predict average precision for concept\nclassification. Finally, the influence of expert vs. non-expert annotations\nacquired in the study are used to evaluate person recognition.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:50:20 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Pustu-Iren", "Kader", ""], ["M\u00fchling", "Markus", ""], ["Korfhage", "Nikolaus", ""], ["Bars", "Joanna", ""], ["Bernh\u00f6ft", "Sabrina", ""], ["H\u00f6rth", "Angelika", ""], ["Freisleben", "Bernd", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1907.10559", "submitter": "Hayder Hamandi", "authors": "Hayder Hamandi, Nabil Sarhan", "title": "QRMODA and BRMODA: Novel Models for Face Recognition Accuracy in\n  Computer Vision Systems with Adapted Video Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge facing Computer Vision systems is providing the ability to\naccurately detect threats and recognize subjects and/or objects under\ndynamically changing network conditions. We propose two novel models that\ncharacterize the face recognition accuracy in terms of video encoding\nparameters. Specifically, we model the accuracy in terms of video resolution,\nquantization, and actual bit rate. We validate the models using two distinct\nvideo datasets and a large image dataset by conducting 1, 668 experiments that\ninvolve simultaneously varying combinations of encoding parameters. We show\nthat both models hold true for the deep learning and statistical based face\nrecognition. Furthermore, we show that the models can be used to capture\ndifferent accuracy metrics, specifically the recall, precision, and F1-score.\nUltimately, we provide meaningful insights on the factors affecting the\nconstants of each proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 16:59:56 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Hamandi", "Hayder", ""], ["Sarhan", "Nabil", ""]]}, {"id": "1907.11379", "submitter": "Yuma Kinoshita", "authors": "Artit Visavakitcharoen, Yuma Kinoshita and Hitoshi Kiya", "title": "A Color Compensation Method Using Inverse Camera Response Function for\n  Multi-exposure Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-exposure image fusion is a method for producing an image with a wide\ndynamic range by fusing multiple images taken under various exposure values. In\nthis paper, we discuss color distortion included in fused images, and propose a\nnovel color compensation method for multi-exposure image fusion. In the\nproposed method, an inverse camera response function (CRF) is estimated by\nusing multi-exposure images, and then a high dynamic range (HDR) radiance map\nis recovered. The color information of the radiance map is applied to images\nfused by conventional multi-exposure imaging to correct the color distortion.\nThe proposed method can be applied to any existing fusion approaches for\nimproving the quality of the fused images.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 04:34:57 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Visavakitcharoen", "Artit", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1907.11496", "submitter": "Xin Wang", "authors": "Xin Wang, Bo Wu, Yun Ye, Yueqi Zhong", "title": "Outfit Compatibility Prediction and Diagnosis with Multi-Layered\n  Comparison Network", "comments": "9 pages, 6 figures, Proceedings of the 27th ACM International\n  Conference on Multimedia", "journal-ref": null, "doi": "10.1145/3343031.3350909", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works about fashion outfit compatibility focus on predicting the\noverall compatibility of a set of fashion items with their information from\ndifferent modalities. However, there are few works explore how to explain the\nprediction, which limits the persuasiveness and effectiveness of the model. In\nthis work, we propose an approach to not only predict but also diagnose the\noutfit compatibility. We introduce an end-to-end framework for this goal, which\nfeatures for: (1) The overall compatibility is learned from all type-specified\npairwise similarities between items, and the backpropagation gradients are used\nto diagnose the incompatible factors. (2) We leverage the hierarchy of CNN and\ncompare the features at different layers to take into account the\ncompatibilities of different aspects from the low level (such as color,\ntexture) to the high level (such as style). To support the proposed method, we\nbuild a new type-specified outfit dataset named Polyvore-T based on Polyvore\ndataset. We compare our method with the prior state-of-the-art in two tasks:\noutfit compatibility prediction and fill-in-the-blank. Experiments show that\nour approach has advantages in both prediction performance and diagnosis\nability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:39:15 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 03:56:30 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Wang", "Xin", ""], ["Wu", "Bo", ""], ["Ye", "Yun", ""], ["Zhong", "Yueqi", ""]]}, {"id": "1907.11857", "submitter": "Yi Zhang", "authors": "Yi Zhang, Cheng Zeng, Hao Cheng, Chongjun Wang, Lei Zhang", "title": "Many could be better than all: A novel instance-oriented algorithm for\n  Multi-modal Multi-label problem", "comments": "To be published in ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the emergence of diverse data collection techniques, objects in real\napplications can be represented as multi-modal features. What's more, objects\nmay have multiple semantic meanings. Multi-modal and Multi-label (MMML) problem\nbecomes a universal phenomenon. The quality of data collected from different\nchannels are inconsistent and some of them may not benefit for prediction. In\nreal life, not all the modalities are needed for prediction. As a result, we\npropose a novel instance-oriented Multi-modal Classifier Chains (MCC) algorithm\nfor MMML problem, which can make convince prediction with partial modalities.\nMCC extracts different modalities for different instances in the testing phase.\nExtensive experiments are performed on one real-world herbs dataset and two\npublic datasets to validate our proposed algorithm, which reveals that it may\nbe better to extract many instead of all of the modalities at hand.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 06:55:24 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhang", "Yi", ""], ["Zeng", "Cheng", ""], ["Cheng", "Hao", ""], ["Wang", "Chongjun", ""], ["Zhang", "Lei", ""]]}, {"id": "1907.12021", "submitter": "William Yang Wang", "authors": "Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni,\n  Matthew Turk, William Yang Wang", "title": "What Should I Ask? Using Conversationally Informative Rewards for\n  Goal-Oriented Visual Dialog", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to engage in goal-oriented conversations has allowed humans to\ngain knowledge, reduce uncertainty, and perform tasks more efficiently.\nArtificial agents, however, are still far behind humans in having goal-driven\nconversations. In this work, we focus on the task of goal-oriented visual\ndialogue, aiming to automatically generate a series of questions about an image\nwith a single objective. This task is challenging since these questions must\nnot only be consistent with a strategy to achieve a goal, but also consider the\ncontextual information in the image. We propose an end-to-end goal-oriented\nvisual dialogue system, that combines reinforcement learning with regularized\ninformation gain. Unlike previous approaches that have been proposed for the\ntask, our work is motivated by the Rational Speech Act framework, which models\nthe process of human inquiry to reach a goal. We test the two versions of our\nmodel on the GuessWhat?! dataset, obtaining significant results that outperform\nthe current state-of-the-art models in the task of generating questions to find\nan undisclosed object in an image.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 06:15:35 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Shukla", "Pushkar", ""], ["Elmadjian", "Carlos", ""], ["Sharan", "Richika", ""], ["Kulkarni", "Vivek", ""], ["Turk", "Matthew", ""], ["Wang", "William Yang", ""]]}, {"id": "1907.12352", "submitter": "Tobias Isenberg", "authors": "Sarkis Halladjian, Haichao Miao, David Kou\\v{r}il, M. Eduard\n  Gr\\\"oller, Ivan Viola, Tobias Isenberg", "title": "ScaleTrotter: Illustrative Visual Travels Across Negative Scales", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934334", "report-no": null, "categories": "cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ScaleTrotter, a conceptual framework for an interactive,\nmulti-scale visualization of biological mesoscale data and, specifically,\ngenome data. ScaleTrotter allows viewers to smoothly transition from the\nnucleus of a cell to the atomistic composition of the DNA, while bridging\nseveral orders of magnitude in scale. The challenges in creating an interactive\nvisualization of genome data are fundamentally different in several ways from\nthose in other domains like astronomy that require a multi-scale representation\nas well. First, genome data has intertwined scale levels---the DNA is an\nextremely long, connected molecule that manifests itself at all scale levels.\nSecond, elements of the DNA do not disappear as one zooms out---instead the\nscale levels at which they are observed group these elements differently.\nThird, we have detailed information and thus geometry for the entire dataset\nand for all scale levels, posing a challenge for interactive visual\nexploration. Finally, the conceptual scale levels for genome data are close in\nscale space, requiring us to find ways to visually embed a smaller scale into a\ncoarser one. We address these challenges by creating a new multi-scale\nvisualization concept. We use a scale-dependent camera model that controls the\nvisual embedding of the scales into their respective parents, the rendering of\na subset of the scale hierarchy, and the location, size, and scope of the view.\nIn traversing the scales, ScaleTrotter is roaming between 2D and 3D visual\nrepresentations that are depicted in integrated visuals. We discuss,\nspecifically, how this form of multi-scale visualization follows from the\nspecific characteristics of the genome data and describe its implementation.\nFinally, we discuss the implications of our work to the general illustrative\ndepiction of multi-scale data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:01:54 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Halladjian", "Sarkis", ""], ["Miao", "Haichao", ""], ["Kou\u0159il", "David", ""], ["Gr\u00f6ller", "M. Eduard", ""], ["Viola", "Ivan", ""], ["Isenberg", "Tobias", ""]]}, {"id": "1907.12490", "submitter": "Rongcheng Tu", "authors": "Rong-Cheng Tu, Xian-Ling Mao, Bing Ma, Yong Hu, Tan Yan, Wei Wei and\n  Heyan Huang", "title": "Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes\n  Jointly Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their high retrieval efficiency and low storage cost, cross-modal\nhashing methods have attracted considerable attention. Generally, compared with\nshallow cross-modal hashing methods, deep cross-modal hashing methods can\nachieve a more satisfactory performance by integrating feature learning and\nhash codes optimizing into a same framework. However, most existing deep\ncross-modal hashing methods either cannot learn a unified hash code for the two\ncorrelated data-points of different modalities in a database instance or cannot\nguide the learning of unified hash codes by the feedback of hashing function\nlearning procedure, to enhance the retrieval accuracy. To address the issues\nabove, in this paper, we propose a novel end-to-end Deep Cross-Modal Hashing\nwith Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC).\nSpecifically, by an iterative optimization algorithm, DCHUC jointly learns\nunified hash codes for image-text pairs in a database and a pair of hash\nfunctions for unseen query image-text pairs. With the iterative optimization\nalgorithm, the learned unified hash codes can be used to guide the hashing\nfunction learning procedure; Meanwhile, the learned hashing functions can\nfeedback to guide the unified hash codes optimizing procedure. Extensive\nexperiments on three public datasets demonstrate that the proposed method\noutperforms the state-of-the-art cross-modal hashing methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:39:37 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Tu", "Rong-Cheng", ""], ["Mao", "Xian-Ling", ""], ["Ma", "Bing", ""], ["Hu", "Yong", ""], ["Yan", "Tan", ""], ["Wei", "Wei", ""], ["Huang", "Heyan", ""]]}, {"id": "1907.12743", "submitter": "Min-Hung Chen", "authors": "Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen,\n  Jian Zheng", "title": "Temporal Attentive Alignment for Large-Scale Video Domain Adaptation", "comments": "ICCV 2019 (Oral) camera-ready + supplementary. Code and data:\n  http://github.com/cmhungsteve/TA3N", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various image-based domain adaptation (DA) techniques have been\nproposed in recent years, domain shift in videos is still not well-explored.\nMost previous works only evaluate performance on small-scale datasets which are\nsaturated. Therefore, we first propose two large-scale video DA datasets with\nmuch larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we\ninvestigate different DA integration methods for videos, and show that\nsimultaneously aligning and learning temporal dynamics achieves effective\nalignment even without sophisticated DA methods. Finally, we propose Temporal\nAttentive Adversarial Adaptation Network (TA3N), which explicitly attends to\nthe temporal dynamics using domain discrepancy for more effective domain\nalignment, achieving state-of-the-art performance on four video DA datasets\n(e.g. 7.9% accuracy gain over \"Source only\" from 73.9% to 81.8% on \"HMDB -->\nUCF\", and 10.3% gain on \"Kinetics --> Gameplay\"). The code and data are\nreleased at http://github.com/cmhungsteve/TA3N.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 05:43:55 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 16:06:39 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 05:17:51 GMT"}, {"version": "v4", "created": "Thu, 8 Aug 2019 05:50:11 GMT"}, {"version": "v5", "created": "Mon, 12 Aug 2019 15:28:47 GMT"}, {"version": "v6", "created": "Sun, 15 Sep 2019 00:48:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chen", "Min-Hung", ""], ["Kira", "Zsolt", ""], ["AlRegib", "Ghassan", ""], ["Yoo", "Jaekwon", ""], ["Chen", "Ruxin", ""], ["Zheng", "Jian", ""]]}, {"id": "1907.12888", "submitter": "Ts\\`i-U\\'i \\.Ik", "authors": "Tzu-Han Hsu, Ching-Hsuan Chen, Nyan Ping Ju, Ts\\`i-U\\'i \\.Ik, Wen-Chih\n  Peng, Chih-Chuan Wang, Yu-Shuen Wang, Yuan-Hsiang Lin, Yu-Chee Tseng,\n  Jiun-Long Huang, Yu-Tai Ching", "title": "CoachAI: A Project for Microscopic Badminton Match Data Collection and\n  Tactical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision based object tracking has been used to annotate and augment\nsports video. For sports learning and training, video replay is often used in\npost-match review and training review for tactical analysis and movement\nanalysis. For automatically and systematically competition data collection and\ntactical analysis, a project called CoachAI has been supported by the Ministry\nof Science and Technology, Taiwan. The proposed project also includes research\nof data visualization, connected training auxiliary devices, and data\nwarehouse. Deep learning techniques will be used to develop video-based\nreal-time microscopic competition data collection based on broadcast\ncompetition video. Machine learning techniques will be used to develop a\ntactical analysis. To reveal data in more understandable forms and to help in\npre-match training, AR/VR techniques will be used to visualize data, tactics,\nand so on. In addition, training auxiliary devices including smart badminton\nrackets and connected serving machines will be developed based on the IoT\ntechnology to further utilize competition data and tactical data and boost\ntraining efficiency. Especially, the connected serving machines will be\ndeveloped to perform specified tactics and to interact with players in their\ntraining.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:33:00 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Hsu", "Tzu-Han", ""], ["Chen", "Ching-Hsuan", ""], ["Ju", "Nyan Ping", ""], ["\u0130k", "Ts\u00ec-U\u00ed", ""], ["Peng", "Wen-Chih", ""], ["Wang", "Chih-Chuan", ""], ["Wang", "Yu-Shuen", ""], ["Lin", "Yuan-Hsiang", ""], ["Tseng", "Yu-Chee", ""], ["Huang", "Jiun-Long", ""], ["Ching", "Yu-Tai", ""]]}, {"id": "1907.12904", "submitter": "Wanjie Sun", "authors": "Wanjie Sun and Zhenzhong Chen", "title": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "comments": "15 pages; not the final version", "journal-ref": null, "doi": "10.1109/TIP.2020.2970248", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network based image super-resolution (SR) models\nhave shown superior performance in recovering the underlying high resolution\n(HR) images from low resolution (LR) images obtained from the predefined\ndownscaling methods. In this paper we propose a learned image downscaling\nmethod based on content adaptive resampler (CAR) with consideration on the\nupscaling process. The proposed resampler network generates content adaptive\nimage resampling kernels that are applied to the original HR input to generate\npixels on the downscaled image. Moreover, a differentiable upscaling (SR)\nmodule is employed to upscale the LR result into its underlying HR counterpart.\nBy back-propagating the reconstruction error down to the original HR input\nacross the entire framework to adjust model parameters, the proposed framework\nachieves a new state-of-the-art SR performance through upscaling guided image\nresamplers which adaptively preserve detailed information that is essential to\nthe upscaling. Experimental results indicate that the quality of the generated\nLR image is comparable to that of the traditional interpolation based method,\nbut the significant SR performance gain is achieved by deep SR models trained\njointly with the CAR model. The code is publicly available on: URL\nhttps://github.com/sunwj/CAR.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 05:35:27 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 13:00:42 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Sun", "Wanjie", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "1907.12914", "submitter": "Farid Ghareh Mohammadi", "authors": "Farid Ghareh Mohammadi, Farzan Shenavarmasouleh, M. Hadi Amini, Hamid\n  R. Arabnia", "title": "Evolutionary Algorithms and Efficient Data Analytics for Image\n  Processing", "comments": "8 pages,5 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steganography algorithms facilitate communication between a source and a\ndestination in a secret manner. This is done by embedding messages/text/data\ninto images without impacting the appearance of the resultant images/videos.\nSteganalysis is the science of determining if an image has secret messages\nembedded/hidden in it. Because there are numerous steganography algorithms, and\nsince each one of them requires a different type of steganalysis, the\nsteganalysis process is extremely challenging. Thus, researchers aim to develop\none universal steganalysis to detect all known and unknown steganography\nalgorithms, ideally in real-time. Universal steganalysis extracts a large\nnumber of features to distinguish stego images from cover images. However, the\nincrease in features leads to the problem of the curse of dimensionality (CoD),\nwhich is considered to be an NP-hard problem. This COD problem additionally\nmakes real-time steganalysis hard. A large number of features generates large\ndatasets for which machine learning cannot generate an optimal model.\nGenerating a machine learning based model also takes a long time which makes\nreal-time processing appear impossible in any optimization for time-intensive\nfields such as visual computing. Possible solutions for CoD are deep learning\nand evolutionary algorithms that overcome the machine learning limitations. In\nthis study, we investigate previously developed evolutionary algorithms for\nboosting real-time image processing and argue that they provide the most\npromising solutions for the CoD problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:13:53 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 05:14:06 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 16:10:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mohammadi", "Farid Ghareh", ""], ["Shenavarmasouleh", "Farzan", ""], ["Amini", "M. Hadi", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "1907.12918", "submitter": "Haipeng Zeng", "authors": "Haipeng Zeng, Xingbo Wang, Aoyu Wu, Yong Wang, Quan Li, Alex Endert\n  and Huamin Qu", "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "comments": "11 pages, 8 figures. Accepted by IEEE VAST 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934656", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions play a key role in human communication and public presentations.\nHuman emotions are usually expressed through multiple modalities. Therefore,\nexploring multimodal emotions and their coherence is of great value for\nunderstanding emotional expressions in presentations and improving presentation\nskills. However, manually watching and studying presentation videos is often\ntedious and time-consuming. There is a lack of tool support to help conduct an\nefficient and in-depth multi-level analysis. Thus, in this paper, we introduce\nEmoCo, an interactive visual analytics system to facilitate efficient analysis\nof emotion coherence across facial, text, and audio modalities in presentation\nvideos. Our visualization system features a channel coherence view and a\nsentence clustering view that together enable users to obtain a quick overview\nof emotion coherence and its temporal evolution. In addition, a detail view and\nword view enable detailed exploration and comparison from the sentence level\nand word level, respectively. We thoroughly evaluate the proposed system and\nvisualization techniques through two usage scenarios based on TED Talk videos\nand interviews with two domain experts. The results demonstrate the\neffectiveness of our system in gaining insights into emotion coherence in\npresentations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 10:27:42 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:46:29 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Zeng", "Haipeng", ""], ["Wang", "Xingbo", ""], ["Wu", "Aoyu", ""], ["Wang", "Yong", ""], ["Li", "Quan", ""], ["Endert", "Alex", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.12945", "submitter": "Tao Sun", "authors": "Tao Sun, Roberto Barrio, Marcos Rodriguez, Hao Jiang", "title": "Inertial nonconvex alternating minimizations for the image deblurring", "comments": "Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2924339", "report-no": null, "categories": "math.OC cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image processing, Total Variation (TV) regularization models are commonly\nused to recover blurred images. One of the most efficient and popular methods\nto solve the convex TV problem is the Alternating Direction Method of\nMultipliers (ADMM) algorithm, recently extended using the inertial proximal\npoint method. Although all the classical studies focus on only a convex\nformulation, recent articles are paying increasing attention to the nonconvex\nmethodology due to its good numerical performance and properties. In this\npaper, we propose to extend the classical formulation with a novel nonconvex\nAlternating Direction Method of Multipliers with the Inertial technique\n(IADMM). Under certain assumptions on the parameters, we prove the convergence\nof the algorithm with the help of the Kurdyka-{\\L}ojasiewicz property. We also\npresent numerical simulations on classical TV image reconstruction problems to\nillustrate the efficiency of the new algorithm and its behavior compared with\nthe well established ADMM method.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:50:45 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Sun", "Tao", ""], ["Barrio", "Roberto", ""], ["Rodriguez", "Marcos", ""], ["Jiang", "Hao", ""]]}, {"id": "1907.13347", "submitter": "Young-Min Song", "authors": "Young-min Song, Kwangjin Yoon, Young-Chul Yoon, Kin-Choong Yow, Moongu\n  Jeon", "title": "Online Multi-Object Tracking Framework with the GMPHD Filter and\n  Occlusion Group Management", "comments": "This paper includes 15 pages and 9 figures, and has been prepared for\n  a journal (not yet submitted anywhere)", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2953276", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient online multi-object tracking framework\nbased on the GMPHD filter and occlusion group management scheme where the GMPHD\nfilter utilizes hierarchical data association to reduce the false negatives\ncaused by miss detection. The hierarchical data association consists of two\nsteps: detection-to-track and track-to-track associations, which can recover\nthe lost tracks and their switched IDs. In addition, the proposed framework is\nequipped with an object grouping management scheme which handles occlusion\nproblems with two main parts. The first part is \"track merging\" which can merge\nthe false positive tracks caused by false positive detections from occlusions,\nwhere the false positive tracks are usually occluded with a measure. The\nmeasure is the occlusion ratio between visual objects,\nsum-of-intersection-over-area (SIOA) we defined instead of the IOU metric. The\nsecond part is \"occlusion group energy minimization (OGEM)\" which prevents the\noccluded true positive tracks from false \"track merging\". We define each group\nof the occluded objects as an energy function and find an optimal hypothesis\nwhich makes the energy minimal. We evaluate the proposed tracker in benchmark\ndatasets such as MOT15 and MOT17 which are built for multi-person tracking. An\nablation study in training dataset shows that not only \"track merging\" and\n\"OGEM\" complement each other but also the proposed tracking method has more\nrobust performance and less sensitive to parameters than baseline methods.\nAlso, SIOA works better than IOU for various sizes of false positives.\nExperimental results show that the proposed tracker efficiently handles\nocclusion situations and achieves competitive performance compared to the\nstate-of-the-art methods. Especially, our method shows the best multi-object\ntracking accuracy among the online and real-time executable methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:36:09 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Song", "Young-min", ""], ["Yoon", "Kwangjin", ""], ["Yoon", "Young-Chul", ""], ["Yow", "Kin-Choong", ""], ["Jeon", "Moongu", ""]]}]