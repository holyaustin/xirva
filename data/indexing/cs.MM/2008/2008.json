[{"id": "2008.00781", "submitter": "Yilun Zhao", "authors": "Yilun Zhao, Jia Guo", "title": "MusiCoder: A Universal Music-Acoustic Encoder Based on Transformers", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-67832-6_34", "report-no": null, "categories": "eess.AS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music annotation has always been one of the critical topics in the field of\nMusic Information Retrieval (MIR). Traditional models use supervised learning\nfor music annotation tasks. However, as supervised machine learning approaches\nincrease in complexity, the increasing need for more annotated training data\ncan often not be matched with available data. In this paper, a new\nself-supervised music acoustic representation learning approach named MusiCoder\nis proposed. Inspired by the success of BERT, MusiCoder builds upon the\narchitecture of self-attention bidirectional transformers. Two pre-training\nobjectives, including Contiguous Frames Masking (CFM) and Contiguous Channels\nMasking (CCM), are designed to adapt BERT-like masked reconstruction\npre-training to continuous acoustic frame domain. The performance of MusiCoder\nis evaluated in two downstream music annotation tasks. The results show that\nMusiCoder outperforms the state-of-the-art models in both music genre\nclassification and auto-tagging tasks. The effectiveness of MusiCoder indicates\na great potential of a new self-supervised learning approach to understand\nmusic: first apply masked reconstruction tasks to pre-train a transformer-based\nmodel with massive unlabeled music acoustic data, and then finetune the model\non specific downstream tasks with labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 11:15:28 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 09:52:26 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Yilun", ""], ["Guo", "Jia", ""]]}, {"id": "2008.00836", "submitter": "Qiao Liu", "authors": "Qiao Liu, Xin Li, Zhenyu He, Chenglong Li, Jun Li, Zikun Zhou, Di\n  Yuan, Jing Li, Kai Yang, Nana Fan, Feng Zheng", "title": "LSOTB-TIR:A Large-Scale High-Diversity Thermal Infrared Object Tracking\n  Benchmark", "comments": "accepted by ACM Mutlimedia Conference, 2020", "journal-ref": null, "doi": "10.1145/3394171.3413922", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we present a Large-Scale and high-diversity general Thermal\nInfraRed (TIR) Object Tracking Benchmark, called LSOTBTIR, which consists of an\nevaluation dataset and a training dataset with a total of 1,400 TIR sequences\nand more than 600K frames. We annotate the bounding box of objects in every\nframe of all sequences and generate over 730K bounding boxes in total. To the\nbest of our knowledge, LSOTB-TIR is the largest and most diverse TIR object\ntracking benchmark to date. To evaluate a tracker on different attributes, we\ndefine 4 scenario attributes and 12 challenge attributes in the evaluation\ndataset. By releasing LSOTB-TIR, we encourage the community to develop deep\nlearning based TIR trackers and evaluate them fairly and comprehensively. We\nevaluate and analyze more than 30 trackers on LSOTB-TIR to provide a series of\nbaselines, and the results show that deep trackers achieve promising\nperformance. Furthermore, we re-train several representative deep trackers on\nLSOTB-TIR, and their results demonstrate that the proposed training dataset\nsignificantly improves the performance of deep TIR trackers. Codes and dataset\nare available at https://github.com/QiaoLiuHit/LSOTB-TIR.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:36:06 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liu", "Qiao", ""], ["Li", "Xin", ""], ["He", "Zhenyu", ""], ["Li", "Chenglong", ""], ["Li", "Jun", ""], ["Zhou", "Zikun", ""], ["Yuan", "Di", ""], ["Li", "Jing", ""], ["Yang", "Kai", ""], ["Fan", "Nana", ""], ["Zheng", "Feng", ""]]}, {"id": "2008.01023", "submitter": "Yu Han", "authors": "Yu Han, Shuai Yang, Wenjing Wang, Jiaying Liu", "title": "From Design Draft to Real Attire: Unaligned Fashion Image Translation", "comments": "Accepted by ACMMM 2020. Our project website is available at:\n  https://victoriahy.github.io/MM2020/", "journal-ref": null, "doi": "10.1145/3394171.3413953", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion manipulation has attracted growing interest due to its great\napplication value, which inspires many researches towards fashion images.\nHowever, little attention has been paid to fashion design draft. In this paper,\nwe study a new unaligned translation problem between design drafts and real\nfashion items, whose main challenge lies in the huge misalignment between the\ntwo modalities. We first collect paired design drafts and real fashion item\nimages without pixel-wise alignment. To solve the misalignment problem, our\nmain idea is to train a sampling network to adaptively adjust the input to an\nintermediate state with structure alignment to the output. Moreover, built upon\nthe sampling network, we present design draft to real fashion item translation\nnetwork (D2RNet), where two separate translation streams that focus on texture\nand shape, respectively, are combined tactfully to get both benefits. D2RNet is\nable to generate realistic garments with both texture and shape consistency to\ntheir design drafts. We show that this idea can be effectively applied to the\nreverse translation problem and present R2DNet accordingly. Extensive\nexperiments on unaligned fashion design translation demonstrate the superiority\nof our method over state-of-the-art methods. Our project website is available\nat: https://victoriahy.github.io/MM2020/ .\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:03:11 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 03:02:40 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 12:27:01 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Han", "Yu", ""], ["Yang", "Shuai", ""], ["Wang", "Wenjing", ""], ["Liu", "Jiaying", ""]]}, {"id": "2008.01190", "submitter": "SeungHeon Doh", "authors": "Seungheon Doh, Jongpil Lee, Tae Hong Park, Juhan Nam", "title": "Musical Word Embedding: Bridging the Gap between Listening Contexts and\n  Music", "comments": "Machine Learning for Media Discovery Workshop, International\n  Conference on Machine Learning (ICML), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding pioneered by Mikolov et al. is a staple technique for word\nrepresentations in natural language processing (NLP) research which has also\nfound popularity in music information retrieval tasks. Depending on the type of\ntext data for word embedding, however, vocabulary size and the degree of\nmusical pertinence can significantly vary. In this work, we (1) train the\ndistributed representation of words using combinations of both general text\ndata and music-specific data and (2) evaluate the system in terms of how they\nassociate listening contexts with musical compositions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 06:42:45 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Doh", "Seungheon", ""], ["Lee", "Jongpil", ""], ["Park", "Tae Hong", ""], ["Nam", "Juhan", ""]]}, {"id": "2008.01291", "submitter": "Ke Chen", "authors": "Ke Chen, Cheng-i Wang, Taylor Berg-Kirkpatrick, Shlomo Dubnov", "title": "Music SketchNet: Controllable Music Generation via Factorized\n  Representations of Pitch and Rhythm", "comments": "8 pages, 8 figures, Proceedings of the 21st International Society for\n  Music Information Retrieval Conference, ISMIR 2020", "journal-ref": "21st International Society for Music Information Retrieval\n  Conference, ISMIR 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drawing an analogy with automatic image completion systems, we propose Music\nSketchNet, a neural network framework that allows users to specify partial\nmusical ideas guiding automatic music generation. We focus on generating the\nmissing measures in incomplete monophonic musical pieces, conditioned on\nsurrounding context, and optionally guided by user-specified pitch and rhythm\nsnippets. First, we introduce SketchVAE, a novel variational autoencoder that\nexplicitly factorizes rhythm and pitch contour to form the basis of our\nproposed model. Then we introduce two discriminative architectures,\nSketchInpainter and SketchConnector, that in conjunction perform the guided\nmusic completion, filling in representations for the missing measures\nconditioned on surrounding context and user-specified snippets. We evaluate\nSketchNet on a standard dataset of Irish folk music and compare with models\nfrom recent works. When used for music completion, our approach outperforms the\nstate-of-the-art both in terms of objective metrics and subjective listening\ntests. Finally, we demonstrate that our model can successfully incorporate\nuser-specified snippets during the generation process.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:49:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Ke", ""], ["Wang", "Cheng-i", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Dubnov", "Shlomo", ""]]}, {"id": "2008.01334", "submitter": "Xin Wen", "authors": "Jie Shao, Xin Wen, Bingchen Zhao and Xiangyang Xue", "title": "Temporal Context Aggregation for Video Retrieval with Contrastive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current research focus on Content-Based Video Retrieval requires\nhigher-level video representation describing the long-range semantic\ndependencies of relevant incidents, events, etc. However, existing methods\ncommonly process the frames of a video as individual images or short clips,\nmaking the modeling of long-range semantic dependencies difficult. In this\npaper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a\nvideo representation learning framework that incorporates long-range temporal\ninformation between frame-level features using the self-attention mechanism. To\ntrain it on video retrieval datasets, we propose a supervised contrastive\nlearning method that performs automatic hard negative mining and utilizes the\nmemory bank mechanism to increase the capacity of negative samples. Extensive\nexperiments are conducted on multiple video retrieval tasks, such as\nCC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant\nperformance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods\nwith video-level features, and deliver competitive results with 22x faster\ninference time comparing with frame-level features.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:24:20 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 08:21:08 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Shao", "Jie", ""], ["Wen", "Xin", ""], ["Zhao", "Bingchen", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2008.01437", "submitter": "Dhruv Verma", "authors": "Dhruv Verma, Kshitij Gulati and Rajiv Ratn Shah", "title": "Addressing the Cold-Start Problem in Outfit Recommendation Using Visual\n  Preference Modelling", "comments": "Sixth IEEE International Conference on Multimedia Big Data (BigMM'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the global transformation of the fashion industry and a rise in the\ndemand for fashion items worldwide, the need for an effectual fashion\nrecommendation has never been more. Despite various cutting-edge solutions\nproposed in the past for personalising fashion recommendation, the technology\nis still limited by its poor performance on new entities, i.e. the cold-start\nproblem. In this paper, we attempt to address the cold-start problem for new\nusers, by leveraging a novel visual preference modelling approach on a small\nset of input images. We demonstrate the use of our approach with\nfeature-weighted clustering to personalise occasion-oriented outfit\nrecommendation. Quantitatively, our results show that the proposed visual\npreference modelling approach outperforms state of the art in terms of clothing\nattribute prediction. Qualitatively, through a pilot study, we demonstrate the\nefficacy of our system to provide diverse and personalised recommendations in\ncold-start scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:07:09 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Verma", "Dhruv", ""], ["Gulati", "Kshitij", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2008.01652", "submitter": "Yanhui Guo", "authors": "Yanhui Guo, Xi Zhang, Xiaolin Wu", "title": "Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos", "comments": "Accepted by Proceedings of the 28th ACM International Conference on\n  Multimedia(ACM MM),2020", "journal-ref": "Proceedings of the 28th ACM International Conference on\n  Multimedia,2020", "doi": "10.1145/3394171.3413709", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep multi-modality neural network for restoring very low\nbit rate videos of talking heads. Such video contents are very common in social\nmedia, teleconferencing, distance education, tele-medicine, etc., and often\nneed to be transmitted with limited bandwidth. The proposed CNN method exploits\nthe correlations among three modalities, video, audio and emotion state of the\nspeaker, to remove the video compression artifacts caused by spatial down\nsampling and quantization. The deep learning approach turns out to be ideally\nsuited for the video restoration task, as the complex non-linear cross-modality\ncorrelations are very difficult to model analytically and explicitly. The new\nmethod is a video post processor that can significantly boost the perceptual\nquality of aggressively compressed talking head videos, while being fully\ncompatible with all existing video compression standards.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:38:59 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Guo", "Yanhui", ""], ["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2008.01780", "submitter": "Sejal Bhalla", "authors": "Dikshant Sagar, Jatin Garg, Prarthana Kansal, Sejal Bhalla, Rajiv Ratn\n  Shah and Yi Yu", "title": "PAI-BPR: Personalized Outfit Recommendation Scheme with Attribute-wise\n  Interpretability", "comments": "10 pages, 5 figures, to be published in IEEE BigMM, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is an important part of human experience. Events such as interviews,\nmeetings, marriages, etc. are often based on clothing styles. The rise in the\nfashion industry and its effect on social influencing have made outfit\ncompatibility a need. Thus, it necessitates an outfit compatibility model to\naid people in clothing recommendation. However, due to the highly subjective\nnature of compatibility, it is necessary to account for personalization. Our\npaper devises an attribute-wise interpretable compatibility scheme with\npersonal preference modelling which captures user-item interaction along with\ngeneral item-item interaction. Our work solves the problem of interpretability\nin clothing matching by locating the discordant and harmonious attributes\nbetween fashion items. Extensive experiment results on IQON3000, a publicly\navailable real-world dataset, verify the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:30:06 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sagar", "Dikshant", ""], ["Garg", "Jatin", ""], ["Kansal", "Prarthana", ""], ["Bhalla", "Sejal", ""], ["Shah", "Rajiv Ratn", ""], ["Yu", "Yi", ""]]}, {"id": "2008.01919", "submitter": "Xiaojun Jia", "authors": "Xiaojun Jia, Xingxing Wei, Xiaochun Cao and Xiaoguang Han", "title": "Adv-watermark: A Novel Watermark Perturbation for Adversarial Examples", "comments": null, "journal-ref": "ACM MM2020", "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated that adding some imperceptible perturbations\nto original images can fool deep learning models. However, the current\nadversarial perturbations are usually shown in the form of noises, and thus\nhave no practical meaning. Image watermark is a technique widely used for\ncopyright protection. We can regard image watermark as a king of meaningful\nnoises and adding it to the original image will not affect people's\nunderstanding of the image content, and will not arouse people's suspicion.\nTherefore, it will be interesting to generate adversarial examples using\nwatermarks. In this paper, we propose a novel watermark perturbation for\nadversarial examples (Adv-watermark) which combines image watermarking\ntechniques and adversarial example algorithms. Adding a meaningful watermark to\nthe clean images can attack the DNN models. Specifically, we propose a novel\noptimization algorithm, which is called Basin Hopping Evolution (BHE), to\ngenerate adversarial watermarks in the black-box attack mode. Thanks to the\nBHE, Adv-watermark only requires a few queries from the threat models to finish\nthe attacks. A series of experiments conducted on ImageNet and CASIA-WebFace\ndatasets show that the proposed method can efficiently generate adversarial\nexamples, and outperforms the state-of-the-art attack methods. Moreover,\nAdv-watermark is more robust against image transformation defense methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 03:28:43 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 19:10:01 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Jia", "Xiaojun", ""], ["Wei", "Xingxing", ""], ["Cao", "Xiaochun", ""], ["Han", "Xiaoguang", ""]]}, {"id": "2008.02427", "submitter": "Yazhou Yao", "authors": "Zeren Sun, Xian-Sheng Hua, Yazhou Yao, Xiu-Shen Wei, Guosheng Hu, Jian\n  Zhang", "title": "Salvage Reusable Samples from Noisy Data for Robust Learning", "comments": "accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the existence of label noise in web images and the high memorization\ncapacity of deep neural networks, training deep fine-grained (FG) models\ndirectly through web images tends to have an inferior recognition ability. In\nthe literature, to alleviate this issue, loss correction methods try to\nestimate the noise transition matrix, but the inevitable false correction would\ncause severe accumulated errors. Sample selection methods identify clean\n(\"easy\") samples based on the fact that small losses can alleviate the\naccumulated errors. However, \"hard\" and mislabeled examples that can both boost\nthe robustness of FG models are also dropped. To this end, we propose a\ncertainty-based reusable sample selection and correction approach, termed as\nCRSSC, for coping with label noise in training deep FG models with web images.\nOur key idea is to additionally identify and correct reusable samples, and then\nleverage them together with clean examples to update the networks. We\ndemonstrate the superiority of the proposed approach from both theoretical and\nexperimental perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 02:07:21 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Sun", "Zeren", ""], ["Hua", "Xian-Sheng", ""], ["Yao", "Yazhou", ""], ["Wei", "Xiu-Shen", ""], ["Hu", "Guosheng", ""], ["Zhang", "Jian", ""]]}, {"id": "2008.02438", "submitter": "Yazhou Yao", "authors": "Chuanyi Zhang, Yazhou Yao, Xiangbo Shu, Zechao Li, Zhenmin Tang, Qi Wu", "title": "Data-driven Meta-set Based Fine-Grained Visual Classification", "comments": "accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing fine-grained image datasets typically requires domain-specific\nexpert knowledge, which is not always available for crowd-sourcing platform\nannotators. Accordingly, learning directly from web images becomes an\nalternative method for fine-grained visual recognition. However, label noise in\nthe web training set can severely degrade the model performance. To this end,\nwe propose a data-driven meta-set based approach to deal with noisy web images\nfor fine-grained recognition. Specifically, guided by a small amount of clean\nmeta-set, we train a selection net in a meta-learning manner to distinguish in-\nand out-of-distribution noisy images. To further boost the robustness of model,\nwe also learn a labeling net to correct the labels of in-distribution noisy\ndata. In this way, our proposed method can alleviate the harmful effects caused\nby out-of-distribution noise and properly exploit the in-distribution noisy\nsamples for training. Extensive experiments on three commonly used fine-grained\ndatasets demonstrate that our approach is much superior to state-of-the-art\nnoise-robust methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 03:04:16 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Zhang", "Chuanyi", ""], ["Yao", "Yazhou", ""], ["Shu", "Xiangbo", ""], ["Li", "Zechao", ""], ["Tang", "Zhenmin", ""], ["Wu", "Qi", ""]]}, {"id": "2008.02661", "submitter": "Amir Shirian", "authors": "A. Shirian, S. Tripathi, T. Guha", "title": "Dynamic Emotion Modeling with Learnable Graphs and Graph Inception\n  Network", "comments": null, "journal-ref": "10.1109/TMM.2021.3059169", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human emotion is expressed, perceived and captured using a variety of dynamic\ndata modalities, such as speech (verbal), videos (facial expressions) and\nmotion sensors (body gestures). We propose a generalized approach to emotion\nrecognition that can adapt across modalities by modeling dynamic data as\nstructured graphs. The motivation behind the graph approach is to build compact\nmodels without compromising on performance. To alleviate the problem of optimal\ngraph construction, we cast this as a joint graph learning and classification\ntask. To this end, we present the Learnable Graph Inception Network (L-GrIN)\nthat jointly learns to recognize emotion and to identify the underlying graph\nstructure in the dynamic data. Our architecture comprises multiple novel\ncomponents: a new graph convolution operation, a graph inception layer,\nlearnable adjacency, and a learnable pooling function that yields a graph-level\nembedding. We evaluate the proposed architecture on five benchmark emotion\nrecognition databases spanning three different modalities (video, audio, motion\ncapture), where each database captures one of the following emotional cues:\nfacial expressions, speech and body gestures. We achieve state-of-the-art\nperformance on all five databases outperforming several competitive baselines\nand relevant existing methods. Our graph architecture shows superior\nperformance with significantly fewer parameters (compared to convolutional or\nrecurrent neural networks) promising its applicability to resource-constrained\ndevices.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:51:31 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 12:21:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Shirian", "A.", ""], ["Tripathi", "S.", ""], ["Guha", "T.", ""]]}, {"id": "2008.02686", "submitter": "Liangfa Wei", "authors": "Liangfa Wei, Jie Zhang, Junfeng Hou and Lirong Dai", "title": "Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based\n  Robust Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual information fusion enables a performance improvement in speech\nrecognition performed in complex acoustic scenarios, e.g., noisy environments.\nIt is required to explore an effective audio-visual fusion strategy for\naudiovisual alignment and modality reliability. Different from the previous\nend-to-end approaches where the audio-visual fusion is performed after encoding\neach modality, in this paper we propose to integrate an attentive fusion block\ninto the encoding process. It is shown that the proposed audio-visual fusion\nmethod in the encoder module can enrich audio-visual representations, as the\nrelevance between the two modalities is leveraged. In line with the\ntransformer-based architecture, we implement the embedded fusion block using a\nmulti-head attention based audiovisual fusion with one-way or two-way\ninteractions. The proposed method can sufficiently combine the two streams and\nweaken the over-reliance on the audio modality. Experiments on the LRS3-TED\ndataset demonstrate that the proposed method can increase the recognition rate\nby 0.55%, 4.51% and 4.61% on average under the clean, seen and unseen noise\nconditions, respectively, compared to the state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 14:39:07 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Wei", "Liangfa", ""], ["Zhang", "Jie", ""], ["Hou", "Junfeng", ""], ["Dai", "Lirong", ""]]}, {"id": "2008.02734", "submitter": "Christopher Tralie", "authors": "Christopher Tralie, Elizabeth Dempsey", "title": "Exact, Parallelizable Dynamic Time Warping Alignment with Linear Memory", "comments": "12 Pages, 6 Figures, 1 Table, ISMIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio alignment is a fundamental preprocessing step in many MIR pipelines.\nFor two audio clips with M and N frames, respectively, the most popular\napproach, dynamic time warping (DTW), has O(MN) requirements in both memory and\ncomputation, which is prohibitive for frame-level alignments at reasonable\nrates. To address this, a variety of memory efficient algorithms exist to\napproximate the optimal alignment under the DTW cost. To our knowledge,\nhowever, no exact algorithms exist that are guaranteed to break the quadratic\nmemory barrier. In this work, we present a divide and conquer algorithm that\ncomputes the exact globally optimal DTW alignment using O(M+N) memory. Its\nruntime is still O(MN), trading off memory for a 2x increase in computation.\nHowever, the algorithm can be parallelized up to a factor of min(M, N) with the\nsame memory constraints, so it can still run more efficiently than the textbook\nversion with an adequate GPU. We use our algorithm to compute exact alignments\non a collection of orchestral music, which we use as ground truth to benchmark\nthe alignment accuracy of several popular approximate alignment schemes at\nscales that were not previously possible.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 15:00:33 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Tralie", "Christopher", ""], ["Dempsey", "Elizabeth", ""]]}, {"id": "2008.02749", "submitter": "Lucia Vadicamo", "authors": "Giuseppe Amato, Paolo Bolettieri, Fabio Carrara, Franca Debole,\n  Fabrizio Falchi, Claudio Gennaro, Lucia Vadicamo, Claudio Vairo", "title": "The VISIONE Video Search System: Exploiting Off-the-Shelf Text Search\n  Engines for Large-Scale Video Retrieval", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe in details VISIONE, a video search system that\nallows users to search for videos using textual keywords, occurrence of objects\nand their spatial relationships, occurrence of colors and their spatial\nrelationships, and image similarity. These modalities can be combined together\nto express complex queries and satisfy user needs. The peculiarity of our\napproach is that we encode all the information extracted from the keyframes,\nsuch as visual deep features, tags, color and object locations, using a\nconvenient textual encoding indexed in a single text retrieval engine. This\noffers great flexibility when results corresponding to various parts of the\nquery (visual, text and locations) have to be merged. In addition, we report an\nextensive analysis of the system retrieval performance, using the query logs\ngenerated during the Video Browser Showdown (VBS) 2019 competition. This\nallowed us to fine-tune the system by choosing the optimal parameters and\nstrategies among the ones that we tested.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 16:32:17 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 14:37:27 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Amato", "Giuseppe", ""], ["Bolettieri", "Paolo", ""], ["Carrara", "Fabio", ""], ["Debole", "Franca", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Vadicamo", "Lucia", ""], ["Vairo", "Claudio", ""]]}, {"id": "2008.03379", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, H. M. de Oliveira, C. O. Cintra", "title": "Rounded Hartley Transform: A Quasi-involution", "comments": "6 pages. Manuscript originally published in 2002 at the International\n  Telecommunications Symposium ITS 2002. Readers are encouraged to access newer\n  results at arXiv:2007.02232", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new multiplication-free transform derived from DHT is introduced: the RHT.\nInvestigations on the properties of the RHT led us to the concept of\nweak-inversion. Using new constructs, we show that RHT is not involutional like\nthe DHT, but exhibits quasi-involutional property, a new definition derived\nfrom the periodicity of matrices. Thus instead of using the actual inverse\ntransform, the RHT is viewed as an involutional transform, allowing the use of\ndirect (multiplication-free) to evaluate the inverse. A fast algorithm to\ncompute RHT is presented. This algorithm show embedded properties. We also\nextended RHT to the two-dimensional case. This permitted us to perform a\npreliminary analysis on the effects of RHT on images. Despite of some SNR loss,\nRHT can be very interesting for applications involving image monitoring\nassociated to decision making, such as military applications or medical\nimaging.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 21:10:19 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cintra", "R. J.", ""], ["de Oliveira", "H. M.", ""], ["Cintra", "C. O.", ""]]}, {"id": "2008.03546", "submitter": "Anyi Rao", "authors": "Jiangyue Xia, Anyi Rao, Qingqiu Huang, Linning Xu, Jiangtao Wen, Dahua\n  Lin", "title": "Online Multi-modal Person Search in Videos", "comments": "ECCV2020. Project page:\n  http://movienet.site/projects/eccv20onlineperson.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of searching certain people in videos has seen increasing potential\nin real-world applications, such as video organization and editing. Most\nexisting approaches are devised to work in an offline manner, where identities\ncan only be inferred after an entire video is examined. This working manner\nprecludes such methods from being applied to online services or those\napplications that require real-time responses. In this paper, we propose an\nonline person search framework, which can recognize people in a video on the\nfly. This framework maintains a multimodal memory bank at its heart as the\nbasis for person recognition, and updates it dynamically with a policy obtained\nby reinforcement learning. Our experiments on a large movie dataset show that\nthe proposed method is effective, not only achieving remarkable improvements\nover online schemes but also outperforming offline methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:48:32 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xia", "Jiangyue", ""], ["Rao", "Anyi", ""], ["Huang", "Qingqiu", ""], ["Xu", "Linning", ""], ["Wen", "Jiangtao", ""], ["Lin", "Dahua", ""]]}, {"id": "2008.03548", "submitter": "Anyi Rao", "authors": "Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei\n  Zhou, Dahua Lin", "title": "A Unified Framework for Shot Type Classification Based on Subject\n  Centric Lens", "comments": "ECCV2020. Project page: https://anyirao.com/projects/ShotType.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shots are key narrative elements of various videos, e.g. movies, TV series,\nand user-generated videos that are thriving over the Internet. The types of\nshots greatly influence how the underlying ideas, emotions, and messages are\nexpressed. The technique to analyze shot types is important to the\nunderstanding of videos, which has seen increasing demand in real-world\napplications in this era. Classifying shot type is challenging due to the\nadditional information required beyond the video content, such as the spatial\ncomposition of a frame and camera movement. To address these issues, we propose\na learning framework Subject Guidance Network (SGNet) for shot type\nrecognition. SGNet separates the subject and background of a shot into two\nstreams, serving as separate guidance maps for scale and movement type\nclassification respectively. To facilitate shot type analysis and model\nevaluations, we build a large-scale dataset MovieShots, which contains 46K\nshots from 7K movie trailers with annotations of their scale and movement\ntypes. Experiments show that our framework is able to recognize these two\nattributes of shot accurately, outperforming all the previous methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:49:40 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rao", "Anyi", ""], ["Wang", "Jiaze", ""], ["Xu", "Linning", ""], ["Jiang", "Xuekun", ""], ["Huang", "Qingqiu", ""], ["Zhou", "Bolei", ""], ["Lin", "Dahua", ""]]}, {"id": "2008.03592", "submitter": "Sefik Emre Eskimez", "authors": "Sefik Emre Eskimez, You Zhang, Zhiyao Duan", "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion\n  Condition", "comments": "Accepted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 20:46:31 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 22:45:01 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Eskimez", "Sefik Emre", ""], ["Zhang", "You", ""], ["Duan", "Zhiyao", ""]]}, {"id": "2008.03715", "submitter": "Chirag Raman", "authors": "Chirag Raman, Stephanie Tan, Hayley Hung", "title": "A Modular Approach for Synchronized Wireless Multimodal Multisensor Data\n  Acquisition in Highly Dynamic Social Settings", "comments": "9 pages, 8 figures, Proceedings of the 28th ACM International\n  Conference on Multimedia (MM '20), October 12--16, 2020, Seattle, WA, USA.\n  First two authors contributed equally", "journal-ref": null, "doi": "10.1145/3394171.3413697", "report-no": null, "categories": "eess.SP cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing data acquisition literature for human behavior research provides\nwired solutions, mainly for controlled laboratory setups. In uncontrolled\nfree-standing conversation settings, where participants are free to walk\naround, these solutions are unsuitable. While wireless solutions are employed\nin the broadcasting industry, they can be prohibitively expensive. In this\nwork, we propose a modular and cost-effective wireless approach for\nsynchronized multisensor data acquisition of social human behavior. Our core\nidea involves a cost-accuracy trade-off by using Network Time Protocol (NTP) as\na source reference for all sensors. While commonly used as a reference in\nubiquitous computing, NTP is widely considered to be insufficiently accurate as\na reference for video applications, where Precision Time Protocol (PTP) or\nGlobal Positioning System (GPS) based references are preferred. We argue and\nshow, however, that the latency introduced by using NTP as a source reference\nis adequate for human behavior research, and the subsequent cost and modularity\nbenefits are a desirable trade-off for applications in this domain. We also\ndescribe one instantiation of the approach deployed in a real-world experiment\nto demonstrate the practicality of our setup in-the-wild.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 12:31:05 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Raman", "Chirag", ""], ["Tan", "Stephanie", ""], ["Hung", "Hayley", ""]]}, {"id": "2008.03889", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang and Ming Jiang", "title": "Norm-in-Norm Loss with Faster Convergence and Better Performance for\n  Image Quality Assessment", "comments": "Accepted by ACM MM 2020, + supplemental materials", "journal-ref": null, "doi": "10.1145/3394171.3413804", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, most image quality assessment (IQA) models are supervised by the\nMAE or MSE loss with empirically slow convergence. It is well-known that\nnormalization can facilitate fast convergence. Therefore, we explore\nnormalization in the design of loss functions for IQA. Specifically, we first\nnormalize the predicted quality scores and the corresponding subjective quality\nscores. Then, the loss is defined based on the norm of the differences between\nthese normalized values. The resulting \"Norm-in-Norm'' loss encourages the IQA\nmodel to make linear predictions with respect to subjective quality scores.\nAfter training, the least squares regression is applied to determine the linear\nmapping from the predicted quality to the subjective quality. It is shown that\nthe new loss is closely connected with two common IQA performance criteria\n(PLCC and RMSE). Through theoretical analysis, it is proved that the embedded\nnormalization makes the gradients of the loss function more stable and more\npredictable, which is conducive to the faster convergence of the IQA model.\nFurthermore, to experimentally verify the effectiveness of the proposed loss,\nit is applied to solve a challenging problem: quality assessment of in-the-wild\nimages. Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that,\ncompared to MAE or MSE loss, the new loss enables the IQA model to converge\nabout 10 times faster and the final model achieves better performance. The\nproposed model also achieves state-of-the-art prediction performance on this\nchallenging problem. For reproducible scientific research, our code is publicly\navailable at https://github.com/lidq92/LinearityIQA.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:01:21 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "2008.04511", "submitter": "Mario Montagud Climent", "authors": "Henrique Galvan Debarba, Mario Montagud, Sylvain Chagu\\'e, Javier\n  Lajara, Ignacio Lacosta, Sergi Fernandez Langa, Caecilia Charbonnier", "title": "Content Format and Quality of Experience in Virtual Reality", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate three forms of virtual reality content\nproduction and consumption. Namely, 360 stereoscopic video, the combination of\na 3D environment with a video billboard for dynamic elements, and a full 3D\nrendered scene. On one hand, video based techniques facilitate the acquisition\nof content, but they can limit the experience of the user since the content is\ncaptured from a fixed point of view. On the other hand, 3D content allows for\npoint of view translation, but real-time photorealistic rendering is not\ntrivial and comes at high production and processing costs. We also compare the\ntwo extremes with an approach that combines dynamic video elements with a 3D\nvirtual environment. We discuss the advantages and disadvantages of these\nsystems, and present the result of a user study with 24 participants. In the\nstudy, we evaluated the quality of experience, including presence, simulation\nsickness and participants' assessment of content quality, of three versions of\na cinematic segment with two actors. We found that, in this context, mixing\nvideo and 3D content produced the best experience.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 04:56:53 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Debarba", "Henrique Galvan", ""], ["Montagud", "Mario", ""], ["Chagu\u00e9", "Sylvain", ""], ["Lajara", "Javier", ""], ["Lacosta", "Ignacio", ""], ["Langa", "Sergi Fernandez", ""], ["Charbonnier", "Caecilia", ""]]}, {"id": "2008.04558", "submitter": "Hiroyuki Kobayashi", "authors": "Hiroyuki Kobayashi and Hitoshi Kiya", "title": "Extension of JPEG XS for Two-Layer Lossless Coding", "comments": "to appear in 2020 IEEE 9th Global Conference on Consumer Electronics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-layer lossless image coding method compatible with JPEG XS is proposed.\nJPEG XS is a new international standard for still image coding that has the\ncharacteristics of very low latency and very low complexity. However, it does\nnot support lossless coding, although it can achieve visual lossless coding.\nThe proposed method has a two-layer structure similar to JPEG XT, which\nconsists of JPEG XS coding and a lossless coding method. As a result, it\nenables us to losslessly restore original images, while maintaining\ncompatibility with JPEG XS.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 07:14:17 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Kobayashi", "Hiroyuki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2008.04585", "submitter": "YueFeng Chen", "authors": "Xiaodan Li, Yining Lang, Yuefeng Chen, Xiaofeng Mao, Yuan He, Shuhui\n  Wang, Hui Xue, Quan Lu", "title": "Sharp Multiple Instance Learning for DeepFake Video Detection", "comments": "Accepted at ACM MM 2020. 11 pages, 8 figures, with appendix", "journal-ref": "Proceedings of the 28th ACM International Conference on\n  Multimedia, 2020", "doi": "10.1145/3394171.3414034", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of facial manipulation techniques, face forgery\nhas received considerable attention in multimedia and computer vision community\ndue to security concerns. Existing methods are mostly designed for single-frame\ndetection trained with precise image-level labels or for video-level prediction\nby only modeling the inter-frame inconsistency, leaving potential high risks\nfor DeepFake attackers. In this paper, we introduce a new problem of partial\nface attack in DeepFake video, where only video-level labels are provided but\nnot all the faces in the fake videos are manipulated. We address this problem\nby multiple instance learning framework, treating faces and input video as\ninstances and bag respectively. A sharp MIL (S-MIL) is proposed which builds\ndirect mapping from instance embeddings to bag prediction, rather than from\ninstance embeddings to instance prediction and then to bag prediction in\ntraditional MIL. Theoretical analysis proves that the gradient vanishing in\ntraditional MIL is relieved in S-MIL. To generate instances that can accurately\nincorporate the partially manipulated faces, spatial-temporal encoded instance\nis designed to fully model the intra-frame and inter-frame inconsistency, which\nfurther helps to promote the detection performance. We also construct a new\ndataset FFPMS for partially attacked DeepFake video detection, which can\nbenefit the evaluation of different methods at both frame and video levels.\nExperiments on FFPMS and the widely used DFDC dataset verify that S-MIL is\nsuperior to other counterparts for partially attacked DeepFake video detection.\nIn addition, S-MIL can also be adapted to traditional DeepFake image detection\ntasks and achieve state-of-the-art performance on single-frame datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 08:52:17 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Xiaodan", ""], ["Lang", "Yining", ""], ["Chen", "Yuefeng", ""], ["Mao", "Xiaofeng", ""], ["He", "Yuan", ""], ["Wang", "Shuhui", ""], ["Xue", "Hui", ""], ["Lu", "Quan", ""]]}, {"id": "2008.04638", "submitter": "Marco Comunita'", "authors": "Marco Comunit\\`a, Andrea Gerino, Veranika Lim, Lorenzo Picinali", "title": "PlugSonic: a web- and mobile-based platform for binaural audio and sonic\n  narratives", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PlugSonic is a suite of web- and mobile-based applications for the curation\nand experience of binaural interactive soundscapes and sonic narratives. It was\ndeveloped as part of the PLUGGY EU project (Pluggable Social Platform for\nHeritage Awareness and Participation) and consists of two main applications:\nPlugSonic Sample, to edit and apply audio effects, and PlugSonic Soundscape, to\ncreate and experience binaural soundscapes. The audio processing within\nPlugSonic is based on the Web Audio API and the 3D Tune-In Toolkit, while the\nexploration of soundscapes in a physical space is obtained using Apple's ARKit.\nIn this paper we present the design choices, the user involvement processes and\nthe implementation details. The main goal of PlugSonic is technology\ndemocratisation; PlugSonic users - whether institutions or citizens - are all\ngiven the instruments needed to create, process and experience 3D soundscapes\nand sonic narrative; without the need for specific devices, external tools\n(software and/or hardware), specialised knowledge or custom development. The\nevaluation, which was conducted with inexperienced users on three tasks -\ncreation, curation and experience - demonstrates how PlugSonic is indeed a\nsimple, effective, yet powerful tool.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 11:42:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Comunit\u00e0", "Marco", ""], ["Gerino", "Andrea", ""], ["Lim", "Veranika", ""], ["Picinali", "Lorenzo", ""]]}, {"id": "2008.04687", "submitter": "Junchen Jiang", "authors": "Xu Zhang, Yiyang Ou, Siddhartha Sen, Junchen Jiang", "title": "SENSEI: Aligning Video Streaming Quality with Dynamic User Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to improve video streaming by leveraging a simple\nobservation: users are more sensitive to low quality in certain parts of a\nvideo than in others. For instance, rebuffering during key moments of a sports\nvideo (e.g., before a goal is scored) is more annoying than rebuffering during\nnormal gameplay. Such dynamic quality sensitivity, however, is rarely captured\nby current approaches, which predict QoE (quality-of-experience) using\none-size-fits-all heuristics that are too simplistic to understand the nuances\nof video content. Instead of proposing yet another heuristic, we take a\ndifferent approach: we run a separate crowdsourcing experiment for each video\nto derive users' quality sensitivity at different parts of the video. Of\ncourse, the cost of doing this at scale can be prohibitive, but we show that\ncareful experiment design combined with a suite of pruning techniques can make\nthe cost negligible compared to how much content providers invest in content\ngeneration and distribution. Our ability to accurately profile time-varying\nuser sensitivity inspires a new approach: dynamically aligning higher (lower)\nquality with higher (lower) sensitivity periods. We present a new video\nstreaming system called SENSEI that incorporates dynamic quality sensitivity\ninto existing quality adaptation algorithms. We apply SENSEI to two\nstate-of-the-art adaptation algorithms. SENSEI can take seemingly unusual\nactions: e.g., lowering bitrate (or initiating a rebuffering event) even when\nbandwidth is sufficient so that it can maintain a higher bitrate without\nrebuffering when quality sensitivity becomes higher in the near future.\nCompared to state-of-the-art approaches, SENSEI improves QoE by 15.1% or\nachieves the same QoE with 26.8% less bandwidth on average.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:18:29 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhang", "Xu", ""], ["Ou", "Yiyang", ""], ["Sen", "Siddhartha", ""], ["Jiang", "Junchen", ""]]}, {"id": "2008.05255", "submitter": "Jiangkai Wu", "authors": "Zichuan Xu, Jiangkai Wu, Qiufen Xia, Pan Zhou, Jiankang Ren, Huizhi\n  Liang", "title": "Identity-Aware Attribute Recognition via Real-Time Distributed Inference\n  in Mobile Edge Clouds", "comments": "9 pages, 8 figures, Proceedings of the 28th ACM International\n  Conference on Multimedia (ACM MM'20), Seattle, WA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning technologies, attribute recognition and\nperson re-identification (re-ID) have attracted extensive attention and\nachieved continuous improvement via executing computing-intensive deep neural\nnetworks in cloud datacenters. However, the datacenter deployment cannot meet\nthe real-time requirement of attribute recognition and person re-ID, due to the\nprohibitive delay of backhaul networks and large data transmissions from\ncameras to datacenters. A feasible solution thus is to employ mobile edge\nclouds (MEC) within the proximity of cameras and enable distributed inference.\nIn this paper, we design novel models for pedestrian attribute recognition with\nre-ID in an MEC-enabled camera monitoring system. We also investigate the\nproblem of distributed inference in the MEC-enabled camera network. To this\nend, we first propose a novel inference framework with a set of distributed\nmodules, by jointly considering the attribute recognition and person re-ID. We\nthen devise a learning-based algorithm for the distributions of the modules of\nthe proposed distributed inference framework, considering the dynamic\nMEC-enabled camera network with uncertainties. We finally evaluate the\nperformance of the proposed algorithm by both simulations with real datasets\nand system implementation in a real testbed. Evaluation results show that the\nperformance of the proposed algorithm with distributed inference framework is\npromising, by reaching the accuracies of attribute recognition and person\nidentification up to 92.9% and 96.6% respectively, and significantly reducing\nthe inference delay by at least 40.6% compared with existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 12:03:27 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Xu", "Zichuan", ""], ["Wu", "Jiangkai", ""], ["Xia", "Qiufen", ""], ["Zhou", "Pan", ""], ["Ren", "Jiankang", ""], ["Liang", "Huizhi", ""]]}, {"id": "2008.05359", "submitter": "Weiqing Min", "authors": "Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng,\n  Shuqiang Jiang", "title": "LogoDet-3K: A Large-Scale Image Dataset for Logo Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logo detection has been gaining considerable attention because of its wide\nrange of applications in the multimedia field, such as copyright infringement\ndetection, brand visibility monitoring, and product brand management on social\nmedia. In this paper, we introduce LogoDet-3K, the largest logo detection\ndataset with full annotation, which has 3,000 logo categories, about 200,000\nmanually annotated logo objects and 158,652 images. LogoDet-3K creates a more\nchallenging benchmark for logo detection, for its higher comprehensive coverage\nand wider variety in both logo categories and annotated objects compared with\nexisting datasets. We describe the collection and annotation process of our\ndataset, analyze its scale and diversity in comparison to other datasets for\nlogo detection. We further propose a strong baseline method Logo-Yolo, which\nincorporates Focal loss and CIoU loss into the state-of-the-art YOLOv3\nframework for large-scale logo detection. Logo-Yolo can solve the problems of\nmulti-scale objects, logo sample imbalance and inconsistent bounding-box\nregression. It obtains about 4% improvement on the average performance compared\nwith YOLOv3, and greater improvements compared with reported several deep\ndetection models on LogoDet-3K. The evaluations on other three existing\ndatasets further verify the effectiveness of our method, and demonstrate better\ngeneralization ability of LogoDet-3K on logo detection and retrieval tasks. The\nLogoDet-3K dataset is used to promote large-scale logo-related research and it\ncan be found at https://github.com/Wangjing1551/LogoDet-3K-Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:57:53 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Jing", ""], ["Min", "Weiqing", ""], ["Hou", "Sujuan", ""], ["Ma", "Shengnan", ""], ["Zheng", "Yuanjie", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "2008.05642", "submitter": "Rongqun Lin", "authors": "Rongqun Lin, Linwei Zhu, Shiqi Wang and Sam Kwong", "title": "Towards Modality Transferable Visual Information Representation with\n  Optimal Model Compression", "comments": "Accepted in ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compactly representing the visual signals is of fundamental importance in\nvarious image/video-centered applications. Although numerous approaches were\ndeveloped for improving the image and video coding performance by removing the\nredundancies within visual signals, much less work has been dedicated to the\ntransformation of the visual signals to another well-established modality for\nbetter representation capability. In this paper, we propose a new scheme for\nvisual signal representation that leverages the philosophy of transferable\nmodality. In particular, the deep learning model, which characterizes and\nabsorbs the statistics of the input scene with online training, could be\nefficiently represented in the sense of rate-utility optimization to serve as\nthe enhancement layer in the bitstream. As such, the overall performance can be\nfurther guaranteed by optimizing the new modality incorporated. The proposed\nframework is implemented on the state-of-the-art video coding standard (i.e.,\nversatile video coding), and significantly better representation capability has\nbeen observed based on extensive evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 01:52:40 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lin", "Rongqun", ""], ["Zhu", "Linwei", ""], ["Wang", "Shiqi", ""], ["Kwong", "Sam", ""]]}, {"id": "2008.05655", "submitter": "Weiqing Min", "authors": "Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei,\n  Xiaolin Wei, Shuqiang Jiang", "title": "ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked\n  Global-Local Attention Network", "comments": "Accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Food recognition has received more and more attention in the multimedia\ncommunity for its various real-world applications, such as diet management and\nself-service restaurants. A large-scale ontology of food images is urgently\nneeded for developing advanced large-scale food recognition algorithms, as well\nas for providing the benchmark dataset for such algorithms. To encourage\nfurther progress in food recognition, we introduce the dataset ISIA Food- 500\nwith 500 categories from the list in the Wikipedia and 399,726 images, a more\ncomprehensive food dataset that surpasses existing popular benchmark datasets\nby category coverage and data volume. Furthermore, we propose a stacked\nglobal-local attention network, which consists of two sub-networks for food\nrecognition. One subnetwork first utilizes hybrid spatial-channel attention to\nextract more discriminative features, and then aggregates these multi-scale\ndiscriminative features from multiple layers into global-level representation\n(e.g., texture and shape information about food). The other one generates\nattentional regions (e.g., ingredient relevant regions) from different regions\nvia cascaded spatial transformers, and further aggregates these multi-scale\nregional features from different layers into local-level representation. These\ntwo types of features are finally fused as comprehensive representation for\nfood recognition. Extensive experiments on ISIA Food-500 and other two popular\nbenchmark datasets demonstrate the effectiveness of our proposed method, and\nthus can be considered as one strong baseline. The dataset, code and models can\nbe found at http://123.57.42.89/FoodComputing-Dataset/ISIA-Food500.html.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:48:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Min", "Weiqing", ""], ["Liu", "Linhu", ""], ["Wang", "Zhiling", ""], ["Luo", "Zhengdong", ""], ["Wei", "Xiaoming", ""], ["Wei", "Xiaolin", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "2008.05672", "submitter": "Chen-Hsiu Huang", "authors": "Chen-Hsiu Huang and Ja-Ling Wu", "title": "JQF: Optimal JPEG Quantization Table Fusion by Simulated Annealing on\n  Texture Images and Predicting Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG has been a widely used lossy image compression codec for nearly three\ndecades. The JPEG standard allows to use customized quantization table;\nhowever, it's still a challenging problem to find an optimal quantization table\nwithin acceptable computational cost. This work tries to solve the dilemma of\nbalancing between computational cost and image specific optimality by\nintroducing a new concept of texture mosaic images. Instead of optimizing a\nsingle image or a collection of representative images, the simulated annealing\ntechnique is applied to texture mosaic images to search for an optimal\nquantization table for each texture category. We use pre-trained VGG-16 CNN\nmodel to learn those texture features and predict the new image's texture\ndistribution, then fuse optimal texture tables to come out with an image\nspecific optimal quantization table. On the Kodak dataset with the quality\nsetting $Q=95$, our experiment shows a size reduction of 23.5% over the JPEG\nstandard table with a slightly 0.35% FSIM decrease, which is visually\nunperceivable. The proposed JQF method achieves per image optimality for JPEG\nencoding with less than one second additional timing cost. The online demo is\navailable at https://matthorn.s3.amazonaws.com/JQF/qtbl_vis.html\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 03:43:35 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 01:03:32 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Huang", "Chen-Hsiu", ""], ["Wu", "Ja-Ling", ""]]}, {"id": "2008.05789", "submitter": "Ying Cheng", "authors": "Ying Cheng, Ruize Wang, Zhihao Pan, Rui Feng, Yuejie Zhang", "title": "Look, Listen, and Attend: Co-Attention Network for Self-Supervised\n  Audio-Visual Representation Learning", "comments": "Accepted by the 28th ACM International Conference on Multimedia (ACM\n  MM 2020)", "journal-ref": null, "doi": "10.1145/3394171.3413869", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When watching videos, the occurrence of a visual event is often accompanied\nby an audio event, e.g., the voice of lip motion, the music of playing\ninstruments. There is an underlying correlation between audio and visual\nevents, which can be utilized as free supervised information to train a neural\nnetwork by solving the pretext task of audio-visual synchronization. In this\npaper, we propose a novel self-supervised framework with co-attention mechanism\nto learn generic cross-modal representations from unlabelled videos in the\nwild, and further benefit downstream tasks. Specifically, we explore three\ndifferent co-attention modules to focus on discriminative visual regions\ncorrelated to the sounds and introduce the interactions between them.\nExperiments show that our model achieves state-of-the-art performance on the\npretext task while having fewer parameters compared with existing methods. To\nfurther evaluate the generalizability and transferability of our approach, we\napply the pre-trained model on two downstream tasks, i.e., sound source\nlocalization and action recognition. Extensive experiments demonstrate that our\nmodel provides competitive results with other self-supervised methods, and also\nindicate that our approach can tackle the challenging scenes which contain\nmultiple sound sources.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 10:08:12 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Cheng", "Ying", ""], ["Wang", "Ruize", ""], ["Pan", "Zhihao", ""], ["Feng", "Rui", ""], ["Zhang", "Yuejie", ""]]}, {"id": "2008.05889", "submitter": "Grigory Antipov", "authors": "Grigory Antipov, Nicolas Gengembre, Olivier Le Blouch, Ga\\\"el Le Lan", "title": "Automatic Quality Assessment for Audio-Visual Verification Systems. The\n  LOVe submission to NIST SRE Challenge 2019", "comments": "5 pages, 1 figure, accepted at INTERSPEECH 2020. Corrected the\n  reference [20]", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusion of scores is a cornerstone of multimodal biometric systems composed of\nindependent unimodal parts. In this work, we focus on quality-dependent fusion\nfor speaker-face verification. To this end, we propose a universal model which\ncan be trained for automatic quality assessment of both face and speaker\nmodalities. This model estimates the quality of representations produced by\nunimodal systems which are then used to enhance the score-level fusion of\nspeaker and face verification modules. We demonstrate the improvements brought\nby this quality-dependent fusion on the recent NIST SRE19 Audio-Visual\nChallenge dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 13:21:48 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 07:33:03 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Antipov", "Grigory", ""], ["Gengembre", "Nicolas", ""], ["Blouch", "Olivier Le", ""], ["Lan", "Ga\u00ebl Le", ""]]}, {"id": "2008.05924", "submitter": "Xingxun Jiang", "authors": "Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia,\n  Cheng Lu, Jiateng Liu", "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions\n  in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, facial expression recognition (FER) in the wild has gained a lot of\nresearchers' attention because it is a valuable topic to enable the FER\ntechniques to move from the laboratory to the real applications. In this paper,\nwe focus on this challenging but interesting topic and make contributions from\nthree aspects. First, we present a new large-scale 'in-the-wild' dynamic facial\nexpression database, DFEW (Dynamic Facial Expression in the Wild), consisting\nof over 16,000 video clips from thousands of movies. These video clips contain\nvarious challenging interferences in practical scenarios such as extreme\nillumination, occlusions, and capricious pose changes. Second, we propose a\nnovel method called Expression-Clustered Spatiotemporal Feature Learning\n(EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct\nextensive benchmark experiments on DFEW using a lot of spatiotemporal deep\nfeature learning methods as well as our proposed EC-STFL. Experimental results\nshow that DFEW is a well-designed and challenging database, and the proposed\nEC-STFL can promisingly improve the performance of existing spatiotemporal deep\nneural networks in coping with the problem of dynamic FER in the wild. Our DFEW\ndatabase is publicly available and can be freely downloaded from\nhttps://dfew-dataset.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:10:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Jiang", "Xingxun", ""], ["Zong", "Yuan", ""], ["Zheng", "Wenming", ""], ["Tang", "Chuangao", ""], ["Xia", "Wanchuang", ""], ["Lu", "Cheng", ""], ["Liu", "Jiateng", ""]]}, {"id": "2008.06007", "submitter": "James Hong", "authors": "James Hong, Will Crichton, Haotian Zhang, Daniel Y. Fu, Jacob Ritchie,\n  Jeremy Barenholtz, Ben Hannel, Xinwei Yao, Michaela Murray, Geraldine Moriba,\n  Maneesh Agrawala, Kayvon Fatahalian", "title": "Analyzing Who and What Appears in a Decade of US Cable TV News", "comments": "14 pages, 22 figures (15 pages, 16 figures in supplemental materials)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cable TV news reaches millions of U.S. households each day, meaning that\ndecisions about who appears on the news and what stories get covered can\nprofoundly influence public opinion and discourse. We analyze a data set of\nnearly 24/7 video, audio, and text captions from three U.S. cable TV networks\n(CNN, FOX, and MSNBC) from January 2010 to July 2019. Using machine learning\ntools, we detect faces in 244,038 hours of video, label each face's presented\ngender, identify prominent public figures, and align text captions to audio. We\nuse these labels to perform screen time and word frequency analyses. For\nexample, we find that overall, much more screen time is given to\nmale-presenting individuals than to female-presenting individuals (2.4x in 2010\nand 1.9x in 2019). We present an interactive web-based tool, accessible at\nhttps://tvnews.stanford.edu, that allows the general public to perform their\nown analyses on the full cable TV news data set.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 16:48:17 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 17:11:11 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 02:03:09 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hong", "James", ""], ["Crichton", "Will", ""], ["Zhang", "Haotian", ""], ["Fu", "Daniel Y.", ""], ["Ritchie", "Jacob", ""], ["Barenholtz", "Jeremy", ""], ["Hannel", "Ben", ""], ["Yao", "Xinwei", ""], ["Murray", "Michaela", ""], ["Moriba", "Geraldine", ""], ["Agrawala", "Maneesh", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "2008.06048", "submitter": "Jeff Ens Mr", "authors": "Jeff Ens, Philippe Pasquier", "title": "MMM : Exploring Conditional Multi-Track Music Generation with the\n  Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the Multi-Track Music Machine (MMM), a generative system based on\nthe Transformer architecture that is capable of generating multi-track music.\nIn contrast to previous work, which represents musical material as a single\ntime-ordered sequence, where the musical events corresponding to different\ntracks are interleaved, we create a time-ordered sequence of musical events for\neach track and concatenate several tracks into a single sequence. This takes\nadvantage of the Transformer's attention-mechanism, which can adeptly handle\nlong-term dependencies. We explore how various representations can offer the\nuser a high degree of control at generation time, providing an interactive demo\nthat accommodates track-level and bar-level inpainting, and offers control over\ntrack instrumentation and note density.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:36:34 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 19:13:39 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ens", "Jeff", ""], ["Pasquier", "Philippe", ""]]}, {"id": "2008.06255", "submitter": "Seung-Hun Nam", "authors": "Seung-Hun Nam, Wonhyuk Ahn, In-Jae Yu, Seung-Min Mun", "title": "WAN: Watermarking Attack Network", "comments": "Seung-Hun Nam and Wonhyuk Ahn contributed equally to this work.\n  Corresponding author: Seung-Hun Nam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-bit watermarking (MW) has been developed to improve robustness against\nsignal processing operations and geometric distortions. To this end, benchmark\ntools that test robustness by applying simulated attacks on watermarked images\nare available. However, limitations in these general attacks exist since they\ncannot exploit specific characteristics of the targeted MW. In addition, these\nattacks are usually devised without consideration of visual quality, which\nrarely occurs in the real world. To address these limitations, we propose a\nwatermarking attack network (WAN), a fully trainable watermarking benchmark\ntool that utilizes the weak points of the target MW and induces an inversion of\nthe watermark bit, thereby considerably reducing the watermark extractability.\nTo hinder the extraction of hidden information while ensuring high visual\nquality, we utilize a residual dense blocks-based architecture specialized in\nlocal and global feature learning. A novel watermarking attack loss is\nintroduced to break the MW systems. We empirically demonstrate that the WAN can\nsuccessfully fool various block-based MW systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:11:46 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 21:32:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Nam", "Seung-Hun", ""], ["Ahn", "Wonhyuk", ""], ["Yu", "In-Jae", ""], ["Mun", "Seung-Min", ""]]}, {"id": "2008.06854", "submitter": "Koteswar Rao Jerripothula", "authors": "Akansha Gautam, Koteswar Rao Jerripothula", "title": "SGG: Spinbot, Grammarly and GloVe based Fake News Detection", "comments": "9 pages, 7 figures, Accepted by IEEE International Conference on\n  Multimedia Big Data (BigMM), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, news consumption using online news portals has increased\nexponentially due to several reasons, such as low cost and easy accessibility.\nHowever, such online platforms inadvertently also become the cause of spreading\nfalse information across the web. They are being misused quite frequently as a\nmedium to disseminate misinformation and hoaxes. Such malpractices call for a\nrobust automatic fake news detection system that can keep us at bay from such\nmisinformation and hoaxes. We propose a robust yet simple fake news detection\nsystem, leveraging the tools for paraphrasing, grammar-checking, and\nword-embedding. In this paper, we try to the potential of these tools in\njointly unearthing the authenticity of a news article. Notably, we leverage\nSpinbot (for paraphrasing), Grammarly (for grammar-checking), and GloVe (for\nword-embedding) tools for this purpose. Using these tools, we were able to\nextract novel features that could yield state-of-the-art results on the Fake\nNews AMT dataset and comparable results on Celebrity datasets when combined\nwith some of the essential features. More importantly, the proposed method is\nfound to be more robust empirically than the existing ones, as revealed in our\ncross-domain analysis and multi-domain analysis.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 08:06:52 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gautam", "Akansha", ""], ["Jerripothula", "Koteswar Rao", ""]]}, {"id": "2008.06861", "submitter": "Koteswar Rao Jerripothula", "authors": "Daksh Goyal, Koteswar Rao Jerripothula, Ankush Mittal", "title": "Detection of Gait Abnormalities caused by Neurological Disorders", "comments": "6 pages, 5 figures, Accepted by IEEE Workshop on Multimedia Signal\n  Processing (MMSP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we leverage gait to potentially detect some of the important\nneurological disorders, namely Parkinson's disease, Diplegia, Hemiplegia, and\nHuntington's Chorea. Persons with these neurological disorders often have a\nvery abnormal gait, which motivates us to target gait for their potential\ndetection. Some of the abnormalities involve the circumduction of legs,\nforward-bending, involuntary movements, etc. To detect such abnormalities in\ngait, we develop gait features from the key-points of the human pose, namely\nshoulders, elbows, hips, knees, ankles, etc. To evaluate the effectiveness of\nour gait features in detecting the abnormalities related to these diseases, we\nbuild a synthetic video dataset of persons mimicking the gait of persons with\nsuch disorders, considering the difficulty in finding a sufficient number of\npeople with these disorders. We name it \\textit{NeuroSynGait} video dataset.\nExperiments demonstrated that our gait features were indeed successful in\ndetecting these abnormalities.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 09:00:36 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Goyal", "Daksh", ""], ["Jerripothula", "Koteswar Rao", ""], ["Mittal", "Ankush", ""]]}, {"id": "2008.06908", "submitter": "Parth Tiwari", "authors": "Parth Tiwari, Yash Jain, Shivansh Mundra, Jenny Harding, Manoj Kumar\n  Tiwari", "title": "Visually Aware Skip-Gram for Image Based Recommendations", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The visual appearance of a product significantly influences purchase\ndecisions on e-commerce websites. We propose a novel framework VASG (Visually\nAware Skip-Gram) for learning user and product representations in a common\nlatent space using product image features. Our model is an amalgamation of the\nSkip-Gram architecture and a deep neural network based Decoder. Here the\nSkip-Gram attempts to capture user preference by optimizing user-product\nco-occurrence in a Heterogeneous Information Network while the Decoder\nsimultaneously learns a mapping to transform product image features to the\nSkip-Gram embedding space. This architecture is jointly optimized in an\nend-to-end, multitask fashion. The proposed framework enables us to make\npersonalized recommendations for cold-start products which have no purchase\nhistory. Experiments conducted on large real-world datasets show that the\nlearned embeddings can generate effective recommendations using nearest\nneighbour searches.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 13:16:29 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Tiwari", "Parth", ""], ["Jain", "Yash", ""], ["Mundra", "Shivansh", ""], ["Harding", "Jenny", ""], ["Tiwari", "Manoj Kumar", ""]]}, {"id": "2008.06941", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhou Zhao, Zhijie Lin, Baoxing Huai and Nicholas Jing Yuan", "title": "Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video\n  Grounding", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal video grounding aims to retrieve the spatio-temporal tube of\na queried object according to the given sentence. Currently, most existing\ngrounding methods are restricted to well-aligned segment-sentence pairs. In\nthis paper, we explore spatio-temporal video grounding on unaligned data and\nmulti-form sentences. This challenging task requires to capture critical object\nrelations to identify the queried target. However, existing approaches cannot\ndistinguish notable objects and remain in ineffective relation modeling between\nunnecessary objects. Thus, we propose a novel object-aware multi-branch\nrelation network for object-aware relation discovery. Concretely, we first\ndevise multiple branches to develop object-aware region modeling, where each\nbranch focuses on a crucial object mentioned in the sentence. We then propose\nmulti-branch relation reasoning to capture critical object relationships\nbetween the main branch and auxiliary branches. Moreover, we apply a diversity\nloss to make each branch only pay attention to its corresponding object and\nboost multi-branch learning. The extensive experiments show the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 15:39:56 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 11:11:32 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhang", "Zhu", ""], ["Zhao", "Zhou", ""], ["Lin", "Zhijie", ""], ["Huai", "Baoxing", ""], ["Yuan", "Nicholas Jing", ""]]}, {"id": "2008.08257", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhijie Lin, Zhou Zhao, Jieming Zhu and Xiuqiang He", "title": "Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment\n  Retrieval in Videos", "comments": "ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video moment retrieval aims to localize the target moment in an video\naccording to the given sentence. The weak-supervised setting only provides the\nvideo-level sentence annotations during training. Most existing weak-supervised\nmethods apply a MIL-based framework to develop inter-sample confrontment, but\nignore the intra-sample confrontment between moments with semantically similar\ncontents. Thus, these methods fail to distinguish the target moment from\nplausible negative moments. In this paper, we propose a novel Regularized\nTwo-Branch Proposal Network to simultaneously consider the inter-sample and\nintra-sample confrontments. Concretely, we first devise a language-aware filter\nto generate an enhanced video stream and a suppressed video stream. We then\ndesign the sharable two-branch proposal module to generate positive proposals\nfrom the enhanced stream and plausible negative proposals from the suppressed\none for sufficient confrontment. Further, we apply the proposal regularization\nto stabilize the training process and improve model performance. The extensive\nexperiments show the effectiveness of our method. Our code is released at here.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 04:42:46 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhang", "Zhu", ""], ["Lin", "Zhijie", ""], ["Zhao", "Zhou", ""], ["Zhu", "Jieming", ""], ["He", "Xiuqiang", ""]]}, {"id": "2008.08360", "submitter": "Junyan Wang", "authors": "Junyan Wang, Yang Bai, Yang Long, Bingzhang Hu, Zhenhua Chai, Yu Guan\n  and Xiaolin Wei", "title": "Query Twice: Dual Mixture Attention Meta Learning for Video\n  Summarization", "comments": "This manuscript has been accepted at ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3414064", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization aims to select representative frames to retain high-level\ninformation, which is usually solved by predicting the segment-wise importance\nscore via a softmax function. However, softmax function suffers in retaining\nhigh-rank representations for complex visual or sequential information, which\nis known as the Softmax Bottleneck problem. In this paper, we propose a novel\nframework named Dual Mixture Attention (DMASum) model with Meta Learning for\nvideo summarization that tackles the softmax bottleneck problem, where the\nMixture of Attention layer (MoA) effectively increases the model capacity by\nemploying twice self-query attention that can capture the second-order changes\nin addition to the initial query-key attention, and a novel Single Frame Meta\nLearning rule is then introduced to achieve more generalization to small\ndatasets with limited training sources. Furthermore, the DMASum significantly\nexploits both visual and sequential attention that connects local key-frame and\nglobal attention in an accumulative way. We adopt the new evaluation protocol\non two public datasets, SumMe, and TVSum. Both qualitative and quantitative\nexperiments manifest significant improvements over the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 10:12:52 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Junyan", ""], ["Bai", "Yang", ""], ["Long", "Yang", ""], ["Hu", "Bingzhang", ""], ["Chai", "Zhenhua", ""], ["Guan", "Yu", ""], ["Wei", "Xiaolin", ""]]}, {"id": "2008.08804", "submitter": "Zhengfang Duanmu", "authors": "Zhengfang Duanmu and Wentao Liu and Zhuoran Li and Diqi Chen and Zhou\n  Wang and Yizhou Wang and Wen Gao", "title": "Assessing the Quality-of-Experience of Adaptive Bitrate Video Streaming", "comments": "13 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of video delivery pipeline poses a grand challenge to the\nevaluation of adaptive bitrate (ABR) streaming algorithms and objective\nquality-of-experience (QoE) models. Here we introduce so-far the largest\nsubject-rated database of its kind, namely WaterlooSQoE-IV, consisting of 1350\nadaptive streaming videos created from diverse source contents, video encoders,\nnetwork traces, ABR algorithms, and viewing devices. We collect human opinions\nfor each video with a series of carefully designed subjective experiments.\nSubsequent data analysis and testing/comparison of ABR algorithms and QoE\nmodels using the database lead to a series of novel observations and\ninteresting findings, in terms of the effectiveness of subjective experiment\nmethodologies, the interactions between user experience and source content,\nviewing device and encoder type, the heterogeneities in the bias and preference\nof user experiences, the behaviors of ABR algorithms, and the performance of\nobjective QoE models. Most importantly, our results suggest that a better\nobjective QoE model, or a better understanding of human perceptual experience\nand behaviour, is the most dominating factor in improving the performance of\nABR algorithms, as opposed to advanced optimization frameworks, machine\nlearning strategies or bandwidth predictors, where a majority of ABR research\nhas been focused on in the past decade. On the other hand, our performance\nevaluation of 11 QoE models shows only a moderate correlation between\nstate-of-the-art QoE models and subjective ratings, implying rooms for\nimprovement in both QoE modeling and ABR algorithms. The database is made\npublicly available at: \\url{https://ece.uwaterloo.ca/~zduanmu/waterloosqoe4/}.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 07:01:52 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Duanmu", "Zhengfang", ""], ["Liu", "Wentao", ""], ["Li", "Zhuoran", ""], ["Chen", "Diqi", ""], ["Wang", "Zhou", ""], ["Wang", "Yizhou", ""], ["Gao", "Wen", ""]]}, {"id": "2008.09151", "submitter": "Liangming Pan", "authors": "Liangming Pan, Jingjing Chen, Jianlong Wu, Shaoteng Liu, Chong-Wah\n  Ngo, Min-Yen Kan, Yu-Gang Jiang, Tat-Seng Chua", "title": "Multi-modal Cooking Workflow Construction for Food Recipes", "comments": "This manuscript has been accepted at ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413765", "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding food recipe requires anticipating the implicit causal effects\nof cooking actions, such that the recipe can be converted into a graph\ndescribing the temporal workflow of the recipe. This is a non-trivial task that\ninvolves common-sense reasoning. However, existing efforts rely on hand-crafted\nfeatures to extract the workflow graph from recipes due to the lack of\nlarge-scale labeled datasets. Moreover, they fail to utilize the cooking\nimages, which constitute an important part of food recipes. In this paper, we\nbuild MM-ReS, the first large-scale dataset for cooking workflow construction,\nconsisting of 9,850 recipes with human-labeled workflow graphs. Cooking steps\nare multi-modal, featuring both text instructions and cooking images. We then\npropose a neural encoder-decoder model that utilizes both visual and textual\ninformation to construct the cooking workflow, which achieved over 20%\nperformance gain over existing hand-crafted baselines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 18:31:25 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Pan", "Liangming", ""], ["Chen", "Jingjing", ""], ["Wu", "Jianlong", ""], ["Liu", "Shaoteng", ""], ["Ngo", "Chong-Wah", ""], ["Kan", "Min-Yen", ""], ["Jiang", "Yu-Gang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2008.09326", "submitter": "Wang Zheng", "authors": "Zheng Wang, Jianwu Li and Ge Song", "title": "DTDN: Dual-task De-raining Network", "comments": null, "journal-ref": null, "doi": "10.1145/3343031.3350945", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing rain streaks from rainy images is necessary for many tasks in\ncomputer vision, such as object detection and recognition. It needs to address\ntwo mutually exclusive objectives: removing rain streaks and reserving\nrealistic details. Balancing them is critical for de-raining methods. We\npropose an end-to-end network, called dual-task de-raining network (DTDN),\nconsisting of two sub-networks: generative adversarial network (GAN) and\nconvolutional neural network (CNN), to remove rain streaks via coordinating the\ntwo mutually exclusive objectives self-adaptively. DTDN-GAN is mainly used to\nremove structural rain streaks, and DTDN-CNN is designed to recover details in\noriginal images. We also design a training algorithm to train these two\nsub-networks of DTDN alternatively, which share same weights but use different\ntraining sets. We further enrich two existing datasets to approximate the\ndistribution of real rain streaks. Experimental results show that our method\noutperforms several recent state-of-the-art methods, based on both benchmark\ntesting datasets and real rainy images.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 06:32:42 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Wang", "Zheng", ""], ["Li", "Jianwu", ""], ["Song", "Ge", ""]]}, {"id": "2008.09506", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Yongxin Wang, Yunze Man, and Kris Kitani", "title": "Graph Neural Networks for 3D Multi-Object Tracking", "comments": "ECCV 2020 workshop paper. Project website:\n  http://www.xinshuoweng.com/projects/GNN3DMOT. arXiv admin note: substantial\n  text overlap with arXiv:2006.07327", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work\noften uses a tracking-by-detection pipeline, where the feature of each object\nis extracted independently to compute an affinity matrix. Then, the affinity\nmatrix is passed to the Hungarian algorithm for data association. A key process\nof this pipeline is to learn discriminative features for different objects in\norder to reduce confusion during data association. To that end, we propose two\ninnovative techniques: (1) instead of obtaining the features for each object\nindependently, we propose a novel feature interaction mechanism by introducing\nGraph Neural Networks; (2) instead of obtaining the features from either 2D or\n3D space as in prior work, we propose a novel joint feature extractor to learn\nappearance and motion features from 2D and 3D space. Through experiments on the\nKITTI dataset, our proposed method achieves state-of-the-art 3D MOT\nperformance. Our project website is at\nhttp://www.xinshuoweng.com/projects/GNN3DMOT.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:55:41 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Weng", "Xinshuo", ""], ["Wang", "Yongxin", ""], ["Man", "Yunze", ""], ["Kitani", "Kris", ""]]}, {"id": "2008.09559", "submitter": "Paresh Saxena", "authors": "Paresh Saxena, Mandan Naresh, Manik Gupta, Anirudh Achanta, Sastri\n  Kota and Smrati Gupta", "title": "NANCY: Neural Adaptive Network Coding methodologY for video distribution\n  over wireless networks", "comments": "Accepted in Globecom, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents NANCY, a system that generates adaptive bit rates (ABR)\nfor video and adaptive network coding rates (ANCR) using reinforcement learning\n(RL) for video distribution over wireless networks. NANCY trains a neural\nnetwork model with rewards formulated as quality of experience (QoE) metrics.\nIt performs joint optimization in order to select: (i) adaptive bit rates for\nfuture video chunks to counter variations in available bandwidth and (ii)\nadaptive network coding rates to encode the video chunk slices to counter\npacket losses in wireless networks. We present the design and implementation of\nNANCY, and evaluate its performance compared to state-of-the-art video rate\nadaptation algorithms including Pensieve and robustMPC. Our results show that\nNANCY provides 29.91% and 60.34% higher average QoE than Pensieve and\nrobustMPC, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 15:55:32 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Saxena", "Paresh", ""], ["Naresh", "Mandan", ""], ["Gupta", "Manik", ""], ["Achanta", "Anirudh", ""], ["Kota", "Sastri", ""], ["Gupta", "Smrati", ""]]}, {"id": "2008.09883", "submitter": "Ghalib Tahir", "authors": "Ghalib Ahmed Tahir, Chu Kiong Loo, Foong Ming Moy and Nadine Kong", "title": "A Review of Critical Features and General Issues of Freely Available\n  mHealth Apps For Dietary Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obesity is known to lower the quality of life substantially. It is often\nassociated with increased chances of non-communicable diseases such as\ndiabetes, cardiovascular problems, various cancers, etc. Evidence suggests that\ndiet-related mobile applications play a vital role in assisting individuals in\nmaking healthier choices and keeping track of food intake. However, due to an\nabundance of similar applications, it becomes pertinent to evaluate each of\nthem in terms of functionality, usability, and possible design issues to truly\ndetermine state-of-the-art solutions for the future. Since these applications\ninvolve implementing multiple user requirements and recommendations from\ndifferent dietitians, the evaluation becomes quite complex. Therefore, this\nstudy aims to review existing dietary applications at length to highlight key\nfeatures and problems that enhance or undermine an application's usability. For\nthis purpose, we have examined the published literature from various scientific\ndatabases of the PUBMED, CINAHL (January 2010-December 2019) and Science Direct\n(2010-2019). We followed PRISMA guidelines, and out of our findings, fifty-six\nprimary studies met our inclusion criteria after identification, screening,\neligibility and full-text evaluation. We analyzed 35 apps from the selected\nstudies and extracted the data of each of the identified apps.Following our\ndetailed analysis on the comprehensiveness of freely available mHealth\napplications, we specified potential future research challenges and stated\nrecommendations to help grow clinically accurate diet-related applications.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 17:27:49 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 08:42:46 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 18:15:37 GMT"}, {"version": "v4", "created": "Sun, 11 Jul 2021 15:03:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tahir", "Ghalib Ahmed", ""], ["Loo", "Chu Kiong", ""], ["Moy", "Foong Ming", ""], ["Kong", "Nadine", ""]]}, {"id": "2008.11042", "submitter": "YuHui Lee", "authors": "Yu-Hui Lee, Shang-Hong Lai", "title": "ByeGlassesGAN: Identity Preserving Eyeglasses Removal for Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel image-to-image GAN framework for eyeglasses\nremoval, called ByeGlassesGAN, which is used to automatically detect the\nposition of eyeglasses and then remove them from face images. Our ByeGlassesGAN\nconsists of an encoder, a face decoder, and a segmentation decoder. The encoder\nis responsible for extracting information from the source face image, and the\nface decoder utilizes this information to generate glasses-removed images. The\nsegmentation decoder is included to predict the segmentation mask of eyeglasses\nand completed face region. The feature vectors generated by the segmentation\ndecoder are shared with the face decoder, which facilitates better\nreconstruction results. Our experiments show that ByeGlassesGAN can provide\nvisually appealing results in the eyeglasses-removed face images even for\nsemi-transparent color eyeglasses or glasses with glare. Furthermore, we\ndemonstrate significant improvement in face recognition accuracy for face\nimages with glasses by applying our method as a pre-processing step in our face\nrecognition experiment.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 14:10:42 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lee", "Yu-Hui", ""], ["Lai", "Shang-Hong", ""]]}, {"id": "2008.11420", "submitter": "Meng Wang", "authors": "Meng Wang and Shiqi Wang and Junru Li and Li Zhang and Yue Wang and\n  Siwei Ma and Sam Kwong", "title": "Low Complexity Trellis-Coded Quantization in Versatile Video Coding", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3051460", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forthcoming Versatile Video Coding (VVC) standard adopts the\ntrellis-coded quantization, which leverages the delicate trellis graph to map\nthe quantization candidates within one block into the optimal path. Despite the\nhigh compression efficiency, the complex trellis search with soft decision\nquantization may hinder the applications due to high complexity and low\nthroughput capacity. To reduce the complexity, in this paper, we propose a low\ncomplexity trellis-coded quantization scheme in a scientifically sound way with\ntheoretical modeling of the rate and distortion. As such, the trellis departure\npoint can be adaptively adjusted, and unnecessarily visited branches are\naccordingly pruned, leading to the shrink of total trellis stages and\nsimplification of transition branches. Extensive experimental results on the\nVVC test model show that the proposed scheme is effective in reducing the\nencoding complexity by 11% and 5% with all intra and random access\nconfigurations, respectively, at the cost of only 0.11% and 0.05% BD-Rate\nincrease. Meanwhile, on average 24% and 27% quantization time savings can be\nachieved under all intra and random access configurations. Due to the excellent\nperformance, the VVC test model has adopted one implementation of the proposed\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 07:21:54 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wang", "Meng", ""], ["Wang", "Shiqi", ""], ["Li", "Junru", ""], ["Zhang", "Li", ""], ["Wang", "Yue", ""], ["Ma", "Siwei", ""], ["Kwong", "Sam", ""]]}, {"id": "2008.11455", "submitter": "Mao Yunhao", "authors": "Yunhao Mao, Meng Wang, Shiqi Wang and Sam Kwong", "title": "High Efficiency Rate Control for Versatile Video Coding Based on\n  Composite Cauchy Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel rate control algorithm for Versatile Video\nCoding (VVC) standard based on its distinct rate-distortion characteristics. By\nmodelling the transform coefficients with the composite Cauchy distribution,\nhigher accuracy compared with traditional distributions has been achieved.\nBased on the transform coefficient modelling, the theoretically derived R-Q and\nD-Q models which have been shown to deliver higher accuracy in characterizing\nRD characteristics for sequences with different content are incorporated into\nthe rate control process. Furthermore, to establish an adaptive bit allocation\nscheme, the dependency between different levels of frames is modelled by a\ndependency factor to describe relationship between the reference and\nto-be-coded frames. Given the derived R-Q and D-Q relationships, as well as the\ndependency factor, an adaptive bit allocation scheme is developed for optimal\nbits allocation. We implement the proposed algorithm on VVC Test Model (VTM)\n3.0. Experiments show that due to proper bit allocation, for low delay\nconfiguration the proposed algorithm can achieve 1.03% BD-Rate saving compared\nwith the default rate control algorithm and 2.96% BD-Rate saving compared with\nfixed QP scheme. Moreover, 1.29% BD-Rate saving and higher control accuracy\nhave also been observed under the random access configuration.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 09:13:36 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 02:33:54 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Mao", "Yunhao", ""], ["Wang", "Meng", ""], ["Wang", "Shiqi", ""], ["Kwong", "Sam", ""]]}, {"id": "2008.11961", "submitter": "Chen-Hsiu Huang", "authors": "Chen-Hsiu Huang, Ja-Ling Wu", "title": "Multi-task deep CNN model for no-reference image quality assessment on\n  smartphone camera photos", "comments": "Proceedings of Computer Vision & Graphic Image Processing (CVGIP),\n  Hsinchu, Taiwan, Aug. 16-18, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone is the most successful consumer electronic product in today's\nmobile social network era. The smartphone camera quality and its image\npost-processing capability is the dominant factor that impacts consumer's\nbuying decision. However, the quality evaluation of photos taken from\nsmartphones remains a labor-intensive work and relies on professional\nphotographers and experts. As an extension of the prior CNN-based NR-IQA\napproach, we propose a multi-task deep CNN model with scene type detection as\nan auxiliary task. With the shared model parameters in the convolution layer,\nthe learned feature maps could become more scene-relevant and enhance the\nperformance. The evaluation result shows improved SROCC performance compared to\ntraditional NR-IQA methods and single task CNN-based models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:33:05 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Huang", "Chen-Hsiu", ""], ["Wu", "Ja-Ling", ""]]}, {"id": "2008.12017", "submitter": "Sajida Kairm Ms", "authors": "Sajida Karim, Hui He, Asif Ali Laghari and Hina Madiha", "title": "Quality of Service (QoS): Measurements of Video Streaming", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.3987056", "report-no": null, "categories": "cs.MM cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nowadays video streaming is growing over the social clouds, where end-users\nalways want to share High Definition (HD) videos among friends. Mostly videos\nwere recorded via smartphones and other HD devices and short time videos have a\nbig file size. The big file size of videos required high bandwidth to upload\nand download on the Internet and also required more time to load in a web page\nfor play. So avoiding this problem social cloud compress videos during the\nupload for smooth play and fast loading in a web page. Compression decreases\nthe video quality which also decreases the quality of experience of end users.\nIn this paper we measure the QoS of different standard video file formats on\nsocial clouds; they varied from each other in resolution, audio/video bitrate,\nand storage size.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:38:46 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Karim", "Sajida", ""], ["He", "Hui", ""], ["Laghari", "Asif Ali", ""], ["Madiha", "Hina", ""]]}, {"id": "2008.12408", "submitter": "Akshay Gadde", "authors": "Sam John, Akshay Gadde and Balu Adsumilli", "title": "Rate distortion optimization over large scale video corpus with machine\n  learning", "comments": "Accepted in 2020 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient codec-agnostic method for bitrate allocation over a\nlarge scale video corpus with the goal of minimizing the average bitrate\nsubject to constraints on average and minimum quality. Our method clusters the\nvideos in the corpus such that videos within one cluster have similar\nrate-distortion (R-D) characteristics. We train a support vector machine\nclassifier to predict the R-D cluster of a video using simple video complexity\nfeatures that are computationally easy to obtain. The model allows us to\nclassify a large sample of the corpus in order to estimate the distribution of\nthe number of videos in each of the clusters. We use this distribution to find\nthe optimal encoder operating point for each R-D cluster. Experiments with AV1\nencoder show that our method can achieve the same average quality over the\ncorpus with $22\\%$ less average bitrate.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:40:22 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["John", "Sam", ""], ["Gadde", "Akshay", ""], ["Adsumilli", "Balu", ""]]}, {"id": "2008.12832", "submitter": "Prerana Mukherjee", "authors": "Ronak Gupta, Prerana Mukherjee, Brejesh Lall, Varshul Gupta", "title": "Semantics Preserving Hierarchy based Retrieval of Indian heritage\n  monuments", "comments": "Accepted in Structuring and Understanding of Multimedia heritAge\n  Contents (SUMAC2020), ACM Multimedia Workshops, Seattle, United States,\n  October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monument classification can be performed on the basis of their appearance and\nshape from coarse to fine categories. Although there is much semantic\ninformation present in the monuments which is reflected in the eras they were\nbuilt, its type or purpose, the dynasty which established it, etc.\nParticularly, Indian subcontinent exhibits a huge deal of variation in terms of\narchitectural styles owing to its rich cultural heritage. In this paper, we\npropose a framework that utilizes hierarchy to preserve semantic information\nwhile performing image classification or image retrieval. We encode the learnt\ndeep embeddings to construct a dictionary of images and then utilize a\nre-ranking framework on the the retrieved results using DeLF features. The\nsemantic information preserved in these embeddings helps to classify unknown\nmonuments at higher level of granularity in hierarchy. We have curated a large,\nnovel Indian heritage monuments dataset comprising of images of historical,\ncultural and religious importance with subtypes of eras, dynasties and\narchitectural styles. We demonstrate the performance of the proposed framework\nin image classification and retrieval tasks and compare it with other competing\nmethods on this dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 20:04:23 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gupta", "Ronak", ""], ["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""], ["Gupta", "Varshul", ""]]}, {"id": "2008.12855", "submitter": "Vaibhav Pandey", "authors": "Ali Rostami, Vaibhav Pandey, Nitish Nag, Vesper Wang, Ramesh Jain", "title": "Personal Food Model", "comments": null, "journal-ref": "Proceedings of the 28th ACM International Conference on Multimedia\n  (MM '20), October 12--16, 2020, Seattle, WA, USA", "doi": "10.1145/3394171.3414691", "report-no": null, "categories": "cs.MM cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food is central to life. Food provides us with energy and foundational\nbuilding blocks for our body and is also a major source of joy and new\nexperiences. A significant part of the overall economy is related to food. Food\nscience, distribution, processing, and consumption have been addressed by\ndifferent communities using silos of computational approaches. In this paper,\nwe adopt a person-centric multimedia and multimodal perspective on food\ncomputing and show how multimedia and food computing are synergistic and\ncomplementary.\n  Enjoying food is a truly multimedia experience involving sight, taste, smell,\nand even sound, that can be captured using a multimedia food logger. The\nbiological response to food can be captured using multimodal data streams using\navailable wearable devices. Central to this approach is the Personal Food\nModel. Personal Food Model is the digitized representation of the food-related\ncharacteristics of an individual. It is designed to be used in food\nrecommendation systems to provide eating-related recommendations that improve\nthe user's quality of life. To model the food-related characteristics of each\nperson, it is essential to capture their food-related enjoyment using a\nPreferential Personal Food Model and their biological response to food using\ntheir Biological Personal Food Model. Inspired by the power of 3-dimensional\ncolor models for visual processing, we introduce a 6-dimensional taste-space\nfor capturing culinary characteristics as well as personal preferences. We use\nevent mining approaches to relate food with other life and biological events to\nbuild a predictive model that could also be used effectively in emerging food\nrecommendation systems.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 21:36:09 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rostami", "Ali", ""], ["Pandey", "Vaibhav", ""], ["Nag", "Nitish", ""], ["Wang", "Vesper", ""], ["Jain", "Ramesh", ""]]}, {"id": "2008.13024", "submitter": "Hao Tang", "authors": "Hao Tang, Song Bai, Nicu Sebe", "title": "Dual Attention GANs for Semantic Image Synthesis", "comments": "Accepted to ACM MM 2020, camera ready (9 pages) + supplementary (10\n  pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the semantic image synthesis task that aims at\ntransferring semantic label maps to photo-realistic images. Existing methods\nlack effective semantic constraints to preserve the semantic information and\nignore the structural correlations in both spatial and channel dimensions,\nleading to unsatisfactory blurry and artifact-prone results. To address these\nlimitations, we propose a novel Dual Attention GAN (DAGAN) to synthesize\nphoto-realistic and semantically-consistent images with fine details from the\ninput layouts without imposing extra training overhead or modifying the network\narchitectures of existing methods. We also propose two novel modules, i.e.,\nposition-wise Spatial Attention Module (SAM) and scale-wise Channel Attention\nModule (CAM), to capture semantic structure attention in spatial and channel\ndimensions, respectively. Specifically, SAM selectively correlates the pixels\nat each position by a spatial attention map, leading to pixels with the same\nsemantic label being related to each other regardless of their spatial\ndistances. Meanwhile, CAM selectively emphasizes the scale-wise features at\neach channel by a channel attention map, which integrates associated features\namong all channel maps regardless of their scales. We finally sum the outputs\nof SAM and CAM to further improve feature representation. Extensive experiments\non four challenging datasets show that DAGAN achieves remarkably better results\nthan state-of-the-art methods, while using fewer model parameters. The source\ncode and trained models are available at https://github.com/Ha0Tang/DAGAN.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 17:49:01 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tang", "Hao", ""], ["Bai", "Song", ""], ["Sebe", "Nicu", ""]]}, {"id": "2008.13269", "submitter": "Hina Tabassum Prof.", "authors": "Hosein Zarini, Ata Khalili, Hina Tabassum, and Mehdi Rasti", "title": "Joint Transmission in QoE-Driven Backhaul-Aware MC-NOMA Cognitive Radio\n  Network", "comments": null, "journal-ref": "2020 IEEE Global Communications Conference (GLOBECOM)", "doi": "10.1109/GLOBECOM42002.2020.9322380", "report-no": null, "categories": "eess.SP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a resource allocation framework to optimize the\ndownlink transmission of a backhaul-aware multi-cell cognitive radio network\n(CRN) which is enabled with multi-carrier non-orthogonal multiple access\n(MC-NOMA). The considered CRN is composed of a single macro base station (MBS)\nand multiple small BSs (SBSs) that are referred to as the primary and secondary\ntiers, respectively. For the primary tier, we consider orthogonal frequency\ndivision multiple access (OFDMA) scheme and also Quality of Service (QoS) to\nevaluate the user satisfaction. On the other hand in secondary tier, MC-NOMA is\nemployed and the user satisfaction for web, video and audio as popular\nmultimedia services is evaluated by Quality-of-Experience (QoE). Furthermore,\neach user in secondary tier can be served simultaneously by multiple SBSs over\na subcarrier via Joint Transmission (JT). In particular, we formulate a joint\noptimization problem of power control and scheduling (i.e., user association\nand subcarrier allocation) in secondary tier to maximize total achievable QoE\nfor the secondary users. An efficient resource allocation mechanism has been\ndeveloped to handle the non-linear form interference and to overcome the\nnon-convexity of QoE serving functions. The scheduling and power control policy\nleverage on Augmented Lagrangian Method (ALM). Simulation results reveal that\nproposed solution approach can control the interference and JT-NOMA improves\ntotal perceived QoE compared to the existing schemes.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 20:44:53 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zarini", "Hosein", ""], ["Khalili", "Ata", ""], ["Tabassum", "Hina", ""], ["Rasti", "Mehdi", ""]]}, {"id": "2008.13381", "submitter": "Ziran Wang", "authors": "Ziran Wang and Kyungtae Han and Prashant Tiwari", "title": "Augmented Reality-Based Advanced Driver-Assistance System for Connected\n  Vehicles", "comments": "2020 IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of advanced communication technology, connected vehicles\nbecome increasingly popular in our transportation systems, which can conduct\ncooperative maneuvers with each other as well as road entities through\nvehicle-to-everything communication. A lot of research interests have been\ndrawn to other building blocks of a connected vehicle system, such as\ncommunication, planning, and control. However, less research studies were\nfocused on the human-machine cooperation and interface, namely how to visualize\nthe guidance information to the driver as an advanced driver-assistance system\n(ADAS). In this study, we propose an augmented reality (AR)-based ADAS, which\nvisualizes the guidance information calculated cooperatively by multiple\nconnected vehicles. An unsignalized intersection scenario is adopted as the use\ncase of this system, where the driver can drive the connected vehicle crossing\nthe intersection under the AR guidance, without any full stop at the\nintersection. A simulation environment is built in Unity game engine based on\nthe road network of San Francisco, and human-in-the-loop (HITL) simulation is\nconducted to validate the effectiveness of our proposed system regarding travel\ntime and energy consumption.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 06:14:28 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Ziran", ""], ["Han", "Kyungtae", ""], ["Tiwari", "Prashant", ""]]}, {"id": "2008.13769", "submitter": "Maitree Leekha", "authors": "Shahid Nawaz Khan, Maitree Leekha, Jainendra Shukla, Rajiv Ratn Shah", "title": "Vyaktitv: A Multimodal Peer-to-Peer Hindi Conversations based Dataset\n  for Personality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically detecting personality traits can aid several applications, such\nas mental health recognition and human resource management. Most datasets\nintroduced for personality detection so far have analyzed these traits for each\nindividual in isolation. However, personality is intimately linked to our\nsocial behavior. Furthermore, surprisingly little research has focused on\npersonality analysis using low resource languages. To this end, we present a\nnovel peer-to-peer Hindi conversation dataset- Vyaktitv. It consists of\nhigh-quality audio and video recordings of the participants, with Hinglish\ntextual transcriptions for each conversation. The dataset also contains a rich\nset of socio-demographic features, like income, cultural orientation, amongst\nseveral others, for all the participants. We release the dataset for public\nuse, as well as perform preliminary statistical analysis along the different\ndimensions. Finally, we also discuss various other applications and tasks for\nwhich the dataset can be employed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:44:28 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Khan", "Shahid Nawaz", ""], ["Leekha", "Maitree", ""], ["Shukla", "Jainendra", ""], ["Shah", "Rajiv Ratn", ""]]}]