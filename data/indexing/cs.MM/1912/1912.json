[{"id": "1912.00649", "submitter": "Jungwoo Pyo", "authors": "Jungwoo Pyo, Joohyun Lee, Youngjune Park, Tien-Cuong Bui, Sang Kyun\n  Cha", "title": "An Attention-Based Speaker Naming Method for Online Adaptation in\n  Non-Fixed Scenarios", "comments": "AAAI 2020 Workshop on Interactive and Conversational Recommendation\n  Systems(WICRS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A speaker naming task, which finds and identifies the active speaker in a\ncertain movie or drama scene, is crucial for dealing with high-level video\nanalysis applications such as automatic subtitle labeling and video\nsummarization. Modern approaches have usually exploited biometric features with\na gradient-based method instead of rule-based algorithms. In a certain\nsituation, however, a naive gradient-based method does not work efficiently.\nFor example, when new characters are added to the target identification list,\nthe neural network needs to be frequently retrained to identify new people and\nit causes delays in model preparation. In this paper, we present an\nattention-based method which reduces the model setup time by updating the newly\nadded data via online adaptation without a gradient update process. We\ncomparatively analyzed with three evaluation metrics(accuracy, memory usage,\nsetup time) of the attention-based method and existing gradient-based methods\nunder various controlled settings of speaker naming. Also, we applied existing\nspeaker naming models and the attention-based model to real video to prove that\nour approach shows comparable accuracy to the existing state-of-the-art models\nand even higher accuracy in some cases.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:30:27 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pyo", "Jungwoo", ""], ["Lee", "Joohyun", ""], ["Park", "Youngjune", ""], ["Bui", "Tien-Cuong", ""], ["Cha", "Sang Kyun", ""]]}, {"id": "1912.01115", "submitter": "Karol Chlasta", "authors": "Karol Chlasta, Krzysztof Wo{\\l}k, Izabela Krejtz", "title": "Automated speech-based screening of depression using deep convolutional\n  neural networks", "comments": "10 pages, 8 figures and 2 tables, HCist 2019 - 8th International\n  Conference on Health and Social Care Information Systems and Technologies\n  (16-18 October 2019, Sousse, Tunisia)", "journal-ref": "Procedia Computer Science 164 (2019) 618-628", "doi": "10.1016/j.procs.2019.12.228", "report-no": null, "categories": "cs.LG cs.CV cs.CY cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and treatment of depression is essential in promoting\nremission, preventing relapse, and reducing the emotional burden of the\ndisease. Current diagnoses are primarily subjective, inconsistent across\nprofessionals, and expensive for individuals who may be in urgent need of help.\nThis paper proposes a novel approach to automated depression detection in\nspeech using convolutional neural network (CNN) and multipart interactive\ntraining. The model was tested using 2568 voice samples obtained from 77\nnon-depressed and 30 depressed individuals. In experiment conducted, data were\napplied to residual CNNs in the form of spectrograms, images auto-generated\nfrom audio samples. The experimental results obtained using different ResNet\narchitectures gave a promising baseline accuracy reaching 77%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:58:40 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Chlasta", "Karol", ""], ["Wo\u0142k", "Krzysztof", ""], ["Krejtz", "Izabela", ""]]}, {"id": "1912.01947", "submitter": "Matti Pouke", "authors": "Matti Pouke, Katherine J. Mimnaugh, Timo Ojala, Steven M. LaValle", "title": "The Plausibility Paradox for Scaled-Down Users in Virtual Environments", "comments": "Accepted to the 27th IEEE Conference on Virtual Reality and 3D User\n  Interfaces (IEEEVR 2020). The title of the paper was changed among other\n  edits necessary for the accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies a new phenomenon: when users interact with simulated\nobjects in a virtual environment where the user is much smaller than usual,\nthere is a mismatch between the object physics that they expect and the object\nphysics that would be correct at that scale. We report the findings of our\nstudy investigating the relationship between perceived realism and a physically\naccurate approximation of reality in a virtual reality experience in which the\nuser has been scaled down by a factor of ten. We conducted a within-subjects\nexperiment in which 44 subjects performed a simple interaction task with\nobjects under two different physics simulation conditions. In one condition,\nthe objects, when dropped and thrown, behaved accurately according to the\nphysics that would be correct at that reduced scale in the real world, our true\nphysics condition. In the other condition, the movie physics condition, the\nobjects behaved in a similar manner as they would if no scaling of the user had\noccurred. We found that a significant majority of the users considered the\nlatter condition to be the more realistic one. We argue that our findings have\nimplications for many virtual reality and telepresence applications involving\noperation with simulated or physical objects in small scales.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 12:18:53 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 08:48:17 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Pouke", "Matti", ""], ["Mimnaugh", "Katherine J.", ""], ["Ojala", "Timo", ""], ["LaValle", "Steven M.", ""]]}, {"id": "1912.02591", "submitter": "Woosung Choi", "authors": "Woosung Choi and Minseok Kim and Jaehwa Chung and Daewon Lee and\n  Soonyoung Jung", "title": "Investigating U-Nets with various Intermediate Blocks for\n  Spectrogram-based Singing Voice Separation", "comments": "8 pages 4 tables 6 figures, accepted to ISMIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing Voice Separation (SVS) tries to separate singing voice from a given\nmixed musical signal. Recently, many U-Net-based models have been proposed for\nthe SVS task, but there were no existing works that evaluate and compare\nvarious types of intermediate blocks that can be used in the U-Net\narchitecture. In this paper, we introduce a variety of intermediate spectrogram\ntransformation blocks. We implement U-nets based on these blocks and train them\non complex-valued spectrograms to consider both magnitude and phase. These\nnetworks are then compared on the SDR metric. When using a particular block\ncomposed of convolutional and fully-connected layers, it achieves\nstate-of-the-art SDR on the MUSDB singing voice separation task by a large\nmargin of 0.9 dB. Our code and models are available online.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:46:19 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 13:56:59 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 16:39:49 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Choi", "Woosung", ""], ["Kim", "Minseok", ""], ["Chung", "Jaehwa", ""], ["Lee", "Daewon", ""], ["Jung", "Soonyoung", ""]]}, {"id": "1912.02802", "submitter": "Alcardo Alex Barakabitze", "authors": "Alcardo Alex Barakabitze and Arslan Ahmad and Rashid Mijumbi and\n  Andrew Hines", "title": "5G network slicing using SDN and NFV- A survey of taxonomy,\n  architectures and future challenges", "comments": "40 Pages, 22 figures, published in computer networks (Open Access)", "journal-ref": "2019", "doi": "10.1016/j.comnet.2019.106984", "report-no": null, "categories": "cs.NI cs.DC cs.MM cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we provide a comprehensive review and updated solutions\nrelated to 5G network slicing using SDN and NFV. Firstly, we present 5G service\nquality and business requirements followed by a description of 5G network\nsoftwarization and slicing paradigms including essential concepts, history and\ndifferent use cases. Secondly, we provide a tutorial of 5G network slicing\ntechnology enablers including SDN, NFV, MEC, cloud/Fog computing, network\nhypervisors, virtual machines & containers. Thidly, we comprehensively survey\ndifferent industrial initiatives and projects that are pushing forward the\nadoption of SDN and NFV in accelerating 5G network slicing. A comparison of\nvarious 5G architectural approaches in terms of practical implementations,\ntechnology adoptions and deployment strategies is presented. Moreover, we\nprovide a discussion on various open source orchestrators and proof of concepts\nrepresenting industrial contribution. The work also investigates the\nstandardization efforts in 5G networks regarding network slicing and\nsoftwarization. Additionally, the article presents the management and\norchestration of network slices in a single domain followed by a comprehensive\nsurvey of management and orchestration approaches in 5G network slicing across\nmultiple domains while supporting multiple tenants. Furthermore, we highlight\nthe future challenges and research directions regarding network softwarization\nand slicing using SDN and NFV in 5G networks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:50:31 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Barakabitze", "Alcardo Alex", ""], ["Ahmad", "Arslan", ""], ["Mijumbi", "Rashid", ""], ["Hines", "Andrew", ""]]}, {"id": "1912.03333", "submitter": "Ammar Mohammadi", "authors": "Ammar Mohammadi", "title": "Reversible Data Hiding in Encrypted Images Using MSBs Integration and\n  Histogram Modification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a reversible data hiding in encrypted image that employs\nbased notions of the RDH in plain-image schemes including histogram\nmodification and prediction-error computation. In the proposed method, original\nimage may be encrypted by desire encryption algorithm. Most significant bit\n(MSB) of encrypted pixels are integrated to vacate room for embedding data\nbits. Integrated ones will be more resistant against failure of reconstruction\nif they are modified for embedding data bits. At the recipient, we employ\nchess-board predictor for lossless reconstruction of the original image by the\naim of prediction-error analysis. Comparing to existent RDHEI algorithms, not\nonly we propose a separable method to extract data bits, but also content-owner\nmay attain a perfect reconstruction of the original image without having data\nhider key. Experimental results confirm that the proposed algorithm outperforms\nstate of the art ones.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 20:18:27 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 18:12:11 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 09:52:27 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2020 09:36:16 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mohammadi", "Ammar", ""]]}, {"id": "1912.03590", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Houwen Peng, Jianlong Fu, Jiebo Luo", "title": "Learning 2D Temporal Adjacent Networks for Moment Localization with\n  Natural Language", "comments": "This paper is accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of retrieving a specific moment from an untrimmed\nvideo by a query sentence. This is a challenging problem because a target\nmoment may take place in relations to other temporal moments in the untrimmed\nvideo. Existing methods cannot tackle this challenge well since they consider\ntemporal moments individually and neglect the temporal dependencies. In this\npaper, we model the temporal relations between video moments by a\ntwo-dimensional map, where one dimension indicates the starting time of a\nmoment and the other indicates the end time. This 2D temporal map can cover\ndiverse video moments with different lengths, while representing their adjacent\nrelations. Based on the 2D map, we propose a Temporal Adjacent Network\n(2D-TAN), a single-shot framework for moment localization. It is capable of\nencoding the adjacent temporal relation, while learning discriminative features\nfor matching video moments with referring expressions. We evaluate the proposed\n2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet\nCaptions, and TACoS, where our 2D-TAN outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 01:34:39 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 13:44:56 GMT"}, {"version": "v3", "created": "Sat, 26 Dec 2020 15:27:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Songyang", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.03878", "submitter": "Bolin Chen", "authors": "Bolin Chen, Weiqi Luo, Peijia Zheng, Jiwu Huang", "title": "Universal Stego Post-processing for Enhancing Image Steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the designing or improving embedding cost becomes a key\nissue for current steganographic methods. Unlike existing works, we propose a\nnovel framework to enhance the steganography security via post-processing on\nthe embedding units (i.e., pixel values and DCT coefficients) of stego\ndirectly. In this paper, we firstly analyze the characteristics of STCs\n(Syndrome-Trellis Codes), and then design the rule for post-processing to\nensure the correct extraction of hidden message. Since the steganography\nartifacts are typically reflected on image residuals, we try to reduce the\nresidual distance between cover and the modified stego in order to enhance\nsteganography security. To this end, we model the post-processing as a\nnon-linear integer programming, and implement it via heuristic search. In\naddition, we carefully determine several important issues in the proposed\npost-processing, such as the candidate embedding units to be modified, the\ndirection and amplitude of post-modification, the adaptive filters for getting\nresiduals, and the distance measure of residuals. Extensive experimental\nresults evaluated on both hand-crafted steganalytic features and deep learning\nbased ones demonstrate that the proposed method can effectively enhance the\nsecurity of most modern steganographic methods both in spatial and JPEG\ndomains.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:14:05 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:36:58 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Chen", "Bolin", ""], ["Luo", "Weiqi", ""], ["Zheng", "Peijia", ""], ["Huang", "Jiwu", ""]]}, {"id": "1912.05014", "submitter": "Sayan Nag", "authors": "Mayukh Bhattacharyya, Sayan Nag", "title": "Hybrid Style Siamese Network: Incorporating style loss in complementary\n  apparels retrieval", "comments": "Paper Accepted in the Third Workshop on Computer Vision for Fashion,\n  Art and Design, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Retrieval grows to be an integral part of fashion e-commerce ecosystem\nas it keeps expanding in multitudes. Other than the retrieval of visually\nsimilar items, the retrieval of visually compatible or complementary items is\nalso an important aspect of it. Normal Siamese Networks tend to work well on\ncomplementary items retrieval. But it fails to identify low level style\nfeatures which make items compatible in human eyes. These low level style\nfeatures are captured to a large extent in techniques used in neural style\ntransfer. This paper proposes a mechanism of utilising those methods in this\nretrieval task and capturing the low level style features through a hybrid\nsiamese network coupled with a hybrid loss. The experimental results indicate\nthat the proposed method outperforms traditional siamese networks in retrieval\ntasks for complementary items.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 05:56:50 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 23:48:47 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bhattacharyya", "Mayukh", ""], ["Nag", "Sayan", ""]]}, {"id": "1912.05636", "submitter": "Sudheer Achary", "authors": "Sudheer Achary, K L Bhanu Moorthy, Syed Ashar Javed, Nikita Shravan,\n  Vineet Gandhi, Anoop Namboodiri", "title": "CineFilter: Unsupervised Filtering for Real Time Autonomous Camera\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous camera systems are often subjected to an optimization/filtering\noperation to smoothen and stabilize the rough trajectory estimates. Most common\nfiltering techniques do reduce the irregularities in data; however, they fail\nto mimic the behavior of a human cameraman. Global filtering methods modeling\nhuman camera operators have been successful; however, they are limited to\noffline settings. In this paper, we propose two online filtering methods called\nCinefilters, which produce smooth camera trajectories that are motivated by\ncinematographic principles. The first filter (CineConvex) uses a sliding\nwindow-based convex optimization formulation, and the second (CineCNN) is a CNN\nbased encoder-decoder model. We evaluate the proposed filters in two different\nsettings, namely a basketball dataset and a stage performance dataset. Our\nmodels outperform previous methods and baselines on both quantitative and\nqualitative metrics. The CineConvex and CineCNN filters operate at about 250fps\nand 1000fps, respectively, with a minor latency (half a second), making them\napt for a variety of real-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:23:59 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 19:25:24 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 11:53:39 GMT"}, {"version": "v4", "created": "Wed, 27 May 2020 10:24:37 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Achary", "Sudheer", ""], ["Moorthy", "K L Bhanu", ""], ["Javed", "Syed Ashar", ""], ["Shravan", "Nikita", ""], ["Gandhi", "Vineet", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "1912.06348", "submitter": "Haojie Liu", "authors": "Haojie Liu, Han shen, Lichao Huang, Ming Lu, Tong Chen, Zhan Ma", "title": "Learned Video Compression via Joint Spatial-Temporal Correlation\n  Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional video compression technologies have been developed over decades\nin pursuit of higher coding efficiency. Efficient temporal information\nrepresentation plays a key role in video coding. Thus, in this paper, we\npropose to exploit the temporal correlation using both first-order optical flow\nand second-order flow prediction. We suggest an one-stage learning approach to\nencapsulate flow as quantized features from consecutive frames which is then\nentropy coded with adaptive contexts conditioned on joint spatial-temporal\npriors to exploit second-order correlations. Joint priors are embedded in\nautoregressive spatial neighbors, co-located hyper elements and temporal\nneighbors using ConvLSTM recurrently. We evaluate our approach for the\nlow-delay scenario with High-Efficiency Video Coding (H.265/HEVC), H.264/AVC\nand another learned video compression method, following the common test\nsettings. Our work offers the state-of-the-art performance, with consistent\ngains across all popular test sequences.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:45:44 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Liu", "Haojie", ""], ["shen", "Han", ""], ["Huang", "Lichao", ""], ["Lu", "Ming", ""], ["Chen", "Tong", ""], ["Ma", "Zhan", ""]]}, {"id": "1912.06540", "submitter": "Shenghua Zhong", "authors": "Songtao Wu, Sheng-hua Zhong, Yan Liu, and Mengyuan Liu", "title": "CIS-Net: A Novel CNN Model for Spatial Image Steganalysis via Cover\n  Image Suppression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image steganalysis is a special binary classification problem that aims to\nclassify natural cover images and suspected stego images which are the results\nof embedding very weak secret message signals into covers. How to effectively\nsuppress cover image content and thus make the classification of cover images\nand stego images easier is the key of this task. Recent researches show that\nConvolutional Neural Networks (CNN) are very effective to detect steganography\nby learning discriminative features between cover images and their stegos.\nSeveral deep CNN models have been proposed via incorporating domain knowledge\nof image steganography/steganalysis into the design of the network and achieve\nstate of the art performance on standard database. Following such direction, we\npropose a novel model called Cover Image Suppression Network (CIS-Net), which\nimproves the performance of spatial image steganalysis by suppressing cover\nimage content as much as possible in model learning. Two novel layers, the\nSingle-value Truncation Layer (STL) and Sub-linear Pooling Layer (SPL), are\nproposed in this work. Specifically, STL truncates input values into a same\nthreshold when they are out of a predefined interval. Theoretically, we have\nproved that STL can reduce the variance of input feature map without\ndeteriorating useful information. For SPL, it utilizes sub-linear power\nfunction to suppress large valued elements introduced by cover image contents\nand aggregates weak embedded signals via average pooling. Extensive experiments\ndemonstrate the proposed network equipped with STL and SPL achieves better\nperformance than rich model classifiers and existing CNN models on challenging\nsteganographic algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 14:54:10 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Wu", "Songtao", ""], ["Zhong", "Sheng-hua", ""], ["Liu", "Yan", ""], ["Liu", "Mengyuan", ""]]}, {"id": "1912.06606", "submitter": "Qifeng Chen", "authors": "Xuanchi Ren, Haoran Li, Zijian Huang, Qifeng Chen", "title": "Music-oriented Dance Video Synthesis with Pose Perceptual Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based approach with pose perceptual loss for automatic\nmusic video generation. Our method can produce a realistic dance video that\nconforms to the beats and rhymes of almost any given music. To achieve this, we\nfirstly generate a human skeleton sequence from music and then apply the\nlearned pose-to-appearance mapping to generate the final video. In the stage of\ngenerating skeleton sequences, we utilize two discriminators to capture\ndifferent aspects of the sequence and propose a novel pose perceptual loss to\nproduce natural dances. Besides, we also provide a new cross-modal evaluation\nto evaluate the dance quality, which is able to estimate the similarity between\ntwo modalities of music and dance. Finally, a user study is conducted to\ndemonstrate that dance video synthesized by the presented approach produces\nsurprisingly realistic results. The results are shown in the supplementary\nvideo at https://youtu.be/0rMuFMZa_K4\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 17:01:21 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Ren", "Xuanchi", ""], ["Li", "Haoran", ""], ["Huang", "Zijian", ""], ["Chen", "Qifeng", ""]]}, {"id": "1912.07101", "submitter": "Omid Jafari", "authors": "Omid Jafari, Parth Nagarkar, Jonathan Monta\\~no", "title": "Efficient Bitmap-based Indexing and Retrieval of Similarity Search Image\n  Queries", "comments": "Submitted to 2020 IEEE Southwest Symposium on Image Analysis and\n  Interpretation (SSIAI 2020)", "journal-ref": "2020 IEEE Southwest Symposium on Image Analysis and Interpretation\n  (SSIAI), Albuquerque, NM, USA, 2020, pp. 58-61", "doi": "10.1109/SSIAI49293.2020.9094616", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding similar images is a necessary operation in many multimedia\napplications. Images are often represented and stored as a set of\nhigh-dimensional features, which are extracted using localized feature\nextraction algorithms. Locality Sensitive Hashing is one of the most popular\napproximate processing techniques for finding similar points in\nhigh-dimensional spaces. Locality Sensitive Hashing (LSH) and its variants are\ndesigned to find similar points, but they are not designed to find objects\n(such as images, which are made up of a collection of points) efficiently. In\nthis paper, we propose an index structure, Bitmap-Image LSH (bImageLSH), for\nefficient processing of high-dimensional images. Using a real dataset, we\nexperimentally show the performance benefit of our novel design while keeping\nthe accuracy of the image results high.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 20:09:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""], ["Monta\u00f1o", "Jonathan", ""]]}, {"id": "1912.07126", "submitter": "Wentao Liu", "authors": "Zhengfang Duanmu (1), Wentao Liu (1), Zhuoran Li (1), Kede Ma (2) and\n  Zhou Wang (1) ((1) University of Waterloo, Canada, (2) City University of\n  Hong Kong, Hong Kong, China)", "title": "Characterizing Generalized Rate-Distortion Performance of Video Coding:\n  An Eigen Analysis Approach", "comments": "The manuscript is submitted to Transactions on Image Processing.\n  Zhengfang Duanmu and Wentao Liu contributed equally to the manuscript", "journal-ref": null, "doi": "10.1109/TIP.2020.2988437", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate-distortion (RD) theory is at the heart of lossy data compression. Here\nwe aim to model the generalized RD (GRD) trade-off between the visual quality\nof a compressed video and its encoding profiles (e.g., bitrate and spatial\nresolution). We first define the theoretical functional space $\\mathcal{W}$ of\nthe GRD function by analyzing its mathematical properties.We show that\n$\\mathcal{W}$ is a convex set in a Hilbert space, inspiring a computational\nmodel of the GRD function, and a method of estimating model parameters from\nsparse measurements. To demonstrate the feasibility of our idea, we collect a\nlarge-scale database of real-world GRD functions, which turn out to live in a\nlow-dimensional subspace of $\\mathcal{W}$. Combining the GRD reconstruction\nframework and the learned low-dimensional space, we create a low-parameter\neigen GRD method to accurately estimate the GRD function of a source video\ncontent from only a few queries. Experimental results on the database show that\nthe learned GRD method significantly outperforms state-of-the-art empirical RD\nestimation methods both in accuracy and efficiency. Last, we demonstrate the\npromise of the proposed model in video codec comparison.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 22:50:00 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 00:52:14 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 10:27:24 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Duanmu", "Zhengfang", ""], ["Liu", "Wentao", ""], ["Li", "Zhuoran", ""], ["Ma", "Kede", ""], ["Wang", "Zhou", ""]]}, {"id": "1912.07854", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai, Dimitris Agrafiotis", "title": "Enhanced Spatially Interleaved Techniques for Multi-View Distributed\n  Video Coding", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-view distributed video coding framework for\nindependent camera encoding and centralized decoding. Spatio-temporal-view\nconcealment methods are developed that exploit the interleaved nature of the\nemployed hybrid KEY/Wyner-Ziv frames for block-wise generation of the side\ninformation (SI). We study a number of view concealment methods and develop a\njoint approach that exploits all available correlation for forming the side\ninformation. We apply a diversity technique for fusing multiple such\npredictions thereby achieving more reliable results. We additionally introduce\nsystems enhancements for further improving the rate distortion performance\nthrough selective feedback, inter-view bitplane projection and frame\nsubtraction. Results show a significant improvement in performance relative to\nH.264 intra coding of up to 25% reduction in bitrate or equivalently 2.5 dB\nincrease in PSNR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 07:35:26 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Agrafiotis", "Dimitris", ""]]}, {"id": "1912.07966", "submitter": "Franz G\\\"otz-Hahn", "authors": "Franz G\\\"otz-Hahn, Vlad Hosu, Hanhe Lin, Dietmar Saupe", "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of\n  Videos in-the-Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality assessment (VQA) methods focus on particular degradation types,\nusually artificially induced on a small set of reference videos. Hence, most\ntraditional VQA methods under-perform in-the-wild. Deep learning approaches\nhave had limited success due to the small size and diversity of existing VQA\ndatasets, either artificial or authentically distorted. We introduce a new\nin-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k.\nIt consists of a coarsely annotated set of 153,841 videos having five quality\nratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally,\nwe propose new efficient VQA approaches (MLSP-VQA) relying on multi-level\nspatially pooled deep-features (MLSP). They are exceptionally well suited for\ntraining at scale, compared to deep transfer learning approaches. Our best\nmethod, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient\n(SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark\ndataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC)\nand hand-crafted feature-based method (0.78 SRCC). We further investigate how\nalternative approaches perform under different levels of label noise, and\ndataset size, showing that MLSP-VQA-FF is the overall best method for videos\nin-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k\nsets the new state-of-the-art for cross-test performance on KoNViD-1k,\nLIVE-VQC, and LIVE-Qualcomm with a 0.83, 0.75, and 0.64 SRCC, respectively. For\nboth KoNViD-1k and LIVE-VQC this inter-dataset testing outperforms\nintra-dataset experiments, showing excellent generalization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:26:32 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 14:25:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["G\u00f6tz-Hahn", "Franz", ""], ["Hosu", "Vlad", ""], ["Lin", "Hanhe", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1912.09137", "submitter": "Alireza Javaheri", "authors": "Alireza Javaheri, Catarina Brites, Fernando Pereira, Joao Ascenso", "title": "Point Cloud Rendering after Coding: Impacts on Subjective and Objective\n  Quality", "comments": "This paper is a preprint of a paper submitted to IEEE Transaction on\n  Multimedia. If accepted, the copy of record will be available at the IEEE\n  Xplore digital library", "journal-ref": null, "doi": "10.1109/TMM.2020.3037481", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, point clouds have shown to be a promising way to represent 3D\nvisual data for a wide range of immersive applications, from augmented reality\nto autonomous cars. Emerging imaging sensors have made easier to perform richer\nand denser point cloud acquisition, notably with millions of points, thus\nraising the need for efficient point cloud coding solutions. In such a\nscenario, it is important to evaluate the impact and performance of several\nprocessing steps in a point cloud communication system, notably the quality\ndegradations associated to point cloud coding solutions. Moreover, since point\nclouds are not directly visualized but rather processed with a rendering\nalgorithm before shown on any display, the perceived quality of point cloud\ndata highly depends on the rendering solution. In this context, the main\nobjective of this paper is to study the impact of several coding and rendering\nsolutions on the perceived user quality and in the performance of available\nobjective quality assessment metrics. Another contribution regards the\nassessment of recent MPEG point cloud coding solutions for several popular\nrendering methods which were never presented before. The conclusions regard the\nvisibility of three types of coding artifacts for the three considered\nrendering approaches as well as the strengths and weakness of objective quality\nmetrics when point clouds are rendered after coding.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:40:26 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 15:30:25 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Javaheri", "Alireza", ""], ["Brites", "Catarina", ""], ["Pereira", "Fernando", ""], ["Ascenso", "Joao", ""]]}, {"id": "1912.09572", "submitter": "Behnam Kiani Kalejahi", "authors": "Ulkar Ahmadova, Laman Mammadova, Behnam Kiani Kalejahi", "title": "Implementation of encryption on telemedicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the era of technology, data security is one of the most important things\nthat both individuals and companies need. Information plays a huge role in our\neveryday life and keeping it safe should be our number one priority. Nowadays\nmost of the information is transferred via the internet. One of the ways to use\nit is telemedicine. With the help of telemedicine, people can have an\nappointment at the doctors without losing their time or money. All of the\ninformation about one's health is transferred through the internet but is it\nthat safe? What techniques are used to provide the safety of our confidential\ninformation? To guarantee that the information is not changed or that in case\nit will be stolen no one can still have access to it.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 07:12:54 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 08:28:39 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ahmadova", "Ulkar", ""], ["Mammadova", "Laman", ""], ["Kalejahi", "Behnam Kiani", ""]]}, {"id": "1912.09669", "submitter": "Lin Zhu", "authors": "Siwei Dong, Lin Zhu, Daoyuan Xu, Yonghong Tian and Tiejun Huang", "title": "An Efficient Coding Method for Spike Camera using Inter-Spike Intervals", "comments": "Accepted in DCC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a novel bio-inspired spike camera has been proposed, which\ncontinuously accumulates luminance intensity and fires spikes while the\ndispatch threshold is reached. Compared to the conventional frame-based cameras\nand the emerging dynamic vision sensors, the spike camera has shown great\nadvantages in capturing fast-moving scene in a frame-free manner with full\ntexture reconstruction capabilities. However, it is difficult to transmit or\nstore the large amount of spike data. To address this problem, we first\ninvestigate the spatiotemporal distribution of inter-spike intervals and\npropose an intensity-based measurement of spike train distance. Then, we design\nan efficient spike coding method, which integrates the techniques of adaptive\ntemporal partitioning, intra-/inter-pixel prediction, quantization and entropy\ncoding into a unified lossy coding framework. Finally, we construct a PKU-Spike\ndataset captured by the spike camera to evaluate the compression performance.\nThe experimental results on the dataset demonstrate that the proposed approach\nis effective in compressing such spike data while maintaining the fidelity.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:20:05 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Dong", "Siwei", ""], ["Zhu", "Lin", ""], ["Xu", "Daoyuan", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""]]}, {"id": "1912.09675", "submitter": "Hui Yuan", "authors": "Hui Yuan, Shiyun Zhao, Junhui Hou, Xuekai Wei, and Sam Kwong", "title": "Spatial and Temporal Consistency-Aware Dynamic Adaptive Streaming for\n  360-Degree Videos", "comments": "16 pages, This paper has been accepted by the IEEE Journal of\n  Selected Topics in Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 360-degree video allows users to enjoy the whole scene by interactively\nswitching viewports. However, the huge data volume of the 360-degree video\nlimits its remote applications via network. To provide high quality of\nexperience (QoE) for remote web users, this paper presents a tile-based\nadaptive streaming method for 360-degree videos. First, we propose a simple yet\neffective rate adaptation algorithm to determine the requested bitrate for\ndownloading the current video segment by considering the balance between the\nbuffer length and video quality. Then, we propose to use a Gaussian model to\npredict the field of view at the beginning of each requested video segment. To\ndeal with the circumstance that the view angle is switched during the display\nof a video segment, we propose to download all the tiles in the 360-degree\nvideo with different priorities based on a Zipf model. Finally, in order to\nallocate bitrates for all the tiles, a two-stage optimization algorithm is\nproposed to preserve the quality of tiles in FoV and guarantee the spatial and\ntemporal smoothness. Experimental results demonstrate the effectiveness and\nadvantage of the proposed method compared with the state-of-the-art methods.\nThat is, our method preserves both the quality and the smoothness of tiles in\nFoV, thus providing the best QoE for users.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:41:02 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yuan", "Hui", ""], ["Zhao", "Shiyun", ""], ["Hou", "Junhui", ""], ["Wei", "Xuekai", ""], ["Kwong", "Sam", ""]]}, {"id": "1912.10070", "submitter": "Jonathan Lwowski", "authors": "Isaac Corley, Jonathan Lwowski, and Justin Hoffman", "title": "Destruction of Image Steganography using Generative Adversarial Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.LG eess.IV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital image steganalysis, or the detection of image steganography, has been\nstudied in depth for years and is driven by Advanced Persistent Threat (APT)\ngroups', such as APT37 Reaper, utilization of steganographic techniques to\ntransmit additional malware to perform further post-exploitation activity on a\ncompromised host. However, many steganalysis algorithms are constrained to work\nwith only a subset of all possible images in the wild or are known to produce a\nhigh false positive rate. This results in blocking any suspected image being an\nunreasonable policy. A more feasible policy is to filter suspicious images\nprior to reception by the host machine. However, how does one optimally filter\nspecifically to obfuscate or remove image steganography while avoiding\ndegradation of visual image quality in the case that detection of the image was\na false positive? We propose the Deep Digital Steganography Purifier (DDSP), a\nGenerative Adversarial Network (GAN) which is optimized to destroy\nsteganographic content without compromising the perceptual quality of the\noriginal image. As verified by experimental results, our model is capable of\nproviding a high rate of destruction of steganographic image content while\nmaintaining a high visual quality in comparison to other state-of-the-art\nfiltering methods. Additionally, we test the transfer learning capability of\ngeneralizing to to obfuscate real malware payloads embedded into different\nimage file formats and types using an unseen steganographic algorithm and prove\nthat our model can in fact be deployed to provide adequate results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:23:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Corley", "Isaac", ""], ["Lwowski", "Jonathan", ""], ["Hoffman", "Justin", ""]]}, {"id": "1912.10088", "submitter": "Zhenqiang Ying", "authors": "Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti\n  Ghadiyaram, Alan Bovik", "title": "From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of\n  Picture Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blind or no-reference (NR) perceptual picture quality prediction is a\ndifficult, unsolved problem of great consequence to the social and streaming\nmedia industries that impacts billions of viewers daily. Unfortunately, popular\nNR prediction models perform poorly on real-world distorted pictures. To\nadvance progress on this problem, we introduce the largest (by far) subjective\npicture quality database, containing about 40000 real-world distorted pictures\nand 120000 patches, on which we collected about 4M human judgments of picture\nquality. Using these picture and patch quality labels, we built deep\nregion-based architectures that learn to produce state-of-the-art global\npicture quality predictions as well as useful local picture quality maps. Our\ninnovations include picture quality prediction architectures that produce\nglobal-to-local inferences as well as local-to-global inferences (via\nfeedback).\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 20:22:55 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ying", "Zhenqiang", ""], ["Niu", "Haoran", ""], ["Gupta", "Praful", ""], ["Mahajan", "Dhruv", ""], ["Ghadiyaram", "Deepti", ""], ["Bovik", "Alan", ""]]}, {"id": "1912.10131", "submitter": "Eda Okur", "authors": "Shachi H Kumar, Eda Okur, Saurav Sahay, Jonathan Huang, Lama Nachman", "title": "Leveraging Topics and Audio Features with Multimodal Attention for Audio\n  Visual Scene-Aware Dialog", "comments": "Presented at the 3rd Visually Grounded Interaction and Language\n  (ViGIL) Workshop, NeurIPS 2019, Vancouver, Canada. arXiv admin note:\n  substantial text overlap with arXiv:1812.08407, arXiv:1912.10132", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancements in Artificial Intelligence (AI), Intelligent\nVirtual Assistants (IVA) such as Alexa, Google Home, etc., have become a\nubiquitous part of many homes. Currently, such IVAs are mostly audio-based, but\ngoing forward, we are witnessing a confluence of vision, speech and dialog\nsystem technologies that are enabling the IVAs to learn audio-visual groundings\nof utterances. This will enable agents to have conversations with users about\nthe objects, activities and events surrounding them. In this work, we present\nthree main architectural explorations for the Audio Visual Scene-Aware Dialog\n(AVSD): 1) investigating `topics' of the dialog as an important contextual\nfeature for the conversation, 2) exploring several multimodal attention\nmechanisms during response generation, 3) incorporating an end-to-end audio\nclassification ConvNet, AclNet, into our architecture. We discuss detailed\nanalysis of the experimental results and show that our model variations\noutperform the baseline system presented for the AVSD task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:55:40 GMT"}], "update_date": "2019-12-27", "authors_parsed": [["Kumar", "Shachi H", ""], ["Okur", "Eda", ""], ["Sahay", "Saurav", ""], ["Huang", "Jonathan", ""], ["Nachman", "Lama", ""]]}, {"id": "1912.10248", "submitter": "Huaizheng Zhang", "authors": "Huaizheng Zhang, Yong Luo, Qiming Ai, Yonggang Wen", "title": "Look, Read and Feel: Benchmarking Ads Understanding with Multimodal\n  Multitask Learning", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the massive market of advertising and the sharply increasing online\nmultimedia content (such as videos), it is now fashionable to promote\nadvertisements (ads) together with the multimedia content. It is exhausted to\nfind relevant ads to match the provided content manually, and hence, some\nautomatic advertising techniques are developed. Since ads are usually hard to\nunderstand only according to its visual appearance due to the contained visual\nmetaphor, some other modalities, such as the contained texts, should be\nexploited for understanding. To further improve user experience, it is\nnecessary to understand both the topic and sentiment of the ads. This motivates\nus to develop a novel deep multimodal multitask framework to integrate multiple\nmodalities to achieve effective topic and sentiment prediction simultaneously\nfor ads understanding. In particular, our model first extracts multimodal\ninformation from ads and learn high-level and comparable representations. The\nvisual metaphor of the ad is decoded in an unsupervised manner. The obtained\nrepresentations are then fed into the proposed hierarchical multimodal\nattention modules to learn task-specific representations for final prediction.\nA multitask loss function is also designed to train both the topic and\nsentiment prediction models jointly in an end-to-end manner. We conduct\nextensive experiments on the latest and large advertisement dataset and achieve\nstate-of-the-art performance for both prediction tasks. The obtained results\ncould be utilized as a benchmark for ads understanding.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 11:19:08 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 09:37:30 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Zhang", "Huaizheng", ""], ["Luo", "Yong", ""], ["Ai", "Qiming", ""], ["Wen", "Yonggang", ""]]}, {"id": "1912.10413", "submitter": "Tanay Singhania", "authors": "Kartik Sharma, Ashutosh Aggarwal, Tanay Singhania, Deepak Gupta,\n  Ashish Khanna", "title": "Hiding Data in Images Using Cryptography and Deep Neural Network", "comments": "20 pages, 9 figures, 5 tables", "journal-ref": null, "doi": "10.33969/AIS.2019.11009", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steganography is an art of obscuring data inside another quotidian file of\nsimilar or varying types. Hiding data has always been of significant importance\nto digital forensics. Previously, steganography has been combined with\ncryptography and neural networks separately. Whereas, this research combines\nsteganography, cryptography with the neural networks all together to hide an\nimage inside another container image of the larger or same size. Although the\ncryptographic technique used is quite simple, but is effective when convoluted\nwith deep neural nets. Other steganography techniques involve hiding data\nefficiently, but in a uniform pattern which makes it less secure. This method\ntargets both the challenges and make data hiding secure and non-uniform.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 10:19:44 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 18:51:59 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sharma", "Kartik", ""], ["Aggarwal", "Ashutosh", ""], ["Singhania", "Tanay", ""], ["Gupta", "Deepak", ""], ["Khanna", "Ashish", ""]]}, {"id": "1912.10789", "submitter": "Ahmad Shawahna", "authors": "Ahmad Shawahna, Md. Enamul Haque, and Alaaeldin Amin", "title": "JPEG Image Compression using the Discrete Cosine Transform: An Overview,\n  Applications, and Hardware Implementation", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital images are becoming large in size containing more information day by\nday to represent the as is state of the original one due to the availability of\nhigh resolution digital cameras, smartphones, and medical tests images.\nTherefore, we need to come up with some technique to convert these images into\nsmaller size without loosing much information from the actual. There are both\nlossy and lossless image compression format available and JPEG is one of the\npopular lossy compression among them. In this paper, we present the\narchitecture and implementation of JPEG compression using VHDL (VHSIC Hardware\nDescription Language) and compare the performance with some contemporary\nimplementation. JPEG compression takes place in five steps with color space\nconversion, down sampling, discrete cosine transformation (DCT), quantization,\nand entropy encoding. The five steps cover for the compression purpose only.\nAdditionally, we implement the reverse order in VHDL to get the original image\nback. We use optimized matrix multiplication and quantization for DCT to\nachieve better performance. Our experimental results show that significant\namount of compression ratio has been achieved with very little change in the\nimages, which is barely noticeable to human eye.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 22:55:26 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Shawahna", "Ahmad", ""], ["Haque", "Md. Enamul", ""], ["Amin", "Alaaeldin", ""]]}, {"id": "1912.10809", "submitter": "Christian Otto", "authors": "Hang Zhou, Christian Otto, Ralph Ewerth", "title": "Visual Summarization of Scholarly Videos using Word Embeddings and\n  Keyphrase Extraction", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-30760-8_28", "report-no": null, "categories": "cs.MM cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective learning with audiovisual content depends on many factors. Besides\nthe quality of the learning resource's content, it is essential to discover the\nmost relevant and suitable video in order to support the learning process most\neffectively. Video summarization techniques facilitate this goal by providing a\nquick overview over the content. It is especially useful for longer recordings\nsuch as conference presentations or lectures. In this paper, we present an\napproach that generates a visual summary of video content based on semantic\nword embeddings and keyphrase extraction. For this purpose, we exploit video\nannotations that are automatically generated by speech recognition and video\nOCR (optical character recognition).\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:02:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhou", "Hang", ""], ["Otto", "Christian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1912.11145", "submitter": "Xing Xu", "authors": "Xing Xu, Zahaib Akhtar, Wyatt Lloyd, Antonio Ortega, Ramesh Govindan", "title": "Reducing Storage in Large-Scale Photo Sharing Services using\n  Recompression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of photo sharing services has increased dramatically in recent\nyears. Increases in users, quantity of photos, and quality/resolution of photos\ncombined with the user expectation that photos are reliably stored indefinitely\ncreates a growing burden on the storage backend of these services. We identify\na new opportunity for storage savings with application-specific compression for\nphoto sharing services: photo recompression.\n  We explore new photo storage management techniques that are fast so they do\nnot adversely affect photo download latency, are complementary to existing\ndistributed erasure coding techniques, can efficiently be converted to the\nstandard JPEG user devices expect, and significantly increase compression. We\nimplement our photo recompression techniques in two novel codecs, ROMP and\nL-ROMP. ROMP is a lossless JPEG recompression codec that compresses typical\nphotos 15% over standard JPEG. L-ROMP is a lossy JPEG recompression codec that\ndistorts photos in a perceptually un-noticeable way and typically achieves 28%\ncompression over standard JPEG. We estimate the benefits of our approach on\nFacebook's photo stack and find that our approaches can reduce the photo\nstorage by 0.3-0.9x the logical size of the stored photos, and offer\nadditional, collateral benefits to the photo caching stack, including 5-11%\nfewer requests to the backend storage, 15-31% reduction in wide-area bandwidth,\nand 16% reduction in external bandwidth.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 23:39:51 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Xu", "Xing", ""], ["Akhtar", "Zahaib", ""], ["Lloyd", "Wyatt", ""], ["Ortega", "Antonio", ""], ["Govindan", "Ramesh", ""]]}, {"id": "1912.11318", "submitter": "Khadija Bouraqia", "authors": "Khadija Bouraqia, Essaid Sabir, Mohamed Sadik and Latif Ladid", "title": "Quality of Experience for Streaming Services: Measurements, Challenges\n  and Insights", "comments": "20 pages, 3 figures, Submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the evolution of network and user handsets'\ntechnologies, have challenged the telecom industry and the Internet ecosystem.\nEspecially, the unprecedented progress of multimedia streaming services like\nYouTube, Vimeo and DailyMotion resulted in an impressive demand growth and a\nsignificant need of Quality of Service (QoS) (e.g., high data rate, low\nlatency/jitter, etc.). Mainly, numerous difficulties are to be considered while\ndelivering a specific service, such as a strict QoS, human-centric features,\nmassive number of devices, heterogeneous devices and networks, and\nuncontrollable environments. Thenceforth, the concept of Quality of Experience\n(QoE) is gaining visibility, and tremendous research efforts have been spent on\nimproving and/or delivering reliable and addedvalue services, at a high user\nexperience. In this paper, we present the importance of QoE in wireless and\nmobile networks (4G, 5G, and beyond), by providing standard definitions and the\nmost important measurement methods developed. Moreover, we exhibit notable\nenhancements and controlling approaches proposed by researchers to meet the\nuser expectation in terms of service experience.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 12:28:59 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Bouraqia", "Khadija", ""], ["Sabir", "Essaid", ""], ["Sadik", "Mohamed", ""], ["Ladid", "Latif", ""]]}, {"id": "1912.11604", "submitter": "Xiaoyi He", "authors": "Weiyao Lin, Xiaoyi He, Xintong Han, Dong Liu, John See, Junni Zou,\n  Hongkai Xiong, and Feng Wu", "title": "Partition-Aware Adaptive Switching Neural Networks for Post-Processing\n  in HEVC", "comments": "to appear in IEEE Transaction on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2019.2962310", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses neural network based post-processing for the\nstate-of-the-art video coding standard, High Efficiency Video Coding (HEVC). We\nfirst propose a partition-aware Convolution Neural Network (CNN) that utilizes\nthe partition information produced by the encoder to assist in the\npost-processing. In contrast to existing CNN-based approaches, which only take\nthe decoded frame as input, the proposed approach considers the coding unit\n(CU) size information and combines it with the distorted decoded frame such\nthat the artifacts introduced by HEVC are efficiently reduced. We further\nintroduce an adaptive-switching neural network (ASN) that consists of multiple\nindependent CNNs to adaptively handle the variations in content and distortion\nwithin compressed-video frames, providing further reduction in visual\nartifacts. Additionally, an iterative training procedure is proposed to train\nthese independent CNNs attentively on different local patch-wise classes.\nExperiments on benchmark sequences demonstrate the effectiveness of our\npartition-aware and adaptive-switching neural networks. The source code can be\nfound at http://min.sjtu.edu.cn/lwydemo/HEVCpostprocessing.html.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 06:11:54 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lin", "Weiyao", ""], ["He", "Xiaoyi", ""], ["Han", "Xintong", ""], ["Liu", "Dong", ""], ["See", "John", ""], ["Zou", "Junni", ""], ["Xiong", "Hongkai", ""], ["Wu", "Feng", ""]]}, {"id": "1912.11691", "submitter": "Fahimeh Fooladgar", "authors": "Fahimeh Fooladgar and Shohreh Kasaei", "title": "Multi-Modal Attention-based Fusion Model for Semantic Segmentation of\n  RGB-Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D scene understanding is mainly considered as a crucial requirement in\ncomputer vision and robotics applications. One of the high-level tasks in 3D\nscene understanding is semantic segmentation of RGB-Depth images. With the\navailability of RGB-D cameras, it is desired to improve the accuracy of the\nscene understanding process by exploiting the depth features along with the\nappearance features. As depth images are independent of illumination, they can\nimprove the quality of semantic labeling alongside RGB images. Consideration of\nboth common and specific features of these two modalities improves the\nperformance of semantic segmentation. One of the main problems in RGB-Depth\nsemantic segmentation is how to fuse or combine these two modalities to achieve\nmore advantages of each modality while being computationally efficient.\nRecently, the methods that encounter deep convolutional neural networks have\nreached the state-of-the-art results by early, late, and middle fusion\nstrategies. In this paper, an efficient encoder-decoder model with the\nattention-based fusion block is proposed to integrate mutual influences between\nfeature maps of these two modalities. This block explicitly extracts the\ninterdependences among concatenated feature maps of these modalities to exploit\nmore powerful feature maps from RGB-Depth images. The extensive experimental\nresults on three main challenging datasets of NYU-V2, SUN RGB-D, and Stanford\n2D-3D-Semantic show that the proposed network outperforms the state-of-the-art\nmodels with respect to computational cost as well as model size. Experimental\nresults also illustrate the effectiveness of the proposed lightweight\nattention-based fusion model in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 16:53:31 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fooladgar", "Fahimeh", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1912.11822", "submitter": "Hui Yuan", "authors": "Hui Yuan, Xiaoqian Hu, Junhui Hou, Xuekai Wei, and Sam Kwong", "title": "An Ensemble Rate Adaptation Framework for Dynamic Adaptive Streaming\n  Over HTTP", "comments": "This article has been accepted by IEEE Transactions on Broadcasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate adaptation is one of the most important issues in dynamic adaptive\nstreaming over HTTP (DASH). Due to the frequent fluctuations of the network\nbandwidth and complex variations of video content, it is difficult to deal with\nthe varying network conditions and video content perfectly by using a single\nrate adaptation method. In this paper, we propose an ensemble rate adaptation\nframework for DASH, which aims to leverage the advantages of multiple methods\ninvolved in the framework to improve the quality of experience (QoE) of users.\nThe proposed framework is simple yet very effective. Specifically, the proposed\nframework is composed of two modules, i.e., the method pool and method\ncontroller. In the method pool, several rate adap tation methods are\nintegrated. At each decision time, only the method that can achieve the best\nQoE is chosen to determine the bitrate of the requested video segment. Besides,\nwe also propose two strategies for switching methods, i.e., InstAnt Method\nSwitching, and InterMittent Method Switching, for the method controller to\ndetermine which method can provide the best QoEs. Simulation results\ndemonstrate that, the proposed framework always achieves the highest QoE for\nthe change of channel environment and video complexity, compared with\nstate-of-the-art rate adaptation methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 09:54:18 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yuan", "Hui", ""], ["Hu", "Xiaoqian", ""], ["Hou", "Junhui", ""], ["Wei", "Xuekai", ""], ["Kwong", "Sam", ""]]}, {"id": "1912.11954", "submitter": "Hui Yuan", "authors": "Hui Yuan, Huayong Fu, Ju Liu, Junhui Hou, and Sam Kwong", "title": "Non-Cooperative Game Theory Based Rate Adaptation for Dynamic Video\n  Streaming over HTTP", "comments": "This paper has been published on IEEE Transactions on Mobile\n  Computing. H. Yuan, H. Fu, J. Liu, J. Hou, and S. Kwong, \"Non-Cooperative\n  Game Theory Based Rate Adaptation for Dynamic Video Streaming over HTTP,\"\n  IEEE Transactions on Mobile Computing, vol.17, no.10, pp. 2334-2348, Oct.\n  2018", "journal-ref": "IEEE Transactions on Mobile Computing, vol.17, no.10, pp.\n  2334-2348, Oct. 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Adaptive Streaming over HTTP (DASH) has demonstrated to be an\nemerging and promising multimedia streaming technique, owing to its capability\nof dealing with the variability of networks. Rate adaptation mechanism, a\nchallenging and open issue, plays an important role in DASH based systems since\nit affects Quality of Experience (QoE) of users, network utilization, etc. In\nthis paper, based on non-cooperative game theory, we propose a novel algorithm\nto optimally allocate the limited export bandwidth of the server to multi-users\nto maximize their QoE with fairness guaranteed. The proposed algorithm is\nproxy-free. Specifically, a novel user QoE model is derived by taking a variety\nof factors into account, like the received video quality, the reference buffer\nlength, and user accumulated buffer lengths, etc. Then, the bandwidth competing\nproblem is formulated as a non-cooperation game with the existence of Nash\nEquilibrium that is theoretically proven. Finally, a distributed iterative\nalgorithm with stability analysis is proposed to find the Nash Equilibrium.\nCompared with state-of-the-art methods, extensive experimental results in terms\nof both simulated and realistic networking scenarios demonstrate that the\nproposed algorithm can produce higher QoE, and the actual buffer lengths of all\nusers keep nearly optimal states, i.e., moving around the reference buffer all\nthe time. Besides, the proposed algorithm produces no playback interruption.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 01:19:14 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yuan", "Hui", ""], ["Fu", "Huayong", ""], ["Liu", "Ju", ""], ["Hou", "Junhui", ""], ["Kwong", "Sam", ""]]}, {"id": "1912.11977", "submitter": "Renzhi Wu", "authors": "Renzhi Wu, Sergey Sukhanov, Christian Debes", "title": "Real Time Pattern Matching with Dynamic Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching in time series data streams is considered to be an essential\ndata mining problem that still stays challenging for many practical scenarios.\nDifferent factors such as noise, varying amplitude scale or shift, signal\nstretches or shrinks in time are all leading to performance degradation of many\nexisting pattern matching algorithms. In this paper, we introduce a dynamic\nz-normalization mechanism allowing for proper signal scaling even under\nsignificant time and amplitude distortions. Based on that, we further propose a\nDynamic Time Warping-based real-time pattern matching method to recover hidden\npatterns that can be distorted in both time and amplitude. We evaluate our\nproposed method on synthetic and real-world scenarios under realistic\nconditions demonstrating its high operational characteristics comparing to\nother state-of-the-art pattern matching methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 04:02:06 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 06:41:29 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wu", "Renzhi", ""], ["Sukhanov", "Sergey", ""], ["Debes", "Christian", ""]]}, {"id": "1912.12362", "submitter": "Simone Santini", "authors": "Maria Rojo Gonz\\'alez and Simone Santini", "title": "Structural characterization of musical harmonies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structural characteristics of harmony is essential for an\neffective use of music as a communication medium. Of the three expressive axes\nof music (melody, rhythm, harmony), harmony is the foundation on which the\nemotional content is built, and its understanding is important in areas such as\nmultimedia and affective computing. The common tool for studying this kind of\nstructure in computing science is the formal grammar but, in the case of music,\ngrammars run into problems due to the ambiguous nature of some of the concepts\ndefined in music theory. In this paper, we consider one of such constructs:\nmodulation, that is, the change of key in the middle of a musical piece, an\nimportant tool used by many authors to enhance the capacity of music to express\nemotions. We develop a hybrid method in which an evidence-gathering numerical\nmethod detects modulation and then, based on the detected tonalities, a\nnon-ambiguous grammar can be used for analyzing the structure of each tonal\ncomponent. Experiments with music from the XVII and XVIII centuries show that\nwe can detect the precise point of modulation with an error of at most two\nchords in almost 97% of the cases. Finally, we show examples of complete\nmodulation and structural analysis of musical harmonies.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 23:15:49 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Gonz\u00e1lez", "Maria Rojo", ""], ["Santini", "Simone", ""]]}, {"id": "1912.12395", "submitter": "Arjun Gupta", "authors": "Arjun Gupta, Dashiell Kosaka, Edwin Pan, Jingning Tang, Ruihao Yao,\n  Sanjay Patel", "title": "OpenRadar: A Toolkit for Prototyping mmWave Radar Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter-Wave (mmWave) radar sensors are gaining popularity for their\nrobust sensing and increasing imaging capabilities. However, current radar\nsignal processing is hardware specific, which makes it impossible to build\nsensor agnostic solutions. OpenRadar serves as an interface to prototype,\nresearch, and benchmark solutions in a modular manner. This enables creating\nsoftware processing stacks in a way that has not yet been extensively explored.\nIn the wake of increased AI adoption, OpenRadar can accelerate the growth of\nthe combined fields of radar and AI. The OpenRadar API was released on Oct 2,\n2019 as an open-source package under the Apache 2.0 license. The codebase\nexists at https://github.com/presenseradar/openradar.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 03:54:47 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gupta", "Arjun", ""], ["Kosaka", "Dashiell", ""], ["Pan", "Edwin", ""], ["Tang", "Jingning", ""], ["Yao", "Ruihao", ""], ["Patel", "Sanjay", ""]]}, {"id": "1912.12467", "submitter": "Alcardo Alex Barakabitze", "authors": "Alcardo Alex Barakabitze, Nabajeet Barman, Arslan Ahmad, Saman\n  Zadtootaghaj, Lingfen Sun, Maria G. Martini, Luigi Atzori", "title": "QoE Management of Multimedia Streaming Services in Future Networks: A\n  Tutorial and Survey", "comments": "42 pages, 21 figures, 10 tables", "journal-ref": null, "doi": "10.1109/COMST.2019.2958784", "report-no": null, "categories": "cs.NI cs.MM eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide in this paper a tutorial and a comprehensive survey of QoE\nmanagement solutions in current and future networks. We start with a high level\ndescription of QoE management for multimedia services, which integrates QoE\nmodelling, monitoring, and optimization. This followed by a discussion of HTTP\nAdaptive Streaming (HAS) solutions as the dominant technique for streaming\nvideos over the best-effort Internet. We then summarize the key elements in\nSDN/NFV along with an overview of ongoing research projects, standardization\nactivities and use cases related to SDN, NFV, and other emerging applications.\nWe provide a survey of the state-of-the-art of QoE management techniques\ncategorized into three different groups: a) QoE-aware/driven strategies using\nSDN and/or NFV; b) QoE-aware/driven approaches for adaptive streaming over\nemerging architectures such as multi-access edge computing, cloud/fog\ncomputing, and information-centric networking; and c) extended QoE management\napproaches in new domains such as immersive augmented and virtual reality,\nmulsemedia and video gaming applications. Based on the review, we present a\nlist of identified future QoE management challenges regarding emerging\nmultimedia applications, network management and orchestration, network slicing\nand collaborative service management in softwarized networks. Finally, we\nprovide a discussion on future research directions with a focus on emerging\nresearch areas in QoE management, such as QoE-oriented business models,\nQoE-based big data strategies, and scalability issues in QoE optimization.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 14:50:48 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Barakabitze", "Alcardo Alex", ""], ["Barman", "Nabajeet", ""], ["Ahmad", "Arslan", ""], ["Zadtootaghaj", "Saman", ""], ["Sun", "Lingfen", ""], ["Martini", "Maria G.", ""], ["Atzori", "Luigi", ""]]}, {"id": "1912.12871", "submitter": "Yang Hao", "authors": "YongJian Bao, Hao Yang, Zhongliang Yang, Sheng Liu, Yongfeng Huang", "title": "Text Steganalysis with Attentional LSTM-CNN", "comments": "5 pages, 1 figures, accepted by ICCCS'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of Natural Language Processing (NLP) technologies,\ntext steganography methods have been significantly innovated recently, which\nposes a great threat to cybersecurity. In this paper, we propose a novel\nattentional LSTM-CNN model to tackle the text steganalysis problem. The\nproposed method firstly maps words into semantic space for better exploitation\nof the semantic feature in texts and then utilizes a combination of\nConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM)\nrecurrent neural networks to capture both local and long-distance contextual\ninformation in steganography texts. In addition, we apply attention mechanism\nto recognize and attend to important clues within suspicious sentences. After\nmerge feature clues from Convolutional Neural Networks (CNNs) and Recurrent\nNeural Networks (RNNs), we use a softmax layer to categorize the input text as\ncover or stego. Experiments showed that our model can achieve the state-of-art\nresult in the text steganalysis task.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 10:24:30 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bao", "YongJian", ""], ["Yang", "Hao", ""], ["Yang", "Zhongliang", ""], ["Liu", "Sheng", ""], ["Huang", "Yongfeng", ""]]}, {"id": "1912.13156", "submitter": "Dingju Zhu", "authors": "Dingju Zhu", "title": "Hiding Information in Big Data based on Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current approach of information hiding based on deep learning model can\nnot directly use the original data as carriers, which means the approach can\nnot make use of the existing data in big data to hiding information. We\nproposed a novel method of information hiding in big data based on deep\nlearning. Our method uses the existing data in big data as carriers and uses\ndeep learning models to hide and extract secret messages in big data. The data\namount of big data is unlimited and thus the data amount of secret messages\nhided in big data can also be unlimited. Before opponents want to extract\nsecret messages from carriers, they need to find the carriers, however finding\nout the carriers from big data is just like finding out a box from the sea.\nDeep learning models are well known as deep black boxes in which the process\nfrom the input to the output is very complex, and thus the deep learning model\nfor information hiding is almost impossible for opponents to reconstruct. The\nresults also show that our method can hide secret messages safely,\nconveniently, quickly and with no limitation on the data amount.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 03:23:54 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 10:32:24 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Zhu", "Dingju", ""]]}]