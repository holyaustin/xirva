[{"id": "1210.0297", "submitter": "Md Sahidullah", "authors": "Md. Sahidullah and Goutam Saha", "title": "Comparison of Speech Activity Detection Techniques for Speaker\n  Recognition", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech activity detection (SAD) is an essential component for a variety of\nspeech processing applications. It has been observed that performances of\nvarious speech based tasks are very much dependent on the efficiency of the\nSAD. In this paper, we have systematically reviewed some popular SAD techniques\nand their applications in speaker recognition. Speaker verification system\nusing different SAD technique are experimentally evaluated on NIST speech\ncorpora using Gaussian mixture model- universal background model (GMM-UBM)\nbased classifier for clean and noisy conditions. It has been found that two\nGaussian modeling based SAD is comparatively better than other SAD techniques\nfor different types of noises.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 07:10:12 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2012 03:52:57 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Sahidullah", "Md.", ""], ["Saha", "Goutam", ""]]}, {"id": "1210.0623", "submitter": "Lexing Xie", "authors": "Lexing Xie, Apostol Natsev, Xuming He, John Kender, Matthew Hill, John\n  R Smith", "title": "Tracking Large-Scale Video Remix in Real-World Events", "comments": "11 pages, accepted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social information networks, such as YouTube, contains traces of both\nexplicit online interaction (such as \"like\", leaving a comment, or subscribing\nto video feed), and latent interactions (such as quoting, or remixing parts of\na video). We propose visual memes, or frequently re-posted short video\nsegments, for tracking such latent video interactions at scale. Visual memes\nare extracted by scalable detection algorithms that we develop, with high\naccuracy. We further augment visual memes with text, via a statistical model of\nlatent topics. We model content interactions on YouTube with visual memes,\ndefining several measures of influence and building predictive models for meme\npopularity. Experiments are carried out on with over 2 million video shots from\nmore than 40,000 videos on two prominent news events in 2009: the election in\nIran and the swine flu epidemic. In these two events, a high percentage of\nvideos contain remixed content, and it is apparent that traditional news media\nand citizen journalists have different roles in disseminating remixed content.\nWe perform two quantitative evaluations for annotating visual memes and\npredicting their popularity. The joint statistical model of visual memes and\nwords outperform a concurrence model, and the average error is ~2% for\npredicting meme volume and ~17% for their lifespan.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 00:43:36 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2013 06:26:27 GMT"}, {"version": "v3", "created": "Sun, 12 May 2013 23:29:25 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Xie", "Lexing", ""], ["Natsev", "Apostol", ""], ["He", "Xuming", ""], ["Kender", "John", ""], ["Hill", "Matthew", ""], ["Smith", "John R", ""]]}, {"id": "1210.1192", "submitter": "Sukhpal  Singh", "authors": "Sukhpal Singh", "title": "Reduction of Blocking Artifacts In JPEG Compressed Image", "comments": "10 Pages including 6 figures, Presented in the TECHNOVISION-10 in\n  G.N.D.E.C., Ludhiana, Punjab, India", "journal-ref": "Conference Proceedings of the National Level Technical Symposium\n  on Emerging Trends in Technology, 9th-10th April, 2010, 234-243", "doi": null, "report-no": "idcs-110 10", "categories": "cs.GR cs.MM", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In JPEG (DCT based) compresses image data by representing the original image\nwith a small number of transform coefficients. It exploits the fact that for\ntypical images a large amount of signal energy is concentrated in a small\nnumber of coefficients. The goal of DCT transform coding is to minimize the\nnumber of retained transform coefficients while keeping distortion at an\nacceptable level.In JPEG, it is done in 8X8 non overlapping blocks. It divides\nan image into blocks of equal size and processes each block independently.\nBlock processing allows the coder to adapt to the local image statistics,\nexploit the correlation present among neighboring image pixels, and to reduce\ncomputational and storage requirements. One of the most degradation of the\nblock transform coding is the blocking artifact. These artifacts appear as a\nregular pattern of visible block boundaries. This degradation is a direct\nresult of the coarse quantization of the coefficients and the independent\nprocessing of the blocks which does not take into account the existing\ncorrelations among adjacent block pixels. In this paper attempt is being made\nto reduce the blocking artifact introduced by the Block DCT Transform in JPEG.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 18:53:36 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 19:32:08 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 03:49:50 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Singh", "Sukhpal", ""]]}, {"id": "1210.2388", "submitter": "Yadong Mu", "authors": "Yadong Mu and Wei Liu and Shuicheng Yan", "title": "Video De-fencing", "comments": "To appear in IEEE transactions on Circuits and Systems for Video\n  Technology (T-CSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and provides an initial solution to a novel video\nediting task, i.e., video de-fencing. It targets automatic restoration of the\nvideo clips that are corrupted by fence-like occlusions during capture. Our key\nobservation lies in the visual parallax between fences and background scenes,\nwhich is caused by the fact that the former are typically closer to the camera.\nUnlike in traditional image inpainting, fence-occluded pixels in the videos\ntend to appear later in the temporal dimension and are therefore recoverable\nvia optimized pixel selection from relevant frames. To eventually produce\nfence-free videos, major challenges include cross-frame sub-pixel image\nalignment under diverse scene depth, and \"correct\" pixel selection that is\nrobust to dominating fence pixels. Several novel tools are developed in this\npaper, including soft fence detection, weighted truncated optical flow method\nand robust temporal median filter. The proposed algorithm is validated on\nseveral real-world video clips with fences.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 19:58:59 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Mu", "Yadong", ""], ["Liu", "Wei", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1210.2815", "submitter": "Suayb Arslan", "authors": "Suayb S. Arslan", "title": "Minimum Distortion Variance Concatenated Block Codes for Embedded Source\n  Transmission", "comments": "6 pages, 4 figures, In Proc. of International Conference on\n  Computing, Networking and Communications, ICNC 2014, Hawaii, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some state-of-art multimedia source encoders produce embedded source bit\nstreams that upon the reliable reception of only a fraction of the total bit\nstream, the decoder is able reconstruct the source up to a basic quality.\nReliable reception of later source bits gradually improve the reconstruction\nquality. Examples include scalable extensions of H.264/AVC and progressive\nimage coders such as JPEG2000. To provide an efficient protection for embedded\nsource bit streams, a concatenated block coding scheme using a minimum mean\ndistortion criterion was considered in the past. Although, the original design\nwas shown to achieve better mean distortion characteristics than previous\nstudies, the proposed coding structure was leading to dramatic quality\nfluctuations. In this paper, a modification of the original design is first\npresented and then the second order statistics of the distortion is taken into\naccount in the optimization. More specifically, an extension scheme is proposed\nusing a minimum distortion variance optimization criterion. This robust system\ndesign is tested for an image transmission scenario. Numerical results show\nthat the proposed extension achieves significantly lower variance than the\noriginal design, while showing similar mean distortion performance using both\nconvolutional codes and low density parity check codes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 06:54:32 GMT"}, {"version": "v2", "created": "Sat, 1 Mar 2014 08:47:17 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Arslan", "Suayb S.", ""]]}, {"id": "1210.4481", "submitter": "Yingzhen Yang", "authors": "Yingzhen Yang, Xinqi Chu, Tian-Tsong Ng, Alex Yong-Sang Chia,\n  Shuicheng Yan, Thomas S. Huang", "title": "Epitome for Automatic Image Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image colorization adds color to grayscale images. It not only increases the\nvisual appeal of grayscale images, but also enriches the information contained\nin scientific images that lack color information. Most existing methods of\ncolorization require laborious user interaction for scribbles or image\nsegmentation. To eliminate the need for human labor, we develop an automatic\nimage colorization method using epitome. Built upon a generative graphical\nmodel, epitome is a condensed image appearance and shape model which also\nproves to be an effective summary of color information for the colorization\ntask. We train the epitome from the reference images and perform inference in\nthe epitome to colorize grayscale images, rendering better colorization results\nthan previous method in our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 06:35:04 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Yang", "Yingzhen", ""], ["Chu", "Xinqi", ""], ["Ng", "Tian-Tsong", ""], ["Chia", "Alex Yong-Sang", ""], ["Yan", "Shuicheng", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1210.5041", "submitter": "Thomas Maugey", "authors": "Thomas Maugey, Ismael Daribo, Gene Cheung, Pascal Frossard", "title": "Navigation domain representation for interactive multiview imaging", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2013.2270183", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling users to interactively navigate through different viewpoints of a\nstatic scene is a new interesting functionality in 3D streaming systems. While\nit opens exciting perspectives towards rich multimedia applications, it\nrequires the design of novel representations and coding techniques in order to\nsolve the new challenges imposed by interactive navigation. Interactivity\nclearly brings new design constraints: the encoder is unaware of the exact\ndecoding process, while the decoder has to reconstruct information from\nincomplete subsets of data since the server can generally not transmit images\nfor all possible viewpoints due to resource constrains. In this paper, we\npropose a novel multiview data representation that permits to satisfy bandwidth\nand storage constraints in an interactive multiview streaming system. In\nparticular, we partition the multiview navigation domain into segments, each of\nwhich is described by a reference image and some auxiliary information. The\nauxiliary information enables the client to recreate any viewpoint in the\nnavigation segment via view synthesis. The decoder is then able to navigate\nfreely in the segment without further data request to the server; it requests\nadditional data only when it moves to a different segment. We discuss the\nbenefits of this novel representation in interactive navigation systems and\nfurther propose a method to optimize the partitioning of the navigation domain\ninto independent segments, under bandwidth and storage constraints.\nExperimental results confirm the potential of the proposed representation;\nnamely, our system leads to similar compression performance as classical\ninter-view coding, while it provides the high level of flexibility that is\nrequired for interactive streaming. Hence, our new framework represents a\npromising solution for 3D data representation in novel interactive multimedia\nservices.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 07:41:17 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2013 09:32:50 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Maugey", "Thomas", ""], ["Daribo", "Ismael", ""], ["Cheung", "Gene", ""], ["Frossard", "Pascal", ""]]}, {"id": "1210.5888", "submitter": "Wojciech Mazurczyk", "authors": "Artur Janicki, Wojciech Mazurczyk, Krzysztof Szczypiorski", "title": "Steganalysis of Transcoding Steganography", "comments": "13 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TranSteg (Trancoding Steganography) is a fairly new IP telephony\nsteganographic method that functions by compressing overt (voice) data to make\nspace for the steganogram by means of transcoding. It offers high\nsteganographic bandwidth, retains good voice quality and is generally harder to\ndetect than other existing VoIP steganographic methods. In TranSteg, after the\nsteganogram reaches the receiver, the hidden information is extracted and the\nspeech data is practically restored to what was originally sent. This is a huge\nadvantage compared with other existing VoIP steganographic methods, where the\nhidden data can be extracted and removed but the original data cannot be\nrestored because it was previously erased due to a hidden data insertion\nprocess. In this paper we address the issue of steganalysis of TranSteg.\nVarious TranSteg scenarios and possibilities of warden(s) localization are\nanalyzed with regards to the TranSteg detection. A steganalysis method based on\nMFCC (Mel-Frequency Cepstral Coefficients) parameters and GMMs (Gaussian\nMixture Models) was developed and tested for various overt/covert codec pairs\nin a single warden scenario with double transcoding. The proposed method\nallowed for efficient detection of some codec pairs (e.g., G.711/G.729), whilst\nsome others remained more resistant to detection (e.g., iLBC/AMR).\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 13:18:19 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Janicki", "Artur", ""], ["Mazurczyk", "Wojciech", ""], ["Szczypiorski", "Krzysztof", ""]]}, {"id": "1210.5941", "submitter": "Pitchammal G", "authors": "J. Veerappan (Department of Electronics and Communication Engineering,\n  Sethu Institute of Technology, Viruthunagar, Tamil Nadu, India), G.\n  Pitchammal (Anna university, Chennai, Tamil nadu, India)", "title": "Multilayer image watermarking scheme for providing high security", "comments": "6 pages, 6 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The main theme of this application is to provide an algorithm color image\nwatermark to manage the attacks such as rotation, scaling and translation. In\nthe existing watermarking algorithms, those exploited robust features are more\nor less related to the pixel position, so they cannot be more robust against\nthe attacks. In order to solve this problem this application focus on certain\nparameters rather than the pixel position for watermarking. Two statistical\nfeatures such as the histogram shape and the mean of Gaussian filtered\nlow-frequency component of images are taken for this proposed application to\nmake the watermarking algorithm robust to attacks and also AES technique is\nused to provide higher security.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 15:55:03 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Veerappan", "J.", "", "Department of Electronics and Communication Engineering,\n  Sethu Institute of Technology, Viruthunagar, Tamil Nadu, India"], ["Pitchammal", "G.", "", "Anna university, Chennai, Tamil nadu, India"]]}, {"id": "1210.7506", "submitter": "Cong Ling", "authors": "Kezhi Li, Lu Gan, Cong Ling", "title": "Convolutional Compressed Sensing Using Deterministic Sequences", "comments": "A major overhaul of the withdrawn paper Orthogonal symmetric Toeplitz\n  matrices for compressed sensing: Statistical isometry property", "journal-ref": null, "doi": "10.1109/TSP.2012.2229994", "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new class of circulant matrices built from deterministic\nsequences is proposed for convolution-based compressed sensing (CS). In\ncontrast to random convolution, the coefficients of the underlying filter are\ngiven by the discrete Fourier transform of a deterministic sequence with good\nautocorrelation. Both uniform recovery and non-uniform recovery of sparse\nsignals are investigated, based on the coherence parameter of the proposed\nsensing matrices. Many examples of the sequences are investigated, particularly\nthe Frank-Zadoff-Chu (FZC) sequence, the \\textit{m}-sequence and the Golay\nsequence. A salient feature of the proposed sensing matrices is that they can\nnot only handle sparse signals in the time domain, but also those in the\nfrequency and/or or discrete-cosine transform (DCT) domain.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 20:34:06 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Li", "Kezhi", ""], ["Gan", "Lu", ""], ["Ling", "Cong", ""]]}, {"id": "1210.8025", "submitter": "Ka Chun Lam", "authors": "Lok Ming Lui, Ka Chun Lam, Tsz Wai Wong, XianFeng Gu", "title": "Beltrami Representation and its applications to texture map and video\n  compression", "comments": "30 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR math.DG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Surface parameterizations and registrations are important in computer\ngraphics and imaging, where 1-1 correspondences between meshes are computed. In\npractice, surface maps are usually represented and stored as 3D coordinates\neach vertex is mapped to, which often requires lots of storage memory. This\ncauses inconvenience in data transmission and data storage. To tackle this\nproblem, we propose an effective algorithm for compressing surface\nhomeomorphisms using Fourier approximation of the Beltrami representation. The\nBeltrami representation is a complex-valued function defined on triangular\nfaces of the surface mesh with supreme norm strictly less than 1. Under\nsuitable normalization, there is a 1-1 correspondence between the set of\nsurface homeomorphisms and the set of Beltrami representations. Hence, every\nbijective surface map is associated with a unique Beltrami representation.\nConversely, given a Beltrami representation, the corresponding bijective\nsurface map can be exactly reconstructed using the Linear Beltrami Solver\nintroduced in this paper. Using the Beltrami representation, the surface\nhomeomorphism can be easily compressed by Fourier approximation, without\ndistorting the bijectivity of the map. The storage memory can be effectively\nreduced, which is useful for many practical problems in computer graphics and\nimaging. In this paper, we proposed to apply the algorithm to texture map\ncompression and video compression. With our proposed algorithm, the storage\nrequirement for the texture properties of a textured surface can be\nsignificantly reduced. Our algorithm can further be applied to compressing\nmotion vector fields for video compression, which effectively improve the\ncompression ratio.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 12:17:12 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Lui", "Lok Ming", ""], ["Lam", "Ka Chun", ""], ["Wong", "Tsz Wai", ""], ["Gu", "XianFeng", ""]]}, {"id": "1210.8165", "submitter": "Madhur Srivastava", "authors": "Madhur Srivastava and Prasanta K. Panigrahi", "title": "Non-uniform Quantization of Detail Components in Wavelet Transformed\n  Image for Lossy JPEG2000 Compression", "comments": "5 pages, 3 figures, conference", "journal-ref": "International Conference on Pattern Recognition Applications and\n  Methods (ICPRAM 2013)", "doi": "10.5220/0004333706040607", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces the idea of non-uniform quantization in the detail\ncomponents of wavelet transformed image. It argues that most of the\ncoefficients of horizontal, vertical and diagonal components lie near to zeros\nand the coefficients representing large differences are few at the extreme ends\nof histogram. Therefore, this paper advocates need for variable step size\nquantization scheme which preserves the edge information at the edge of\nhistogram and removes redundancy with the minimal number of quantized values.\nTo support the idea, preliminary results are provided using a non-uniform\nquantization algorithm. We believe that successful implementation of\nnon-uniform quantization in detail components in JPEG-2000 still image standard\nwill improve image quality and compression efficiency with lesser number of\nquantized values.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 21:05:16 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Srivastava", "Madhur", ""], ["Panigrahi", "Prasanta K.", ""]]}, {"id": "1210.8318", "submitter": "H.R.  Chennamma", "authors": "H. R. Chennamma and Lalitha Rangarajan", "title": "Mugshot Identification from Manipulated Facial Images", "comments": "8 pages, 5 figures, 1 table, journal. arXiv admin note: substantial\n  text overlap with arXiv:1106.4907", "journal-ref": "International Journal of Machine Intelligence, Volume 4, Issue 1,\n  pp. 407, 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing on digital images is ubiquitous. Identification of deliberately\nmodified facial images is a new challenge for face identification system. In\nthis paper, we address the problem of identification of a face or person from\nheavily altered facial images. In this face identification problem, the input\nto the system is a manipulated or transformed face image and the system reports\nback the determined identity from a database of known individuals. Such a\nsystem can be useful in mugshot identification in which mugshot database\ncontains two views (frontal and profile) of each criminal. We considered only\nfrontal view from the available database for face identification and the query\nimage is a manipulated face generated by face transformation software tool\navailable online. We propose SIFT features for efficient face identification in\nthis scenario. Further comparative analysis has been given with well known\neigenface approach. Experiments have been conducted with real case images to\nevaluate the performance of both methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 12:55:57 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Chennamma", "H. R.", ""], ["Rangarajan", "Lalitha", ""]]}]