[{"id": "1706.00079", "submitter": "Sourish Chaudhuri", "authors": "Ken Hoover, Sourish Chaudhuri, Caroline Pantofaru, Malcolm Slaney, Ian\n  Sturdy", "title": "Putting a Face to the Voice: Fusing Audio and Visual Signals Across a\n  Video to Determine Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a system that associates faces with voices in a\nvideo by fusing information from the audio and visual signals. The thesis\nunderlying our work is that an extremely simple approach to generating (weak)\nspeech clusters can be combined with visual signals to effectively associate\nfaces and voices by aggregating statistics across a video. This approach does\nnot need any training data specific to this task and leverages the natural\ncoherence of information in the audio and visual streams. It is particularly\napplicable to tracking speakers in videos on the web where a priori information\nabout the environment (e.g., number of speakers, spatial signals for\nbeamforming) is not available. We performed experiments on a real-world dataset\nusing this analysis framework to determine the speaker in a video. Given a\nground truth labeling determined by human rater consensus, our approach had\n~71% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:35:26 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Hoover", "Ken", ""], ["Chaudhuri", "Sourish", ""], ["Pantofaru", "Caroline", ""], ["Slaney", "Malcolm", ""], ["Sturdy", "Ian", ""]]}, {"id": "1706.00153", "submitter": "Yuxin Peng", "authors": "Xin Huang, Yuxin Peng, and Mingkuan Yuan", "title": "Cross-modal Common Representation Learning by Hybrid Transfer Network", "comments": "To appear in the proceedings of 26th International Joint Conference\n  on Artificial Intelligence (IJCAI), Melbourne, Australia, Aug. 19-25, 2017. 8\n  pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN-based cross-modal retrieval is a research hotspot to retrieve across\ndifferent modalities as image and text, but existing methods often face the\nchallenge of insufficient cross-modal training data. In single-modal scenario,\nsimilar problem is usually relieved by transferring knowledge from large-scale\nauxiliary datasets (as ImageNet). Knowledge from such single-modal datasets is\nalso very useful for cross-modal retrieval, which can provide rich general\nsemantic information that can be shared across different modalities. However,\nit is challenging to transfer useful knowledge from single-modal (as image)\nsource domain to cross-modal (as image/text) target domain. Knowledge in source\ndomain cannot be directly transferred to both two different modalities in\ntarget domain, and the inherent cross-modal correlation contained in target\ndomain provides key hints for cross-modal retrieval which should be preserved\nduring transfer process. This paper proposes Cross-modal Hybrid Transfer\nNetwork (CHTN) with two subnetworks: Modal-sharing transfer subnetwork utilizes\nthe modality in both source and target domains as a bridge, for transferring\nknowledge to both two modalities simultaneously; Layer-sharing correlation\nsubnetwork preserves the inherent cross-modal semantic correlation to further\nadapt to cross-modal retrieval task. Cross-modal data can be converted to\ncommon representation by CHTN for retrieval, and comprehensive experiment on 3\ndatasets shows its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 02:53:57 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 14:08:19 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""], ["Yuan", "Mingkuan", ""]]}, {"id": "1706.00291", "submitter": "Manish Narwaria Dr", "authors": "Manish Narwaria, Lukas Krasula, and Patrick Le Callet", "title": "Data Analysis in Multimedia Quality Assessment: Revisiting the\n  Statistical Tests", "comments": null, "journal-ref": "IEEE Transactions on Multimedia 2018", "doi": "10.1109/TMM.2018.2794266", "report-no": null, "categories": "cs.MM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessment of multimedia quality relies heavily on subjective assessment, and\nis typically done by human subjects in the form of preferences or continuous\nratings. Such data is crucial for analysis of different multimedia processing\nalgorithms as well as validation of objective (computational) methods for the\nsaid purpose. To that end, statistical testing provides a theoretical framework\ntowards drawing meaningful inferences, and making well grounded conclusions and\nrecommendations. While parametric tests (such as t test, ANOVA, and error\nestimates like confidence intervals) are popular and widely used in the\ncommunity, there appears to be a certain degree of confusion in the application\nof such tests. Specifically, the assumption of normality and homogeneity of\nvariance is often not well understood. Therefore, the main goal of this paper\nis to revisit them from a theoretical perspective and in the process provide\nuseful insights into their practical implications. Experimental results on both\nsimulated and real data are presented to support the arguments made. A software\nimplementing the said recommendations is also made publicly available, in order\nto achieve the goal of reproducible research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 13:35:13 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Narwaria", "Manish", ""], ["Krasula", "Lukas", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1706.00447", "submitter": "Allan Pinto", "authors": "Allan Pinto, Daniel Moreira, Aparna Bharati, Joel Brogan, Kevin\n  Bowyer, Patrick Flynn, Walter Scheirer and Anderson Rocha", "title": "Provenance Filtering for Multimedia Phylogeny", "comments": "5 pages, Accepted in IEEE International Conference on Image\n  Processing (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Departing from traditional digital forensics modeling, which seeks to analyze\nsingle objects in isolation, multimedia phylogeny analyzes the evolutionary\nprocesses that influence digital objects and collections over time. One of its\nintegral pieces is provenance filtering, which consists of searching a\npotentially large pool of objects for the most related ones with respect to a\ngiven query, in terms of possible ancestors (donors or contributors) and\ndescendants. In this paper, we propose a two-tiered provenance filtering\napproach to find all the potential images that might have contributed to the\ncreation process of a given query $q$. In our solution, the first (coarse) tier\naims to find the most likely \"host\" images --- the major donor or background\n--- contributing to a composite/doctored image. The search is then refined in\nthe second tier, in which we search for more specific (potentially small) parts\nof the query that might have been extracted from other images and spliced into\nthe query image. Experimental results with a dataset containing more than a\nmillion images show that the two-tiered solution underpinned by the context of\nthe query is highly useful for solving this difficult task.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 18:12:57 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Pinto", "Allan", ""], ["Moreira", "Daniel", ""], ["Bharati", "Aparna", ""], ["Brogan", "Joel", ""], ["Bowyer", "Kevin", ""], ["Flynn", "Patrick", ""], ["Scheirer", "Walter", ""], ["Rocha", "Anderson", ""]]}, {"id": "1706.00757", "submitter": "Juan Colmenares", "authors": "Ying Lu and Juan A. Colmenares", "title": "Efficient Detection of Points of Interest from Georeferenced Visual\n  Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people take photos and videos with smartphones and more recently with\n360-degree cameras at popular places and events, and share them in social\nmedia. Such visual content is produced in large volumes in urban areas, and it\nis a source of information that online users could exploit to learn what has\ngot the interest of the general public on the streets of the cities where they\nlive or plan to visit. A key step to providing users with that information is\nto identify the most popular k spots in specified areas. In this paper, we\npropose a clustering and incremental sampling (C&IS) approach that trades off\naccuracy of top-k results for detection speed. It uses clustering to determine\nareas with high density of visual content, and incremental sampling, controlled\nby stopping criteria, to limit the amount of computational work. It leverages\nspatial metadata, which represent the scenes in the visual content, to rapidly\ndetect the hotspots, and uses a recently proposed Gaussian probability model to\ndescribe the capture intention distribution in the query area. We evaluate the\napproach with metadata, derived from a non-synthetic, user-generated dataset,\nfor regular mobile and 360-degree visual content. Our results show that the\nC&IS approach offers 2.8x-19x reductions in processing time over an optimized\nbaseline, while in most cases correctly identifying 4 out of 5 top locations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:03:06 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Lu", "Ying", ""], ["Colmenares", "Juan A.", ""]]}, {"id": "1706.01788", "submitter": "Lamberto Ballan", "authors": "Irene Amerini and Tiberio Uricchio and Lamberto Ballan and Roberto\n  Caldelli", "title": "Localization of JPEG double compression through multi-domain\n  convolutional neural networks", "comments": "Accepted to CVPRW 2017, Workshop on Media Forensics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an attacker wants to falsify an image, in most of cases she/he will\nperform a JPEG recompression. Different techniques have been developed based on\ndiverse theoretical assumptions but very effective solutions have not been\ndeveloped yet. Recently, machine learning based approaches have been started to\nappear in the field of image forensics to solve diverse tasks such as\nacquisition source identification and forgery detection. In this last case, the\naim ahead would be to get a trained neural network able, given a to-be-checked\nimage, to reliably localize the forged areas. With this in mind, our paper\nproposes a step forward in this direction by analyzing how a single or double\nJPEG compression can be revealed and localized using convolutional neural\nnetworks (CNNs). Different kinds of input to the CNN have been taken into\nconsideration, and various experiments have been carried out trying also to\nevidence potential issues to be further investigated.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 14:34:50 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Amerini", "Irene", ""], ["Uricchio", "Tiberio", ""], ["Ballan", "Lamberto", ""], ["Caldelli", "Roberto", ""]]}, {"id": "1706.02361", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi and George Fazekas and Kyunghyun Cho and Mark Sandler", "title": "The Effects of Noisy Labels on Deep Convolutional Neural Networks for\n  Music Tagging", "comments": "The section that overlapped with arXiv:1709.01922 is completely\n  removed since the earlier version. This is the camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNN) have been successfully applied to music\nclassification including music tagging. However, there are several open\nquestions regarding the training, evaluation, and analysis of DNNs. In this\narticle, we investigate specific aspects of neural networks, the effects of\nnoisy labels, to deepen our understanding of their properties. We analyse and\n(re-)validate a large music tagging dataset to investigate the reliability of\ntraining and evaluation. Using a trained network, we compute label vector\nsimilarities which is compared to groundtruth similarity.\n  The results highlight several important aspects of music tagging and neural\nnetworks. We show that networks can be effective despite relatively large error\nrates in groundtruth datasets, while conjecturing that label noise can be the\ncause of varying tag-wise performance differences. Lastly, the analysis of our\ntrained network provides valuable insight into the relationships between music\ntags. These results highlight the benefit of using data-driven methods to\naddress automatic music tagging.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:54:39 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 23:47:42 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 15:54:38 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1706.02901", "submitter": "Che-Wei Huang", "authors": "Che-Wei Huang, Shrikanth. S. Narayanan", "title": "Characterizing Types of Convolution in Deep Convolutional Recurrent\n  Neural Networks for Robust Speech Emotion Recognition", "comments": "Revised Submission to IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are being actively investigated in a wide\nrange of speech and audio processing applications including speech recognition,\naudio event detection and computational paralinguistics, owing to their ability\nto reduce factors of variations, for learning from speech. However, studies\nhave suggested to favor a certain type of convolutional operations when\nbuilding a deep convolutional neural network for speech applications although\nthere has been promising results using different types of convolutional\noperations. In this work, we study four types of convolutional operations on\ndifferent input features for speech emotion recognition under noisy and clean\nconditions in order to derive a comprehensive understanding. Since affective\nbehavioral information has been shown to reflect temporally varying of mental\nstate and convolutional operation are applied locally in time, all deep neural\nnetworks share a deep recurrent sub-network architecture for further temporal\nmodeling. We present detailed quantitative module-wise performance analysis to\ngain insights into information flows within the proposed architectures. In\nparticular, we demonstrate the interplay of affective information and the other\nirrelevant information during the progression from one module to another.\nFinally we show that all of our deep neural networks provide state-of-the-art\nperformance on the eNTERFACE'05 corpus.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 15:17:21 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 19:38:21 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Huang", "Che-Wei", ""], ["Narayanan", "Shrikanth. S.", ""]]}, {"id": "1706.02981", "submitter": "Husameldin Mukhtar", "authors": "Husameldin Mukhtar", "title": "Link Adaptation for Wireless Video Communication Systems", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM cs.NI cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This PhD thesis considers the performance evaluation and enhancement of video\ncommunication over wireless channels. The system model considers hybrid\nautomatic repeat request (HARQ) with Chase combining and turbo product codes\n(TPC). The thesis proposes algorithms and techniques to optimize the\nthroughput, transmission power and complexity of HARQ-based wireless video\ncommunication. A semi-analytical solution is developed to model the performance\nof delay-constrained HARQ systems. The semi-analytical and Monte Carlo\nsimulation results reveal that significant complexity reduction can be achieved\nby noting that the coding gain advantage of the soft over hard decoding is\nreduced when Chase combining is used, and it actually vanishes completely for\nparticular codes. Moreover, the thesis proposes a novel power optimization\nalgorithm that achieves a significant power saving of up to 80%. Joint\nthroughput maximization and complexity reduction is considered as well. A CRC\n(cyclic redundancy check)-free HARQ is proposed to improve the system\nthroughput when short packets are transmitted. In addition, the computational\ncomplexity/delay is reduced when the packets transmitted are long. Finally, a\ncontent-aware and occupancy-based HARQ scheme is proposed to ensure minimum\nvideo quality distortion with continuous playback.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 07:58:37 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Mukhtar", "Husameldin", ""]]}, {"id": "1706.03249", "submitter": "Rishabh Mehrotra", "authors": "Rishabh Mehrotra and Prasanta Bhattacharya", "title": "Characterizing and Predicting Supply-side Engagement on\n  Crowd-contributed Video Sharing Platforms", "comments": "8 pages, ICTIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video sharing and entertainment websites have rapidly grown in popularity and\nnow constitute some of the most visited websites on the Internet. Despite the\nactive user engagement on these online video-sharing platforms, most of recent\nresearch on online media platforms have restricted themselves to networking\nbased social media sites, like Facebook or Twitter. We depart from previous\nstudies in the online media space that have focused exclusively on demand-side\nuser engagement, by modeling the supply-side of the crowd-contributed videos on\nthis platform. The current study is among the first to perform a large-scale\nempirical study using longitudinal video upload data from a large online video\nplatform. The modeling and subsequent prediction of video uploads is made\ncomplicated by the heterogeneity of video types (e.g. popular vs. niche video\ngenres), and the inherent time trend effects associated with media uploads. We\nidentify distinct genre-clusters from our dataset and employ a self-exciting\nHawkes point-process model on each of these clusters to fully specify and\nestimate the video upload process. Additionally, we go beyond prediction to\ndisentangle potential factors that govern user engagement and determine the\nvideo upload rates, which improves our analysis with additional explanatory\npower. Our findings show that using a relatively parsimonious point-process\nmodel, we are able to achieve higher model fit, and predict video uploads to\nthe platform with a higher accuracy than competing models. The findings from\nthis study can benefit platform owners in better understanding how their\nsupply-side users engage with their site over time. We also offer a robust\nmethod for performing media upload prediction that is likely to be\ngeneralizable across media platforms which demonstrate similar temporal and\ngenre-level heterogeneity.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 16:26:48 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Mehrotra", "Rishabh", ""], ["Bhattacharya", "Prasanta", ""]]}, {"id": "1706.04508", "submitter": "Zuxuan Wu", "authors": "Yu-Gang Jiang, Zuxuan Wu, Jinhui Tang, Zechao Li, Xiangyang Xue,\n  Shih-Fu Chang", "title": "Modeling Multimodal Clues in a Hybrid Deep Learning Framework for Video\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos are inherently multimodal. This paper studies the problem of how to\nfully exploit the abundant multimodal clues for improved video categorization.\nWe introduce a hybrid deep learning framework that integrates useful clues from\nmultiple modalities, including static spatial appearance information, motion\npatterns within a short time window, audio information as well as long-range\ntemporal dynamics. More specifically, we utilize three Convolutional Neural\nNetworks (CNNs) operating on appearance, motion and audio signals to extract\ntheir corresponding features. We then employ a feature fusion network to derive\na unified representation with an aim to capture the relationships among\nfeatures. Furthermore, to exploit the long-range temporal dynamics in videos,\nwe apply two Long Short Term Memory networks with extracted appearance and\nmotion features as inputs. Finally, we also propose to refine the prediction\nscores by leveraging contextual relationships among video semantics. The hybrid\ndeep learning framework is able to exploit a comprehensive set of multimodal\nfeatures for video classification. Through an extensive set of experiments, we\ndemonstrate that (1) LSTM networks which model sequences in an explicitly\nrecurrent manner are highly complementary with CNN models; (2) the feature\nfusion network which produces a fused representation through modeling feature\nrelationships outperforms alternative fusion strategies; (3) the semantic\ncontext of video classes can help further refine the predictions for improved\nperformance. Experimental results on two challenging benchmarks, the UCF-101\nand the Columbia Consumer Videos (CCV), provide strong quantitative evidence\nthat our framework achieves promising results: $93.1\\%$ on the UCF-101 and\n$84.5\\%$ on the CCV, outperforming competing methods with clear margins.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 14:23:08 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Jiang", "Yu-Gang", ""], ["Wu", "Zuxuan", ""], ["Tang", "Jinhui", ""], ["Li", "Zechao", ""], ["Xue", "Xiangyang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1706.04804", "submitter": "Gazi Illahi", "authors": "Gazi Illahi, Matti Siekkinen, and Enrico Masala", "title": "Foveated Video Streaming for Cloud Gaming", "comments": "Submitted to: IEEE 19th International Workshop on Multimedia Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good user experience with interactive cloud-based multimedia applications,\nsuch as cloud gaming and cloud-based VR, requires low end-to-end latency and\nlarge amounts of downstream network bandwidth at the same time. In this paper,\nwe present a foveated video streaming system for cloud gaming. The system\nadapts video stream quality by adjusting the encoding parameters on the fly to\nmatch the player's gaze position. We conduct measurements with a prototype that\nwe developed for a cloud gaming system in conjunction with eye tracker\nhardware. Evaluation results suggest that such foveated streaming can reduce\nbandwidth requirements by even more than 50% depending on parametrization of\nthe foveated video coding and that it is feasible from the latency perspective.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 10:14:25 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Illahi", "Gazi", ""], ["Siekkinen", "Matti", ""], ["Masala", "Enrico", ""]]}, {"id": "1706.05143", "submitter": "Terrence Adams", "authors": "Terrence Adams", "title": "AI-Powered Social Bots", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives an overview of impersonation bots that generate output in\none, or possibly, multiple modalities. We also discuss rapidly advancing areas\nof machine learning and artificial intelligence that could lead to\nfrighteningly powerful new multi-modal social bots. Our main conclusion is that\nmost commonly known bots are one dimensional (i.e., chatterbot), and far from\ndeceiving serious interrogators. However, using recent advances in machine\nlearning, it is possible to unleash incredibly powerful, human-like armies of\nsocial bots, in potentially well coordinated campaigns of deception and\ninfluence.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 04:54:47 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Adams", "Terrence", ""]]}, {"id": "1706.05781", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Deokjin Joo, Juho Kim", "title": "Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of\n  Deep Neural Network Models with Keras", "comments": "ICML 2017 machine learning for music discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Kapre, Keras layers for audio and music signal preprocessing.\nMusic research using deep neural networks requires a heavy and tedious\npreprocessing stage, for which audio processing parameters are often ignored in\nparameter optimisation. To solve this problem, Kapre implements time-frequency\nconversions, normalisation, and data augmentation as Keras layers. We report\nsimple benchmark results, showing real-time on-GPU preprocessing adds a\nreasonable amount of computation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 04:42:14 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Choi", "Keunwoo", ""], ["Joo", "Deokjin", ""], ["Kim", "Juho", ""]]}, {"id": "1706.06064", "submitter": "Wengang Zhou", "authors": "Wengang Zhou, Houqiang Li, and Qi Tian", "title": "Recent Advance in Content-based Image Retrieval: A Literature Survey", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive increase and ubiquitous accessibility of visual data on the Web\nhave led to the prosperity of research activity in image search or retrieval.\nWith the ignorance of visual content as a ranking clue, methods with text\nsearch techniques for visual retrieval may suffer inconsistency between the\ntext words and visual content. Content-based image retrieval (CBIR), which\nmakes use of the representation of visual content to identify relevant images,\nhas attracted sustained attention in recent two decades. Such a problem is\nchallenging due to the intention gap and the semantic gap problems. Numerous\ntechniques have been developed for content-based image retrieval in the last\ndecade. The purpose of this paper is to categorize and evaluate those\nalgorithms proposed during the period of 2003 to 2016. We conclude with several\npromising directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:14:48 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 08:20:19 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Tian", "Qi", ""]]}, {"id": "1706.06651", "submitter": "Nitin Khanna Dr.", "authors": "Hardik Jain, Gaurav Gupta, Sharad Joshi, Nitin Khanna", "title": "Passive Classification of Source Printer using Text-line-level Geometric\n  Distortion Signatures from Scanned Images of Printed Documents", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this digital era, one thing that still holds the convention is a printed\narchive. Printed documents find their use in many critical domains such as\ncontract papers, legal tenders and proof of identity documents. As more\nadvanced printing, scanning and image editing techniques are becoming\navailable, forgeries on these legal tenders pose a serious threat. Ability to\neasily and reliably identify source printer of a printed document can help a\nlot in reducing this menace. During printing procedure, printer hardware\nintroduces certain distortions in printed characters' locations and shapes\nwhich are invisible to naked eyes. These distortions are referred as geometric\ndistortions, their profile (or signature) is generally unique for each printer\nand can be used for printer classification purpose. This paper proposes a set\nof features for characterizing text-line-level geometric distortions, referred\nas geometric distortion signatures and presents a novel system to use them for\nidentification of the origin of a printed document. Detailed experiments\nperformed on a set of thirteen printers demonstrate that the proposed system\nachieves state of the art performance and gives much higher accuracy under\nsmall training size constraint. For four training and six test pages of three\ndifferent fonts, the proposed method gives 99\\% classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 20:11:45 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Jain", "Hardik", ""], ["Gupta", "Gaurav", ""], ["Joshi", "Sharad", ""], ["Khanna", "Nitin", ""]]}, {"id": "1706.06810", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Juhan Nam", "title": "Multi-Level and Multi-Scale Feature Aggregation Using Sample-level Deep\n  Convolutional Neural Networks for Music Classification", "comments": "ICML Music Discovery Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music tag words that describe music audio by text have different levels of\nabstraction. Taking this issue into account, we propose a music classification\napproach that aggregates multi-level and multi-scale features using pre-trained\nfeature extractors. In particular, the feature extractors are trained in\nsample-level deep convolutional neural networks using raw waveforms. We show\nthat this approach achieves state-of-the-art results on several music\nclassification datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 09:57:24 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lee", "Jongpil", ""], ["Nam", "Juhan", ""]]}, {"id": "1706.07422", "submitter": "Nitin Khanna Dr.", "authors": "Sharad Joshi, Nitin Khanna", "title": "Single Classifier-based Passive System for Source Printer Classification\n  using Local Texture Features", "comments": "11 pages", "journal-ref": null, "doi": "10.1109/TIFS.2017.2779441", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of examining printed documents for potential forgeries\nand copyright infringement is the identification of source printer as it can be\nhelpful for ascertaining the leak and detecting forged documents. This paper\nproposes a system for classification of source printer from scanned images of\nprinted documents using all the printed letters simultaneously. This system\nuses local texture patterns based features and a single classifier for\nclassifying all the printed letters. Letters are extracted from scanned images\nusing connected component analysis followed by morphological filtering without\nthe need of using an OCR. Each letter is sub-divided into a flat region and an\nedge region, and local tetra patterns are estimated separately for these two\nregions. A strategically constructed pooling technique is used to extract the\nfinal feature vectors. The proposed method has been tested on both a publicly\navailable dataset of 10 printers and a new dataset of 18 printers scanned at a\nresolution of 600 dpi as well as 300 dpi printed in four different fonts. The\nresults indicate shape independence property in the proposed method as using a\nsingle classifier it outperforms existing handcrafted feature-based methods and\nneeds much smaller number of training pages by using all the printed letters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:53:57 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Joshi", "Sharad", ""], ["Khanna", "Nitin", ""]]}, {"id": "1706.07576", "submitter": "Chao Xia", "authors": "Xia Chao, Guan Qingxiao, Zhao Xianfeng", "title": "Further Study on GFR Features for JPEG Steganalysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GFR (Gabor Filter Residual) features, built as histograms of quantized\nresiduals obtained with 2D Gabor filters, can achieve competitive detection\nperformance against adaptive JPEG steganography. In this paper, an improved\nversion of the GFR is proposed. First, a novel histogram merging method is\nproposed according to the symmetries between different Gabor filters, thus\nmaking the features more compact and robust. Second, a new weighted histogram\nmethod is proposed by considering the position of the residual value in a\nquantization interval, making the features more sensitive to the slight changes\nin residual values. The experiments are given to demonstrate the effectiveness\nof our proposed methods. Finally, we design a CNN to duplicate the detector\nwith the improved GFR features and the ensemble classifier, thus optimizing the\ndesign of the filters used to form residuals in JPEG-phase-aware features.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 06:44:31 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Chao", "Xia", ""], ["Qingxiao", "Guan", ""], ["Xianfeng", "Zhao", ""]]}, {"id": "1706.07613", "submitter": "Yann Bayle", "authors": "Yann Bayle and Matthias Robine and Pierre Hanna", "title": "Toward Faultless Content-Based Playlists Generation for Instrumentals", "comments": "single-column 20 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study deals with content-based musical playlists generation focused on\nSongs and Instrumentals. Automatic playlist generation relies on collaborative\nfiltering and autotagging algorithms. Autotagging can solve the cold start\nissue and popularity bias that are critical in music recommender systems.\nHowever, autotagging remains to be improved and cannot generate satisfying\nmusic playlists. In this paper, we suggest improvements toward better\nautotagging-generated playlists compared to state-of-the-art. To assess our\nmethod, we focus on the Song and Instrumental tags. Song and Instrumental are\ntwo objective and opposite tags that are under-studied compared to genres or\nmoods, which are subjective and multi-modal tags. In this paper, we consider an\nindustrial real-world musical database that is unevenly distributed between\nSongs and Instrumentals and bigger than databases used in previous studies. We\nset up three incremental experiments to enhance automatic playlist generation.\nOur suggested approach generates an Instrumental playlist with up to three\ntimes less false positives than cutting edge methods. Moreover, we provide a\ndesign of experiment framework to foster research on Songs and Instrumentals.\nWe give insight on how to improve further the quality of generated playlists\nand to extend our methods to other musical tags. Furthermore, we provide the\nsource code to guarantee reproducible research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 09:27:35 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:05:00 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Bayle", "Yann", ""], ["Robine", "Matthias", ""], ["Hanna", "Pierre", ""]]}, {"id": "1706.07842", "submitter": "Yaqi Liu", "authors": "Yaqi Liu, Qingxiao Guan, Xianfeng Zhao, and Yun Cao", "title": "Image Forgery Localization Based on Multi-Scale Convolutional Neural\n  Networks", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TGRS.2018.2848473", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to utilize Convolutional Neural Networks (CNNs) and\nthe segmentation-based multi-scale analysis to locate tampered areas in digital\nimages. First, to deal with color input sliding windows of different scales, a\nunified CNN architecture is designed. Then, we elaborately design the training\nprocedures of CNNs on sampled training patches. With a set of robust\nmulti-scale tampering detectors based on CNNs, complementary tampering\npossibility maps can be generated. Last but not least, a segmentation-based\nmethod is proposed to fuse the maps and generate the final decision map. By\nexploiting the benefits of both the small-scale and large-scale analyses, the\nsegmentation-based multi-scale analysis can lead to a performance leap in\nforgery localization of CNNs. Numerous experiments are conducted to demonstrate\nthe effectiveness and efficiency of our method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 09:40:27 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 01:27:45 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 11:11:24 GMT"}, {"version": "v4", "created": "Wed, 7 Feb 2018 10:45:45 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Liu", "Yaqi", ""], ["Guan", "Qingxiao", ""], ["Zhao", "Xianfeng", ""], ["Cao", "Yun", ""]]}, {"id": "1706.07911", "submitter": "Yi Zhu", "authors": "Yi Zhu, Sen Liu, Shawn Newsam", "title": "Large-Scale Mapping of Human Activity using Geo-Tagged Videos", "comments": "Accepted at ACM SIGSPATIAL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first work to perform spatio-temporal mapping of human\nactivity using the visual content of geo-tagged videos. We utilize a recent\ndeep-learning based video analysis framework, termed hidden two-stream\nnetworks, to recognize a range of activities in YouTube videos. This framework\nis efficient and can run in real time or faster which is important for\nrecognizing events as they occur in streaming video or for reducing latency in\nanalyzing already captured video. This is, in turn, important for using video\nin smart-city applications. We perform a series of experiments to show our\napproach is able to accurately map activities both spatially and temporally. We\nalso demonstrate the advantages of using the visual content over the\ntags/titles.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 05:59:37 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 18:24:56 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 19:05:37 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Zhu", "Yi", ""], ["Liu", "Sen", ""], ["Newsam", "Shawn", ""]]}, {"id": "1706.07929", "submitter": "Jian Zhang", "authors": "Jian Zhang, Bernard Ghanem", "title": "ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image\n  Compressive Sensing", "comments": "10 pages, 6 figures, 4 Tables. To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of developing a fast yet accurate algorithm for compressive\nsensing (CS) reconstruction of natural images, we combine in this paper the\nmerits of two existing categories of CS methods: the structure insights of\ntraditional optimization-based methods and the speed of recent network-based\nones. Specifically, we propose a novel structured deep network, dubbed\nISTA-Net, which is inspired by the Iterative Shrinkage-Thresholding Algorithm\n(ISTA) for optimizing a general $\\ell_1$ norm CS reconstruction model. To cast\nISTA into deep network form, we develop an effective strategy to solve the\nproximal mapping associated with the sparsity-inducing regularizer using\nnonlinear transforms. All the parameters in ISTA-Net (\\eg nonlinear transforms,\nshrinkage thresholds, step sizes, etc.) are learned end-to-end, rather than\nbeing hand-crafted. Moreover, considering that the residuals of natural images\nare more compressible, an enhanced version of ISTA-Net in the residual domain,\ndubbed {ISTA-Net}$^+$, is derived to further improve CS reconstruction.\nExtensive CS experiments demonstrate that the proposed ISTA-Nets outperform\nexisting state-of-the-art optimization-based and network-based CS methods by\nlarge margins, while maintaining fast computational speed. Our source codes are\navailable: \\textsl{http://jianzhang.tech/projects/ISTA-Net}.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 09:02:21 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 09:24:34 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhang", "Jian", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1706.08136", "submitter": "Christophe Guyeux", "authors": "Rola Al-Sharif, Christophe Guyeux, Yousra Ahmed Fadil, Abdallah\n  Makhoul, Ali Jaber", "title": "On the usefulness of information hiding techniques for wireless sensor\n  networks security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wireless sensor network (WSN) typically consists of base stations and a\nlarge number of wireless sensors. The sensory data gathered from the whole\nnetwork at a certain time snapshot can be visualized as an image. As a result,\ninformation hiding techniques can be applied to this \"sensory data image\".\nSteganography refers to the technology of hiding data into digital media\nwithout drawing any suspicion, while steganalysis is the art of detecting the\npresence of steganography. This article provides a brief review of\nsteganography and steganalysis applications for wireless sensor networks\n(WSNs). Then we show that the steganographic techniques are both related to\nsensed data authentication in wireless sensor networks, and when considering\nthe attacker point of view, which has not yet been investigated in the\nliterature. Our simulation results show that the sink level is unable to detect\nan attack carried out by the nsF5 algorithm on sensed data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 16:35:16 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Al-Sharif", "Rola", ""], ["Guyeux", "Christophe", ""], ["Fadil", "Yousra Ahmed", ""], ["Makhoul", "Abdallah", ""], ["Jaber", "Ali", ""]]}, {"id": "1706.08675", "submitter": "Dorien Herremans", "authors": "Dorien Herremans, Ching-Hua Chuan", "title": "Proceedings of the First International Workshop on Deep Learning and\n  Music", "comments": null, "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017", "doi": "10.13140/RG.2.2.22227.99364/1", "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proceedings of the First International Workshop on Deep Learning and Music,\njoint with IJCNN, Anchorage, US, May 17-18, 2017\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 05:28:06 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Herremans", "Dorien", ""], ["Chuan", "Ching-Hua", ""]]}, {"id": "1706.08764", "submitter": "Christophe Guyeux", "authors": "Jacques M. Bahi, Jean-Fran\\c{c}ois Couchot, Nicolas Friot, Christophe\n  Guyeux", "title": "A Robust Data Hiding Process Contributing to the Development of a\n  Semantic Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel steganographic scheme based on chaotic iterations is\nproposed. This research work takes place into the information hiding framework,\nand focus more specifically on robust steganography. Steganographic algorithms\ncan participate in the development of a semantic web: medias being on the\nInternet can be enriched by information related to their contents, authors,\netc., leading to better results for the search engines that can deal with such\ntags. As media can be modified by users for various reasons, it is preferable\nthat these embedding tags can resist to changes resulting from some classical\ntransformations as for example cropping, rotation, image conversion, and so on.\nThis is why a new robust watermarking scheme for semantic search engines is\nproposed in this document. For the sake of completeness, the robustness of this\nscheme is finally compared to existing established algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 10:23:21 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Bahi", "Jacques M.", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Friot", "Nicolas", ""], ["Guyeux", "Christophe", ""]]}, {"id": "1706.09088", "submitter": "Dorien Herremans", "authors": "Dorien Herremans, Ching-Hua Chuan", "title": "Modeling Musical Context with Word2vec", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 11-18 (2017)", "doi": null, "report-no": "DLM/2017/1", "categories": "cs.SD cs.IR cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semantic vector space model for capturing complex polyphonic\nmusical context. A word2vec model based on a skip-gram representation with\nnegative sampling was used to model slices of music from a dataset of\nBeethoven's piano sonatas. A visualization of the reduced vector space using\nt-distributed stochastic neighbor embedding shows that the resulting embedded\nvector space captures tonal relationships, even without any explicit\ninformation about the musical contents of the slices. Secondly, an excerpt of\nthe Moonlight Sonata from Beethoven was altered by replacing slices based on\ncontext similarity. The resulting music shows that the selected slice based on\nsimilar word2vec context also has a relatively short tonal distance from the\noriginal slice.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 00:46:50 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 02:33:06 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Herremans", "Dorien", ""], ["Chuan", "Ching-Hua", ""]]}, {"id": "1706.09317", "submitter": "Qian Wang", "authors": "Qian Wang and Ke Chen", "title": "Alternative Semantic Representations for Zero-Shot Human Action\n  Recognition", "comments": "Technical Report, School of Computer Science, The University of\n  Manchester, Accepted to ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proper semantic representation for encoding side information is key to the\nsuccess of zero-shot learning. In this paper, we explore two alternative\nsemantic representations especially for zero-shot human action recognition:\ntextual descriptions of human actions and deep features extracted from still\nimages relevant to human actions. Such side information are accessible on Web\nwith little cost, which paves a new way in gaining side information for\nlarge-scale zero-shot human action recognition. We investigate different\nencoding methods to generate semantic representations for human actions from\nsuch side information. Based on our zero-shot visual recognition method, we\nconducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic\nrepresentations . The results suggest that our proposed text- and image-based\nsemantic representations outperform traditional attributes and word vectors\nconsiderably for zero-shot human action recognition. In particular, the\nimage-based semantic representations yield the favourable performance even\nthough the representation is extracted from a small number of images per class.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:32:57 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Wang", "Qian", ""], ["Chen", "Ke", ""]]}, {"id": "1706.09551", "submitter": "Andrew Pfalz", "authors": "A. Pfalz, E. Berdahl", "title": "Toward Inverse Control of Physics-Based Sound Synthesis", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN, 1(1). pp 56-61 (2017)", "doi": null, "report-no": "DLM/2017/7", "categories": "cs.SD cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory networks (LSTMs) can be trained to realize inverse\ncontrol of physics-based sound synthesizers. Physics-based sound synthesizers\nsimulate the laws of physics to produce output sound according to input gesture\nsignals. When a user's gestures are measured in real time, she or he can use\nthem to control physics-based sound synthesizers, thereby creating simulated\nvirtual instruments. An intriguing question is how to program a computer to\nlearn to play such physics-based models. This work demonstrates that LSTMs can\nbe trained to accomplish this inverse control task with four physics-based\nsound synthesizers.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:33:56 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Pfalz", "A.", ""], ["Berdahl", "E.", ""]]}, {"id": "1706.09552", "submitter": "Hendrik Vincent Koops", "authors": "H.V. Koops, W.B. de Haas, J. Bransen, A. Volk", "title": "Chord Label Personalization through Deep Learning of Integrated Harmonic\n  Interval-based Representations", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proc. of the Int. Workshop on Deep Learning and Music. Anchorage,\n  US. 1(1). pp19-25 (2017)", "doi": null, "report-no": "DLM/2017/2", "categories": "cs.SD cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing accuracy of automatic chord estimation systems, the\navailability of vast amounts of heterogeneous reference annotations, and\ninsights from annotator subjectivity research make chord label personalization\nincreasingly important. Nevertheless, automatic chord estimation systems are\nhistorically exclusively trained and evaluated on a single reference\nannotation. We introduce a first approach to automatic chord label\npersonalization by modeling subjectivity through deep learning of a harmonic\ninterval-based chord label representation. After integrating these\nrepresentations from multiple annotators, we can accurately personalize chord\nlabels for individual annotators from a single model and the annotators' chord\nlabel vocabulary. Furthermore, we show that chord personalization using\nmultiple reference annotations outperforms using a single reference annotation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:38:02 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Koops", "H. V.", ""], ["de Haas", "W. B.", ""], ["Bransen", "J.", ""], ["Volk", "A.", ""]]}, {"id": "1706.09553", "submitter": "Shija Geng", "authors": "S. Geng, G. Ren, M. Ogihara", "title": "Transforming Musical Signals through a Genre Classifying Convolutional\n  Neural Network", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proc. of the First Int. Workshop on Deep Learning and Music joint\n  with IJCNN. Anchorage, US. 1(1). pp 48-49 (2017)", "doi": null, "report-no": "DLM/2017/4", "categories": "cs.SD cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully applied on both\ndiscriminative and generative modeling for music-related tasks. For a\nparticular task, the trained CNN contains information representing the decision\nmaking or the abstracting process. One can hope to manipulate existing music\nbased on this 'informed' network and create music with new features\ncorresponding to the knowledge obtained by the network. In this paper, we\npropose a method to utilize the stored information from a CNN trained on\nmusical genre classification task. The network was composed of three\nconvolutional layers, and was trained to classify five-second song clips into\nfive different genres. After training, randomly selected clips were modified by\nmaximizing the sum of outputs from the network layers. In addition to the\npotential of such CNNs to produce interesting audio transformation, more\ninformation about the network and the original music could be obtained from the\nanalysis of the generated features since these features indicate how the\nnetwork 'understands' the music.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:39:00 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Geng", "S.", ""], ["Ren", "G.", ""], ["Ogihara", "M.", ""]]}, {"id": "1706.09555", "submitter": "Zhe-Cheng Fan", "authors": "Z.C. Fan, T.S. Chan, Y.H. Yang, and J.S. R. Jang", "title": "Music Signal Processing Using Vector Product Neural Networks", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proc. of the First Int. Workshop on Deep Learning and Music joint\n  with IJCNN. Anchorage, US. 1(1). pp 36-30 (2017)", "doi": null, "report-no": "DLM/2017/5", "categories": "cs.SD cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural network model for music signal processing using\nvector product neurons and dimensionality transformations. Here, the inputs are\nfirst mapped from real values into three-dimensional vectors then fed into a\nthree-dimensional vector product neural network where the inputs, outputs, and\nweights are all three-dimensional values. Next, the final outputs are mapped\nback to the reals. Two methods for dimensionality transformation are proposed,\none via context windows and the other via spectral coloring. Experimental\nresults on the iKala dataset for blind singing voice separation confirm the\nefficacy of our model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:41:30 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Fan", "Z. C.", ""], ["Chan", "T. S.", ""], ["Yang", "Y. H.", ""], ["Jang", "J. S. R.", ""]]}, {"id": "1706.09556", "submitter": "Alessio Bazzica", "authors": "A. Bazzica, J.C. van Gemert, C.C.S. Liem, A. Hanjalic", "title": "Vision-based Detection of Acoustic Timed Events: a Case Study on\n  Clarinet Note Onsets", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proc of the First Int Workshop on Deep Learning and Music.\n  Anchorage, US. 1(1). pp 31-36 (2017)", "doi": null, "report-no": "DLM/2017/8", "categories": "cs.NE cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic events often have a visual counterpart. Knowledge of visual\ninformation can aid the understanding of complex auditory scenes, even when\nonly a stereo mixdown is available in the audio domain, \\eg identifying which\nmusicians are playing in large musical ensembles. In this paper, we consider a\nvision-based approach to note onset detection. As a case study we focus on\nchallenging, real-world clarinetist videos and carry out preliminary\nexperiments on a 3D convolutional neural network based on multiple streams and\npurposely avoiding temporal pooling. We release an audiovisual dataset with 4.5\nhours of clarinetist videos together with cleaned annotations which include\nabout 36,000 onsets and the coordinates for a number of salient points and\nregions of interest. By performing several training trials on our dataset, we\nlearned that the problem is challenging. We found that the CNN model is highly\nsensitive to the optimization algorithm and hyper-parameters, and that treating\nthe problem as binary classification may prevent the joint optimization of\nprecision and recall. To encourage further research, we publicly share our\ndataset, annotations and all models and detail which issues we came across\nduring our preliminary experiments.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:43:37 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bazzica", "A.", ""], ["van Gemert", "J. C.", ""], ["Liem", "C. C. S.", ""], ["Hanjalic", "A.", ""]]}, {"id": "1706.09558", "submitter": "Patrick Hutchings", "authors": "P. Hutchings", "title": "Talking Drums: Generating drum grooves with neural networks", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 43-47 (2017)", "doi": null, "report-no": "DLM/2017/3", "categories": "cs.SD cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented is a method of generating a full drum kit part for a provided\nkick-drum sequence. A sequence to sequence neural network model used in natural\nlanguage translation was adopted to encode multiple musical styles and an\nonline survey was developed to test different techniques for sampling the\noutput of the softmax function. The strongest results were found using a\nsampling technique that drew from the three most probable outputs at each\nsubdivision of the drum pattern but the consistency of output was found to be\nheavily dependent on style.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 03:03:35 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Hutchings", "P.", ""]]}, {"id": "1706.09559", "submitter": "Lonce Wyse", "authors": "L. Wyse", "title": "Audio Spectrogram Representations for Processing with Convolutional\n  Neural Networks", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 37-41 (2017)", "doi": null, "report-no": "DLM/2017/9", "categories": "cs.SD cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the decisions that arise when designing a neural network for any\napplication is how the data should be represented in order to be presented to,\nand possibly generated by, a neural network. For audio, the choice is less\nobvious than it seems to be for visual images, and a variety of representations\nhave been used for different applications including the raw digitized sample\nstream, hand-crafted features, machine discovered features, MFCCs and variants\nthat include deltas, and a variety of spectral representations. This paper\nreviews some of these representations and issues that arise, focusing\nparticularly on spectrograms for generating audio using neural networks for\nstyle transfer.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 03:04:06 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Wyse", "L.", ""]]}, {"id": "1706.09588", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Yuki Mitsufuji", "title": "Multi-scale Multi-band DenseNets for Audio Source Separation", "comments": "to appear at WASPAA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of audio source separation. To handle the\ncomplex and ill-posed nature of the problems of audio source separation, the\ncurrent state-of-the-art approaches employ deep neural networks to obtain\ninstrumental spectra from a mixture. In this study, we propose a novel network\narchitecture that extends the recently developed densely connected\nconvolutional network (DenseNet), which has shown excellent results on image\nclassification tasks. To deal with the specific problem of audio source\nseparation, an up-sampling layer, block skip connection and band-dedicated\ndense blocks are incorporated on top of DenseNet. The proposed approach takes\nadvantage of long contextual information and outperforms state-of-the-art\nresults on SiSEC 2016 competition by a large margin in terms of\nsignal-to-distortion ratio. Moreover, the proposed architecture requires\nsignificantly fewer parameters and considerably less training time compared\nwith other methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 05:56:06 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Takahashi", "Naoya", ""], ["Mitsufuji", "Yuki", ""]]}, {"id": "1706.09641", "submitter": "Krzysztof Szczypiorski", "authors": "Jedrzej Bieniasz and Krzysztof Szczypiorski", "title": "SocialStegDisc: Application of steganography in social networks to\n  create a file system", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept named SocialStegDisc was introduced as an application of the\noriginal idea of StegHash method. This new kind of mass-storage was\ncharacterized by unlimited space. The design also attempted to improve the\noperation of StegHash by trade-off between memory requirements and computation\ntime. Applying the mechanism of linked list provided the set of operations on\nfiles: creation, reading, deletion and modification. Features, limitations and\nopportunities were discussed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 09:26:59 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bieniasz", "Jedrzej", ""], ["Szczypiorski", "Krzysztof", ""]]}, {"id": "1706.10143", "submitter": "Tiantian He", "authors": "Tiantian He, Yankai Liu, Rong Xie, Xin Tang, Li Song", "title": "Evaluation of No Reference Bitstream-based Video Quality Assessment\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different parametric models for video quality assessment have been\nproposed in the past few years. This paper presents a review of nine recent\nmodels which cover a wide range of methodologies and have been validated for\nestimating video quality due to different degradation factors. Each model is\nbriefly described with key algorithms and relevant parametric formulas. The\ngeneralization capability of each model to estimate video quality in\nreal-application scenarios is evaluated and compared with other models, using a\ndataset created with video sequences from practical applications. These video\nsequences cover a wide range of possible realistic encoding parameters, labeled\nwith mean opinion scores (MOS) via subjective test. The weakness and strength\nof each model are remarked. Finally, future work towards a more general\nparametric model that could apply for a wider range of applications is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 11:43:34 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["He", "Tiantian", ""], ["Liu", "Yankai", ""], ["Xie", "Rong", ""], ["Tang", "Xin", ""], ["Song", "Li", ""]]}]