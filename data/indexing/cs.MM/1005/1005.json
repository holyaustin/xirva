[{"id": "1005.0092", "submitter": "Andrei Sukhov M", "authors": "E.S. Sagatov, A.M. Sukhov, P. Calyam", "title": "Influence of distortions of key frames on video transfer in wireless\n  networks", "comments": "6 pages, 4 figures, 2 Tables", "journal-ref": null, "doi": "10.1109/ISVC.2010.5656258", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper it is shown that for substantial increase of video quality in\nwireless network it is necessary to execute two obligatory points on\nmodernization of the communication scheme. The player on the received part\nshould throw back automatically duplicated RTP packets, server of streaming\nvideo should duplicate the packets containing the information of key frames.\nCoefficients of the mathematical model describing video quality in wireless\nnetwork have been found for WiFi and 3G standards and codecs MPEG-2 and MPEG-4\n(DivX). The special experimental technique which has allowed collecting and\nprocessing the data has been developed for calculation of values of factors.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 17:15:36 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Sagatov", "E. S.", ""], ["Sukhov", "A. M.", ""], ["Calyam", "P.", ""]]}, {"id": "1005.0771", "submitter": "Ashley Smith", "authors": "Lamjed Touil, Abdessalem Ben Abdelali, Abdellatif Mtibaa and Elbey\n  Bourennane", "title": "Towards Hardware implementation of video applications in new\n  telecommunications devices", "comments": "Lamjed Touil, Abdessalem Ben Abdelali, Abdellatif Mibaa and Elbey\n  Bourennane, \"Towards Hardware implementation of video applications in new\n  telecommunications devices\", Journal of Telecommunications, Volume 2, Issue\n  1, p75-85, April 2010", "journal-ref": "Journal of Telecommunications, Volume 2, Issue 1, p75-85, April\n  2010", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the areas, most demanding in terms of calculation is the\ntelecommunication and video applications are now included in several\ntelecommunication devices such as set-top boxes, mobile phones. Embedded videos\napplications in new generations of telecommunication devices need a processing\ncapacity that can not be achieved by the conventional processor, to work around\nthis problem the use of programmable technology has a lot of interest. First,\nField Programmable Gate Arrays (FPGAs) present many performance benefits for\nreal-time image processing applications. The FPGA structure is able to exploit\nspatial and temporal parallelism. In this paper, we present a new method for\nimplementation of the Color Structure Descriptor (CSD) using the FPGA circuit.\nIn fact the (CSD) provides satisfactory image indexing and retrieval results\namong all colorbased descriptors in MPEG-7. But the real time implementation of\nthis descriptor is still having problems. In this paper we propose a method for\nadapting this descriptor for possible implementation under the constraints of\nthe video processing in real time. We have verified the real-time\nimplementation of the (CSD) with an image size of 120*80 pixels.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 19:14:07 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Touil", "Lamjed", ""], ["Abdelali", "Abdessalem Ben", ""], ["Mtibaa", "Abdellatif", ""], ["Bourennane", "Elbey", ""]]}, {"id": "1005.1684", "submitter": "John Scoville", "authors": "John Scoville", "title": "On Macroscopic Complexity and Perceptual Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.MM cs.SD math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical limits of 'lossy' data compression algorithms are considered.\nThe complexity of an object as seen by a macroscopic observer is the size of\nthe perceptual code which discards all information that can be lost without\naltering the perception of the specified observer. The complexity of this\nmacroscopically observed state is the simplest description of any microstate\ncomprising that macrostate. Inference and pattern recognition based on\nmacrostate rather than microstate complexities will take advantage of the\ncomplexity of the macroscopic observer to ignore irrelevant noise.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 22:41:10 GMT"}, {"version": "v10", "created": "Tue, 14 Jun 2011 18:27:34 GMT"}, {"version": "v11", "created": "Mon, 4 Jul 2011 23:01:29 GMT"}, {"version": "v12", "created": "Thu, 7 Jul 2011 02:14:22 GMT"}, {"version": "v2", "created": "Mon, 24 May 2010 02:41:23 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2010 06:29:43 GMT"}, {"version": "v4", "created": "Wed, 22 Sep 2010 03:51:59 GMT"}, {"version": "v5", "created": "Thu, 23 Sep 2010 01:10:18 GMT"}, {"version": "v6", "created": "Thu, 7 Oct 2010 23:23:37 GMT"}, {"version": "v7", "created": "Wed, 20 Oct 2010 18:40:28 GMT"}, {"version": "v8", "created": "Tue, 26 Apr 2011 22:50:28 GMT"}, {"version": "v9", "created": "Tue, 10 May 2011 11:46:04 GMT"}], "update_date": "2011-07-08", "authors_parsed": [["Scoville", "John", ""]]}, {"id": "1005.1757", "submitter": "Secretary Aircc Journal", "authors": "Ubaid Abbasi and Toufik Ahmed (University of Bordeaux, France)", "title": "Architecture for Cooperative Prefetching in P2P Video-on- Demand System", "comments": "13 Pages, IJCNC", "journal-ref": "International Journal of Computer Networks & Communications 2.3\n  (2010) 126-138", "doi": "10.5121/ijcnc.2010.2310", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Most P2P VoD schemes focused on service architectures and overlays\noptimization without considering segments rarity and the performance of\nprefetching strategies. As a result, they cannot better support VCRoriented\nservice in heterogeneous environment having clients using free VCR controls.\nDespite the remarkable popularity in VoD systems, there exist no prior work\nthat studies the performance gap between different prefetching strategies. In\nthis paper, we analyze and understand the performance of different prefetching\nstrategies. Our analytical characterization brings us not only a better\nunderstanding of several fundamental tradeoffs in prefetching strategies, but\nalso important insights on the design of P2P VoD system. On the basis of this\nanalysis, we finally proposed a cooperative prefetching strategy called\n\"cooching\". In this strategy, the requested segments in VCR interactivities are\nprefetched into session beforehand using the information collected through\ngossips. We evaluate our strategy through extensive simulations. The results\nindicate that the proposed strategy outperforms the existing prefetching\nmechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2010 08:42:18 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Abbasi", "Ubaid", "", "University of Bordeaux, France"], ["Ahmed", "Toufik", "", "University of Bordeaux, France"]]}, {"id": "1005.2672", "submitter": "Carst Tankink", "authors": "Carst Tankink and Herman Geuvers and James McKinna and Freek Wiedijk", "title": "Proviola: A Tool for Proof Re-animation", "comments": "Accepted for the 9th International Conference on Mathematical\n  Knowledge Management (MKM 2010), 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DL cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve on existing models of interaction with a proof assistant (PA), in\nparticular for storage and replay of proofs, we in- troduce three related\nconcepts, those of: a proof movie, consisting of frames which record both user\ninput and the corresponding PA response; a camera, which films a user's\ninteractive session with a PA as a movie; and a proviola, which replays a movie\nframe-by-frame to a third party. In this paper we describe the movie data\nstructure and we discuss a proto- type implementation of the camera and\nproviola based on the ProofWeb system. ProofWeb uncouples the interaction with\na PA via a web- interface (the client) from the actual PA that resides on the\nserver. Our camera films a movie by \"listening\" to the ProofWeb communication.\nThe first reason for developing movies is to uncouple the reviewing of a formal\nproof from the PA used to develop it: the movie concept enables users to\ndiscuss small code fragments without the need to install the PA or to load a\nwhole library into it. Other advantages include the possibility to develop a\nseparate com- mentary track to discuss or explain the PA interaction. We assert\nthat a combined camera+proviola provides a generic layer between a client\n(user) and a server (PA). Finally we claim that movies are the right type of\ndata to be stored in an encyclopedia of formalized mathematics, based on our\nexperience in filming the Coq standard library.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2010 12:54:28 GMT"}], "update_date": "2010-05-18", "authors_parsed": [["Tankink", "Carst", ""], ["Geuvers", "Herman", ""], ["McKinna", "James", ""], ["Wiedijk", "Freek", ""]]}, {"id": "1005.4014", "submitter": "William Jackson", "authors": "Gilbert Phuah Leong Siang, Nor Azman Ismail and Pang Yee Yong", "title": "A Study on Potential of Integrating Multimodal Interaction into Musical\n  Conducting Education", "comments": "http://www.journalofcomputing.org", "journal-ref": "Journal of Computing, Volume 2, Issue 5, May 2010", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of computer technology, computer music has begun\nto appear in the laboratory. Many potential utility of computer music is\ngradually increasing. The purpose of this paper is attempted to analyze the\npossibility of integrating multimodal interaction such as vision-based hand\ngesture and speech interaction into musical conducting education. To achieve\nthis purpose, this paper is focus on discuss some related research and the\ntraditional musical conducting education. To do so, six musical conductors had\nbeen interviewed to share their musical conducting learning/ teaching\nexperience. These interviews had been analyzed in this paper to show the\nsyllabus and the focus of musical conducting education for beginners.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 17:18:29 GMT"}], "update_date": "2010-05-24", "authors_parsed": [["Siang", "Gilbert Phuah Leong", ""], ["Ismail", "Nor Azman", ""], ["Yong", "Pang Yee", ""]]}, {"id": "1005.4267", "submitter": "Chriss Romy", "authors": "Uday Pratap Singh, Sanjeev Jain, Gulfishan Firdose Ahmed", "title": "Content Base Image Retrieval Using Phong Shading", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The digital image data is rapidly expanding in quantity and heterogeneity.\nThe traditional information retrieval techniques does not meet the user's\ndemand, so there is need to develop an efficient system for content based image\nretrieval. Content based image retrieval means retrieval of images from\ndatabase on the basis of visual features of image like as color, texture etc.\nIn our proposed method feature are extracted after applying Phong shading on\ninput image. Phong shading, flattering out the dull surfaces of the image The\nfeatures are extracted using color, texture & edge density methods. Feature\nextracted values are used to find the similarity between input query image and\nthe data base image. It can be measure by the Euclidean distance formula. The\nexperimental result shows that the proposed approach has a better retrieval\nresults with phong shading.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 07:28:24 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Singh", "Uday Pratap", ""], ["Jain", "Sanjeev", ""], ["Ahmed", "Gulfishan Firdose", ""]]}, {"id": "1005.4564", "submitter": "Isabel Rodet", "authors": "Annie Luciani (ACROE, ICA), Matthieu Evrard (ICA), Damien Courouss\\'e\n  (ICA), Nicolas Castagn\\'e (ACROE, ICA), Claude Cadoz (ACROE, ICA), Jean-Loup\n  Florens (ACROE)", "title": "A basic gesture and motion format for virtual reality multisensory\n  applications", "comments": "GRAPP'06, Set\\`ubal : Portugal (2006)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of encoding movements such as those produced by human gestures\nmay become central in the coming years, given the growing importance of\nmovement data exchanges between heterogeneous systems and applications (musical\napplications, 3D motion control, virtual reality interaction, etc.). For the\npast 20 years, various formats have been proposed for encoding movement,\nespecially gestures. Though, these formats, at different degrees, were designed\nin the context of quite specific applications (character animation, motion\ncapture, musical gesture, biomechanical concerns...). The article introduce a\nnew file format, called GMS (for 'Gesture and Motion Signal'), with the aim of\nbeing more low-level and generic, by defining the minimal features a format\ncarrying movement/gesture information needs, rather than by gathering all the\ninformation generally given by the existing formats. The article argues that,\ngiven its growing presence in virtual reality situations, the \"gesture signal\"\nitself must be encoded, and that a specific format is needed. The proposed\nformat features the inner properties of such signals: dimensionality,\nstructural features, types of variables, and spatial and temporal properties.\nThe article first reviews the various situations with multisensory virtual\nobjects in which gesture controls intervene. The proposed format is then\ndeduced, as a mean to encode such versatile and variable \"gestural and animated\nscene\".\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2010 13:16:29 GMT"}], "update_date": "2010-05-26", "authors_parsed": [["Luciani", "Annie", "", "ACROE, ICA"], ["Evrard", "Matthieu", "", "ICA"], ["Courouss\u00e9", "Damien", "", "ICA"], ["Castagn\u00e9", "Nicolas", "", "ACROE, ICA"], ["Cadoz", "Claude", "", "ACROE, ICA"], ["Florens", "Jean-Loup", "", "ACROE"]]}, {"id": "1005.5436", "submitter": "Secretary Aircc Journal", "authors": "M. Dakshayini(1), T. R. Gopala Krishnan Nair(2), ((1)Dr.MGR\n  University, India, (2)Research and Industry Incubation Centre - Bangalore,\n  India)", "title": "Client-to-Client Streaming Scheme for VOD Applications", "comments": "10 Pages, IJMA", "journal-ref": "International journal of Multimedia & Its Applications 2.2 (2010)\n  46-55", "doi": "10.5121/ijma.2010.2204", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we propose an efficient client-to-client streaming approach to\ncooperatively stream the video using chaining technique with unicast\ncommunication among the clients. This approach considers two major issues of\nVoD 1) Prefix caching scheme to accommodate more number of videos closer to\nclient, so that the request-service delay for the user can be minimized. 2)\nCooperative proxy and client chaining scheme for streaming the videos using\nunicasting. This approach minimizes the client rejection rate and bandwidth\nrequirement on server to proxy and proxy to client path. Our simulation results\nshow that the proposed approach achieves reduced client waiting time and\noptimal prefix caching of videos minimizing server to proxy path bandwidth\nusage by utilizing the client to client bandwidth, which is occasionally used\nwhen compared to busy server to proxy path bandwidth.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 08:00:39 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Dakshayini", "M.", ""], ["Nair", "T. R. Gopala Krishnan", ""]]}, {"id": "1005.5613", "submitter": "Secretary Aircc Journal", "authors": "Murtaza Ali Khan (Royal University for Women, Bahrain)", "title": "An Automated Algorithm for Approximation of Temporal Video Data Using\n  Linear B'EZIER Fitting", "comments": "14 Pages, IJMA 2010", "journal-ref": "International journal of Multimedia & Its Applications 2.2 (2010)\n  81-94", "doi": "10.5121/ijma.2010.2207", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents an efficient method for approximation of temporal video\ndata using linear Bezier fitting. For a given sequence of frames, the proposed\nmethod estimates the intensity variations of each pixel in temporal dimension\nusing linear Bezier fitting in Euclidean space. Fitting of each segment ensures\nupper bound of specified mean squared error. Break and fit criteria is employed\nto minimize the number of segments required to fit the data. The proposed\nmethod is well suitable for lossy compression of temporal video data and\nautomates the fitting process of each pixel. Experimental results show that the\nproposed method yields good results both in terms of objective and subjective\nquality measurement parameters without causing any blocking artifacts.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2010 08:11:59 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Khan", "Murtaza Ali", "", "Royal University for Women, Bahrain"]]}]