[{"id": "2010.00246", "submitter": "Zheng Gu", "authors": "Zheng Gu, Chuanqi Dong, Jing Huo, Wenbin Li, Yang Gao", "title": "CariMe: Unpaired Caricature Generation with Multiple Exaggerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature generation aims to translate real photos into caricatures with\nartistic styles and shape exaggerations while maintaining the identity of the\nsubject. Different from the generic image-to-image translation, drawing a\ncaricature automatically is a more challenging task due to the existence of\nvarious spacial deformations. Previous caricature generation methods are\nobsessed with predicting definite image warping from a given photo while\nignoring the intrinsic representation and distribution for exaggerations in\ncaricatures. This limits their ability on diverse exaggeration generation. In\nthis paper, we generalize the caricature generation problem from instance-level\nwarping prediction to distribution-level deformation modeling. Based on this\nassumption, we present the first exploration for unpaired CARIcature generation\nwith Multiple Exaggerations (CariMe). Technically, we propose a\nMulti-exaggeration Warper network to learn the distribution-level mapping from\nphoto to facial exaggerations. This makes it possible to generate diverse and\nreasonable exaggerations from randomly sampled warp codes given one input\nphoto. To better represent the facial exaggeration and produce fine-grained\nwarping, a deformation-field-based warping method is also proposed, which helps\nus to capture more detailed exaggerations than other point-based warping\nmethods. Experiments and two perceptual studies prove the superiority of our\nmethod comparing with other state-of-the-art methods, showing the improvement\nof our work on caricature generation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 08:14:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Gu", "Zheng", ""], ["Dong", "Chuanqi", ""], ["Huo", "Jing", ""], ["Li", "Wenbin", ""], ["Gao", "Yang", ""]]}, {"id": "2010.00302", "submitter": "Anna Melman", "authors": "Anna Melman, Oleg Evsutin, Alexander Shelupanov", "title": "An authorship protection technology for electronic documents based on\n  image watermarking", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of information technology, information security technologies\nhold a special place. They ensure the security of the use of information\ntechnology. One of the urgent tasks is the protection of electronic documents\nduring their transfer in information systems. This paper proposes a technology\nfor protecting electronic documents containing digital images. The main idea is\nthat the electronic document authorship protection can be implemented by\ndigital watermark embedding in the images that are contained in this document.\nThe paper considers three cases of using the proposed technology: full copying\nof an electronic document, copying of images contained in the document, and\ncopying of text. It is shown that in all three cases the authorship\nconfirmation can be successfully implemented. Computational experiments are\nconducted with robust watermarking algorithms that can be used within the\ntechnology. A scenario of technology implementation is proposed, which provides\nfor the joint use of different class algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 11:05:47 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Melman", "Anna", ""], ["Evsutin", "Oleg", ""], ["Shelupanov", "Alexander", ""]]}, {"id": "2010.00400", "submitter": "Ruben Tolosana", "authors": "Javier Hernandez-Ortega, Ruben Tolosana, Julian Fierrez and Aythami\n  Morales", "title": "DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation", "comments": null, "journal-ref": "Proc. 35th AAAI Conference on Artificial Intelligence Workshops,\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work introduces a novel DeepFake detection framework based on\nphysiological measurement. In particular, we consider information related to\nthe heart rate using remote photoplethysmography (rPPG). rPPG methods analyze\nvideo sequences looking for subtle color changes in the human skin, revealing\nthe presence of human blood under the tissues. In this work we investigate to\nwhat extent rPPG is useful for the detection of DeepFake videos.\n  The proposed fake detector named DeepFakesON-Phys uses a Convolutional\nAttention Network (CAN), which extracts spatial and temporal information from\nvideo frames, analyzing and combining both sources to better detect fake\nvideos. This detection approach has been experimentally evaluated using the\nlatest public databases in the field: Celeb-DF and DFDC. The results achieved,\nabove 98% AUC (Area Under the Curve) on both databases, outperform the state of\nthe art and prove the success of fake detectors based on physiological\nmeasurement to detect the latest DeepFake videos.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:37:58 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:47:24 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 14:34:23 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Tolosana", "Ruben", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""]]}, {"id": "2010.00984", "submitter": "Felice Antonio Merra", "authors": "Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, Felice Antonio\n  Merra", "title": "An Empirical Study of DNNs Robustification Inefficacy in Protecting\n  Visual Recommenders", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-based recommender systems (VRSs) enhance recommendation performance by\nintegrating users' feedback with the visual features of product images\nextracted from a deep neural network (DNN). Recently, human-imperceptible\nimages perturbations, defined \\textit{adversarial attacks}, have been\ndemonstrated to alter the VRSs recommendation performance, e.g., pushing/nuking\ncategory of products. However, since adversarial training techniques have\nproven to successfully robustify DNNs in preserving classification accuracy, to\nthe best of our knowledge, two important questions have not been investigated\nyet: 1) How well can these defensive mechanisms protect the VRSs performance?\n2) What are the reasons behind ineffective/effective defenses? To answer these\nquestions, we define a set of defense and attack settings, as well as\nrecommender models, to empirically investigate the efficacy of defensive\nmechanisms. The results indicate alarming risks in protecting a VRS through the\nDNN robustification. Our experiments shed light on the importance of visual\nfeatures in very effective attack scenarios. Given the financial impact of VRSs\non many companies, we believe this work might rise the need to investigate how\nto successfully protect visual-based recommenders. Source code and data are\navailable at\nhttps://anonymous.4open.science/r/868f87ca-c8a4-41ba-9af9-20c41de33029/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:29:41 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Di Noia", "Tommaso", ""], ["Malitesta", "Daniele", ""], ["Merra", "Felice Antonio", ""]]}, {"id": "2010.01158", "submitter": "Zhenyu Wu", "authors": "Zhenyu Wu, Duc Hoang, Shih-Yao Lin, Yusheng Xie, Liangjian Chen,\n  Yen-Yu Lin, Zhangyang Wang, Wei Fan", "title": "MM-Hand: 3D-Aware Multi-Modal Guided Hand Generative Network for 3D Hand\n  Pose Synthesis", "comments": "Accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D hand pose from a monocular RGB image is important but\nchallenging. A solution is training on large-scale RGB hand images with\naccurate 3D hand keypoint annotations. However, it is too expensive in\npractice. Instead, we have developed a learning-based approach to synthesize\nrealistic, diverse, and 3D pose-preserving hand images under the guidance of 3D\npose information. We propose a 3D-aware multi-modal guided hand generative\nnetwork (MM-Hand), together with a novel geometry-based curriculum learning\nstrategy. Our extensive experimental results demonstrate that the 3D-annotated\nimages generated by MM-Hand qualitatively and quantitatively outperform\nexisting options. Moreover, the augmented data can consistently improve the\nquantitative performance of the state-of-the-art 3D hand pose estimators on two\nbenchmark datasets. The code will be available at\nhttps://github.com/ScottHoang/mm-hand.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 18:27:34 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wu", "Zhenyu", ""], ["Hoang", "Duc", ""], ["Lin", "Shih-Yao", ""], ["Xie", "Yusheng", ""], ["Chen", "Liangjian", ""], ["Lin", "Yen-Yu", ""], ["Wang", "Zhangyang", ""], ["Fan", "Wei", ""]]}, {"id": "2010.01323", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young, Katie Crowley", "title": "The Design of Tangible Digital Musical Instruments", "comments": "MusTWork 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present guidelines that highlight the impact of haptic feedback upon\nthe experiences of computer musicians using Digital Musical Instruments (DMIs).\nIn this context, haptic feedback offers a tangible, bi-directional exchange\nbetween a musician and a DMI. We propose that by adhering to and exploring\nthese guidelines the application of haptic feedback can enhance and augment the\nphysical and affective experiences of a musician in interactions with these\ndevices. It has been previously indicated that in the design of haptic DMIs,\nthe experiences and expectations of a musician must be considered for the\ncreation of tangible DMIs and that haptic feedback can be used to address the\nphysical-digital divide that currently exists between users of such\ninstruments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:08:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Young", "Gareth W.", ""], ["Crowley", "Katie", ""]]}, {"id": "2010.01326", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young, Dave Murphy", "title": "Digital Musical Instrument Analysis: The Haptic Bowl", "comments": "CMMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This experiment is a case study that applies a HCI-informed DMI Evaluation\nFramework. This framework applies existing HCI evaluation methods to the\nassessment of prototype Digital Musical Instruments (DMIs). The overall study\nwill involve a three-part analysis - a description and categorisation of the\ndevice, a functionality evaluation that included an examination of usability\nand user experience, and finally an exploration of the device's effectiveness\nas a digital instrument. Here we present the findings of the first two parts of\nthe framework, outlining the constituent components of the interface and\ntesting the functionality of the device. The final stage of analysis will\ninvolve a longitudinal study and will be carried out in order to assess the\nmusical affordances of the device.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:18:33 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Young", "Gareth W.", ""], ["Murphy", "Dave", ""]]}, {"id": "2010.01328", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young, Dave Murphy", "title": "HCI Models for Digital Musical Instruments: Methodologies for Rigorous\n  Testing of Digital Musical Instruments", "comments": "CMMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present an analysis of literature relating to the evaluation\nmethodologies of Digital Musical Instruments (DMIs) derived from the field of\nHuman-Computer Interaction (HCI). We then apply choice aspects from these\nexisting evaluation models and apply them to an optimized evaluation for\nassessing new DMIs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:22:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Young", "Gareth W.", ""], ["Murphy", "Dave", ""]]}, {"id": "2010.01424", "submitter": "Pengchuan Zhang", "authors": "Yi Wei, Zhe Gan, Wenbo Li, Siwei Lyu, Ming-Ching Chang, Lei Zhang,\n  Jianfeng Gao, Pengchuan Zhang", "title": "MagGAN: High-Resolution Face Attribute Editing with Mask-Guided\n  Generative Adversarial Network", "comments": "published at ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mask-guided Generative Adversarial Network (MagGAN) for\nhigh-resolution face attribute editing, in which semantic facial masks from a\npre-trained face parser are used to guide the fine-grained image editing\nprocess. With the introduction of a mask-guided reconstruction loss, MagGAN\nlearns to only edit the facial parts that are relevant to the desired attribute\nchanges, while preserving the attribute-irrelevant regions (e.g., hat, scarf\nfor modification `To Bald'). Further, a novel mask-guided conditioning strategy\nis introduced to incorporate the influence region of each attribute change into\nthe generator. In addition, a multi-level patch-wise discriminator structure is\nproposed to scale our model for high-resolution ($1024 \\times 1024$) face\nediting. Experiments on the CelebA benchmark show that the proposed method\nsignificantly outperforms prior state-of-the-art approaches in terms of both\nimage quality and editing performance.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 20:56:16 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wei", "Yi", ""], ["Gan", "Zhe", ""], ["Li", "Wenbo", ""], ["Lyu", "Siwei", ""], ["Chang", "Ming-Ching", ""], ["Zhang", "Lei", ""], ["Gao", "Jianfeng", ""], ["Zhang", "Pengchuan", ""]]}, {"id": "2010.01679", "submitter": "Mallikarjun Byrasandra Ramalinga Reddy", "authors": "Mallikarjun B R and Ayush Tewari and Hans-Peter Seidel and Mohamed\n  Elgharib and Christian Theobalt", "title": "Learning Complete 3D Morphable Face Models from Images and Videos", "comments": "Project Page - https://gvv.mpi-inf.mpg.de/projects/LeMoMo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most 3D face reconstruction methods rely on 3D morphable models, which\ndisentangle the space of facial deformations into identity geometry,\nexpressions and skin reflectance. These models are typically learned from a\nlimited number of 3D scans and thus do not generalize well across different\nidentities and expressions. We present the first approach to learn complete 3D\nmodels of face identity geometry, albedo and expression just from images and\nvideos. The virtually endless collection of such data, in combination with our\nself-supervised learning-based approach allows for learning face models that\ngeneralize beyond the span of existing approaches. Our network design and loss\nfunctions ensure a disentangled parameterization of not only identity and\nalbedo, but also, for the first time, an expression basis. Our method also\nallows for in-the-wild monocular reconstruction at test time. We show that our\nlearned models better generalize and lead to higher quality image-based\nreconstructions than existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:51:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["R", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Seidel", "Hans-Peter", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2010.01944", "submitter": "Selma Rizvic", "authors": "Selma Rizvic, Dusanka Boskovic, Fabio Bruno, Barbara Davidde\n  Petriaggi, Sanda Sljivo, Marco Cozza", "title": "Actors in VR storytelling", "comments": "Pre-print version", "journal-ref": null, "doi": "10.1109/VS-Games.2019.8864520", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Reality (VR) storytelling enhances the immersion of users into\nvirtual environments (VE). Its use in virtual cultural heritage presentations\nhelps the revival of the genius loci (the spirit of the place) of cultural\nmonuments. This paper aims to show that the use of actors in VR storytelling\nadds to the quality of user experience and improves the edutainment value of\nvirtual cultural heritage applications. We will describe the Baiae dry visit\napplication which takes us to a time travel in the city considered by the Roman\nelite as \"Little Rome (Pusilla Roma)\" and presently is only partially preserved\nunder the sea.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:14:37 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Rizvic", "Selma", ""], ["Boskovic", "Dusanka", ""], ["Bruno", "Fabio", ""], ["Petriaggi", "Barbara Davidde", ""], ["Sljivo", "Sanda", ""], ["Cozza", "Marco", ""]]}, {"id": "2010.02015", "submitter": "Priyadarshini Kumari", "authors": "Praseedha Krishnan Aniyath, Sreeni Kamalalayam Gopalan, Priyadarshini\n  K and Subhasis Chaudhuri", "title": "Combined Hapto-Visual and Auditory Rendering of Cultural Heritage\n  Objects", "comments": "Accepted to ACCVw 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a multi-modal rendering framework comprising of\nhapto-visual and auditory data. The prime focus is to haptically render point\ncloud data representing virtual 3-D models of cultural significance and also to\nhandle their affine transformations. Cultural heritage objects could\npotentially be very large and one may be required to render the object at\nvarious scales of details. Further, surface effects such as texture and\nfriction are incorporated in order to provide a realistic haptic perception to\nthe users. Moreover, the proposed framework includes an appropriate sound\nsynthesis to bring out the acoustic properties of the object. It also includes\na graphical user interface with varied options such as choosing the desired\norientation of 3-D objects and selecting the desired level of spatial\nresolution adaptively at runtime. A fast, point proxy-based haptic rendering\ntechnique is proposed with proxy update loop running 100 times faster than the\nrequired haptic update frequency of 1 kHz. The surface properties are\nintegrated in the system by applying a bilateral filter on the depth data of\nthe virtual 3-D models. Position dependent sound synthesis is incorporated with\nthe incorporation of appropriate audio clips.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:43:46 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Aniyath", "Praseedha Krishnan", ""], ["Gopalan", "Sreeni Kamalalayam", ""], ["K", "Priyadarshini", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.02748", "submitter": "Jonah Probell", "authors": "Jonah Probell", "title": "Neural Generation of Blocks for Video Coding", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-trained generative neural networks (GNN) are very efficient at\ncompressing visual information for static images in their learned parameters\nbut not as efficient as inter- and intra-prediction for most video content.\nHowever, for content entering a frame, such as during panning or zooming out,\nand content with curves, irregular shapes, or fine detail, generation by a GNN\ncan give better compression efficiency (lower rate-distortion). This paper\nproposes encoding content-specific learned parameters of a GNN within a video\nbitstream at specific times and using the GNN to generate content for specific\nranges of blocks and frames. The blocks to generate are just the ones for which\ngeneration gives more efficient compression than inter- or intra- prediction.\nThis approach maximizes the usefulness of the information contained in the\nlearned parameters.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:40:33 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Probell", "Jonah", ""]]}, {"id": "2010.02822", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari, Sreeni K.G and Subhasis Chaudhuri", "title": "Scalable Rendering of Variable Density Point Cloud Data", "comments": "Accepted to World Haptics Conference (WHC), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel proxy-based method of the adaptive haptic\nrendering of a variable density 3D point cloud data at different levels of\ndetail without pre-computing the mesh structure. We also incorporate features\nlike rotation, translation, and friction to provide a better realistic\nexperience to the user. A proxy-based rendering technique is used to avoid the\npop-through problem while rendering thin parts of the object. Instead of a\npoint proxy, a spherical proxy of a variable radius is used, which avoids the\nsinking of proxy during the haptic interaction of sparse data. The radius of\nthe proxy is adaptively varied depending upon the local density of the point\ndata using kernel bandwidth estimation. During the interaction, the proxy moves\nin small steps tangentially over the point cloud such that the new position\nalways minimizes the distance between the proxy and the haptic interaction\npoint (HIP). The raw point cloud data re-sampled in a regular 3D lattice of\nvoxels are loaded to the haptic space after proper smoothing to avoid aliasing\neffects. The rendering technique is validated with several subjects, and it is\nobserved that this functionality supplements the user's experience by allowing\nthe user to interact with an object at multiple resolutions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:37:08 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 07:39:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["G", "Sreeni K.", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.02959", "submitter": "Herv\\'e Le Borgne", "authors": "Yannick Le Cacheux and Herv\\'e Le Borgne and Michel Crucianu", "title": "Using Sentences as Semantic Representations in Large Scale Zero-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning aims to recognize instances of unseen classes, for which\nno visual instance is available during training, by learning multimodal\nrelations between samples from seen classes and corresponding class semantic\nrepresentations. These class representations usually consist of either\nattributes, which do not scale well to large datasets, or word embeddings,\nwhich lead to poorer performance. A good trade-off could be to employ short\nsentences in natural language as class descriptions. We explore different\nsolutions to use such short descriptions in a ZSL setting and show that while\nsimple methods cannot achieve very good results with sentences alone, a\ncombination of usual word embeddings and sentences can significantly outperform\ncurrent state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:22:21 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cacheux", "Yannick Le", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Crucianu", "Michel", ""]]}, {"id": "2010.03169", "submitter": "Priyadarshini Kumari", "authors": "Sreeni K.G, Priyadarshini K, Praseedha A.K and Subhasis Chaudhuri", "title": "Haptic Rendering of Cultural Heritage Objects at Different Scales", "comments": "Accepted to EuroHaptics. arXiv admin note: text overlap with\n  arXiv:2010.02015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the issue of a virtual representation of objects of\ncultural heritage for haptic interaction. Our main focus is to provide haptic\naccess to artistic objects of any physical scale to the differently-abled\npeople. This is a low-cost system and, in conjunction with a stereoscopic\nvisual display, gives a better immersive experience even to the sighted\npersons. To achieve this, we propose a simple multilevel, proxy-based\nhapto-visual rendering technique for point cloud data, which includes the\nmuch-desired scalability feature which enables the users to change the scale of\nthe objects adaptively during the haptic interaction. For the proposed haptic\nrendering technique, the proxy updation loop runs at a rate 100 times faster\nthan the required haptic updation frequency of 1KHz. We observe that this\nfunctionality augments very well with the realism of the experience.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:32:47 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["G", "Sreeni K.", ""], ["K", "Priyadarshini", ""], ["K", "Praseedha A.", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.03183", "submitter": "Pavlos Sermpezis", "authors": "Savvas Kastanakis, Pavlos Sermpezis, Vasileios Kotronis, Daniel\n  Menasch\\'e, Thrasyvoulos Spyropoulos", "title": "Network-aware Recommendations in the Wild: Methodology, Realistic\n  Evaluations, Experiments", "comments": "arXiv admin note: text overlap with arXiv:1806.02704", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint caching and recommendation has been recently proposed as a new paradigm\nfor increasing the efficiency of mobile edge caching. Early findings\ndemonstrate significant gains for the network performance. However, previous\nworks evaluated the proposed schemes exclusively on simulation environments.\nHence, it still remains uncertain whether the claimed benefits would change in\nreal settings. In this paper, we propose a methodology that enables to evaluate\njoint network and recommendation schemes in real content services by only using\npublicly available information. We apply our methodology to the YouTube\nservice, and conduct extensive measurements to investigate the potential\nperformance gains. Our results show that significant gains can be achieved in\npractice; e.g., 8 to 10 times increase in the cache hit ratio from cache-aware\nrecommendations. Finally, we build an experimental testbed and conduct\nexperiments with real users; we make available our code and datasets to\nfacilitate further research. To our best knowledge, this is the first realistic\nevaluation (over a real service, with real measurements and user experiments)\nof the joint caching and recommendations paradigm. Our findings provide\nexperimental evidence for the feasibility and benefits of this paradigm,\nvalidate assumptions of previous works, and provide insights that can drive\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:18:00 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kastanakis", "Savvas", ""], ["Sermpezis", "Pavlos", ""], ["Kotronis", "Vasileios", ""], ["Menasch\u00e9", "Daniel", ""], ["Spyropoulos", "Thrasyvoulos", ""]]}, {"id": "2010.04645", "submitter": "Emmanouil Potetsianakis", "authors": "Emmanuel Thomas, Emmanouil Potetsianakis, Thomas Stockhammer, Imed\n  Bouazizi, Mary-Luc Champel", "title": "MPEG Media Enablers For Richer XR Experiences", "comments": null, "journal-ref": "IBC (2020)", "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of immersive media applications, the requirements for the\nrepresentation and the consumption of such content has dramatically increased.\nThe ever-increasing size of the media asset combined with the stringent\nmotion-to-photon latency requirement makes the equation of a high quality of\nexperience for XR streaming services difficult to solve. The MPEG-I standards\naim at facilitating the wide deployment of immersive applications. This paper\ndescribes part 13, Video Decoding Interface, and part 14, Scene Description for\nMPEG Media of MPEG-I which address decoder management and the virtual scene\ncomposition, respectively. These new parts intend to make complex media\nrendering operations and hardware resources management hidden from the\napplication, hence lowering the barrier for XR application to become mainstream\nand accessible to XR experience developers and designers. Both parts are\nexpected to be published by ISO at the end of 2021.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:39:08 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Thomas", "Emmanuel", ""], ["Potetsianakis", "Emmanouil", ""], ["Stockhammer", "Thomas", ""], ["Bouazizi", "Imed", ""], ["Champel", "Mary-Luc", ""]]}, {"id": "2010.04676", "submitter": "Antonio Busson", "authors": "Paulo R. C. Mendes, Eduardo S. Vieira, \\'Alan L. V. Guedes, Antonio J.\n  G. Busson, and S\\'ergio Colcher", "title": "A Clustering-Based Method for Automatic Educational Video Recommendation\n  Using Deep Face-Features of Lecturers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering and accessing specific content within educational video bases is\na challenging task, mainly because of the abundance of video content and its\ndiversity. Recommender systems are often used to enhance the ability to find\nand select content. But, recommendation mechanisms, especially those based on\ntextual information, exhibit some limitations, such as being error-prone to\nmanually created keywords or due to imprecise speech recognition. This paper\npresents a method for generating educational video recommendation using deep\nface-features of lecturers without identifying them. More precisely, we use an\nunsupervised face clustering mechanism to create relations among the videos\nbased on the lecturer's presence. Then, for a selected educational video taken\nas a reference, we recommend the ones where the presence of the same lecturers\nis detected. Moreover, we rank these recommended videos based on the amount of\ntime the referenced lecturers were present. For this task, we achieved a mAP\nvalue of 99.165%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:53:16 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Mendes", "Paulo R. C.", ""], ["Vieira", "Eduardo S.", ""], ["Guedes", "\u00c1lan L. V.", ""], ["Busson", "Antonio J. G.", ""], ["Colcher", "S\u00e9rgio", ""]]}, {"id": "2010.04862", "submitter": "Dong Wang", "authors": "Dong Wang", "title": "Remarks on Optimal Scores for Speaker Recognition", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we first establish the theory of optimal scores for speaker\nrecognition. Our analysis shows that the minimum Bayes risk (MBR) decisions for\nboth the speaker identification and speaker verification tasks can be based on\na normalized likelihood (NL). When the underlying generative model is a linear\nGaussian, the NL score is mathematically equivalent to the PLDA likelihood\nratio, and the empirical scores based on cosine distance and Euclidean distance\ncan be seen as approximations of this linear Gaussian NL score under some\nconditions. We discuss a number of properties of the NL score and perform a\nsimple simulation experiment to demonstrate the properties of the NL score.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 01:28:24 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 03:33:49 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wang", "Dong", ""]]}, {"id": "2010.05466", "submitter": "Di Hu", "authors": "Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding,\n  Weiyao Lin and Dejing Dou", "title": "Discriminative Sounding Objects Localization via Self-supervised\n  Audiovisual Matching", "comments": "To appear in NeurIPS 2020. Previous Title: Learning to\n  Discriminatively Localize Sounding Objects in a Cocktail-party Scenario", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminatively localizing sounding objects in cocktail-party, i.e., mixed\nsound scenes, is commonplace for humans, but still challenging for machines. In\nthis paper, we propose a two-stage learning framework to perform\nself-supervised class-aware sounding object localization. First, we propose to\nlearn robust object representations by aggregating the candidate sound\nlocalization results in the single source scenes. Then, class-aware object\nlocalization maps are generated in the cocktail-party scenarios by referring\nthe pre-learned object knowledge, and the sounding objects are accordingly\nselected by matching audio and visual object category distributions, where the\naudiovisual consistency is viewed as the self-supervised signal. Experimental\nresults in both realistic and synthesized cocktail-party videos demonstrate\nthat our model is superior in filtering out silent objects and pointing out the\nlocation of sounding objects of different classes. Code is available at\nhttps://github.com/DTaoo/Discriminative-Sounding-Objects-Localization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:51:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hu", "Di", ""], ["Qian", "Rui", ""], ["Jiang", "Minyue", ""], ["Tan", "Xiao", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""], ["Lin", "Weiyao", ""], ["Dou", "Dejing", ""]]}, {"id": "2010.05468", "submitter": "Dongxu Li", "authors": "Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Ben Swift, Hanna\n  Suominen, Hongdong Li", "title": "TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for\n  Sign Language Translation", "comments": "NeurIPS 2020 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language translation (SLT) aims to interpret sign video sequences into\ntext-based natural language sentences. Sign videos consist of continuous\nsequences of sign gestures with no clear boundaries in between. Existing SLT\nmodels usually represent sign visual features in a frame-wise manner so as to\navoid needing to explicitly segmenting the videos into isolated signs. However,\nthese methods neglect the temporal information of signs and lead to substantial\nambiguity in translation. In this paper, we explore the temporal semantic\nstructures of signvideos to learn more discriminative features. To this end, we\nfirst present a novel sign video segment representation which takes into\naccount multiple temporal granularities, thus alleviating the need for accurate\nvideo segmentation. Taking advantage of the proposed segment representation, we\ndevelop a novel hierarchical sign video feature learning method via a temporal\nsemantic pyramid network, called TSPNet. Specifically, TSPNet introduces an\ninter-scale attention to evaluate and enhance local semantic consistency of\nsign segments and an intra-scale attention to resolve semantic ambiguity by\nusing non-local video context. Experiments show that our TSPNet outperforms the\nstate-of-the-art with significant improvements on the BLEU score (from 9.58 to\n13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT\ndataset. Our implementation is available at\nhttps://github.com/verashira/TSPNet.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:58:09 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Dongxu", ""], ["Xu", "Chenchen", ""], ["Yu", "Xin", ""], ["Zhang", "Kaihao", ""], ["Swift", "Ben", ""], ["Suominen", "Hanna", ""], ["Li", "Hongdong", ""]]}, {"id": "2010.05760", "submitter": "Antonio Busson", "authors": "Antonio J G Busson, Paulo R C Mendes, Daniel de S Moraes, \\'Alvaro M\n  da Veiga, \\'Alan L V Guedes and S\\'ergio Colcher", "title": "Video Quality Enhancement Using Deep Learning-Based Prediction Models\n  for Quantized DCT Coefficients in MPEG I-frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have successfully applied some types of Convolutional Neural\nNetworks (CNNs) to reduce the noticeable distortion resulting from the lossy\nJPEG/MPEG compression technique. Most of them are built upon the processing\nmade on the spatial domain. In this work, we propose a MPEG video decoder that\nis purely based on the frequency-to-frequency domain: it reads the quantized\nDCT coefficients received from a low-quality I-frames bitstream and, using a\ndeep learning-based model, predicts the missing coefficients in order to\nrecompose the same frames with enhanced quality. In experiments with a video\ndataset, our best model was able to improve from frames with quantized DCT\ncoefficients corresponding to a Quality Factor (QF) of 10 to enhanced quality\nframes with QF slightly near to 20.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:41:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Busson", "Antonio J G", ""], ["Mendes", "Paulo R C", ""], ["Moraes", "Daniel de S", ""], ["da Veiga", "\u00c1lvaro M", ""], ["Guedes", "\u00c1lan L V", ""], ["Colcher", "S\u00e9rgio", ""]]}, {"id": "2010.07637", "submitter": "Yuzhao Mao", "authors": "Yuzhao Mao, Qi Sun, Guang Liu, Xiaojie Wang, Weiguo Gao, Xuan Li,\n  Jianping Shen", "title": "DialogueTRM: Exploring the Intra- and Inter-Modal Emotional Behaviors in\n  the Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion Recognition in Conversations (ERC) is essential for building\nempathetic human-machine systems. Existing studies on ERC primarily focus on\nsummarizing the context information in a conversation, however, ignoring the\ndifferentiated emotional behaviors within and across different modalities.\nDesigning appropriate strategies that fit the differentiated multi-modal\nemotional behaviors can produce more accurate emotional predictions. Thus, we\npropose the DialogueTransformer to explore the differentiated emotional\nbehaviors from the intra- and inter-modal perspectives. For intra-modal, we\nconstruct a novel Hierarchical Transformer that can easily switch between\nsequential and feed-forward structures according to the differentiated context\npreference within each modality. For inter-modal, we constitute a novel\nMulti-Grained Interactive Fusion that applies both neuron- and vector-grained\nfeature interactions to learn the differentiated contributions across all\nmodalities. Experimental results show that DialogueTRM outperforms the\nstate-of-the-art by a significant margin on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 10:10:41 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Mao", "Yuzhao", ""], ["Sun", "Qi", ""], ["Liu", "Guang", ""], ["Wang", "Xiaojie", ""], ["Gao", "Weiguo", ""], ["Li", "Xuan", ""], ["Shen", "Jianping", ""]]}, {"id": "2010.07739", "submitter": "Yiting Xia", "authors": "Yiting Xia, Yiwei Jiang, Tao Ye", "title": "Music Classification in MIDI Format based on LSTM Mdel", "comments": "in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music classification between music made by AI or human composers can be done\nby deep learning networks. We first transformed music samples in midi format to\nnatural language sequences, then classified these samples by mLSTM\n(multiplicative Long Short Term Memory) + logistic regression. The accuracy of\nthe result evaluated by 10-fold cross validation can reach 90%. Our work\nindicates that music generated by AI and human composers do have different\ncharacteristics, which can be learned by deep learning networks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 13:30:40 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Xia", "Yiting", ""], ["Jiang", "Yiwei", ""], ["Ye", "Tao", ""]]}, {"id": "2010.07775", "submitter": "Zexu Pan", "authors": "Zexu Pan, Ruijie Tao, Chenglin Xu, Haizhou Li", "title": "Muse: Multi-modal target speaker extraction with visual cues", "comments": "Accepted by ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker extraction algorithm relies on the speech sample from the target\nspeaker as the reference point to focus its attention. Such a reference speech\nis typically pre-recorded. On the other hand, the temporal synchronization\nbetween speech and lip movement also serves as an informative cue. Motivated by\nthis idea, we study a novel technique to use speech-lip visual cues to extract\nreference target speech directly from mixture speech during inference time,\nwithout the need of pre-recorded reference speech. We propose a multi-modal\nspeaker extraction network, named MuSE, that is conditioned only on a lip image\nsequence. MuSE not only outperforms other competitive baselines in terms of\nSI-SDR and PESQ, but also shows consistent improvement in cross-dataset\nevaluations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:10:37 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 03:52:30 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 04:40:43 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Pan", "Zexu", ""], ["Tao", "Ruijie", ""], ["Xu", "Chenglin", ""], ["Li", "Haizhou", ""]]}, {"id": "2010.08021", "submitter": "Udit Arora", "authors": "Aman Khullar, Udit Arora", "title": "MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical\n  Attention", "comments": "To appear in the first EMNLP Workshop on NLP Beyond Text, 2020. Aman\n  Khullar and Udit Arora have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents MAST, a new model for Multimodal Abstractive Text\nSummarization that utilizes information from all three modalities -- text,\naudio and video -- in a multimodal video. Prior work on multimodal abstractive\ntext summarization only utilized information from the text and video\nmodalities. We examine the usefulness and challenges of deriving information\nfrom the audio modality and present a sequence-to-sequence trimodal\nhierarchical attention-based model that overcomes these challenges by letting\nthe model pay more attention to the text modality. MAST outperforms the current\nstate of the art model (video-text) by 2.51 points in terms of Content F1 score\nand 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal\nlanguage understanding.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:08:20 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Khullar", "Aman", ""], ["Arora", "Udit", ""]]}, {"id": "2010.08091", "submitter": "Hongru Liang", "authors": "Hongru Liang, Wenqiang Lei, Paul Yaozhu Chan, Zhenglu Yang, Maosong\n  Sun, Tat-Seng Chua", "title": "PiRhDy: Learning Pitch-, Rhythm-, and Dynamics-aware Embeddings for\n  Symbolic Music", "comments": "ACM Multimedia 2020 -- best paper", "journal-ref": null, "doi": "10.1145/3394171.3414032", "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Definitive embeddings remain a fundamental challenge of computational\nmusicology for symbolic music in deep learning today. Analogous to natural\nlanguage, music can be modeled as a sequence of tokens. This motivates the\nmajority of existing solutions to explore the utilization of word embedding\nmodels to build music embeddings. However, music differs from natural languages\nin two key aspects: (1) musical token is multi-faceted -- it comprises of\npitch, rhythm and dynamics information; and (2) musical context is\ntwo-dimensional -- each musical token is dependent on both melodic and harmonic\ncontexts. In this work, we provide a comprehensive solution by proposing a\nnovel framework named PiRhDy that integrates pitch, rhythm, and dynamics\ninformation seamlessly. PiRhDy adopts a hierarchical strategy which can be\ndecomposed into two steps: (1) token (i.e., note event) modeling, which\nseparately represents pitch, rhythm, and dynamics and integrates them into a\nsingle token embedding; and (2) context modeling, which utilizes melodic and\nharmonic knowledge to train the token embedding. A thorough study was made on\neach component and sub-strategy of PiRhDy. We further validate our embeddings\nin three downstream tasks -- melody completion, accompaniment suggestion, and\ngenre classification. Results indicate a significant advancement of the neural\napproach towards symbolic music as well as PiRhDy's potential as a pretrained\ntool for a broad range of symbolic music applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 01:25:48 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Liang", "Hongru", ""], ["Lei", "Wenqiang", ""], ["Chan", "Paul Yaozhu", ""], ["Yang", "Zhenglu", ""], ["Sun", "Maosong", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2010.08119", "submitter": "Xinyu Huang", "authors": "Xinyu Huang, Lijun He, Xing Chen, Liejun Wang, Fan Li", "title": "Revenue and Energy Efficiency-Driven Delay Constrained Computing Task\n  Offloading and Resource Allocation in a Vehicular Edge Computing Network: A\n  Deep Reinforcement Learning Approach", "comments": "15 pages, 13 figures, submitted to IEEE Internet of Things", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For in-vehicle application,task type and vehicle state information, i.e.,\nvehicle speed, bear a significant impact on the task delay requirement.\nHowever, the joint impact of task type and vehicle speed on the task delay\nconstraint has not been studied, and this lack of study may cause a mismatch\nbetween the requirement of the task delay and allocated computation and\nwireless resources. In this paper, we propose a joint task type and vehicle\nspeed-aware task offloading and resource allocation strategy to decrease the\nvehicl's energy cost for executing tasks and increase the revenue of the\nvehicle for processing tasks within the delay constraint. First, we establish\nthe joint task type and vehicle speed-aware delay constraint model. Then, the\ndelay, energy cost and revenue for task execution in the vehicular edge\ncomputing (VEC) server, local terminal and terminals of other vehicles are\ncalculated. Based on the energy cost and revenue from task execution,the\nutility function of the vehicle is acquired. Next, we formulate a joint\noptimization of task offloading and resource allocation to maximize the utility\nlevel of the vehicles subject to the constraints of task delay, computation\nresources and wireless resources. To obtain a near-optimal solution of the\nformulated problem, a joint offloading and resource allocation based on the\nmulti-agent deep deterministic policy gradient (JORA-MADDPG) algorithm is\nproposed to maximize the utility level of vehicles. Simulation results show\nthat our algorithm can achieve superior performance in task completion delay,\nvehicles' energy cost and processing revenue.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:45:05 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Huang", "Xinyu", ""], ["He", "Lijun", ""], ["Chen", "Xing", ""], ["Wang", "Liejun", ""], ["Li", "Fan", ""]]}, {"id": "2010.08123", "submitter": "You Li", "authors": "You Li, Zhuowen Lin", "title": "Melody Classifier with Stacked-LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attempts to use generative models for music generation have been common in\nrecent years, and some of them have achieved good results. Pieces generated by\nsome of these models are almost indistinguishable from those being composed by\nhuman composers. However, the research on the evaluation system for\nmachine-generated music is still at a relatively early stage, and there is no\nuniform standard for such tasks. This paper proposes a stacked-LSTM binary\nclassifier based on a language model, which can be used to distinguish the\nhuman composer's work from the machine-generated melody by learning the MIDI\nfile's pitch, position, and duration.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:01:14 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 20:37:52 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Li", "You", ""], ["Lin", "Zhuowen", ""]]}, {"id": "2010.08737", "submitter": "Giorgos Kordopatis-Zilos Mr.", "authors": "Pavlos Avgoustinakis, Giorgos Kordopatis-Zilos, Symeon Papadopoulos,\n  Andreas L. Symeonidis, Ioannis Kompatsiaris", "title": "Audio-based Near-Duplicate Video Retrieval with Audio Similarity\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of audio-based near-duplicate video\nretrieval. We propose the Audio Similarity Learning (AuSiL) approach that\neffectively captures temporal patterns of audio similarity between video pairs.\nFor the robust similarity calculation between two videos, we first extract\nrepresentative audio-based video descriptors by leveraging transfer learning\nbased on a Convolutional Neural Network (CNN) trained on a large scale dataset\nof audio events, and then we calculate the similarity matrix derived from the\npairwise similarity of these descriptors. The similarity matrix is subsequently\nfed to a CNN network that captures the temporal structures existing within its\ncontent. We train our network following a triplet generation process and\noptimizing the triplet loss function. To evaluate the effectiveness of the\nproposed approach, we have manually annotated two publicly available video\ndatasets based on the audio duplicity between their videos. The proposed\napproach achieves very competitive results compared to three state-of-the-art\nmethods. Also, unlike the competing methods, it is very robust to the retrieval\nof audio duplicates generated with speed transformations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 08:12:18 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 12:33:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Avgoustinakis", "Pavlos", ""], ["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Symeonidis", "Andreas L.", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2010.08919", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Qian Lin, Jan P. Allebach", "title": "Boosting High-Level Vision with Joint Compression Artifacts Reduction\n  and Super-Resolution", "comments": "8 pages, 6 figures, 5 tables. Accepted by the 25th ICPR (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limits of bandwidth and storage space, digital images are usually\ndown-scaled and compressed when transmitted over networks, resulting in loss of\ndetails and jarring artifacts that can lower the performance of high-level\nvisual tasks. In this paper, we aim to generate an artifact-free\nhigh-resolution image from a low-resolution one compressed with an arbitrary\nquality factor by exploring joint compression artifacts reduction (CAR) and\nsuper-resolution (SR) tasks. First, we propose a context-aware joint CAR and SR\nneural network (CAJNN) that integrates both local and non-local features to\nsolve CAR and SR in one-stage. Finally, a deep reconstruction network is\nadopted to predict high quality and high-resolution images. Evaluation on CAR\nand SR benchmark datasets shows that our CAJNN model outperforms previous\nmethods and also takes 26.2% shorter runtime. Based on this model, we explore\naddressing two critical challenges in high-level computer vision: optical\ncharacter recognition of low-resolution texts, and extremely tiny face\ndetection. We demonstrate that CAJNN can serve as an effective image\npreprocessing method and improve the accuracy for real-scene text recognition\n(from 85.30% to 85.75%) and the average precision for tiny face detection (from\n0.317 to 0.611).\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 04:17:08 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:26:40 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Lin", "Qian", ""], ["Allebach", "Jan P.", ""]]}, {"id": "2010.09235", "submitter": "Haoran Wei", "authors": "Haoran Wei, Fei Tao, Runze Su, Sen Yang, Ji Liu", "title": "Ensemble Chinese End-to-End Spoken Language Understanding for Abnormal\n  Event Detection from audio stream", "comments": "Submitting to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional spoken language understanding (SLU) consist of two stages, the\nfirst stage maps speech to text by automatic speech recognition (ASR), and the\nsecond stage maps text to intent by natural language understanding (NLU).\nEnd-to-end SLU maps speech directly to intent through a single deep learning\nmodel. Previous end-to-end SLU models are primarily used for English\nenvironment due to lacking large scale SLU dataset in Chines, and use only one\nASR model to extract features from speech. With the help of Kuaishou\ntechnology, a large scale SLU dataset in Chinese is collected to detect\nabnormal event in their live audio stream. Based on this dataset, this paper\nproposed a ensemble end-to-end SLU model used for Chinese environment. This\nensemble SLU models extracted hierarchies features using multiple pre-trained\nASR models, leading to better representation of phoneme level and word level\ninformation. This proposed approached achieve 9.7% increase of accuracy\ncompared to previous end-to-end SLU model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 05:59:14 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wei", "Haoran", ""], ["Tao", "Fei", ""], ["Su", "Runze", ""], ["Yang", "Sen", ""], ["Liu", "Ji", ""]]}, {"id": "2010.09290", "submitter": "Fang Tao Li", "authors": "Fangtao Li, Wenzhe Wang, Zihe Liu, Haoran Wang, Chenghao Yan, Bin Wu", "title": "Frame Aggregation and Multi-Modal Fusion Framework for Video-Based\n  Person Recognition", "comments": "Accepted by MMM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person recognition is challenging due to persons being blocked\nand blurred, and the variation of shooting angle. Previous research always\nfocused on person recognition on still images, ignoring similarity and\ncontinuity between video frames. To tackle the challenges above, we propose a\nnovel Frame Aggregation and Multi-Modal Fusion (FAMF) framework for video-based\nperson recognition, which aggregates face features and incorporates them with\nmulti-modal information to identify persons in videos. For frame aggregation,\nwe propose a novel trainable layer based on NetVLAD (named AttentionVLAD),\nwhich takes arbitrary number of features as input and computes a fixed-length\naggregation feature based on feature quality. We show that introducing an\nattention mechanism to NetVLAD can effectively decrease the impact of\nlow-quality frames. For the multi-model information of videos, we propose a\nMulti-Layer Multi-Modal Attention (MLMA) module to learn the correlation of\nmulti-modality by adaptively updating Gram matrix. Experimental results on\niQIYI-VID-2019 dataset show that our framework outperforms other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:06:40 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 09:01:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Fangtao", ""], ["Wang", "Wenzhe", ""], ["Liu", "Zihe", ""], ["Wang", "Haoran", ""], ["Yan", "Chenghao", ""], ["Wu", "Bin", ""]]}, {"id": "2010.09489", "submitter": "Dorien Herremans", "authors": "Dorien Herremans, Tom Bergmans", "title": "Hit Song Prediction Based on Early Adopter Data and Audio Features", "comments": null, "journal-ref": "The 18th International Society for Music Information Retrieval\n  Conference (ISMIR)2018 - LBD", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Billions of USD are invested in new artists and songs by the music industry\nevery year. This research provides a new strategy for assessing the hit\npotential of songs, which can help record companies support their investment\ndecisions. A number of models were developed that use both audio data, and a\nnovel feature based on social media listening behaviour. The results show that\nmodels based on early adopter behaviour perform well when predicting top 20\ndance hits.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:42:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Herremans", "Dorien", ""], ["Bergmans", "Tom", ""]]}, {"id": "2010.09641", "submitter": "Tony Zhao", "authors": "Tony Zhao, Jaeyoung Choi, Gerald Friedland", "title": "DIME: An Online Tool for the Visual Comparison of Cross-Modal Retrieval\n  Models", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-37734-2_61", "report-no": null, "categories": "cs.MM cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-modal retrieval relies on accurate models to retrieve relevant results\nfor queries across modalities such as image, text, and video. In this paper, we\nbuild upon previous work by tackling the difficulty of evaluating models both\nquantitatively and qualitatively quickly. We present DIME (Dataset, Index,\nModel, Embedding), a modality-agnostic tool that handles multimodal datasets,\ntrained models, and data preprocessors to support straightforward model\ncomparison with a web browser graphical user interface. DIME inherently\nsupports building modality-agnostic queryable indexes and extraction of\nrelevant feature embeddings, and thus effectively doubles as an efficient\ncross-modal tool to explore and search through datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:35:30 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhao", "Tony", ""], ["Choi", "Jaeyoung", ""], ["Friedland", "Gerald", ""]]}, {"id": "2010.09907", "submitter": "Majid Harouni", "authors": "Majid Harouni and Hadi Yazdani Baghmaleki", "title": "Color Image Segmentation Metrics", "comments": "19 pages, 11 figures, 6 tables, 29 equations, book chapter, 2 authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An automatic image segmentation procedure is an inevitable part of many image\nanalyses and computer vision which deeply affect the rest of the system;\ntherefore, a set of interactive segmentation evaluation methods can\nsubstantially simplify the system development process. This entry presents the\nstate of the art of quantitative evaluation metrics for color image\nsegmentation methods by performing an analytical and comparative review of the\nmeasures. The decision-making process in selecting a suitable evaluation metric\nis still very serious because each metric tends to favor a different\nsegmentation method for each benchmark dataset. Furthermore, a conceptual\ncomparison of these metrics is provided at a high level of abstraction and is\ndiscussed for understanding the quantitative changes in different image\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:47:32 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Harouni", "Majid", ""], ["Baghmaleki", "Hadi Yazdani", ""]]}, {"id": "2010.09925", "submitter": "Pingping Zhang Dr", "authors": "Yinjie Lei and Duo Peng and Pingping Zhang and Qiuhong Ke and Haifeng\n  Li", "title": "Hierarchical Paired Channel Fusion Network for Street Scene Change\n  Detection", "comments": "To appear in Transactions on Image Processing, including 13 pages, 13\n  figures, 9 tables", "journal-ref": null, "doi": "10.1109/TIP.2020.3031173", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street Scene Change Detection (SSCD) aims to locate the changed regions\nbetween a given street-view image pair captured at different times, which is an\nimportant yet challenging task in the computer vision community. The intuitive\nway to solve the SSCD task is to fuse the extracted image feature pairs, and\nthen directly measure the dissimilarity parts for producing a change map.\nTherefore, the key for the SSCD task is to design an effective feature fusion\nmethod that can improve the accuracy of the corresponding change maps. To this\nend, we present a novel Hierarchical Paired Channel Fusion Network (HPCFNet),\nwhich utilizes the adaptive fusion of paired feature channels. Specifically,\nthe features of a given image pair are jointly extracted by a Siamese\nConvolutional Neural Network (SCNN) and hierarchically combined by exploring\nthe fusion of channel pairs at multiple feature levels. In addition, based on\nthe observation that the distribution of scene changes is diverse, we further\npropose a Multi-Part Feature Learning (MPFL) strategy to detect diverse\nchanges. Based on the MPFL strategy, our framework achieves a novel approach to\nadapt to the scale and location diversities of the scene change regions.\nExtensive experiments on three public datasets (i.e., PCD, VL-CMU-CD and\nCDnet2014) demonstrate that the proposed framework achieves superior\nperformance which outperforms other state-of-the-art methods with a\nconsiderable margin.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:51:28 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Lei", "Yinjie", ""], ["Peng", "Duo", ""], ["Zhang", "Pingping", ""], ["Ke", "Qiuhong", ""], ["Li", "Haifeng", ""]]}, {"id": "2010.10135", "submitter": "Hartmut Koenitz", "authors": "Hartmut Koenitz, Mirjam Palosaari Eladhari, Sandy Louchart, Frank Nack", "title": "INDCOR white paper 1: A shared vocabulary for IDN (Interactive Digital\n  Narratives)", "comments": null, "journal-ref": null, "doi": null, "report-no": "INDCOR-DL-no: WP-01-1.0", "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COST Action 18230 INDCOR (Interactive Narrative Design for Complexity\nRepresentations) is an interdisciplinary network of researchers and\npractitioners intended to further the use of interactive digital narratives\n(IDN1) to represent highly complex topics. IDN possess crucial advantages in\nthis regard, but more knowledge is needed to realize these advantages in broad\nusage by media producers and the general public. The lack of a shared\nvocabulary is a crucial obstacle on the path to a generalized, accessible body\nof IDN knowledge. This white paper frames the situation from the perspective of\nINDCOR and describes the creation of an online encyclopedia as a means to\novercome this issue. Two similar and successful projects (The Living Handbook\nof Narratology and the Stanford Encyclopedia of Philosophy) serve as examples\nfor this effort, showing how community-authored encyclopedias can provide\nhigh-quality content. The authors introduce a taxonomy based on an overarching\nanalytical framework (SPP model) as the foundational element of the\nencyclopedia, and detail editorial procedures for the project, including a\npeer-review process, designed to assure high academic quality and relevance of\nencyclopedia entries. Also, a sample entry provides guidance for authors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:00:46 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 00:20:28 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Koenitz", "Hartmut", ""], ["Eladhari", "Mirjam Palosaari", ""], ["Louchart", "Sandy", ""], ["Nack", "Frank", ""]]}, {"id": "2010.10144", "submitter": "Alan Smeaton", "authors": "Alan F. Smeaton, Naveen Garaga Krishnamurthy, Amruth Hebbasuru\n  Suryanarayana", "title": "Keystroke Dynamics as Part of Lifelogging", "comments": "Accepted to 27th International Conference on Multimedia Modeling,\n  Prague, Czech Republic, June 2021", "journal-ref": null, "doi": "10.1007/978-3-030-67835-7_16", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the case for including keystroke dynamics in\nlifelogging. We describe how we have used a simple keystroke logging\napplication called Loggerman, to create a dataset of longitudinal keystroke\ntiming data spanning a period of more than 6 months for 4 participants. We\nperform a detailed analysis of this data by examining the timing information\nassociated with bigrams or pairs of adjacently-typed alphabetic characters. We\nshow how there is very little day-on-day variation of the keystroke timing\namong the top-200 bigrams for some participants and for others there is a lot\nand this correlates with the amount of typing each would do on a daily basis.\nWe explore how daily variations could correlate with sleep score from the\nprevious night but find no significant relation-ship between the two. Finally\nwe describe the public release of this data as well including as a series of\npointers for future work including correlating keystroke dynamics with mood and\nfatigue during the day.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:23:45 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Smeaton", "Alan F.", ""], ["Krishnamurthy", "Naveen Garaga", ""], ["Suryanarayana", "Amruth Hebbasuru", ""]]}, {"id": "2010.10637", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Linghao Jin, Xu Han, Jane You", "title": "Mutual Information Regularized Identity-aware Facial\n  ExpressionRecognition in Compressed Video", "comments": "Published in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to extract effective expression representations that invariant to the\nidentity-specific attributes is a long-lasting problem for facial expression\nrecognition (FER). Most of the previous methods process the RGB images of a\nsequence, while we argue that the off-the-shelf and valuable expression-related\nmuscle movement is already embedded in the compression format. In this paper,\nwe target to explore the inter-subject variations eliminated facial expression\nrepresentation in the compressed video domain. In the up to two orders of\nmagnitude compressed domain, we can explicitly infer the expression from the\nresidual frames and possibly extract identity factors from the I frame with a\npre-trained face recognition network. By enforcing the marginal independence of\nthem, the expression feature is expected to be purer for the expression and be\nrobust to identity shifts. Specifically, we propose a novel collaborative\nmin-min game for mutual information (MI) minimization in latent space. We do\nnot need the identity label or multiple expression samples from the same person\nfor identity elimination. Moreover, when the apex frame is annotated in the\ndataset, the complementary constraint can be further added to regularize the\nfeature-level game. In testing, only the compressed residual frames are\nrequired to achieve expression prediction. Our solution can achieve comparable\nor better performance than the recent decoded image-based methods on the\ntypical FER benchmarks with about 3 times faster inference.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 21:42:18 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 15:09:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Jin", "Linghao", ""], ["Han", "Xu", ""], ["You", "Jane", ""]]}, {"id": "2010.10658", "submitter": "Peter Zelchenko", "authors": "Peter Zelchenko, Xiaohan Fu, Xiangqian Li, Alex Ivanov, Zhenyu Gu", "title": "Display object alignment may influence location recall in unexpected\n  ways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a presumption in human-computer interaction that laying out menus\nand most other material in neat rows and columns helps users get work done. The\nrule has been so implicit in the field of design as to allow for no debate.\nHowever, the idea that perfect collinearity benefits creates an advantage for\nboth either search and or recall has rarely been tested. Drawing from separate\nbranches of cognitive literature, we tested a minimal brainstorming interface\nwith either aligned or eccentrically arranged layouts on 96 college students.\nIncidental exact recall of recently worked locations improved in the eccentric\ncondition. And in both conditions there were frequent near-miss recall errors\nto neighboring aligned objects and groups of objects. Further analysis found\nonly marginal performance advantages specifically for females with the\neccentric design. However, NASA-TLX subjective measures showed that in\neccentric, females reported higher performance, less effort, and yet also\nhigher frustration; while males reported lower performance with about the same\neffort, and lower frustration.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:51:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zelchenko", "Peter", ""], ["Fu", "Xiaohan", ""], ["Li", "Xiangqian", ""], ["Ivanov", "Alex", ""], ["Gu", "Zhenyu", ""]]}, {"id": "2010.10706", "submitter": "Yuanjie Dang", "authors": "Yuanjie Dang", "title": "Can We Enable the Drone to be a Filmmaker?", "comments": "7 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones are enabling new forms of cinematography. However, quadrotor\ncinematography requires accurate comprehension of the scene, technical skill of\nflying, artistic skill of composition and simultaneous realization of all the\nrequirements in real time. These requirements could pose real challenge to\ndrone amateurs because unsuitable camera viewpoint and motion could result in\nunpleasing visual composition and affect the target's visibility. In this\npaper, we propose a novel autonomous drone camera system which captures action\nscenes using proper camera viewpoint and motion. The key novelty is that our\nsystem can dynamically generate smooth drone camera trajectory associated with\nhuman movement while obeying visual composition principles. We evaluate the\nperformance of our cinematography system on simulation and real scenario. The\nexperimental results demonstrate that our system can capture more expressive\nvideo footage of human action than that of the state-of-the-art drone camera\nsystem. To the best of our knowledge, this is the first cinematography system\nthat enables people to leverage the mobility of quadrotor to autonomously\ncapture high-quality footage of action scene based on subject's movements.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 01:36:28 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Dang", "Yuanjie", ""]]}, {"id": "2010.10721", "submitter": "Lu Xu", "authors": "Lu Xu and Jinhai Xiang", "title": "ComboLoss for Facial Attractiveness Analysis with Squeeze-and-Excitation\n  Networks", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss function is crucial for model training and feature representation\nlearning, conventional models usually regard facial attractiveness recognition\ntask as a regression problem, and adopt MSE loss or Huber variant loss as\nsupervision to train a deep convolutional neural network (CNN) to predict\nfacial attractiveness score. Little work has been done to systematically\ncompare the performance of diverse loss functions. In this paper, we firstly\nsystematically analyze model performance under diverse loss functions. Then a\nnovel loss function named ComboLoss is proposed to guide the SEResNeXt50\nnetwork. The proposed method achieves state-of-the-art performance on SCUT-FBP,\nHotOrNot and SCUT-FBP5500 datasets with an improvement of 1.13%, 2.1% and 0.57%\ncompared with prior arts, respectively. Code and models are available at\nhttps://github.com/lucasxlu/ComboLoss.git.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 02:27:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Xu", "Lu", ""], ["Xiang", "Jinhai", ""]]}, {"id": "2010.11098", "submitter": "Konstantinos Drossos", "authors": "An Tran and Konstantinos Drossos and Tuomas Virtanen", "title": "WaveTransformer: A Novel Architecture for Audio Captioning Based on\n  Learning Temporal and Time-Frequency Information", "comments": "Submitted for review at ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated audio captioning (AAC) is a novel task, where a method takes as an\ninput an audio sample and outputs a textual description (i.e. a caption) of its\ncontents. Most AAC methods are adapted from from image captioning of machine\ntranslation fields. In this work we present a novel AAC novel method,\nexplicitly focused on the exploitation of the temporal and time-frequency\npatterns in audio. We employ three learnable processes for audio encoding, two\nfor extracting the local and temporal information, and one to merge the output\nof the previous two processes. To generate the caption, we employ the widely\nused Transformer decoder. We assess our method utilizing the freely available\nsplits of Clotho dataset. Our results increase previously reported highest\nSPIDEr to 17.3, from 16.2.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:02:25 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Tran", "An", ""], ["Drossos", "Konstantinos", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "2010.11550", "submitter": "Keyu Wen", "authors": "Keyu Wen, Xiaodong Gu, Qingrong Cheng", "title": "Learning Dual Semantic Relations with Graph Attention for Image-Text\n  Matching", "comments": "14pages, 9 figures. Accepted at: IEEE Transactions on Circuits and\n  Systems for Video Technology (Early Access Print) | |Codes Available at:\n  https://github.com/kywen1119/DSRAN", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3030656", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-Text Matching is one major task in cross-modal information processing.\nThe main challenge is to learn the unified visual and textual representations.\nPrevious methods that perform well on this task primarily focus on not only the\nalignment between region features in images and the corresponding words in\nsentences, but also the alignment between relations of regions and relational\nwords. However, the lack of joint learning of regional features and global\nfeatures will cause the regional features to lose contact with the global\ncontext, leading to the mismatch with those non-object words which have global\nmeanings in some sentences. In this work, in order to alleviate this issue, it\nis necessary to enhance the relations between regions and the relations between\nregional and global concepts to obtain a more accurate visual representation so\nas to be better correlated to the corresponding text. Thus, a novel multi-level\nsemantic relations enhancement approach named Dual Semantic Relations Attention\nNetwork(DSRAN) is proposed which mainly consists of two modules, separate\nsemantic relations module and the joint semantic relations module. DSRAN\nperforms graph attention in both modules respectively for region-level\nrelations enhancement and regional-global relations enhancement at the same\ntime. With these two modules, different hierarchies of semantic relations are\nlearned simultaneously, thus promoting the image-text matching process by\nproviding more information for the final visual representation. Quantitative\nexperimental results have been performed on MS-COCO and Flickr30K and our\nmethod outperforms previous approaches by a large margin due to the\neffectiveness of the dual semantic relations learning scheme. Codes are\navailable at https://github.com/kywen1119/DSRAN.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:21:32 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wen", "Keyu", ""], ["Gu", "Xiaodong", ""], ["Cheng", "Qingrong", ""]]}, {"id": "2010.11732", "submitter": "Paulo Renato Conceicao Mendes", "authors": "Paulo R C Mendes, Antonio J G Busson, S\\'ergio Colcher, Daniel\n  Schwabe, \\'Alan L V Guedes, Carlos Laufer", "title": "A Cluster-Matching-Based Method for Video Face Recognition", "comments": "13 pages", "journal-ref": null, "doi": "10.1145/3428658.3430967", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition systems are present in many modern solutions and thousands\nof applications in our daily lives. However, current solutions are not easily\nscalable, especially when it comes to the addition of new targeted people. We\npropose a cluster-matching-based approach for face recognition in video. In our\napproach, we use unsupervised learning to cluster the faces present in both the\ndataset and targeted videos selected for face recognition. Moreover, we design\na cluster matching heuristic to associate clusters in both sets that is also\ncapable of identifying when a face belongs to a non-registered person. Our\nmethod has achieved a recall of 99.435% and a precision of 99.131% in the task\nof video face recognition. Besides performing face recognition, it can also be\nused to determine the video segments where each person is present.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:44:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Mendes", "Paulo R C", ""], ["Busson", "Antonio J G", ""], ["Colcher", "S\u00e9rgio", ""], ["Schwabe", "Daniel", ""], ["Guedes", "\u00c1lan L V", ""], ["Laufer", "Carlos", ""]]}, {"id": "2010.11734", "submitter": "Menghan Hu", "authors": "Yunlu Wang, Cheng Yang, Menghan Hu, Jian Zhang, Qingli Li, Guangtao\n  Zhai, Xiao-Ping Zhang", "title": "Identification of deep breath while moving forward based on multiple\n  body regions and graph signal analysis", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unobtrusive solution that can automatically identify\ndeep breath when a person is walking past the global depth camera. Existing\nnon-contact breath assessments achieve satisfactory results under restricted\nconditions when human body stays relatively still. When someone moves forward,\nthe breath signals detected by depth camera are hidden within signals of trunk\ndisplacement and deformation, and the signal length is short due to the short\nstay time, posing great challenges for us to establish models. To overcome\nthese challenges, multiple region of interests (ROIs) based signal extraction\nand selection method is proposed to automatically obtain the signal informative\nto breath from depth video. Subsequently, graph signal analysis (GSA) is\nadopted as a spatial-temporal filter to wipe the components unrelated to\nbreath. Finally, a classifier for identifying deep breath is established based\non the selected breath-informative signal. In validation experiments, the\nproposed approach outperforms the comparative methods with the accuracy,\nprecision, recall and F1 of 75.5%, 76.2%, 75.0% and 75.2%, respectively. This\nsystem can be extended to public places to provide timely and ubiquitous help\nfor those who may have or are going through physical or mental trouble.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 08:26:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wang", "Yunlu", ""], ["Yang", "Cheng", ""], ["Hu", "Menghan", ""], ["Zhang", "Jian", ""], ["Li", "Qingli", ""], ["Zhai", "Guangtao", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "2010.11744", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young and David Murphy and Jeffrey Weeter", "title": "A Qualitative Analysis of Haptic Feedback in Music Focused Exercises", "comments": "6 pages", "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2017", "doi": "10.5281/zenodo.1176222", "report-no": null, "categories": "cs.HC cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the findings of a pilot-study that analysed the role of haptic\nfeedback in a musical context. To examine the role of haptics in Digital\nMusical Instrument (DMI) design an experiment was formulated to measure the\nusers' perception of device usability across four separate feedback stages:\nfully haptic (force and tactile combined), constant force only, vibrotactile\nonly, and no feedback. The study was piloted over extended periods with the\nintention of exploring the application and integration of DMIs in real-world\nmusical contexts. Applying a music orientated analysis of this type enabled the\ninvestigative process to not only take place over a comprehensive period, but\nallowed for the exploration of DMI integration in everyday compositional\npractices. As with any investigation that involves creativity, it was important\nthat the participants did not feel rushed or restricted. That is, they were\ngiven sufficient time to explore and assess the different feedback types\nwithout constraint. This provided an accurate and representational set of\nqualitative data for validating the participants' experience with the different\nfeedback types they were presented with.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:00:14 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:34:31 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Young", "Gareth W.", ""], ["Murphy", "David", ""], ["Weeter", "Jeffrey", ""]]}, {"id": "2010.11886", "submitter": "K L Bhanu Moorthy", "authors": "K L Bhanu Moorthy, Moneish Kumar, Ramanathan Subramaniam, Vineet\n  Gandhi", "title": "GAZED- Gaze-guided Cinematic Editing of Wide-Angle Monocular Video\n  Recordings", "comments": "10 pages", "journal-ref": "In Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems (CHI '20). Association for Computing Machinery, New York,\n  NY, USA, 1-11", "doi": "10.1145/3313831.3376544", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GAZED- eye GAZe-guided EDiting for videos captured by a solitary,\nstatic, wide-angle and high-resolution camera. Eye-gaze has been effectively\nemployed in computational applications as a cue to capture interesting scene\ncontent; we employ gaze as a proxy to select shots for inclusion in the edited\nvideo. Given the original video, scene content and user eye-gaze tracks are\ncombined to generate an edited video comprising cinematically valid actor shots\nand shot transitions to generate an aesthetic and vivid representation of the\noriginal narrative. We model cinematic video editing as an energy minimization\nproblem over shot selection, whose constraints capture cinematographic editing\nconventions. Gazed scene locations primarily determine the shots constituting\nthe edited video. Effectiveness of GAZED against multiple competing methods is\ndemonstrated via a psychophysical study involving 12 users and twelve\nperformance videos.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:27:03 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Moorthy", "K L Bhanu", ""], ["Kumar", "Moneish", ""], ["Subramaniam", "Ramanathan", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2010.11985", "submitter": "Jianing Yang", "authors": "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir\n  Zadeh, Soujanya Poria, Louis-Philippe Morency", "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal\n  Language Sequences", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication is multimodal in nature; it is through multiple\nmodalities such as language, voice, and facial expressions, that opinions and\nemotions are expressed. Data in this domain exhibits complex multi-relational\nand temporal interactions. Learning from this data is a fundamentally\nchallenging research problem. In this paper, we propose Modal-Temporal\nAttention Graph (MTAG). MTAG is an interpretable graph-based neural model that\nprovides a suitable framework for analyzing multimodal sequential data. We\nfirst introduce a procedure to convert unaligned multimodal sequence data into\na graph with heterogeneous nodes and edges that captures the rich interactions\nacross modalities and through time. Then, a novel graph fusion operation,\ncalled MTAG fusion, along with a dynamic pruning and read-out technique, is\ndesigned to efficiently process this modal-temporal graph and capture various\ninteractions. By learning to focus only on the important interactions within\nthe graph, MTAG achieves state-of-the-art performance on multimodal sentiment\nanalysis and emotion recognition benchmarks, while utilizing significantly\nfewer model parameters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:58:50 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 18:44:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yang", "Jianing", ""], ["Wang", "Yongxin", ""], ["Yi", "Ruitao", ""], ["Zhu", "Yuying", ""], ["Rehman", "Azaan", ""], ["Zadeh", "Amir", ""], ["Poria", "Soujanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2010.12139", "submitter": "Ben Sangbae Chon", "authors": "Soochul Park and Ben Sangbae Chon", "title": "GSEP: A robust vocal and accompaniment separation system using gated\n  CBHG module and loudness normalization", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of audio signal processing research, source separation has been\na popular research topic for a long time and the recent adoption of the deep\nneural networks have shown a significant improvement in performance. The\nimprovement vitalizes the industry to productize audio deep learning based\nproducts and services including Karaoke in the music streaming apps and\ndialogue enhancement in the UHDTV. For these early markets, we defined a set of\ndesign principles of the vocal and accompaniment separation model in terms of\nrobustness, quality, and cost. In this paper, we introduce GSEP (Gaudio source\nSEParation system), a robust vocal and accompaniment separation system using a\nGated- CBHG module, mask warping, and loudness normalization and it was\nverified that the proposed system satisfies all three principles and\noutperforms the state-of-the-art systems both in objective measure and\nsubjective assessment through experiments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 03:04:07 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 06:46:54 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Park", "Soochul", ""], ["Chon", "Ben Sangbae", ""]]}, {"id": "2010.12216", "submitter": "Hang Zhu", "authors": "Hang Zhu and Zihao Wang", "title": "Feature matching in Ultrasound images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature matching is an important technique to identify a single object in\ndifferent images. It helps machines to construct recognition of a specific\nobject from multiple perspectives. For years, feature matching has been\ncommonly used in various computer vision applications, like traffic\nsurveillance, self-driving, and other systems. With the arise of Computer-Aided\nDiagnosis(CAD), the need for feature matching techniques also emerges in the\nmedical imaging field. In this paper, we present a deep learning-based method\nspecially for ultrasound images. It will be examined against existing methods\nthat have outstanding results on regular images. As the ultrasound images are\ndifferent from regular images in many fields like texture, noise type, and\ndimension, traditional methods will be evaluated and optimized to be applied to\nultrasound images.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 07:43:27 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhu", "Hang", ""], ["Wang", "Zihao", ""]]}, {"id": "2010.12325", "submitter": "Iris Ren", "authors": "Iris Ren, Anja Volk, Wouter Swierstra, Remco C. Veltkamp", "title": "A Computational Evaluation of Musical Pattern Discovery Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pattern discovery algorithms in the music domain aim to find meaningful\ncomponents in musical compositions. Over the years, although many algorithms\nhave been developed for pattern discovery in music data, it remains a\nchallenging task. To gain more insight into the efficacy of these algorithms,\nwe introduce three computational methods for examining their output: Pattern\nPolling, to combine the patterns; Comparative Classification, to differentiate\nthe patterns; Synthetic Data, to inject predetermined patterns. In combining\nand differentiating the patterns extracted by algorithms, we expose how they\ndiffer from the patterns annotated by humans as well as between algorithms\nthemselves, with rhythmic features contributing the most to the algorithm-human\nand algorithm-algorithm discrepancies. Despite the difficulty in reconciling\nand evaluating the divergent patterns extracted from algorithms, we identify\nsome possibilities for addressing them. In particular, we generate controllable\nsynthesised data with predetermined patterns planted into random data, thereby\nleaving us better able to inspect, compare, validate, and select the\nalgorithms. We provide a concrete example of synthesising data for\nunderstanding the algorithms and expand our discussion to the potential and\nlimitations of such an approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:07:21 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ren", "Iris", ""], ["Volk", "Anja", ""], ["Swierstra", "Wouter", ""], ["Veltkamp", "Remco C.", ""]]}, {"id": "2010.12540", "submitter": "Mohamed Maher Mr", "authors": "Mohamed Maher (1), Perseverance Munga Ngoy (1), Aleksandrs Rebriks\n  (1), Cagri Ozcinar (1), Josue Cuevas (3), Rajasekhar Sanagavarapu (3),\n  Gholamreza Anbarjafari (1 and 2) ((1) iCV Lab, University of Tartu, Tartu,\n  Estonia, (2) Faculty of Engineering, Hasan Kalyoncu University, Gaziantep,\n  Turkey, (3) Rakuten Inc., Big Data Department, Machine Learning Group, Tokyo,\n  Japan)", "title": "Comprehensive Empirical Evaluation of Deep Learning Approaches for\n  Session-based Recommendation in E-Commerce", "comments": "48 pages, 17 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting sales of e-commerce services is guaranteed once users find more\nmatching items to their interests in a short time. Consequently, recommendation\nsystems have become a crucial part of any successful e-commerce services.\nAlthough various recommendation techniques could be used in e-commerce, a\nconsiderable amount of attention has been drawn to session-based recommendation\nsystems during the recent few years. This growing interest is due to the\nsecurity concerns in collecting personalized user behavior data, especially\nafter the recent general data protection regulations. In this work, we present\na comprehensive evaluation of the state-of-the-art deep learning approaches\nused in the session-based recommendation. In session-based recommendation, a\nrecommendation system counts on the sequence of events made by a user within\nthe same session to predict and endorse other items that are more likely to\ncorrelate with his/her preferences. Our extensive experiments investigate\nbaseline techniques (\\textit{e.g.,} nearest neighbors and pattern mining\nalgorithms) and deep learning approaches (\\textit{e.g.,} recurrent neural\nnetworks, graph neural networks, and attention-based networks). Our evaluations\nshow that advanced neural-based models and session-based nearest neighbor\nalgorithms outperform the baseline techniques in most of the scenarios.\nHowever, we found that these models suffer more in case of long sessions when\nthere exists drift in user interests, and when there is no enough data to model\ndifferent items correctly during training. Our study suggests that using hybrid\nmodels of different approaches combined with baseline algorithms could lead to\nsubstantial results in session-based recommendations based on dataset\ncharacteristics. We also discuss the drawbacks of current session-based\nrecommendation algorithms and further open research directions in this field.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 17:22:35 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Maher", "Mohamed", "", "1 and 2"], ["Ngoy", "Perseverance Munga", "", "1 and 2"], ["Rebriks", "Aleksandrs", "", "1 and 2"], ["Ozcinar", "Cagri", "", "1 and 2"], ["Cuevas", "Josue", "", "1 and 2"], ["Sanagavarapu", "Rajasekhar", "", "1 and 2"], ["Anbarjafari", "Gholamreza", "", "1 and 2"]]}, {"id": "2010.12662", "submitter": "Yunjie Zhang", "authors": "Yunjie Zhang, Fei Tao, Xudong Liu, Runze Su, Xiaorong Mei, Weicong\n  Ding, Zhichen Zhao, Lei Yuan, Ji Liu", "title": "Short Video-based Advertisements Evaluation System: Self-Organizing\n  Learning Approach", "comments": "Submitting to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising of short video apps, such as TikTok, Snapchat and Kwai,\nadvertisement in short-term user-generated videos (UGVs) has become a trending\nform of advertising. Prediction of user behavior without specific user profile\nis required by advertisers, as they expect to acquire advertisement performance\nin advance in the scenario of cold start. Current recommender system do not\ntake raw videos as input; additionally, most previous work of Multi-Modal\nMachine Learning may not deal with unconstrained videos like UGVs. In this\npaper, we proposed a novel end-to-end self-organizing framework for user\nbehavior prediction. Our model is able to learn the optimal topology of neural\nnetwork architecture, as well as optimal weights, through training data. We\nevaluate our proposed method on our in-house dataset. The experimental results\nreveal that our model achieves the best performance in all our experiments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:52:24 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Yunjie", ""], ["Tao", "Fei", ""], ["Liu", "Xudong", ""], ["Su", "Runze", ""], ["Mei", "Xiaorong", ""], ["Ding", "Weicong", ""], ["Zhao", "Zhichen", ""], ["Yuan", "Lei", ""], ["Liu", "Ji", ""]]}, {"id": "2010.12968", "submitter": "Zijian Kuang", "authors": "Zijian Kuang and Xinran Tie", "title": "Improved Actor Relation Graph based Group Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding is to recognize and classify different actions or\nactivities appearing in the video. A lot of previous work, such as video\ncaptioning, has shown promising performance in producing general video\nunderstanding. However, it is still challenging to generate a fine-grained\ndescription of human actions and their interactions using state-of-the-art\nvideo captioning techniques. The detailed description of human actions and\ngroup activities is essential information, which can be used in real-time CCTV\nvideo surveillance, health care, sports video analysis, etc. This study\nproposes a video understanding method that mainly focused on group activity\nrecognition by learning the pair-wise actor appearance similarity and actor\npositions. We propose to use Normalized cross-correlation (NCC) and the sum of\nabsolute differences (SAD) to calculate the pair-wise appearance similarity and\nbuild the actor relationship graph to allow the graph convolution network to\nlearn how to classify group activities. We also propose to use MobileNet as the\nbackbone to extract features from each video frame. A visualization model is\nfurther introduced to visualize each input video frame with predicted bounding\nboxes on each human object and predict individual action and collective\nactivity.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 19:46:49 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 16:56:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kuang", "Zijian", ""], ["Tie", "Xinran", ""]]}, {"id": "2010.13035", "submitter": "Michael Lyons", "authors": "Tomohiro Tokunaga, Michael J. Lyons", "title": "Enactive Mandala: Audio-visualizing Brain Waves", "comments": "2 pages, 2 figures", "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2013", "doi": "10.5281/zenodo.1178678", "report-no": null, "categories": "cs.HC cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are exploring the design and implementation of artificial expressions,\nkinetic audio-visual representations of real-time physiological data that\nreflect emotional and cognitive state. In this work, we demonstrate a\nprototype, the Enactive Mandala, which maps real-time EEG signals to modulate\nambient music and animated visual music. Transparent real-time audio-visual\nfeedback of brainwave qualities supports intuitive insight into the connection\nbetween thoughts and physiological states.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 04:48:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Tokunaga", "Tomohiro", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.13059", "submitter": "Chao Liu", "authors": "Chao Liu and Heming Sun and Jiro Katto and Xiaoyang Zeng and Yibo Fan", "title": "A QP-adaptive Mechanism for CNN-based Filter in Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN)-based filters have achieved great success\nin video coding. However, in most previous works, individual models are needed\nfor each quantization parameter (QP) band. This paper presents a generic method\nto help an arbitrary CNN-filter handle different quantization noise. We model\nthe quantization noise problem and implement a feasible solution on CNN, which\nintroduces the quantization step (Qstep) into the convolution. When the\nquantization noise increases, the ability of the CNN-filter to suppress noise\nimproves accordingly. This method can be used directly to replace the (vanilla)\nconvolution layer in any existing CNN-filters. By using only 25% of the\nparameters, the proposed method achieves better performance than using multiple\nmodels with VTM-6.3 anchor. Besides, an additional BD-rate reduction of 0.2% is\nachieved by our proposed method for chroma components.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 08:02:38 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Chao", ""], ["Sun", "Heming", ""], ["Katto", "Jiro", ""], ["Zeng", "Xiaoyang", ""], ["Fan", "Yibo", ""]]}, {"id": "2010.13260", "submitter": "Babak Naderi", "authors": "Babak Naderi, Gabriel Mittag, Rafael Zequeira Jim\\a'enez, Sebastian\n  M\\\"oller", "title": "Effect of Language Proficiency on Subjective Evaluation of Noise\n  Suppression Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech communication systems based on Voice-over-IP technology are frequently\nused by native as well as non-native speakers of a target language, e.g. in\ninternational phone calls or telemeetings. Frequently, such calls also occur in\na noisy environment, making noise suppression modules necessary to increase\nperceived quality of experience. Whereas standard tests for assessing perceived\nquality make use of native listeners, we assume that noise-reduced speech and\nresidual noise may affect native and non-native listeners of a target language\nin different ways. To test this assumption, we report results of two subjective\ntests conducted with English and German native listeners who judge the quality\nof speech samples recorded by native English, German, and Mandarin speakers,\nwhich are degraded with different background noise levels and noise suppression\neffects. The experiments were conducted following the standardized ITU-T Rec.\nP.835 approach, however implemented in a crowdsourcing setting according to\nITU-T Rec. P.808. Our results show a significant influence of language on\nspeech signal ratings and, consequently, on the overall perceived quality in\nspecific conditions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 00:31:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Naderi", "Babak", ""], ["Mittag", "Gabriel", ""], ["Jim\\a'enez", "Rafael Zequeira", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2010.13468", "submitter": "Hung-Shin Lee", "authors": "Chung-En Sun, Yi-Wei Chen, Hung-Shin Lee, Yen-Hsing Chen, Hsin-Min\n  Wang", "title": "Melody Harmonization Using Orderless NADE, Chord Balancing, and Blocked\n  Gibbs Sampling", "comments": "Accepted by ICASSP 2021, and Demo is available at:\n  https://chord-generation.herokuapp.com/demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coherence and interestingness are two criteria for evaluating the performance\nof melody harmonization, which aims to generate a chord progression from a\nsymbolic melody. In this study, we apply the concept of orderless NADE, which\ntakes the melody and its partially masked chord sequence as the input of the\nBiLSTM-based networks to learn the masked ground truth, to the training\nprocess. In addition, the class weights are used to compensate for some\nreasonable chord labels that are rarely seen in the training set. Consistent\nwith the stochasticity in training, blocked Gibbs sampling with proper numbers\nof masking/generating loops is used in the inference phase to progressively\ntrade the coherence of the generated chord sequence off against its\ninterestingness. The experiments were conducted on a dataset of 18,005\nmelody/chord pairs. Our proposed model outperforms the state-of-the-art system\nMTHarmonizer in five of six different objective metrics based on chord/melody\nharmonicity and chord progression. The subjective test results with more than\n100 participants also show the superiority of our model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 10:18:46 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 08:25:42 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Sun", "Chung-En", ""], ["Chen", "Yi-Wei", ""], ["Lee", "Hung-Shin", ""], ["Chen", "Yen-Hsing", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "2010.13540", "submitter": "Zhesong Yu", "authors": "Zhesong Yu, Xingjian Du, Bilei Zhu, Zejun Ma", "title": "Contrastive Unsupervised Learning for Audio Fingerprinting", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of video-sharing platforms has attracted more and more people to\nshoot videos and upload them to the Internet. These videos mostly contain a\ncarefully-edited background audio track, where serious speech change, pitch\nshifting and various types of audio effects may involve, and existing audio\nidentification systems may fail to recognize the audio. To solve this problem,\nin this paper, we introduce the idea of contrastive learning to the task of\naudio fingerprinting (AFP). Contrastive learning is an unsupervised approach to\nlearn representations that can effectively group similar samples and\ndiscriminate dissimilar ones. In our work, we consider an audio track and its\ndifferently distorted versions as similar while considering different audio\ntracks as dissimilar. Based on the momentum contrast (MoCo) framework, we\ndevise a contrastive learning method for AFP, which can generate fingerprints\nthat are both discriminative and robust. A set of experiments showed that our\nAFP method is effective for audio identification, with robustness to serious\naudio distortions, including the challenging speed change and pitch shifting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:49:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yu", "Zhesong", ""], ["Du", "Xingjian", ""], ["Zhu", "Bilei", ""], ["Ma", "Zejun", ""]]}, {"id": "2010.13715", "submitter": "Pavan Madhusudana", "authors": "Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli and\n  Alan C. Bovik", "title": "ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate\n  Dependent Video Quality Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of conducting frame rate dependent video quality\nassessment (VQA) on videos of diverse frame rates, including high frame rate\n(HFR) videos. More generally, we study how perceptual quality is affected by\nframe rate, and how frame rate and compression combine to affect perceived\nquality. We devise an objective VQA model called Space-Time GeneRalized\nEntropic Difference (GREED) which analyzes the statistics of spatial and\ntemporal band-pass video coefficients. A generalized Gaussian distribution\n(GGD) is used to model band-pass responses, while entropy variations between\nreference and distorted videos under the GGD model are used to capture video\nquality variations arising from frame rate changes. The entropic differences\nare calculated across multiple temporal and spatial subbands, and merged using\na learned regressor. We show through extensive experiments that GREED achieves\nstate-of-the-art performance on the LIVE-YT-HFR Database when compared with\nexisting VQA models. The features used in GREED are highly generalizable and\nobtain competitive performance even on standard, non-HFR VQA databases. The\nimplementation of GREED has been made available online:\nhttps://github.com/pavancm/GREED\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:54:33 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Madhusudana", "Pavan C.", ""], ["Birkbeck", "Neil", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2010.14129", "submitter": "Yang Yu", "authors": "Yang Yu, Rongrong Ni and Yao Zhao", "title": "Mining Generalized Features for Detecting AI-Manipulated Fake Faces", "comments": "14 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, AI-manipulated face techniques have developed rapidly and\nconstantly, which has raised new security issues in society. Although existing\ndetection methods consider different categories of fake faces, the performance\non detecting the fake faces with \"unseen\" manipulation techniques is still poor\ndue to the distribution bias among cross-manipulation techniques. To solve this\nproblem, we propose a novel framework that focuses on mining intrinsic features\nand further eliminating the distribution bias to improve the generalization\nability. Firstly, we focus on mining the intrinsic clues in the channel\ndifference image (CDI) and spectrum image (SI) from the camera imaging process\nand the indispensable step in AI manipulation process. Then, we introduce the\nOctave Convolution (OctConv) and an attention-based fusion module to\neffectively and adaptively mine intrinsic features from CDI and SI. Finally, we\ndesign an alignment module to eliminate the bias of manipulation techniques to\nobtain a more generalized detection framework. We evaluate the proposed\nframework on four categories of fake faces datasets with the most popular and\nstate-of-the-art manipulation techniques, and achieve very competitive\nperformances. To further verify the generalization ability of the proposed\nframework, we conduct experiments on cross-manipulation techniques, and the\nresults show the advantages of our method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:41:16 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yu", "Yang", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""]]}, {"id": "2010.14168", "submitter": "Yuanbo Hou", "authors": "Yuanbo Hou, Yi Deng, Bilei Zhu, Zejun Ma and Dick Botteldooren", "title": "Rule-embedded network for audio-visual voice activity detection in live\n  musical video streams", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anchor's voice in live musical streams is an important\npreprocessing for music and speech signal processing. Existing approaches to\nvoice activity detection (VAD) primarily rely on audio, however, audio-based\nVAD is difficult to effectively focus on the target voice in noisy\nenvironments. With the help of visual information, this paper proposes a\nrule-embedded network to fuse the audio-visual (A-V) inputs to help the model\nbetter detect target voice. The core role of the rule in the model is to\ncoordinate the relation between the bi-modal information and use visual\nrepresentations as the mask to filter out the information of non-target sound.\nExperiments show that: 1) with the help of cross-modal fusion by the proposed\nrule, the detection result of A-V branch outperforms that of audio branch; 2)\nthe performance of bi-modal model far outperforms that of audio-only models,\nindicating that the incorporation of both audio and visual signals is highly\nbeneficial for VAD. To attract more attention to the cross-modal music and\naudio signal processing, a new live musical video corpus with frame-level label\nis introduced.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 10:00:07 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 14:55:00 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hou", "Yuanbo", ""], ["Deng", "Yi", ""], ["Zhu", "Bilei", ""], ["Ma", "Zejun", ""], ["Botteldooren", "Dick", ""]]}, {"id": "2010.14565", "submitter": "Li-Chia Yang", "authors": "Li-Chia Yang, Alexander Lerch", "title": "Remixing Music with Visual Conditioning", "comments": null, "journal-ref": "2020 IEEE International Symposium on Multimedia", "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visually conditioned music remixing system by incorporating deep\nvisual and audio models. The method is based on a state of the art audio-visual\nsource separation model which performs music instrument source separation with\nvideo information. We modified the model to work with user-selected images\ninstead of videos as visual input during inference to enable separation of\naudio-only content. Furthermore, we propose a remixing engine that generalizes\nthe task of source separation into music remixing. The proposed method is able\nto achieve improved audio quality compared to remixing performed by the\nseparate-and-add method with a state-of-the-art audio-visual source separation\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:12:08 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Yang", "Li-Chia", ""], ["Lerch", "Alexander", ""]]}, {"id": "2010.14709", "submitter": "Yihao Chen", "authors": "Yihao Chen, Alexander Lerch", "title": "Melody-Conditioned Lyrics Generation with SeqGANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic lyrics generation has received attention from both music and AI\ncommunities for years. Early rule-based approaches have~---due to increases in\ncomputational power and evolution in data-driven models---~mostly been replaced\nwith deep-learning-based systems. Many existing approaches, however, either\nrely heavily on prior knowledge in music and lyrics writing or oversimplify the\ntask by largely discarding melodic information and its relationship with the\ntext. We propose an end-to-end melody-conditioned lyrics generation system\nbased on Sequence Generative Adversarial Networks (SeqGAN), which generates a\nline of lyrics given the corresponding melody as the input. Furthermore, we\ninvestigate the performance of the generator with an additional input\ncondition: the theme or overarching topic of the lyrics to be generated. We\nshow that the input conditions have no negative impact on the evaluation\nmetrics while enabling the network to produce more meaningful results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:35:40 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chen", "Yihao", ""], ["Lerch", "Alexander", ""]]}, {"id": "2010.14805", "submitter": "Keunwoo Choi Mr", "authors": "Qiuqiang Kong, Keunwoo Choi, Yuxuan Wang", "title": "Large-Scale MIDI-based Composer Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music classification is a task to classify a music piece into labels such as\ngenres or composers. We propose large-scale MIDI based composer classification\nsystems using GiantMIDI-Piano, a transcription-based dataset. We propose to use\npiano rolls, onset rolls, and velocity rolls as input representations and use\ndeep neural networks as classifiers. To our knowledge, we are the first to\ninvestigate the composer classification problem with up to 100 composers. By\nusing convolutional recurrent neural networks as models, our MIDI based\ncomposer classification system achieves a 10-composer and a 100-composer\nclassification accuracies of 0.648 and 0.385 (evaluated on 30-second clips) and\n0.739 and 0.489 (evaluated on music pieces), respectively. Our MIDI based\ncomposer system outperforms several audio-based baseline classification\nsystems, indicating the effectiveness of using compact MIDI representations for\ncomposer classification.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:07:55 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kong", "Qiuqiang", ""], ["Choi", "Keunwoo", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2010.15288", "submitter": "Masood Seyed Mortazavi", "authors": "Masood S. Mortazavi", "title": "Speech-Image Semantic Alignment Does Not Depend on Any Prior\n  Classification Tasks", "comments": null, "journal-ref": "Proceedings of INTERSPEECH 2020", "doi": "10.21437/Interspeech.2020", "report-no": null, "categories": "cs.LG cs.CV cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantically-aligned $(speech, image)$ datasets can be used to explore\n\"visually-grounded speech\". In a majority of existing investigations, features\nof an image signal are extracted using neural networks \"pre-trained\" on other\ntasks (e.g., classification on ImageNet). In still others, pre-trained networks\nare used to extract audio features prior to semantic embedding. Without\n\"transfer learning\" through pre-trained initialization or pre-trained feature\nextraction, previous results have tended to show low rates of recall in $speech\n\\rightarrow image$ and $image \\rightarrow speech$ queries.\n  Choosing appropriate neural architectures for encoders in the speech and\nimage branches and using large datasets, one can obtain competitive recall\nrates without any reliance on any pre-trained initialization or feature\nextraction: $(speech,image)$ semantic alignment and $speech \\rightarrow image$\nand $image \\rightarrow speech$ retrieval are canonical tasks worthy of\nindependent investigation of their own and allow one to explore other\nquestions---e.g., the size of the audio embedder can be reduced significantly\nwith little loss of recall rates in $speech \\rightarrow image$ and $image\n\\rightarrow speech$ queries.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 00:14:33 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Mortazavi", "Masood S.", ""]]}, {"id": "2010.15343", "submitter": "Jasper Sebastiaan Wijnands", "authors": "Jasper S. Wijnands, Haifeng Zhao, Kerry A. Nice, Jason Thompson,\n  Katherine Scully, Jingqiu Guo, Mark Stevenson", "title": "Identifying safe intersection design through unsupervised feature\n  extraction from satellite imagery", "comments": "16 pages, 10 figures. Computer-Aided Civil and Infrastructure\n  Engineering (2020)", "journal-ref": null, "doi": "10.1111/mice.12623", "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Health Organization has listed the design of safer intersections as\na key intervention to reduce global road trauma. This article presents the\nfirst study to systematically analyze the design of all intersections in a\nlarge country, based on aerial imagery and deep learning. Approximately 900,000\nsatellite images were downloaded for all intersections in Australia and\ncustomized computer vision techniques emphasized the road infrastructure. A\ndeep autoencoder extracted high-level features, including the intersection's\ntype, size, shape, lane markings, and complexity, which were used to cluster\nsimilar designs. An Australian telematics data set linked infrastructure design\nto driving behaviors captured during 66 million kilometers of driving. This\nshowed more frequent hard acceleration events (per vehicle) at four- than\nthree-way intersections, relatively low hard deceleration frequencies at\nT-intersections, and consistently low average speeds on roundabouts. Overall,\ndomain-specific feature extraction enabled the identification of infrastructure\nimprovements that could result in safer driving behaviors, potentially reducing\nroad trauma.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 03:42:09 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wijnands", "Jasper S.", ""], ["Zhao", "Haifeng", ""], ["Nice", "Kerry A.", ""], ["Thompson", "Jason", ""], ["Scully", "Katherine", ""], ["Guo", "Jingqiu", ""], ["Stevenson", "Mark", ""]]}, {"id": "2010.15869", "submitter": "Shahan Ali Memon", "authors": "Shahan Ali Memon", "title": "Acoustic Correlates of the Voice Qualifiers: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our voices are as distinctive as our faces and fingerprints. There is a\nspectrum of non-disjoint traits that make our voices unique and identifiable,\nsuch as the fundamental frequency, the intensity, and most interestingly the\nquality of the speech. Voice quality refers to the characteristic features of\nan individual's voice. Previous research has from time-to-time proven the\nubiquity of voice quality in making different paralinguistic inferences. These\ninferences range from identifying personality traits, to health conditions and\nbeyond. In this manuscript, we first map the paralinguistic voice qualifiers to\ntheir acoustic correlates in the light of the previous research and literature.\nWe also determine the openSMILE correlates one could possibly use to measure\nthose correlates. In the second part, we give a set of example paralinguistic\ninferences that can be made using different acoustic and perceptual voice\nquality features.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:14:54 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Memon", "Shahan Ali", ""]]}, {"id": "2010.16030", "submitter": "Minz Won", "authors": "Minz Won, Sergio Oramas, Oriol Nieto, Fabien Gouyon, Xavier Serra", "title": "Multimodal Metric Learning for Tag-based Music Retrieval", "comments": "5 pages, 2 figures, submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tag-based music retrieval is crucial to browse large-scale music libraries\nefficiently. Hence, automatic music tagging has been actively explored, mostly\nas a classification task, which has an inherent limitation: a fixed vocabulary.\nOn the other hand, metric learning enables flexible vocabularies by using\npretrained word embeddings as side information. Also, metric learning has\nalready proven its suitability for cross-modal retrieval tasks in other domains\n(e.g., text-to-image) by jointly learning a multimodal embedding space. In this\npaper, we investigate three ideas to successfully introduce multimodal metric\nlearning for tag-based music retrieval: elaborate triplet sampling, acoustic\nand cultural music information, and domain-specific word embeddings. Our\nexperimental results show that the proposed ideas enhance the retrieval system\nquantitatively, and qualitatively. Furthermore, we release the MSD500, a subset\nof the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually\nannotated tag categories, and user taste profiles.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:46:28 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Won", "Minz", ""], ["Oramas", "Sergio", ""], ["Nieto", "Oriol", ""], ["Gouyon", "Fabien", ""], ["Serra", "Xavier", ""]]}, {"id": "2010.16073", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad and Naimul khan", "title": "CNN based Multistage Gated Average Fusion (MGAF) for Human Action\n  Recognition Using Depth and Inertial Sensors", "comments": "arXiv admin note: text overlap with arXiv:1910.11482", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Network (CNN) provides leverage to extract and fuse\nfeatures from all layers of its architecture. However, extracting and fusing\nintermediate features from different layers of CNN structure is still\nuninvestigated for Human Action Recognition (HAR) using depth and inertial\nsensors. To get maximum benefit of accessing all the CNN's layers, in this\npaper, we propose novel Multistage Gated Average Fusion (MGAF) network which\nextracts and fuses features from all layers of CNN using our novel and\ncomputationally efficient Gated Average Fusion (GAF) network, a decisive\nintegral element of MGAF. At the input of the proposed MGAF, we transform the\ndepth and inertial sensor data into depth images called sequential front view\nimages (SFI) and signal images (SI) respectively. These SFI are formed from the\nfront view information generated by depth data. CNN is employed to extract\nfeature maps from both input modalities. GAF network fuses the extracted\nfeatures effectively while preserving the dimensionality of fused feature as\nwell. The proposed MGAF network has structural extensibility and can be\nunfolded to more than two modalities. Experiments on three publicly available\nmultimodal HAR datasets demonstrate that the proposed MGAF outperforms the\nprevious state of the art fusion methods for depth-inertial HAR in terms of\nrecognition accuracy while being computationally much more efficient. We\nincrease the accuracy by an average of 1.5 percent while reducing the\ncomputational cost by approximately 50 percent over the previous state of the\nart.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 11:49:13 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["khan", "Naimul", ""]]}, {"id": "2010.16211", "submitter": "Heng Yao", "authors": "Mian Zou, Heng Yao, Chuan Qin, and Xinpeng Zhang", "title": "Statistical Analysis of Signal-Dependent Noise: Application in Blind\n  Localization of Image Splicing Forgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual noise is often regarded as a disturbance in image quality, whereas it\ncan also provide a crucial clue for image-based forensic tasks. Conventionally,\nnoise is assumed to comprise an additive Gaussian model to be estimated and\nthen used to reveal anomalies. However, for real sensor noise, it should be\nmodeled as signal-dependent noise (SDN). In this work, we apply SDN to splicing\nforgery localization tasks. Through statistical analysis of the SDN model, we\nassume that noise can be modeled as a Gaussian approximation for a certain\nbrightness and propose a likelihood model for a noise level function. By\nbuilding a maximum a posterior Markov random field (MAP-MRF) framework, we\nexploit the likelihood of noise to reveal the alien region of spliced objects,\nwith a probability combination refinement strategy. To ensure a completely\nblind detection, an iterative alternating method is adopted to estimate the MRF\nparameters. Experimental results demonstrate that our method is effective and\nprovides a comparative localization performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 11:53:53 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 09:34:16 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zou", "Mian", ""], ["Yao", "Heng", ""], ["Qin", "Chuan", ""], ["Zhang", "Xinpeng", ""]]}]