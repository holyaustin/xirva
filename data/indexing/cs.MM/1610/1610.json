[{"id": "1610.00609", "submitter": "Vineet Gokhale", "authors": "Vineet Gokhale, Jayakrishnan Nair and Subhasis Chaudhuri", "title": "Congestion Control for Network-Aware Telehaptic Communication", "comments": "25 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telehaptic applications involve delay-sensitive multimedia communication\nbetween remote locations with distinct Quality of Service (QoS) requirements\nfor different media components. These QoS constraints pose a variety of\nchallenges, especially when the communication occurs over a shared network,\nwith unknown and time-varying cross-traffic. In this work, we propose a\ntransport layer congestion control protocol for telehaptic applications\noperating over shared networks, termed as dynamic packetization module (DPM).\nDPM is a lossless, network-aware protocol which tunes the telehaptic\npacketization rate based on the level of congestion in the network. To monitor\nthe network congestion, we devise a novel network feedback module, which\ncommunicates the end-to-end delays encountered by the telehaptic packets to the\nrespective transmitters with negligible overhead. Via extensive simulations, we\nshow that DPM meets the QoS requirements of telehaptic applications over a wide\nrange of network cross-traffic conditions. We also report qualitative results\nof a real-time telepottery experiment with several human subjects, which reveal\nthat DPM preserves the quality of telehaptic activity even under heavily\ncongested network scenarios. Finally, we compare the performance of DPM with\nseveral previously proposed telehaptic communication protocols and demonstrate\nthat DPM outperforms these protocols.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 16:06:20 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 03:37:39 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Gokhale", "Vineet", ""], ["Nair", "Jayakrishnan", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1610.01955", "submitter": "Krzysztof Szczypiorski", "authors": "Krzysztof Szczypiorski and Tomasz Tyl", "title": "MoveSteg: A Method of Network Steganography Detection", "comments": "7 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new method for detecting a source point of time based\nnetwork steganography - MoveSteg. A steganography carrier could be an example\nof multimedia stream made with packets. These packets are then delayed\nintentionally to send hidden information using time based steganography\nmethods. The presented analysis describes a method that allows finding the\nsource of steganography stream in network that is under our management.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 17:12:37 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Szczypiorski", "Krzysztof", ""], ["Tyl", "Tomasz", ""]]}, {"id": "1610.02256", "submitter": "Xin Jin", "authors": "Xin Jin, Le Wu, Xiaodong Li, Xiaokun Zhang, Jingying Chi, Siwei Peng,\n  Shiming Ge, Geng Zhao, Shuying Li", "title": "ILGNet: Inception Modules with Connected Local and Global Features for\n  Efficient Image Aesthetic Quality Classification using Domain Adaptation", "comments": "under review, IET-Computer Vision, Previous WCSP2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a challenging problem of aesthetic image\nclassification, which is to label an input image as high or low aesthetic\nquality. We take both the local and global features of images into\nconsideration. A novel deep convolutional neural network named ILGNet is\nproposed, which combines both the Inception modules and an connected layer of\nboth Local and Global features. The ILGnet is based on GoogLeNet. Thus, it is\neasy to use a pre-trained GoogLeNet for large-scale image classification\nproblem and fine tune our connected layers on an large scale database of\naesthetic related images: AVA, i.e. \\emph{domain adaptation}. The experiments\nreveal that our model achieves the state of the arts in AVA database. Both the\ntraining and testing speeds of our model are higher than those of the original\nGoogLeNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:46:45 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 02:23:09 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 23:43:04 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Jin", "Xin", ""], ["Wu", "Le", ""], ["Li", "Xiaodong", ""], ["Zhang", "Xiaokun", ""], ["Chi", "Jingying", ""], ["Peng", "Siwei", ""], ["Ge", "Shiming", ""], ["Zhao", "Geng", ""], ["Li", "Shuying", ""]]}, {"id": "1610.02263", "submitter": "Zakaria Ye", "authors": "Zakaria Ye and Rachid El-Azouzi and Tania Jimenez and Francesco De\n  Pellegrini", "title": "Backward-Shifted Coding (BSC) based on Scalable Video Coding for HAS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main task of HTTP Adaptive Streaming is to adapt video quality\ndynamically under variable network conditions. This is a key feature for\nmultimedia delivery especially when quality of service cannot be granted\nnetwork-wide and, e.g., throughput may suffer short term fluctuations.\n  Hence, robust bitrate adaptation schemes become crucial in order to improve\nvideo quality. The objective, in this context, is to control the filling level\nof the playback buffer and maximize the quality of the video, while avoiding\nunnecessary video quality variations.\n  In this paper we study bitrate adaptation algorithms based on\nBackward-Shifted Coding (BSC), a scalable video coding scheme able to greatly\nimprove video quality. We design bitrate adaptation algorithms that balance\nvideo rate smoothness and high network capacity utilization, leveraging both on\nthroughput-based and buffer-based adaptation mechanisms.\n  Extensive simulations using synthetic and real-world video traffic traces\nshow that the proposed scheme performs remarkably well even under challenging\nnetwork conditions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 13:01:30 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Ye", "Zakaria", ""], ["El-Azouzi", "Rachid", ""], ["Jimenez", "Tania", ""], ["De Pellegrini", "Francesco", ""]]}, {"id": "1610.02443", "submitter": "Sachin Mehta", "authors": "Sachin Mehta, Balakrishnan Prabhakaran, Rajarathnam Nallusamy, Derrick\n  Newton", "title": "mPDF: Framework for Watermarking PDF Files using Image Watermarking\n  Algorithms", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement in digital technologies have made it possible to produce\nperfect copies of digital content. In this environment, malicious users\nreproduce the digital content and share it without compensation to the content\nowner. Content owners are concerned about the potential loss of revenue and\nreputation from piracy, especially when the content is available over the\nInternet. Digital watermarking has emerged as a deterrent measure towards such\nmalicious activities. Several methods have been proposed for copyright\nprotection and fingerprinting of digital images. However, these methods are not\napplicable to text documents as these documents lack rich texture information\nwhich is abundantly available in digital images. In this paper, a framework\n(mPDF) is proposed which facilitates the usage of digital image watermarking\nalgorithms on text documents. The proposed method divides a text document into\ntexture and non-texture blocks using an energy-based approach. After\nclassification, a watermark is embedded inside the texture blocks in a content\nadaptive manner. The proposed method is integrated with five known image\nwatermarking methods and its performance is studied in terms of quality and\nrobustness. Experiments are conducted on documents in 11 different languages.\nExperimental results clearly show that the proposed method facilitates the\nusage of image watermarking algorithms on text documents and is robust against\nattacks such as print & scan, print screen, and skew. Also, the proposed method\novercomes the drawbacks of existing text watermarking methods such as manual\ninspection and language dependency.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 23:06:09 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Mehta", "Sachin", ""], ["Prabhakaran", "Balakrishnan", ""], ["Nallusamy", "Rajarathnam", ""], ["Newton", "Derrick", ""]]}, {"id": "1610.02488", "submitter": "Jean-Marc Valin", "authors": "Yushin Cho, Thomas J. Daede, Nathan E. Egge, Guillaume Martres,\n  Tristan Matthews, Christopher Montgomery, Timothy B. Terriberry, Jean-Marc\n  Valin", "title": "Perceptually-Driven Video Coding with the Daala Video Codec", "comments": "19 pages, Proceedings of SPIE Workshop on Applications of Digital\n  Image Processing (ADIP), 2016", "journal-ref": null, "doi": "10.1117/12.2238417", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Daala project is a royalty-free video codec that attempts to compete with\nthe best patent-encumbered codecs. Part of our strategy is to replace core\ntools of traditional video codecs with alternative approaches, many of them\ndesigned to take perceptual aspects into account, rather than optimizing for\nsimple metrics like PSNR. This paper documents some of our experiences with\nthese tools, which ones worked and which did not. We evaluate which tools are\neasy to integrate into a more traditional codec design, and show results in the\ncontext of the codec being developed by the Alliance for Open Media.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 05:34:56 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Cho", "Yushin", ""], ["Daede", "Thomas J.", ""], ["Egge", "Nathan E.", ""], ["Martres", "Guillaume", ""], ["Matthews", "Tristan", ""], ["Montgomery", "Christopher", ""], ["Terriberry", "Timothy B.", ""], ["Valin", "Jean-Marc", ""]]}, {"id": "1610.02516", "submitter": "Ren Yang", "authors": "Ren Yang, Mai Xu, Zulin Wang, Yiping Duan, Xiaoming Tao", "title": "Saliency-Guided Complexity Control for HEVC Decoding", "comments": "IEEE Transactions on Broadcasting", "journal-ref": null, "doi": "10.1109/TBC.2018.2795459", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest High Efficiency Video Coding (HEVC) standard significantly\nimproves coding efficiency over its previous video coding standards. The\nexpense of such improvement is enormous computational complexity, from both\nencoding and decoding sides. Since computational capability and power capacity\nare diverse across portable devices, it is necessary to reduce decoding\ncomplexity to a target with tolerable quality loss, so called complexity\ncontrol. This paper proposes a Saliency-Guided Complexity Control (SGCC)\napproach for HEVC decoding, which reduces the decoding complexity to the target\nwith minimal perceptual quality loss. First, we establish the SGCC formulation\nto minimize perceptual quality loss at the constraint on reduced decoding\ncomplexity, which is achieved via disabling Deblocking Filter (DF) and\nsimplifying Motion Compensation (MC) of some non-salient Coding Tree Units\n(CTUs). One important component in this formulation is the modelled\nrelationship between decoding complexity reduction and DF disabling/MC\nsimplification, which determines the control accuracy of our approach. Another\ncomponent is the modelled relationship between quality loss and DF disabling/MC\nsimplification, responsible for optimizing perceptual quality. By solving the\nSGCC formulation for a given target complexity, we can obtain the DF and MC\nsettings of each CTU, and then decoding complexity can be reduced to the\ntarget. Finally, the experimental results validate the effectiveness of our\nSGCC approach, from the aspects of control performance, complexity-distortion\nperformance, fluctuation of quality loss and subjective quality.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 12:09:38 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 15:42:24 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 02:34:44 GMT"}, {"version": "v4", "created": "Thu, 10 Aug 2017 01:15:55 GMT"}, {"version": "v5", "created": "Tue, 9 Jan 2018 12:15:59 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Yang", "Ren", ""], ["Xu", "Mai", ""], ["Wang", "Zulin", ""], ["Duan", "Yiping", ""], ["Tao", "Xiaoming", ""]]}, {"id": "1610.02692", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Issey Masuda, Santiago Pascual de la Puente and Xavier Giro-i-Nieto", "title": "Open-Ended Visual Question-Answering", "comments": "Bachelor thesis report graded with A with honours at ETSETB Telecom\n  BCN school, Universitat Polit\\`ecnica de Catalunya (UPC). June 2016. Source\n  code and models are publicly available at\n  http://imatge-upc.github.io/vqa-2016-cvprw/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis report studies methods to solve Visual Question-Answering (VQA)\ntasks with a Deep Learning framework. As a preliminary step, we explore Long\nShort-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to\ntackle Question-Answering (text based). We then modify the previous model to\naccept an image as an input in addition to the question. For this purpose, we\nexplore the VGG-16 and K-CNN convolutional neural networks to extract visual\nfeatures from the image. These are merged with the word embedding or with a\nsentence embedding of the question to predict the answer. This work was\nsuccessfully submitted to the Visual Question Answering Challenge 2016, where\nit achieved a 53,62% of accuracy in the test dataset. The developed software\nhas followed the best programming practices and Python code style, providing a\nconsistent baseline in Keras for different configurations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 16:38:31 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Masuda", "Issey", ""], ["de la Puente", "Santiago Pascual", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1610.04346", "submitter": "Muhammad Fahad Khan", "authors": "Muhammad Fahad Khan, Faisal Baig, Saira Beg", "title": "Steganography between Silence Intervals of Audio in Video Content Using\n  Chaotic Maps", "comments": "11 pages, 3 figures", "journal-ref": "Khan, Muhammad Fahad, Faisal Baig, and Saira Beg. \"Steganography\n  between silence intervals of audio in video content using chaotic maps.\"\n  Circuits, Systems, and Signal Processing 33.12 (2014): 3901-3919", "doi": "10.1007/s00034-014-9830-5", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steganography is the art of hiding data, in such a way that it is\nundetectable under traffic-pattern analysis and the data hidden is only known\nto the receiver and the sender. In this paper new method of text steganography\nover the silence interval of audio in a video file, is presented. In the\nproposed method first the audio signal is extracted from the video. After doing\naudio enhancement, the data on the audio signal is steganographed using new\ntechnique and then audio signal is rewritten in video file again.\nhttp://www.learnrnd.com/All_latest_research_findings.php\n  To enhance the security level we apply chaotic maps on arbitrary text.\nFurthermore, the algorithm in this paper, gives a technique which states that\nundetectable stegotext and cover-text has same probability distribution and no\nstatistical test can detect the presence of the hidden message.\nhttp://www.learnrnd.com/detail.php?id=Biohack_Eyes_through_Chlorin_e6_eye_drop_:Stanford_University_Research\n  Moreover, hidden message does not affect the transmission rate of video file\nat all.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 07:27:46 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Khan", "Muhammad Fahad", ""], ["Baig", "Faisal", ""], ["Beg", "Saira", ""]]}, {"id": "1610.06347", "submitter": "Oliver Giudice", "authors": "Oliver Giudice, Antonino Paratore, Marco Moltisanti, Sebastiano\n  Battiato", "title": "A Classification Engine for Image Ballistics of Social Data", "comments": "6 pages, 1 figure", "journal-ref": "Image Analysis and Processing - ICIAP 2017: 19th International\n  Conference, Catania, Italy, September 11-15, 2017, Proceedings, Part II,\n  Springer International Publishing", "doi": "10.1007/978-3-319-68548-9_57", "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Forensics has already achieved great results for the source camera\nidentification task on images. Standard approaches for data coming from Social\nNetwork Platforms cannot be applied due to different processes involved (e.g.,\nscaling, compression, etc.). Over 1 billion images are shared each day on the\nInternet and obtaining information about their history from the moment they\nwere acquired could be exploited for investigation purposes. In this paper, a\nclassification engine for the reconstruction of the history of an image, is\npresented. Specifically, exploiting K-NN and decision trees classifiers and\na-priori knowledge acquired through image analysis, we propose an automatic\napproach that can understand which Social Network Platform has processed an\nimage and the software application used to perform the image upload. The engine\nmakes use of proper alterations introduced by each platform as features.\nResults, in terms of global accuracy on a dataset of 2720 images, confirm the\neffectiveness of the proposed strategy.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 10:27:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Giudice", "Oliver", ""], ["Paratore", "Antonino", ""], ["Moltisanti", "Marco", ""], ["Battiato", "Sebastiano", ""]]}, {"id": "1610.07386", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, and Shohreh\n  Kasaei", "title": "An Efficient Adaptive Boundary Matching Algorithm for Video Error\n  Concealment", "comments": "Iranian Journal of Electrical & Electronic Engineering, Vol. 10, No.\n  3, Sep. 2014", "journal-ref": "IJEEE. 2014; 10 (3) :188-202", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sending compressed video data in error-prone environments (like the Internet\nand wireless networks) might cause data degradation. Error concealment\ntechniques try to conceal the received data in the decoder side. In this paper,\nan adaptive boundary matching algorithm is presented for recovering the damaged\nmotion vectors (MVs). This algorithm uses an outer boundary matching or\ndirectional temporal boundary matching method to compare every boundary of\ncandidate macroblocks (MBs), adaptively. It gives a specific weight according\nto the accuracy of each boundary of the damaged MB. Moreover, if each of the\nadjacent MBs is already concealed, different weights are given to the\nboundaries. Finally, the MV with minimum adaptive boundary distortion is\nselected as the MV of the damaged MB. Experimental results show that the\nproposed algorithm can improve both objective and subjective quality of\nreconstructed frames without any considerable computational complexity. The\naverage PSNR in some frames of test sequences increases about 5.20, 5.78, 5.88,\n4.37, 4.41, and 3.50 dB compared to average MV, classic boundary matching,\ndirectional boundary matching, directional temporal boundary matching, outer\nboundary matching, and dynamical temporal error concealment algorithm,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:30:52 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1610.07387", "submitter": "GeeYong Suk", "authors": "Soo-Jin Kim, Gee-Yong Suk, Jong-Seok Lee, Chan-Byoung Chae", "title": "QoE-aware Scalable Video Transmission in MIMO~Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important concept in wireless systems has been quality of experience\n(QoE)-aware video transmission. Such communications are considered not only\nconnection-based communications but also content-aware communications, since\nthe video quality is closely related to the content itself. It becomes\nnecessary therefore for video communications to utilize a cross-layer design\n(also known as joint source and channel coding). To provide efficient methods\nof allocating network resources, the wireless network uses its cross-layer\nknowledge to perform unequal error protection (UEP) solutions. In this article,\nwe summarize the latest video transmission technologies that are based on\nscalable video coding (SVC) over multiple-input multiple-output (MIMO) systems\nwith cross-layer designs. To provide insight into video transmission in\nwireless networks, we investigate UEP solutions in the delivering of video over\nmassive MIMO systems. Our results show that in terms of quality of experience\n(QoE), SVC layer prioritization, which was considered important in the prior\nwork, is not always beneficial in massive MIMO systems; consideration must be\ngiven to the content characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:36:25 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Kim", "Soo-Jin", ""], ["Suk", "Gee-Yong", ""], ["Lee", "Jong-Seok", ""], ["Chae", "Chan-Byoung", ""]]}, {"id": "1610.07753", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei", "title": "A Novel Boundary Matching Algorithm for Video Temporal Error Concealment", "comments": "arXiv admin note: text overlap with arXiv:1610.07386", "journal-ref": "International Journal of Image, Graphics, and Signal Processing\n  (IJIGSP), vol. 6, no. 6, pp. 1-10, May. 2014", "doi": "10.5815/ijigsp.2014.06.01", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast growth of communication networks, the video data transmission\nfrom these networks is extremely vulnerable. Error concealment is a technique\nto estimate the damaged data by employing the correctly received data at the\ndecoder. In this paper, an efficient boundary matching algorithm for estimating\ndamaged motion vectors (MVs) is proposed. The proposed algorithm performs error\nconcealment for each damaged macro block (MB) according to the list of\nidentified priority of each frame. It then uses a classic boundary matching\ncriterion or the proposed boundary matching criterion adaptively to identify\nmatching distortion in each boundary of candidate MB. Finally, the candidate MV\nwith minimum distortion is selected as an MV of damaged MB and the list of\npriorities is updated. Experimental results show that the proposed algorithm\nimproves both objective and subjective qualities of reconstructed frames\nwithout any significant increase in computational cost. The PSNR for test\nsequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to\nthe classic boundary matching, directional boundary matching, and directional\ntemporal boundary matching algorithm, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:11:41 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1610.09003", "submitter": "Carl Vondrick", "authors": "Yusuf Aytar, Lluis Castrejon, Carl Vondrick, Hamed Pirsiavash, Antonio\n  Torralba", "title": "Cross-Modal Scene Networks", "comments": "See more at http://cmplaces.csail.mit.edu/. arXiv admin note: text\n  overlap with arXiv:1607.07295", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can recognize scenes across many different modalities beyond natural\nimages. In this paper, we investigate how to learn cross-modal scene\nrepresentations that transfer across modalities. To study this problem, we\nintroduce a new cross-modal scene dataset. While convolutional neural networks\ncan categorize scenes well, they also learn an intermediate representation not\naligned across modalities, which is undesirable for cross-modal transfer\napplications. We present methods to regularize cross-modal convolutional neural\nnetworks so that they have a shared representation that is agnostic of the\nmodality. Our experiments suggest that our scene representation can help\ntransfer representations across modalities for retrieval. Moreover, our\nvisualizations suggest that units emerge in the shared representation that tend\nto activate on consistent concepts independently of the modality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 20:24:36 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Aytar", "Yusuf", ""], ["Castrejon", "Lluis", ""], ["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1610.09152", "submitter": "Sophie Fosson", "authors": "Giulia Fracastoro, Sophie Marie Fosson, Enrico Magli", "title": "Steerable Discrete Cosine Transform", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2623489", "report-no": null, "categories": "cs.IT cs.MM math.IT math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In image compression, classical block-based separable transforms tend to be\ninefficient when image blocks contain arbitrarily shaped discontinuities. For\nthis reason, transforms incorporating directional information are an appealing\nalternative. In this paper, we propose a new approach to this problem, namely a\ndiscrete cosine transform (DCT) that can be steered in any chosen direction.\nSuch transform, called steerable DCT (SDCT), allows to rotate in a flexible way\npairs of basis vectors, and enables precise matching of directionality in each\nimage block, achieving improved coding efficiency. The optimal rotation angles\nfor SDCT can be represented as solution of a suitable rate-distortion (RD)\nproblem. We propose iterative methods to search such solution, and we develop a\nfully fledged image encoder to practically compare our techniques with other\ncompeting transforms. Analytical and numerical results prove that SDCT\noutperforms both DCT and state-of-the-art directional transforms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 10:09:12 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 10:59:50 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Fracastoro", "Giulia", ""], ["Fosson", "Sophie Marie", ""], ["Magli", "Enrico", ""]]}]