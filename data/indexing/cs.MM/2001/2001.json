[{"id": "2001.00001", "submitter": "Maria Mannone", "authors": "Maria Mannone, Federico Favali, Balandino Di Donato, Luca Turchet", "title": "Quantum GestART: Identifying and Applying Correlations between\n  Mathematics, Art, and Perceptual Organization", "comments": "Accepted for publication, Journal of Mathematics and Music. New\n  references added in this version", "journal-ref": "Journal of Mathematics and Music, 2020", "doi": "10.1080/17459737.2020.1726691", "report-no": null, "categories": "math.HO cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mathematics can help analyze the arts and inspire new artwork. Mathematics\ncan also help make transformations from one artistic medium to another,\nconsidering exceptions and choices, as well as artists' individual and unique\ncontributions. We propose a method based on diagrammatic thinking and quantum\nformalism. We exploit decompositions of complex forms into a set of simple\nshapes, discretization of complex images, and Dirac notation, imagining a world\nof \"prototypes\" that can be connected to obtain a fine or coarse-graining\napproximation of a given visual image. Visual prototypes are exchanged with\nauditory ones, and the information (position, size) characterizing visual\nprototypes is connected with the information (onset, duration, loudness, pitch\nrange) characterizing auditory prototypes. The topic is contextualized within a\nphilosophical debate (discreteness and comparison of apparently unrelated\nobjects), it develops through mathematical formalism, and it leads to\nprogramming, to spark interdisciplinary thinking and ignite creativity within\nSTEAM.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 11:49:27 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 08:19:59 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Mannone", "Maria", ""], ["Favali", "Federico", ""], ["Di Donato", "Balandino", ""], ["Turchet", "Luca", ""]]}, {"id": "2001.00179", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales,\n  Javier Ortega-Garcia", "title": "DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection", "comments": null, "journal-ref": "Information Fusion, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The free access to large-scale public databases, together with the fast\nprogress of deep learning techniques, in particular Generative Adversarial\nNetworks, have led to the generation of very realistic fake content with its\ncorresponding implications towards society in this era of fake news. This\nsurvey provides a thorough review of techniques for manipulating face images\nincluding DeepFake methods, and methods to detect such manipulations. In\nparticular, four types of facial manipulation are reviewed: i) entire face\nsynthesis, ii) identity swap (DeepFakes), iii) attribute manipulation, and iv)\nexpression swap. For each manipulation group, we provide details regarding\nmanipulation techniques, existing public databases, and key benchmarks for\ntechnology evaluation of fake detection methods, including a summary of results\nfrom those evaluations. Among all the aspects discussed in the survey, we pay\nspecial attention to the latest generation of DeepFakes, highlighting its\nimprovements and challenges for fake detection.\n  In addition to the survey information, we also discuss open issues and future\ntrends that should be considered to advance in the field.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 09:54:34 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 07:22:46 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 18:17:43 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "2001.00847", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u, Rafael F. Schaefer, and H. Vincent Poor", "title": "Biometric and Physical Identifiers with Correlated Noise for\n  Controllable Private Authentication", "comments": "Shorter version to appear in the IEEE International Symposium on\n  Information Theory 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM eess.SP math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of secret-key based authentication under privacy and storage\nconstraints on the source sequence is considered. The identifier measurement\nchannels during authentication are assumed to be controllable via a\ncost-constrained action sequence. Single-letter inner and outer bounds for the\nkey-leakage-storage-cost regions are derived for a generalization of a classic\ntwo-terminal key agreement model with an eavesdropper that observes a sequence\nthat is correlated with the sequences observed by the legitimate terminals. The\nadditions to the model are that the encoder observes a noisy version of a\nremote source, and the noisy output and the remote source output together with\nan action sequence are given as inputs to the measurement channel at the\ndecoder. Thus, correlation is introduced between the noise components on the\nencoder and decoder measurements. The model with a secret key generated by an\nencoder is extended to the randomized models, where a secret-key is embedded to\nthe encoder. The results are relevant for several user and device\nauthentication scenarios including physical and biometric identifiers with\nmultiple measurements that provide diversity and multiplexing gains. To\nillustrate the behavior of the rate region, achievable (secret-key rate,\nstorage-rate, cost) tuples are given for binary identifiers and measurement\nchannels that can be represented as a mixture of binary symmetric subchannels.\nThe gains from using an action sequence such as a large secret-key rate at a\nsignificantly small hardware cost, are illustrated to motivate the use of\nlow-complexity transform-coding algorithms with cost-constrained actions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 14:29:58 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 13:49:18 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 08:38:18 GMT"}, {"version": "v4", "created": "Thu, 23 Jul 2020 10:17:25 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Schaefer", "Rafael F.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2001.01403", "submitter": "Zhang Cong", "authors": "Jie Li, Cong Zhang, Zhi Liu, Wei Sun, Qiyue Li", "title": "Joint Communication and Computational Resource Allocation for QoE-driven\n  Point Cloud Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud video is the most popular representation of hologram, which is\nthe medium to precedent natural content in VR/AR/MR and is expected to be the\nnext generation video. Point cloud video system provides users immersive\nviewing experience with six degrees of freedom and has wide applications in\nmany fields such as online education, entertainment. To further enhance these\napplications, point cloud video streaming is in critical demand. The inherent\nchallenges lie in the large size by the necessity of recording the\nthree-dimensional coordinates besides color information, and the associated\nhigh computation complexity of encoding. To this end, this paper proposes a\ncommunication and computation resource allocation scheme for QoE-driven point\ncloud video streaming. In particular, we maximize system resource utilization\nby selecting different quantities, transmission forms and quality level tiles\nto maximize the quality of experience. Extensive simulations are conducted and\nthe simulation results show the superior performance over the existing schemes\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 05:20:40 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 03:23:42 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Li", "Jie", ""], ["Zhang", "Cong", ""], ["Liu", "Zhi", ""], ["Sun", "Wei", ""], ["Li", "Qiyue", ""]]}, {"id": "2001.01720", "submitter": "Stefan Lattner", "authors": "Stefan Lattner", "title": "Modeling Musical Structure with Artificial Neural Networks", "comments": "152 pages, 28 figures, 10 tables. PhD thesis, Johannes Kepler\n  University Linz, October 2019. Includes results from\n  https://www.ijcai.org/Proceedings/15/Papers/348.pdf, arXiv:1612.04742,\n  arXiv:1708.05325, arXiv:1806.08236, and arXiv:1806.08686 (see Section 1.2 for\n  detailed information)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, artificial neural networks (ANNs) have become a universal\ntool for tackling real-world problems. ANNs have also shown great success in\nmusic-related tasks including music summarization and classification,\nsimilarity estimation, computer-aided or autonomous composition, and automatic\nmusic analysis. As structure is a fundamental characteristic of Western music,\nit plays a role in all these tasks. Some structural aspects are particularly\nchallenging to learn with current ANN architectures. This is especially true\nfor mid- and high-level self-similarity, tonal and rhythmic relationships. In\nthis thesis, I explore the application of ANNs to different aspects of musical\nstructure modeling, identify some challenges involved and propose strategies to\naddress them. First, using probability estimations of a Restricted Boltzmann\nMachine (RBM), a probabilistic bottom-up approach to melody segmentation is\nstudied. Then, a top-down method for imposing a high-level structural template\nin music generation is presented, which combines Gibbs sampling using a\nconvolutional RBM with gradient-descent optimization on the intermediate\nsolutions. Furthermore, I motivate the relevance of musical transformations in\nstructure modeling and show how a connectionist model, the Gated Autoencoder\n(GAE), can be employed to learn transformations between musical fragments. For\nlearning transformations in sequences, I propose a special predictive training\nof the GAE, which yields a representation of polyphonic music as a sequence of\nintervals. Furthermore, the applicability of these interval representations to\na top-down discovery of repeated musical sections is shown. Finally, a\nrecurrent variant of the GAE is proposed, and its efficacy in music prediction\nand modeling of low-level repetition structure is demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 18:35:57 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Lattner", "Stefan", ""]]}, {"id": "2001.02002", "submitter": "Hanhe Lin", "authors": "Hanhe Lin, Vlad Hosu, Chunling Fan, Yun Zhang, Yuchen Mu, Raouf\n  Hamzaoui, Dietmar Saupe", "title": "SUR-FeatNet: Predicting the Satisfied User Ratio Curvefor Image\n  Compression with Deep Feature Learning", "comments": null, "journal-ref": "Quality and User Experience (2020) 5:5", "doi": "10.1007/s41233-020-00034-1", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The satisfied user ratio (SUR) curve for a lossy image compression scheme,\ne.g., JPEG, characterizes the complementary cumulative distribution function of\nthe just noticeable difference (JND), the smallest distortion level that can be\nperceived by a subject when a reference image is compared to a distorted one. A\nsequence of JNDs can be defined with a suitable successive choice of reference\nimages. We propose the first deep learning approach to predict SUR curves. We\nshow how to apply maximum likelihood estimation and the Anderson-Darling test\nto select a suitable parametric model for the distribution function. We then\nuse deep feature learning to predict samples of the SUR curve and apply the\nmethod of least squares to fit the parametric model to the predicted samples.\nOur deep learning approach relies on a siamese convolutional neural network,\ntransfer learning, and deep feature learning, using pairs consisting of a\nreference image and a compressed image for training. Experiments on the MCL-JCI\ndataset showed state-of-the-art performance. For example, the mean\nBhattacharyya distances between the predicted and ground truth first, second,\nand third JND distributions were 0.0810, 0.0702, and 0.0522, respectively, and\nthe corresponding average absolute differences of the peak signal-to-noise\nratio at a median of the first JND distribution were 0.58, 0.69, and 0.58 dB.\nFurther experiments on the JND-Pano dataset showed that the method transfers\nwell to high resolution panoramic images viewed on head-mounted displays.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 12:37:07 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 08:55:22 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Lin", "Hanhe", ""], ["Hosu", "Vlad", ""], ["Fan", "Chunling", ""], ["Zhang", "Yun", ""], ["Mu", "Yuchen", ""], ["Hamzaoui", "Raouf", ""], ["Saupe", "Dietmar", ""]]}, {"id": "2001.02653", "submitter": "Th\\'eo Taburet", "authors": "Taburet Th\\'eo, Bas Patrick, Sawaya Wadih, Jessica Fridrich", "title": "Natural Steganography in JPEG Domain with a Linear Development Pipeline", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve high practical security, Natural Steganography (NS) uses\ncover images captured at ISO sensitivity $ISO_{1}$ and generates stego images\nmimicking ISO sensitivity $ISO_{2}>ISO_{1}$. This is achieved by adding a stego\nsignal to the cover that mimics the sensor photonic noise. This paper proposes\nan embedding mechanism to perform NS in the JPEG domain after linear\ndevelopments by explicitly computing the correlations between DCT coefficients\nbefore quantization. In order to compute the covariance matrix of the photonic\nnoise in the DCT domain, we first develop the matrix representation of\ndemosaicking, luminance averaging, pixel section, and 2D-DCT. A detailed\nanalysis of the resulting covariance matrix is done in order to explain the\norigins of the correlations between the coefficients of $3\\times3$ DCT blocks.\nAn embedding scheme is then presented that takes in order to take into account\nall the correlations. It employs 4 sub-lattices and 64 lattices per\nsub-lattices. The modification probabilities of each DCT coefficient are then\nderived by computing conditional probabilities from the multivariate Gaussian\ndistribution using the Cholesky decomposition of the covariance matrix. This\nderivation is also used to compute the embedding capacity of each image. Using\na specific database called E1 Base, we show that in the JPEG domain NS\n(J-Cov-NS) enables to achieve high capacity (more than 2 bits per non-zero AC\nDCT) and with high practical security ($P_{\\mathrm{E}}\\simeq40\\%$ using DCTR\nfrom QF 75 to QF 100).\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 17:50:32 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 23:24:17 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Th\u00e9o", "Taburet", ""], ["Patrick", "Bas", ""], ["Wadih", "Sawaya", ""], ["Fridrich", "Jessica", ""]]}, {"id": "2001.03251", "submitter": "Shadrokh Samavi", "authors": "Mahnoosh Bagheri, Majid Mohrekesh, Nader Karimi, Shadrokh Samavi", "title": "Adaptive Control of Embedding Strength in Image Watermarking using\n  Neural Networks", "comments": "4 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image watermarking has been widely used in different applications\nsuch as copyright protection of digital media, such as audio, image, and video\nfiles. Two opposing criteria of robustness and transparency are the goals of\nwatermarking methods. In this paper, we propose a framework for determining the\nappropriate embedding strength factor. The framework can use most DWT and DCT\nbased blind watermarking approaches. We use Mask R-CNN on the COCO dataset to\nfind a good strength factor for each sub-block. Experiments show that this\nmethod is robust against different attacks and has good transparency.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:08:34 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Bagheri", "Mahnoosh", ""], ["Mohrekesh", "Majid", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2001.03353", "submitter": "Yiwei Zhang", "authors": "Yiwei Zhang, Xueting Wang, Yoshiaki Sakai, and Toshihiko Yamasaki", "title": "Measuring Similarity between Brands using Followers' Post in Social\n  Media", "comments": "Accepted to ACM Multimedia Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new measure to estimate the similarity between\nbrands via posts of brands' followers on social network services (SNS). Our\nmethod was developed with the intention of exploring the brands that customers\nare likely to jointly purchase. Nowadays, brands use social media for targeted\nadvertising because influencing users' preferences can greatly affect the\ntrends in sales. We assume that data on SNS allows us to make quantitative\ncomparisons between brands. Our proposed algorithm analyzes the daily photos\nand hashtags posted by each brand's followers. By clustering them and\nconverting them to histograms, we can calculate the similarity between brands.\nWe evaluated our proposed algorithm with purchase logs, credit card\ninformation, and answers to the questionnaires. The experimental results show\nthat the purchase data maintained by a mall or a credit card company can\npredict the co-purchase very well, but not the customer's willingness to buy\nproducts of new brands. On the other hand, our method can predict the users'\ninterest on brands with a correlation value over 0.53, which is pretty high\nconsidering that such interest to brands are high subjective and individual\ndependent.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 09:04:37 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Zhang", "Yiwei", ""], ["Wang", "Xueting", ""], ["Sakai", "Yoshiaki", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2001.03536", "submitter": "RanSheng Feng", "authors": "Jie Li, Ransheng Feng, Zhi Liu, Wei Sun, Qiyue Li", "title": "QoE-driven Coupled Uplink and Downlink Rate Adaptation for 360-degree\n  Video Live Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360-degree video provides an immersive 360-degree viewing experience and has\nbeen widely used in many areas. The 360-degree video live streaming systems\ninvolve capturing, compression, uplink (camera to video server) and downlink\n(video server to user) transmissions. However, few studies have jointly\ninvestigated such complex systems, especially the rate adaptation for the\ncoupled uplink and downlink in the 360-degree video streaming under limited\nbandwidth constraints. In this letter, we propose a quality of experience\n(QoE)-driven 360-degree video live streaming system, in which a video server\nperforms rate adaptation based on the uplink and downlink bandwidths and\ninformation concerning each user's real-time field-of-view (FOV). We formulate\nit as a nonlinear integer programming problem and propose an algorithm, which\ncombines the Karush-Kuhn-Tucker (KKT) condition and branch and bound method, to\nsolve it. The numerical results show that the proposed optimization model can\nimprove users' QoE significantly in comparison with other baseline schemes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:06:24 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Li", "Jie", ""], ["Feng", "Ransheng", ""], ["Liu", "Zhi", ""], ["Sun", "Wei", ""], ["Li", "Qiyue", ""]]}, {"id": "2001.03542", "submitter": "Glenn Van Wallendael", "authors": "Jolien De Letter, Anissa All, Lieven De Marez, Vasileios Avramelos,\n  Peter Lambert, Glenn Van Wallendael", "title": "Exploratory Study on User's Dynamic Visual Acuity and Quality Perception\n  of Impaired Images", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we assess the impact of head movement on user's visual acuity\nand their quality perception of impaired images. There are physical limitations\non the amount of visual information a person can perceive and physical\nlimitations regarding the speed at which our body, and as a consequence our\nhead, can explore a scene. In these limitations lie fundamental solutions for\nthe communication of multimedia systems. As such, subjects were asked to\nevaluate the perceptual quality of static images presented on a TV screen while\ntheir head was in a dynamic (moving) state. The idea is potentially applicable\nto virtual reality applications and therefore, we also measured the image\nquality perception of each subject on a head mounted display. Experiments show\nthe significant decrease in visual acuity and quality perception when the\nuser's head is not static, and give an indication on how much the quality can\nbe reduced without the user noticing any impairments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:15:06 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["De Letter", "Jolien", ""], ["All", "Anissa", ""], ["De Marez", "Lieven", ""], ["Avramelos", "Vasileios", ""], ["Lambert", "Peter", ""], ["Van Wallendael", "Glenn", ""]]}, {"id": "2001.04316", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Konstantinos Vougioukas, Pingchuan Ma, Stavros\n  Petridis, Maja Pantic", "title": "Visually Guided Self Supervised Learning of Speech Representations", "comments": "Accepted at ICASSP 2020 v2: Updated to the ICASSP 2020 camera ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self supervised representation learning has recently attracted a lot of\nresearch interest for both the audio and visual modalities. However, most works\ntypically focus on a particular modality or feature alone and there has been\nvery limited work that studies the interaction between the two modalities for\nlearning self supervised representations. We propose a framework for learning\naudio representations guided by the visual modality in the context of\naudiovisual speech. We employ a generative audio-to-video training scheme in\nwhich we animate a still image corresponding to a given audio clip and optimize\nthe generated video to be as close as possible to the real video of the speech\nsegment. Through this process, the audio encoder network learns useful speech\nrepresentations that we evaluate on emotion recognition and speech recognition.\nWe achieve state of the art results for emotion recognition and competitive\nresults for speech recognition. This demonstrates the potential of visual\nsupervision for learning audio representations as a novel way for\nself-supervised learning which has not been explored in the past. The proposed\nunsupervised audio features can leverage a virtually unlimited amount of\ntraining data of unlabelled audiovisual speech and have a large number of\npotentially promising applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:53:22 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 12:51:50 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Shukla", "Abhinav", ""], ["Vougioukas", "Konstantinos", ""], ["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2001.04463", "submitter": "Kangle Deng", "authors": "Kangle Deng and Aayush Bansal and Deva Ramanan", "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders", "comments": "ICLR 2021; Project page -- https://www.cs.cmu.edu/~exemplar-ae/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:56:45 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 17:59:14 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 05:24:44 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Deng", "Kangle", ""], ["Bansal", "Aayush", ""], ["Ramanan", "Deva", ""]]}, {"id": "2001.04580", "submitter": "Xiyang Luo", "authors": "Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, Peyman Milanfar", "title": "Distortion Agnostic Deep Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking is the process of embedding information into an image that can\nsurvive under distortions, while requiring the encoded image to have little or\nno perceptual difference from the original image. Recently, deep learning-based\nmethods achieved impressive results in both visual quality and message payload\nunder a wide variety of image distortions. However, these methods all require\ndifferentiable models for the image distortions at training time, and may\ngeneralize poorly to unknown distortions. This is undesirable since the types\nof distortions applied to watermarked images are usually unknown and\nnon-differentiable. In this paper, we propose a new framework for\ndistortion-agnostic watermarking, where the image distortion is not explicitly\nmodeled during training. Instead, the robustness of our system comes from two\nsources: adversarial training and channel coding. Compared to training on a\nfixed set of distortions and noise levels, our method achieves comparable or\nbetter results on distortions available during training, and better performance\non unknown distortions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 01:04:59 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Luo", "Xiyang", ""], ["Zhan", "Ruohan", ""], ["Chang", "Huiwen", ""], ["Yang", "Feng", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2001.04883", "submitter": "C. Estelle Smith", "authors": "C. Estelle Smith, Eduardo Nevarez, Haiyi Zhu", "title": "Disseminating Research News in HCI: Perceived Hazards, How-To's, and\n  Opportunities for Innovation", "comments": "10 pages, 2 figures, accepted paper to CHI 2020 conference", "journal-ref": null, "doi": "10.1145/3313831.3376744", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass media afford researchers critical opportunities to disseminate research\nfindings and trends to the general public. Yet researchers also perceive that\ntheir work can be miscommunicated in mass media, thus generating unintended\nunderstandings of HCI research by the general public. We conduct a Grounded\nTheory analysis of interviews with 12 HCI researchers and find that\nmiscommunication can occur at four origins along the socio-technical\ninfrastructure known as the Media Production Pipeline (MPP) for science news.\nResults yield researchers' perceived hazards of disseminating their work\nthrough mass media, as well as strategies for fostering effective communication\nof research. We conclude with implications for augmenting or innovating new MPP\ntechnologies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:34:24 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Smith", "C. Estelle", ""], ["Nevarez", "Eduardo", ""], ["Zhu", "Haiyi", ""]]}, {"id": "2001.05200", "submitter": "Ikram Achar", "authors": "Rabie Hachemi, Ikram Achar, Biasi Wiga, Mahfoud Sidi Ali Mebarek", "title": "Evaluating image matching methods for book cover identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of identifying a book only by looking at its cover, but\nhow can computers do the same? In this paper, we explore different feature\ndetectors and matching methods for book cover identification, and compare their\nperformances in terms of both speed and accuracy. This will allow, for example,\nlibraries to develop interactive services based on cover book picture. Only one\nsingle image of a cover book needs to be available through a database. Tests\nhave been performed by taking into account different transformations of each\nbook cover image. Encouraging results have been achieved.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 09:52:38 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Hachemi", "Rabie", ""], ["Achar", "Ikram", ""], ["Wiga", "Biasi", ""], ["Mebarek", "Mahfoud Sidi Ali", ""]]}, {"id": "2001.05201", "submitter": "Wayne Wu", "authors": "Linsen Song, Wayne Wu, Chen Qian, Ran He, Chen Change Loy", "title": "Everybody's Talkin': Let Me Talk as You Want", "comments": "Technical report. Project page:\n  https://wywu.github.io/projects/EBT/EBT.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to edit a target portrait footage by taking a sequence of\naudio as input to synthesize a photo-realistic video. This method is unique\nbecause it is highly dynamic. It does not assume a person-specific rendering\nnetwork yet capable of translating arbitrary source audio into arbitrary video\noutput. Instead of learning a highly heterogeneous and nonlinear mapping from\naudio to the video directly, we first factorize each target video frame into\northogonal parameter spaces, i.e., expression, geometry, and pose, via\nmonocular 3D face reconstruction. Next, a recurrent network is introduced to\ntranslate source audio into expression parameters that are primarily related to\nthe audio content. The audio-translated expression parameters are then used to\nsynthesize a photo-realistic human subject in each video frame, with the\nmovement of the mouth regions precisely mapped to the source audio. The\ngeometry and pose parameters of the target human portrait are retained,\ntherefore preserving the context of the original video footage. Finally, we\nintroduce a novel video rendering network and a dynamic programming method to\nconstruct a temporally coherent and photo-realistic video. Extensive\nexperiments demonstrate the superiority of our method over existing approaches.\nOur method is end-to-end learnable and robust to voice variations in the source\naudio.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 09:54:23 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Song", "Linsen", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["He", "Ran", ""], ["Loy", "Chen Change", ""]]}, {"id": "2001.05864", "submitter": "Yiyan Chen", "authors": "Yiyan Chen, Li Tao, Xueting Wang and Toshihiko Yamasaki", "title": "Weakly Supervised Video Summarization by Hierarchical Reinforcement\n  Learning", "comments": "mmasia 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional video summarization approaches based on reinforcement learning\nhave the problem that the reward can only be received after the whole summary\nis generated. Such kind of reward is sparse and it makes reinforcement learning\nhard to converge. Another problem is that labelling each frame is tedious and\ncostly, which usually prohibits the construction of large-scale datasets. To\nsolve these problems, we propose a weakly supervised hierarchical reinforcement\nlearning framework, which decomposes the whole task into several subtasks to\nenhance the summarization quality. This framework consists of a manager network\nand a worker network. For each subtask, the manager is trained to set a subgoal\nonly by a task-level binary label, which requires much fewer labels than\nconventional approaches. With the guide of the subgoal, the worker predicts the\nimportance scores for video frames in the subtask by policy gradient according\nto both global reward and innovative defined sub-rewards to overcome the sparse\nproblem. Experiments on two benchmark datasets show that our proposal has\nachieved the best performance, even better than supervised approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 07:47:02 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 15:31:24 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Yiyan", ""], ["Tao", "Li", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2001.05970", "submitter": "Viet Duong", "authors": "Viet Duong, Phu Pham, Ritwik Bose, Jiebo Luo", "title": "#MeToo on Campus: Studying College Sexual Assault at Scale Using Data\n  Reported on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the emergence of the #MeToo trend on social media has empowered\nthousands of people to share their own sexual harassment experiences. This\nviral trend, in conjunction with the massive personal information and content\navailable on Twitter, presents a promising opportunity to extract data driven\ninsights to complement the ongoing survey based studies about sexual harassment\nin college. In this paper, we analyze the influence of the #MeToo trend on a\npool of college followers. The results show that the majority of topics\nembedded in those #MeToo tweets detail sexual harassment stories, and there\nexists a significant correlation between the prevalence of this trend and\nofficial reports on several major geographical regions. Furthermore, we\ndiscover the outstanding sentiments of the #MeToo tweets using deep semantic\nmeaning representations and their implications on the affected users\nexperiencing different types of sexual harassment. We hope this study can raise\nfurther awareness regarding sexual misconduct in academia.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:05:46 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Duong", "Viet", ""], ["Pham", "Phu", ""], ["Bose", "Ritwik", ""], ["Luo", "Jiebo", ""]]}, {"id": "2001.06466", "submitter": "Serhan G\\\"ul", "authors": "Serhan G\\\"ul, Dimitri Podborski, Thomas Buchholz, Thomas Schierl,\n  Cornelius Hellge", "title": "Low-latency Cloud-based Volumetric Video Streaming Using Head Motion\n  Prediction", "comments": "7 pages, 4 figures", "journal-ref": "30th ACM Workshop on Network and Operating Systems Support for\n  Digital Audio and Video (NOSSDAV) 2020", "doi": "10.1145/3386290.3396933", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric video is an emerging key technology for immersive representation\nof 3D spaces and objects. Rendering volumetric video requires lots of\ncomputational power which is challenging especially for mobile devices. To\nmitigate this, we developed a streaming system that renders a 2D view from the\nvolumetric video at a cloud server and streams a 2D video stream to the client.\nHowever, such network-based processing increases the motion-to-photon (M2P)\nlatency due to the additional network and processing delays. In order to\ncompensate the added latency, prediction of the future user pose is necessary.\nWe developed a head motion prediction model and investigated its potential to\nreduce the M2P latency for different look-ahead times. Our results show that\nthe presented model reduces the rendering errors caused by the M2P latency\ncompared to a baseline system in which no prediction is performed.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:35:31 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 13:45:30 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["G\u00fcl", "Serhan", ""], ["Podborski", "Dimitri", ""], ["Buchholz", "Thomas", ""], ["Schierl", "Thomas", ""], ["Hellge", "Cornelius", ""]]}, {"id": "2001.06765", "submitter": "Amit Kumar Jaiswal", "authors": "Amit Kumar Jaiswal, Haiming Liu, Ingo Frommholz", "title": "Information Foraging for Enhancing Implicit Feedback in Content-based\n  Image Recommendation", "comments": "FIRE '19: Proceedings of the 11th Forum for Information Retrieval\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User implicit feedback plays an important role in recommender systems.\nHowever, finding implicit features is a tedious task. This paper aims to\nidentify users' preferences through implicit behavioural signals for image\nrecommendation based on the Information Scent Model of Information Foraging\nTheory. In the first part, we hypothesise that the users' perception is\nimproved with visual cues in the images as behavioural signals that provide\nusers' information scent during information seeking. We designed a\ncontent-based image recommendation system to explore which image attributes\n(i.e., visual cues or bookmarks) help users find their desired image. We found\nthat users prefer recommendations predicated by visual cues and therefore\nconsider the visual cues as good information scent for their information\nseeking. In the second part, we investigated if visual cues in the images\ntogether with the images itself can be better perceived by the users than each\nof them on its own. We evaluated the information scent artifacts in image\nrecommendation on the Pinterest image collection and the WikiArt dataset. We\nfind our proposed image recommendation system supports the implicit signals\nthrough Information Foraging explanation of the information scent model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 03:36:33 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Jaiswal", "Amit Kumar", ""], ["Liu", "Haiming", ""], ["Frommholz", "Ingo", ""]]}, {"id": "2001.06888", "submitter": "Meysam Asgari-Chenaghlu", "authors": "Meysam Asgari-Chenaghlu, M.Reza Feizi-Derakhshi, Leili Farzinvash, M.\n  A. Balafar, Cina Motamed", "title": "A multimodal deep learning approach for named entity recognition from\n  social media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) from social media posts is a challenging task.\nUser generated content that forms the nature of social media, is noisy and\ncontains grammatical and linguistic errors. This noisy content makes it much\nharder for tasks such as named entity recognition. We propose two novel deep\nlearning approaches utilizing multimodal deep learning and Transformers. Both\nof our approaches use image features from short social media posts to provide\nbetter results on the NER task. On the first approach, we extract image\nfeatures using InceptionV3 and use fusion to combine textual and image\nfeatures. This presents more reliable name entity recognition when the images\nrelated to the entities are provided by the user. On the second approach, we\nuse image features combined with text and feed it into a BERT like Transformer.\nThe experimental results, namely, the precision, recall and F1 score metrics\nshow the superiority of our work compared to other state-of-the-art NER\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 19:37:45 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 20:00:12 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 12:29:04 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Asgari-Chenaghlu", "Meysam", ""], ["Feizi-Derakhshi", "M. Reza", ""], ["Farzinvash", "Leili", ""], ["Balafar", "M. A.", ""], ["Motamed", "Cina", ""]]}, {"id": "2001.07194", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Shaunak Mishra, Manisha Verma, Narayan Bhamidipati, and\n  Wei Wang", "title": "Recommending Themes for Ad Creative Design via Visual-Linguistic\n  Representations", "comments": "7 pages, 8 figures, 2 tables, accepted by The Web Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380001", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a perennial need in the online advertising industry to refresh ad\ncreatives, i.e., images and text used for enticing online users towards a\nbrand. Such refreshes are required to reduce the likelihood of ad fatigue among\nonline users, and to incorporate insights from other successful campaigns in\nrelated product categories. Given a brand, to come up with themes for a new ad\nis a painstaking and time consuming process for creative strategists.\nStrategists typically draw inspiration from the images and text used for past\nad campaigns, as well as world knowledge on the brands. To automatically infer\nad themes via such multimodal sources of information in past ad campaigns, we\npropose a theme (keyphrase) recommender system for ad creative strategists. The\ntheme recommender is based on aggregating results from a visual question\nanswering (VQA) task, which ingests the following: (i) ad images, (ii) text\nassociated with the ads as well as Wikipedia pages on the brands in the ads,\nand (iii) questions around the ad. We leverage transformer based cross-modality\nencoders to train visual-linguistic representations for our VQA task. We study\ntwo formulations for the VQA task along the lines of classification and\nranking; via experiments on a public dataset, we show that cross-modal\nrepresentations lead to significantly better classification accuracy and\nranking precision-recall metrics. Cross-modal representations show better\nperformance compared to separate image and text representations. In addition,\nthe use of multimodal information shows a significant lift over using only\ntextual or visual information.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 18:04:10 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 23:05:46 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhou", "Yichao", ""], ["Mishra", "Shaunak", ""], ["Verma", "Manisha", ""], ["Bhamidipati", "Narayan", ""], ["Wang", "Wei", ""]]}, {"id": "2001.07295", "submitter": "Clayton Morrison", "authors": "Adarsh Pyarelal and Marco A. Valenzuela-Escarcega and Rebecca Sharp\n  and Paul D. Hein, Jon Stephens, Pratik Bhandari, HeuiChan Lim, Saumya Debray,\n  Clayton T. Morrison", "title": "AutoMATES: Automated Model Assembly from Text, Equations, and Software", "comments": "8 pages, 6 figures, accepted to Modeling the World's Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of complicated systems can be represented in different ways - in\nscientific papers, they are represented using natural language text as well as\nequations. But to be of real use, they must also be implemented as software,\nthus making code a third form of representing models. We introduce the\nAutoMATES project, which aims to build semantically-rich unified\nrepresentations of models from scientific code and publications to facilitate\nthe integration of computational models from different domains and allow for\nmodeling large, complicated systems that span multiple domains and levels of\nabstraction.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 00:33:40 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Pyarelal", "Adarsh", ""], ["Valenzuela-Escarcega", "Marco A.", ""], ["Sharp", "Rebecca", ""], ["Hein", "Paul D.", ""], ["Stephens", "Jon", ""], ["Bhandari", "Pratik", ""], ["Lim", "HeuiChan", ""], ["Debray", "Saumya", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "2001.07494", "submitter": "Mahieddine Djoudi", "authors": "Ghalia Merzougui, Roumaissa Dehkal, Maheiddine Djoudi (TECHN\\'E - EA\n  6316)", "title": "Evaluation of a course mediatised with Xerte", "comments": null, "journal-ref": "International Conference of Computing for Engineering and Sciences\n  (ICCES'2015), Jul 2015, Istanbul,, Turkey", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive multimedia educational content has recently been of interest to\nattract attention on the learner and increase understanding by the latter. In\nparallel several open source authoring tools offer a quick and easy production\nof this type of content. As such, our contribution is to mediatize a course\ni.e. 'English' with the authoring system 'Xerte' which is intended both for\nsimple users and developers in ActionScript. An experiment of course is\nconducted on a sample of a private school's students. At the end of this\nexperience, we administered a questionnaire to evaluate the device, the results\nobtained, evidenced by the favorable reception of interactive multimedia\nintegration in educational content.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 13:04:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Merzougui", "Ghalia", "", "TECHN\u00c9 - EA\n  6316"], ["Dehkal", "Roumaissa", "", "TECHN\u00c9 - EA\n  6316"], ["Djoudi", "Maheiddine", "", "TECHN\u00c9 - EA\n  6316"]]}, {"id": "2001.07886", "submitter": "Jack Stokes III", "authors": "Paul England, Henrique S. Malvar, Eric Horvitz, Jack W. Stokes,\n  C\\'edric Fournet, Rebecca Burke-Aguero, Amaury Chamayou, Sylvan Clebsch,\n  Manuel Costa, John Deutscher, Shabnam Erfani, Matt Gaylor, Andrew Jenks,\n  Kevin Kane, Elissa Redmiles, Alex Shamis, Isha Sharma, Sam Wenker, Anika\n  Zaman", "title": "AMP: Authentication of Media via Provenance", "comments": "Add detailed manifest description, Add provenance, Improve text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in graphics and machine learning have led to the general\navailability of easy-to-use tools for modifying and synthesizing media. The\nproliferation of these tools threatens to cast doubt on the veracity of all\nmedia. One approach to thwarting the flow of fake media is to detect modified\nor synthesized media through machine learning methods. While detection may help\nin the short term, we believe that it is destined to fail as the quality of\nfake media generation continues to improve. Soon, neither humans nor algorithms\nwill be able to reliably distinguish fake versus real content. Thus, pipelines\nfor assuring the source and integrity of media will be required---and\nincreasingly relied upon. We propose AMP, a system that ensures the\nauthentication of media via certifying provenance. AMP creates one or more\npublisher-signed manifests for a media instance uploaded by a content provider.\nThese manifests are stored in a database allowing fast lookup from applications\nsuch as browsers. For reference, the manifests are also registered and signed\nby a permissioned ledger, implemented using the Confidential Consortium\nFramework (CCF). CCF employs both software and hardware techniques to ensure\nthe integrity and transparency of all registered manifests. AMP, through its\nuse of CCF, enables a consortium of media providers to govern the service while\nmaking all its operations auditable. The authenticity of the media can be\ncommunicated to the user via visual elements in the browser, indicating that an\nAMP manifest has been successfully located and verified.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 05:51:30 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 01:05:31 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 21:17:14 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["England", "Paul", ""], ["Malvar", "Henrique S.", ""], ["Horvitz", "Eric", ""], ["Stokes", "Jack W.", ""], ["Fournet", "C\u00e9dric", ""], ["Burke-Aguero", "Rebecca", ""], ["Chamayou", "Amaury", ""], ["Clebsch", "Sylvan", ""], ["Costa", "Manuel", ""], ["Deutscher", "John", ""], ["Erfani", "Shabnam", ""], ["Gaylor", "Matt", ""], ["Jenks", "Andrew", ""], ["Kane", "Kevin", ""], ["Redmiles", "Elissa", ""], ["Shamis", "Alex", ""], ["Sharma", "Isha", ""], ["Wenker", "Sam", ""], ["Zaman", "Anika", ""]]}, {"id": "2001.08730", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Shivansh Pate, and Vinay P. Namboodiri", "title": "Robust Explanations for Visual Question Answering", "comments": "WACV-2020 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a method to obtain robust explanations for visual\nquestion answering(VQA) that correlate well with the answers. Our model\nexplains the answers obtained through a VQA model by providing visual and\ntextual explanations. The main challenges that we address are i) Answers and\ntextual explanations obtained by current methods are not well correlated and\nii) Current methods for visual explanation do not focus on the right location\nfor explaining the answer. We address both these challenges by using a\ncollaborative correlated module which ensures that even if we do not train for\nnoise based attacks, the enhanced correlation ensures that the right\nexplanation and answer can be generated. We further show that this also aids in\nimproving the generated visual and textual explanations. The use of the\ncorrelated module can be thought of as a robust method to verify if the answer\nand explanations are coherent. We evaluate this model using VQA-X dataset. We\nobserve that the proposed method yields better textual and visual justification\nthat supports the decision. We showcase the robustness of the model against a\nnoise-based perturbation attack using corresponding visual and textual\nexplanations. A detailed empirical analysis is shown. Here we provide source\ncode link for our model \\url{https://github.com/DelTA-Lab-IITK/CCM-WACV}.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:43:34 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Patro", "Badri N.", ""], ["Pate", "Shivansh", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2001.08779", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Vinod K. Kurmi, Sandeep Kumar, and Vinay P. Namboodiri", "title": "Deep Bayesian Network for Visual Question Generation", "comments": "WACV-2020 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generating natural questions from an image is a semantic task that requires\nusing vision and language modalities to learn multimodal representations.\nImages can have multiple visual and language cues such as places, captions, and\ntags. In this paper, we propose a principled deep Bayesian learning framework\nthat combines these cues to produce natural questions. We observe that with the\naddition of more cues and by minimizing uncertainty in the among cues, the\nBayesian network becomes more confident. We propose a Minimizing Uncertainty of\nMixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues\nexperts for generating probabilistic questions. This is a Bayesian framework\nand the results show a remarkable similarity to natural questions as validated\nby a human study. We observe that with the addition of more cues and by\nminimizing uncertainty among the cues, the Bayesian framework becomes more\nconfident. Ablation studies of our model indicate that a subset of cues is\ninferior at this task and hence the principled fusion of cues is preferred.\nFurther, we observe that the proposed approach substantially improves over\nstate-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE,\nand CIDEr). Here we provide project link for Deep Bayesian VQG\n\\url{https://delta-lab-iitk.github.io/BVQG/}\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 19:37:20 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Patro", "Badri N.", ""], ["Kurmi", "Vinod K.", ""], ["Kumar", "Sandeep", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2001.09545", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "aiTPR: Attribute Interaction-Tensor Product Representation for Image\n  Caption", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region visual features enhance the generative capability of the machines\nbased on features, however they lack proper interaction attentional perceptions\nand thus ends up with biased or uncorrelated sentences or pieces of\nmisinformation. In this work, we propose Attribute Interaction-Tensor Product\nRepresentation (aiTPR) which is a convenient way of gathering more information\nthrough orthogonal combination and learning the interactions as physical\nentities (tensors) and improving the captions. Compared to previous works,\nwhere features are added up to undefined feature spaces, TPR helps in\nmaintaining sanity in combinations and orthogonality helps in defining familiar\nspaces. We have introduced a new concept layer that defines the objects and\nalso their interactions that can play a crucial role in determination of\ndifferent descriptions. The interaction portions have contributed heavily for\nbetter caption quality and has out-performed different previous works on this\ndomain and MSCOCO dataset. We introduced, for the first time, the notion of\ncombining regional image features and abstracted interaction likelihood\nembedding for image captioning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 00:19:41 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "2001.10190", "submitter": "Tomohiko Nakamura", "authors": "Tomohiko Nakamura and Hiroshi Saruwatari", "title": "Time-Domain Audio Source Separation Based on Wave-U-Net Combined with\n  Discrete Wavelet Transform", "comments": "5 pages, to appear in IEEE International Conference on Acoustics,\n  Speech, and Signal Processing 2020 (ICASSP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a time-domain audio source separation method using down-sampling\n(DS) and up-sampling (US) layers based on a discrete wavelet transform (DWT).\nThe proposed method is based on one of the state-of-the-art deep neural\nnetworks, Wave-U-Net, which successively down-samples and up-samples feature\nmaps. We find that this architecture resembles that of multiresolution\nanalysis, and reveal that the DS layers of Wave-U-Net cause aliasing and may\ndiscard information useful for the separation. Although the effects of these\nproblems may be reduced by training, to achieve a more reliable source\nseparation method, we should design DS layers capable of overcoming the\nproblems. With this belief, focusing on the fact that the DWT has an\nanti-aliasing filter and the perfect reconstruction property, we design the\nproposed layers. Experiments on music source separation show the efficacy of\nthe proposed method and the importance of simultaneously considering the\nanti-aliasing filters and the perfect reconstruction property.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 06:43:21 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Nakamura", "Tomohiko", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "2001.10590", "submitter": "Milad Taleby Ahvanooey", "authors": "Amir Vatani, Milad Taleby Ahvanooey, Mostafa Rahimi", "title": "An Effective Automatic Image Annotation Model Via Attention Model and\n  Data Equilibrium", "comments": "9 pages, 3 figures", "journal-ref": "Int. J. Adv. Comput. Sci. Appl, 9(3), pp.269-277 (2018)", "doi": "10.14569/IJACSA.2018.090338", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, a huge number of images are available. However, retrieving a\nrequired image for an ordinary user is a challenging task in computer vision\nsystems. During the past two decades, many types of research have been\nintroduced to improve the performance of the automatic annotation of images,\nwhich are traditionally focused on content-based image retrieval. Although,\nrecent research demonstrates that there is a semantic gap between content-based\nimage retrieval and image semantics understandable by humans. As a result,\nexisting research in this area has caused to bridge the semantic gap between\nlow-level image features and high-level semantics. The conventional method of\nbridging the semantic gap is through the automatic image annotation (AIA) that\nextracts semantic features using machine learning techniques. In this paper, we\npropose a novel AIA model based on the deep learning feature extraction method.\nThe proposed model has three phases, including a feature extractor, a tag\ngenerator, and an image annotator. First, the proposed model extracts\nautomatically the high and low-level features based on dual-tree continues\nwavelet transform (DT-CWT), singular value decomposition, distribution of color\nton, and the deep neural network. Moreover, the tag generator balances the\ndictionary of the annotated keywords by a new log-entropy auto-encoder (LEAE)\nand then describes these keywords by word embedding. Finally, the annotator\nworks based on the long-short-term memory (LSTM) network in order to obtain the\nimportance degree of specific features of the image. The experiments conducted\non two benchmark datasets confirm that the superiority of the proposed model\ncompared to the previous models in terms of performance criteria.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 05:59:57 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Vatani", "Amir", ""], ["Ahvanooey", "Milad Taleby", ""], ["Rahimi", "Mostafa", ""]]}, {"id": "2001.10832", "submitter": "Rohith Aralikatti", "authors": "Rohith Aralikatti, Sharad Roy, Abhinav Thanda, Dilip Kumar Margam,\n  Pujitha Appan Kandala, Tanay Sharma and Shankar M Venkatesan", "title": "Audio-Visual Decision Fusion for WFST-based and seq2seq Models", "comments": "Submitted for review to ICASSP 2020 on October 21st, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under noisy conditions, speech recognition systems suffer from high Word\nError Rates (WER). In such cases, information from the visual modality\ncomprising the speaker lip movements can help improve the performance. In this\nwork, we propose novel methods to fuse information from audio and visual\nmodalities at inference time. This enables us to train the acoustic and visual\nmodels independently. First, we train separate RNN-HMM based acoustic and\nvisual models. A common WFST generated by taking a special union of the HMM\ncomponents is used for decoding using a modified Viterbi algorithm. Second, we\ntrain separate seq2seq acoustic and visual models. The decoding step is\nperformed simultaneously for both modalities using shallow fusion while\nmaintaining a common hypothesis beam. We also present results for a novel\nseq2seq fusion without the weighing parameter. We present results at varying\nSNR and show that our methods give significant improvements over acoustic-only\nWER.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 13:45:08 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Aralikatti", "Rohith", ""], ["Roy", "Sharad", ""], ["Thanda", "Abhinav", ""], ["Margam", "Dilip Kumar", ""], ["Kandala", "Pujitha Appan", ""], ["Sharma", "Tanay", ""], ["Venkatesan", "Shankar M", ""]]}, {"id": "2001.11406", "submitter": "Helard Becerra Martinez Dr", "authors": "Helard Martinez, M. C. Farias, A. Hines", "title": "NAViDAd: A No-Reference Audio-Visual Quality Metric Based on a Deep\n  Autoencoder", "comments": "5 pages", "journal-ref": "2019 27th European Signal Processing Conference (EUSIPCO), IEEE,\n  2019, pp 1-5", "doi": "10.23919/EUSIPCO.2019.8902975", "report-no": null, "categories": "cs.MM cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of models for quality prediction of both audio and video\nsignals is a fairly mature field. But, although several multimodal models have\nbeen proposed, the area of audio-visual quality prediction is still an emerging\narea. In fact, despite the reasonable performance obtained by combination and\nparametric metrics, currently there is no reliable pixel-based audio-visual\nquality metric. The approach presented in this work is based on the assumption\nthat autoencoders, fed with descriptive audio and video features, might produce\na set of features that is able to describe the complex audio and video\ninteractions. Based on this hypothesis, we propose a No-Reference Audio-Visual\nQuality Metric Based on a Deep Autoencoder (NAViDAd). The model visual features\nare natural scene statistics (NSS) and spatial-temporal measures of the video\ncomponent. Meanwhile, the audio features are obtained by computing the\nspectrogram representation of the audio component. The model is formed by a\n2-layer framework that includes a deep autoencoder layer and a classification\nlayer. These two layers are stacked and trained to build the deep neural\nnetwork model. The model is trained and tested using a large set of stimuli,\ncontaining representative audio and video artifacts. The model performed well\nwhen tested against the UnB-AV and the LiveNetflix-II databases. %Results shows\nthat this type of approach produces quality scores that are highly correlated\nto subjective quality scores.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 15:40:08 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 19:08:49 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Martinez", "Helard", ""], ["Farias", "M. C.", ""], ["Hines", "A.", ""]]}, {"id": "2001.11847", "submitter": "Sara Mandelli", "authors": "Sara Mandelli, Davide Cozzolino, Paolo Bestagini, Luisa Verdoliva,\n  Stefano Tubaro", "title": "CNN-based fast source device identification", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2020.3008855", "report-no": null, "categories": "cs.NE cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source identification is an important topic in image forensics, since it\nallows to trace back the origin of an image. This represents a precious\ninformation to claim intellectual property but also to reveal the authors of\nillicit materials. In this paper we address the problem of device\nidentification based on sensor noise and propose a fast and accurate solution\nusing convolutional neural networks (CNNs). Specifically, we propose a\n2-channel-based CNN that learns a way of comparing camera fingerprint and image\nnoise at patch level. The proposed solution turns out to be much faster than\nthe conventional approach and to ensure an increased accuracy. This makes the\napproach particularly suitable in scenarios where large databases of images are\nanalyzed, like over social networks. In this vein, since images uploaded on\nsocial media usually undergo at least two compression stages, we include\ninvestigations on double JPEG compressed images, always reporting higher\naccuracy than standard approaches.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 14:01:45 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 16:01:01 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 13:57:26 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Mandelli", "Sara", ""], ["Cozzolino", "Davide", ""], ["Bestagini", "Paolo", ""], ["Verdoliva", "Luisa", ""], ["Tubaro", "Stefano", ""]]}]