[{"id": "1909.00741", "submitter": "Sreyasi Nag Chowdhury", "authors": "Sreyasi Nag Chowdhury, Niket Tandon, Hakan Ferhatosmanoglu, Gerhard\n  Weikum", "title": "VISIR: Visual and Semantic Image Label Refinement", "comments": "Published in WSDM 2018", "journal-ref": "ACM ISBN 978-1-4503-5581-0/18/02 2018", "doi": "10.1145/3159652.3159693", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media explosion has populated the Internet with a wealth of\nimages. There are two existing paradigms for image retrieval: 1) content-based\nimage retrieval (CBIR), which has traditionally used visual features for\nsimilarity search (e.g., SIFT features), and 2) tag-based image retrieval\n(TBIR), which has relied on user tagging (e.g., Flickr tags). CBIR now gains\nsemantic expressiveness by advances in deep-learning-based detection of visual\nlabels. TBIR benefits from query-and-click logs to automatically infer more\ninformative labels. However, learning-based tagging still yields noisy labels\nand is restricted to concrete objects, missing out on generalizations and\nabstractions. Click-based tagging is limited to terms that appear in the\ntextual context of an image or in queries that lead to a click. This paper\naddresses the above limitations by semantically refining and expanding the\nlabels suggested by learning-based object detection. We consider the semantic\ncoherence between the labels for different objects, leverage lexical and\ncommonsense knowledge, and cast the label assignment into a constrained\noptimization problem solved by an integer linear program. Experiments show that\nour method, called VISIR, improves the quality of the state-of-the-art visual\nlabeling tools like LSDA and YOLO.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:41:44 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chowdhury", "Sreyasi Nag", ""], ["Tandon", "Niket", ""], ["Ferhatosmanoglu", "Hakan", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1909.00952", "submitter": "Hilmi Enes Egilmez", "authors": "Hilmi E. Egilmez, Yung-Hsuan Chao, Antonio Ortega", "title": "Graph-based Transforms for Video Coding", "comments": "To appear in IEEE Trans. on Image Processing (14 pages)", "journal-ref": null, "doi": "10.1109/TIP.2020.3026627", "report-no": null, "categories": "eess.IV cs.LG cs.MM cs.SY eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many state-of-the-art compression systems, signal transformation is an\nintegral part of the encoding and decoding process, where transforms provide\ncompact representations for the signals of interest. This paper introduces a\nclass of transforms called graph-based transforms (GBTs) for video compression,\nand proposes two different techniques to design GBTs. In the first technique,\nwe formulate an optimization problem to learn graphs from data and provide\nsolutions for optimal separable and nonseparable GBT designs, called GL-GBTs.\nThe optimality of the proposed GL-GBTs is also theoretically analyzed based on\nGaussian-Markov random field (GMRF) models for intra and inter predicted block\nsignals. The second technique develops edge-adaptive GBTs (EA-GBTs) in order to\nflexibly adapt transforms to block signals with image edges (discontinuities).\nThe advantages of EA-GBTs are both theoretically and empirically demonstrated.\nOur experimental results demonstrate that the proposed transforms can\nsignificantly outperform the traditional Karhunen-Loeve transform (KLT).\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 04:53:53 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 08:44:44 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Egilmez", "Hilmi E.", ""], ["Chao", "Yung-Hsuan", ""], ["Ortega", "Antonio", ""]]}, {"id": "1909.01161", "submitter": "Yaqi Xie", "authors": "Yaqi Xie, Ziwei Xu, Mohan S. Kankanhalli, Kuldeep S. Meel, Harold Soh", "title": "Embedding Symbolic Knowledge into Deep Networks", "comments": "*Equal contribution; Accepted at conference Neural Information\n  Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to leverage prior symbolic knowledge to improve the\nperformance of deep models. We propose a graph embedding network that projects\npropositional formulae (and assignments) onto a manifold via an augmented Graph\nConvolutional Network (GCN). To generate semantically-faithful embeddings, we\ndevelop techniques to recognize node heterogeneity, and semantic regularization\nthat incorporate structural constraints into the embedding. Experiments show\nthat our approach improves the performance of models trained to perform\nentailment checking and visual relation prediction. Interestingly, we observe a\nconnection between the tractability of the propositional theory representation\nand the ease of embedding. Future exploration of this connection may elucidate\nthe relationship between knowledge compilation and vector representation\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:23:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:38:40 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 10:26:34 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 10:53:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Xie", "Yaqi", ""], ["Xu", "Ziwei", ""], ["Kankanhalli", "Mohan S.", ""], ["Meel", "Kuldeep S.", ""], ["Soh", "Harold", ""]]}, {"id": "1909.01285", "submitter": "Kevin Zhang", "authors": "Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, Kalyan\n  Veeramachaneni", "title": "Robust Invisible Video Watermarking with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of video watermarking is to embed a message within a video file in a\nway such that it minimally impacts the viewing experience but can be recovered\neven if the video is redistributed and modified, allowing media producers to\nassert ownership over their content. This paper presents RivaGAN, a novel\narchitecture for robust video watermarking which features a custom\nattention-based mechanism for embedding arbitrary data as well as two\nindependent adversarial networks which critique the video quality and optimize\nfor robustness. Using this technique, we are able to achieve state-of-the-art\nresults in deep learning-based video watermarking and produce watermarked\nvideos which have minimal visual distortion and are robust against common video\nprocessing operations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:28:40 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Kevin Alex", ""], ["Xu", "Lei", ""], ["Cuesta-Infante", "Alfredo", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "1909.01738", "submitter": "Jiahua Xu", "authors": "Jiahua Xu, Wei Zhou, Zhibo Chen, Suiyi Ling and Patrick Le Callet", "title": "Binocular Rivalry Oriented Predictive Auto-Encoding Network for Blind\n  Stereoscopic Image Quality Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereoscopic image quality measurement (SIQM) has become increasingly\nimportant for guiding stereo image processing and commutation systems due to\nthe widespread usage of 3D contents. Compared with conventional methods which\nare relied on hand-crafted features, deep learning oriented measurements have\nachieved remarkable performance in recent years. However, most existing deep\nSIQM evaluators are not specifically built for stereoscopic contents and\nconsider little prior domain knowledge of the 3D human visual system (HVS) in\nnetwork design. In this paper, we develop a Predictive Auto-encoDing Network\n(PAD-Net) for blind/No-Reference stereoscopic image quality measurement. In the\nfirst stage, inspired by the predictive coding theory that the cognition system\ntries to match bottom-up visual signal with top-down predictions, we adopt the\nencoder-decoder architecture to reconstruct the distorted inputs. Besides,\nmotivated by the binocular rivalry phenomenon, we leverage the likelihood and\nprior maps generated from the predictive coding process in the Siamese\nframework for assisting SIQM. In the second stage, quality regression network\nis applied to the fusion image for acquiring the perceptual quality prediction.\nThe performance of PAD-Net has been extensively evaluated on three benchmark\ndatabases and the superiority has been well validated on both symmetrically and\nasymmetrically distorted stereoscopic images under various distortion types.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:43:32 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 08:01:42 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 07:34:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Xu", "Jiahua", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""], ["Ling", "Suiyi", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1909.01860", "submitter": "Shiv Ram Dubey", "authors": "Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, Snehasis Mukherjee", "title": "Visual Question Answering using Deep Learning: A Survey and Performance\n  Analysis", "comments": "Accepted in Fifth IAPR International Conference on Computer Vision\n  and Image Processing (CVIP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Visual Question Answering (VQA) task combines challenges for processing\ndata with both Visual and Linguistic processing, to answer basic `common sense'\nquestions about given images. Given an image and a question in natural\nlanguage, the VQA system tries to find the correct answer to it using visual\nelements of the image and inference gathered from textual questions. In this\nsurvey, we cover and discuss the recent datasets released in the VQA domain\ndealing with various types of question-formats and robustness of the\nmachine-learning models. Next, we discuss about new deep learning models that\nhave shown promising results over the VQA datasets. At the end, we present and\ndiscuss some of the results computed by us over the vanilla VQA model, Stacked\nAttention Network and the VQA Challenge 2017 winner model. We also provide the\ndetailed analysis along with the challenges and future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 07:03:03 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 01:11:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Srivastava", "Yash", ""], ["Murali", "Vaishnav", ""], ["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1909.02294", "submitter": "Dawid Mieloch", "authors": "Dawid Mieloch, Olgierd Stankiewicz and Marek Doma\\'nski", "title": "Depth Map Estimation for Free-Viewpoint Television", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2019.2963487", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new method of depth estimation dedicated for\nfree-viewpoint television (FTV). The estimation is performed for segments and\nthus their size can be used to control a trade-off between the quality of depth\nmaps and the processing time of their estimation. The proposed algorithm can\ntake as its input multiple arbitrarily positioned views which are\nsimultaneously used to produce multiple inter view consistent output depth\nmaps. The presented depth estimation method uses novel parallelization and\ntemporal consistency enhancement methods that significantly reduce the\nprocessing time of depth estimation. An experimental assessment of the\nproposals has been performed, based on the analysis of virtual view quality in\nFTV. The results show that the proposed method provides an improvement of the\ndepth map quality over the state of-the-art method, simultaneously reducing the\ncomplexity of depth estimation. The consistency of depth maps, which is crucial\nfor the quality of the synthesized video and thus the quality of experience of\nnavigating through a 3D scene, is also vastly improved.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:06:28 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Mieloch", "Dawid", ""], ["Stankiewicz", "Olgierd", ""], ["Doma\u0144ski", "Marek", ""]]}, {"id": "1909.02358", "submitter": "Wei Zhou", "authors": "Wei Zhou, Likun Shi, Zhibo Chen", "title": "Tensor Oriented No-Reference Light Field Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field image (LFI) quality assessment is becoming more and more\nimportant, which helps to better guide the acquisition, processing and\napplication of immersive media. However, due to the inherent high dimensional\ncharacteristics of LFI, the LFI quality assessment turns into a\nmulti-dimensional problem that requires consideration of the quality\ndegradation in both spatial and angular dimensions. Therefore, we propose a\nnovel Tensor oriented No-reference Light Field image Quality evaluator\n(Tensor-NLFQ) based on tensor theory. Specifically, since the LFI is regarded\nas a low-rank 4D tensor, the principle components of four oriented sub-aperture\nview stacks are obtained via Tucker decomposition. Then, the Principal\nComponent Spatial Characteristic (PCSC) is designed to measure the\nspatial-dimensional quality of LFI considering its global naturalness and local\nfrequency properties. Finally, the Tensor Angular Variation Index (TAVI) is\nproposed to measure angular consistency quality by analyzing the structural\nsimilarity distribution between the first principal component and each view in\nthe view stack. Extensive experimental results on four publicly available LFI\nquality databases demonstrate that the proposed Tensor-NLFQ model outperforms\nstate-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 12:27:44 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhou", "Wei", ""], ["Shi", "Likun", ""], ["Chen", "Zhibo", ""]]}, {"id": "1909.02423", "submitter": "Vincent Labatut", "authors": "Xavier Bost (LIA), Serigne Gueye (LIA), Vincent Labatut (LIA), Martha\n  Larson (DMIR), Georges Linar\\`es (LIA), Damien Malinas (CNELIAS), Rapha\\\"el\n  Roth (CNELIAS)", "title": "Remembering Winter Was Coming: Character-Oriented Video Summaries of TV\n  Series", "comments": null, "journal-ref": "Multimedia Tools and Applications, Springer, 2019,\n  78(24):35373-35399", "doi": "10.1007/s11042-019-07969-4", "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's popular TV series tend to develop continuous, complex plots spanning\nseveral seasons, but are often viewed in controlled and discontinuous\nconditions. Consequently, most viewers need to be re-immersed in the story\nbefore watching a new season. Although discussions with friends and family can\nhelp, we observe that most viewers make extensive use of summaries to re-engage\nwith the plot. Automatic generation of video summaries of TV series' complex\nstories requires, first, modeling the dynamics of the plot and, second,\nextracting relevant sequences. In this paper, we tackle plot modeling by\nconsidering the social network of interactions between the characters involved\nin the narrative: substantial, durable changes in a major character's social\nenvironment suggest a new development relevant for the summary. Once\nidentified, these major stages in each character's storyline can be used as a\nbasis for completing the summary with related sequences. Our algorithm combines\nsuch social network analysis with filmmaking grammar to automatically generate\ncharacter-oriented video summaries of TV series from partially annotated data.\nWe carry out evaluation with a user study in a real-world scenario: a large\nsample of viewers were asked to rank video summaries centered on five\ncharacters of the popular TV series Game of Thrones, a few weeks before the\nnew, sixth season was released. Our results reveal the ability of\ncharacter-oriented summaries to re-engage viewers in television series and\nconfirm the contributions of modeling the plot content and exploiting stylistic\npatterns to identify salient sequences.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:00:45 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:24:57 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 07:10:52 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Gueye", "Serigne", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Larson", "Martha", "", "DMIR"], ["Linar\u00e8s", "Georges", "", "LIA"], ["Malinas", "Damien", "", "CNELIAS"], ["Roth", "Rapha\u00ebl", "", "CNELIAS"]]}, {"id": "1909.02772", "submitter": "Tran Huyen Thi Thanh", "authors": "Huyen T. T. Tran, Nam Pham Ngoc, Tobias Ho{\\ss}feld, Michael Seufert,\n  and Truong Cong Thang", "title": "Cumulative Quality Modeling for HTTP Adaptive Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the abundance of Web platforms and broadband connections, HTTP\nAdaptive Streaming has become the de facto choice for multimedia delivery\nnowadays. However, the visual quality of adaptive video streaming may fluctuate\nstrongly during a session due to bandwidth fluctuations. So, it is important to\nevaluate the quality of a streaming session over time. In this paper, we\npropose a model to estimate the cumulative quality for HTTP Adaptive Streaming.\nIn the model, a sliding window of video segments is employed as the basic\nbuilding block. Through statistical analysis using a subjective dataset, we\nidentify three important components of the cumulative quality model, namely the\nminimum window quality, the last window quality, and the average window\nquality. Experiment results show that the proposed model achieves high\nprediction performance and outperforms related quality models. In addition,\nanother advantage of the proposed model is its simplicity and effectiveness for\ndeployment in real-time estimation. The source code of the proposed model has\nbeen made available to the public at https://github.com/TranHuyen1191/CQM.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:49:25 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 01:15:53 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 06:02:36 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Tran", "Huyen T. T.", ""], ["Ngoc", "Nam Pham", ""], ["Ho\u00dffeld", "Tobias", ""], ["Seufert", "Michael", ""], ["Thang", "Truong Cong", ""]]}, {"id": "1909.03766", "submitter": "Guizhong Liu", "authors": "Xing Chen, Lijun He, Shang Xu, Shibo Hu, Qingzhou Li, Guizhong Liu", "title": "Hit Ratio Driven Mobile Edge Caching Scheme for Video on Demand Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more scholars focus on mobile edge computing (MEC) technology,\nbecause the strong storage and computing capabilities of MEC servers can reduce\nthe long transmission delay, bandwidth waste, energy consumption, and privacy\nleaks in the data transmission process. In this paper, we study the cache\nplacement problem to determine how to cache videos and which videos to be\ncached in a mobile edge computing system. First, we derive the video request\nprobability by taking into account video popularity, user preference and the\ncharacteristic of video representations. Second, based on the acquired request\nprobability, we formulate a cache placement problem with the objective to\nmaximize the cache hit ratio subject to the storage capacity constraints.\nFinally, in order to solve the formulated problem, we transform it into a\ngrouping knapsack problem and develop a dynamic programming algorithm to obtain\nthe optimal caching strategy. Simulation results show that the proposed\nalgorithm can greatly improve the cache hit ratio.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:19:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Chen", "Xing", ""], ["He", "Lijun", ""], ["Xu", "Shang", ""], ["Hu", "Shibo", ""], ["Li", "Qingzhou", ""], ["Liu", "Guizhong", ""]]}, {"id": "1909.03909", "submitter": "Ting Yao", "authors": "Yehao Li and Ting Yao and Yingwei Pan and Hongyang Chao and Tao Mei", "title": "Deep Metric Learning with Density Adaptivity", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distance metric learning is mostly considered from the\nperspective of learning an embedding space, where the distances between pairs\nof examples are in correspondence with a similarity metric. With the rise and\nsuccess of Convolutional Neural Networks (CNN), deep metric learning (DML)\ninvolves training a network to learn a nonlinear transformation to the\nembedding space. Existing DML approaches often express the supervision through\nmaximizing inter-class distance and minimizing intra-class variation. However,\nthe results can suffer from overfitting problem, especially when the training\nexamples of each class are embedded together tightly and the density of each\nclass is very high. In this paper, we integrate density, i.e., the measure of\ndata concentration in the representation, into the optimization of DML\nframeworks to adaptively balance inter-class similarity and intra-class\nvariation by training the architecture in an end-to-end manner. Technically,\nthe knowledge of density is employed as a regularizer, which is pluggable to\nany DML architecture with different objective functions such as contrastive\nloss, N-pair loss and triplet loss. Extensive experiments on three public\ndatasets consistently demonstrate clear improvements by amending three types of\nembedding with the density adaptivity. More remarkably, our proposal increases\nRecall@1 from 67.95% to 77.62%, from 52.01% to 55.64% and from 68.20% to 70.56%\non Cars196, CUB-200-2011 and Stanford Online Products dataset, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:04:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Chao", "Hongyang", ""], ["Mei", "Tao", ""]]}, {"id": "1909.04117", "submitter": "Sandro Rama Fiorini", "authors": "Sandro Rama Fiorini, Wallas Sousa dos Santos, Rodrigo Costa Mesquita,\n  Guilherme Ferreira Lima, Marcio F. Moreno", "title": "General Fragment Model for Information Artifacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of semantic descriptions in data intensive domains require a\nsystematic model for linking semantic descriptions with their manifestations in\nfragments of heterogeneous information and data objects. Such information\nheterogeneity requires a fragment model that is general enough to support the\nspecification of anchors from conceptual models to multiple types of\ninformation artifacts. While diverse proposals of anchoring models exist in the\nliterature, they are usually focused in audiovisual information. We propose a\ngeneralized fragment model that can be instantiated to different kinds of\ninformation artifacts. Our objective is to systematize the way in which\nfragments and anchors can be described in conceptual models, without committing\nto a specific vocabulary.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:29:17 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Fiorini", "Sandro Rama", ""], ["Santos", "Wallas Sousa dos", ""], ["Mesquita", "Rodrigo Costa", ""], ["Lima", "Guilherme Ferreira", ""], ["Moreno", "Marcio F.", ""]]}, {"id": "1909.04369", "submitter": "Krzysztof Rusek", "authors": "Lucjan Janowski, Bogdan \\'Cmiel, Krzysztof Rusek, Jakub Nawa{\\l}a, Zhi\n  Li", "title": "Generalized Score Distribution", "comments": "13 pages, 14 Figures Submitted to Journal of Survey Statistics and\n  Methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of discrete probability distributions contains distributions with\nlimited support, i.e. possible argument values are limited to a set of numbers\n(typically consecutive). Examples of such data are results from subjective\nexperiments utilizing the Absolute Category Rating (ACR) technique, where\npossible answers (argument values) are $\\{1, 2, \\cdots, 5\\}$ or typical Likert\nscale $\\{-3, -2, \\cdots, 3\\}$. An interesting subclass of those distributions\nare distributions limited to two parameters: describing the mean value and the\nspread of the answers, and having no more than one change in the probability\nmonotonicity. In this paper we propose a general distribution passing those\nlimitations called Generalized Score Distribution (GSD). The proposed GSD\ncovers all spreads of the answers, from very small, given by the Bernoulli\ndistribution, to the maximum given by a Beta Binomial distribution. We also\nshow that GSD correctly describes subjective experiments scores from video\nquality evaluations with probability of 99.7\\%. A Google Collaboratory website\nwith implementation of the GSD estimation, simulation, and visualization is\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:37:14 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Janowski", "Lucjan", ""], ["\u0106miel", "Bogdan", ""], ["Rusek", "Krzysztof", ""], ["Nawa\u0142a", "Jakub", ""], ["Li", "Zhi", ""]]}, {"id": "1909.04573", "submitter": "Samet Taspinar", "authors": "Samet Taspinar, Manoranjan Mohanty, and Nasir Memon", "title": "Camera Fingerprint Extraction via Spatial Domain Averaged Frames", "comments": "11 pages, 9 figures, submitted to IEEE Transactions on Information\n  Forensics and Security journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo Response Non-Uniformity (PRNU) based camera attribution is an effective\nmethod to determine the source camera of visual media (an image or a video). To\napply this method, images or videos need to be obtained from a camera to create\na \"camera fingerprint\" which then can be compared against the PRNU of the query\nmedia whose origin is under question. The fingerprint extraction process can be\ntime-consuming when a large number of video frames or images have to be\ndenoised. This may need to be done when the individual images have been\nsubjected to high compression or other geometric processing such as video\nstabilization. This paper investigates a simple, yet effective and efficient\ntechnique to create a camera fingerprint when so many still images need to be\ndenoised. The technique utilizes Spatial Domain Averaged (SDA) frames. An\nSDA-frame is the arithmetic mean of multiple still images. When it is used for\nfingerprint extraction, the number of denoising operations can be significantly\ndecreased with little or no performance loss. Experimental results show that\nthe proposed method can work more than 50 times faster than conventional\nmethods while providing similar matching results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:36:07 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Taspinar", "Samet", ""], ["Mohanty", "Manoranjan", ""], ["Memon", "Nasir", ""]]}, {"id": "1909.04685", "submitter": "Ajay Shrestha", "authors": "Ramita Maharjan, Ajay Kumar Shrestha and Rejina Basnet", "title": "Image Steganography: Protection of Digital Properties against\n  Eavesdropping", "comments": "8 pages, 11 figures, 9TH International conference on software,\n  knowledge, information management and applications (SKIMA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is the art of hiding the fact that communication is taking\nplace, by hiding information in other information. Different types of carrier\nfile formats can be used, but digital images are the most popular ones because\nof their frequency on the internet. For hiding secret information in images,\nthere exists a large variety of steganography techniques. Some are more complex\nthan others and all of them have respective strong and weak points. Many\napplications may require absolute invisibility of the secret information. This\npaper intends to give an overview of image steganography, it's usage and\ntechniques, basically, to store the confidential information within images such\nas details of working strategy, secret missions, criminal and confidential\ninformation in various organizations that work for the national security such\nas army, police, FBI, secret service etc. We develop a desktop application that\nincorporates Advanced Encryption Standard for encryption of the original\nmessage, and Spatially Desynchronized Steganography Algorithm for hiding the\ntext file inside the image.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 18:00:53 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Maharjan", "Ramita", ""], ["Shrestha", "Ajay Kumar", ""], ["Basnet", "Rejina", ""]]}, {"id": "1909.04800", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Anupriy, Vinay P. Namboodiri", "title": "Probabilistic framework for solving Visual Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a probabilistic framework for solving the task of\n`Visual Dialog'. Solving this task requires reasoning and understanding of\nvisual modality, language modality, and common sense knowledge to answer.\nVarious architectures have been proposed to solve this task by variants of\nmulti-modal deep learning techniques that combine visual and language\nrepresentations. However, we believe that it is crucial to understand and\nanalyze the sources of uncertainty for solving this task. Our approach allows\nfor estimating uncertainty and also aids a diverse generation of answers. The\nproposed approach is obtained through a probabilistic representation module\nthat provides us with representations for image, question and conversation\nhistory, a module that ensures that diverse latent representations for\ncandidate answers are obtained given the probabilistic representations and an\nuncertainty representation module that chooses the appropriate answer that\nminimizes uncertainty. We thoroughly evaluate the model with a detailed\nablation analysis, comparison with state of the art and visualization of the\nuncertainty that aids in the understanding of the method. Using the proposed\nprobabilistic framework, we thus obtain an improved visual dialog system that\nis also more explainable.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 00:25:12 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 07:30:39 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Patro", "Badri N.", ""], ["Anupriy", "", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1909.05202", "submitter": "Rui Li", "authors": "Rui Li, Zhibin Pan, Yang Wang", "title": "Correlation-based Initialization Algorithm for Tensor-based HSI\n  Compression Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition (TD) is widely used in hyperspectral image (HSI)\ncompression. The initialization of factor matrix in tensor decomposition can\ndetermine the HSI compression performance. It is worth noting that HSI is\nhighly correlated in bands. However, this phenomenon is ignored by the previous\nTD method. Aiming at improving the HSI compression performance, we propose a\nmethod called correlation-based TD initialization algorithm. As HSI is well\napproximated by means of a reference band. In accordance with the SVD result of\nthe reference band, the initialized factor matrices of TD are produced. We\ncompare our methods with random and SVD-based initialization methods. The\nexperimental results reveal that our correlation-based TD initialization method\nis capable of significantly reducing the computational cost of TD while keeping\nthe initialization quality and compression performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 07:11:59 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Li", "Rui", ""], ["Pan", "Zhibin", ""], ["Wang", "Yang", ""]]}, {"id": "1909.05638", "submitter": "Lahiru D. Chamain Hewa Gamage", "authors": "Lahiru D. Chamain, Zhi Ding", "title": "Faster and Accurate Classification for JPEG2000 Compressed Images in\n  Networked Applications", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG2000 (j2k) is a highly popular format for image and video\ncompression.With the rapidly growing applications of cloud based image\nclassification, most existing j2k-compatible schemes would stream compressed\ncolor images from the source before reconstruction at the processing center as\ninputs to deep CNNs. We propose to remove the computationally costly\nreconstruction step by training a deep CNN image classifier using the CDF 9/7\nDiscrete Wavelet Transformed (DWT) coefficients directly extracted from\nj2k-compressed images. We demonstrate additional computation savings by\nutilizing shallower CNN to achieve classification of good accuracy in the DWT\ndomain. Furthermore, we show that traditional augmentation transforms such as\nflipping/shifting are ineffective in the DWT domain and present different\naugmentation transformations to achieve more accurate classification without\nany additional cost. This way, faster and more accurate classification is\npossible for j2k encoded images without image reconstruction. Through\nexperiments on CIFAR-10 and Tiny ImageNet data sets, we show that the\nperformance of the proposed solution is consistent for image transmission over\nlimited channel bandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:03:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chamain", "Lahiru D.", ""], ["Ding", "Zhi", ""]]}, {"id": "1909.05693", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Zizhou Jia, Hui Chen, Leida Li, Guiguang Ding, Kurt\n  Keutzer", "title": "PDANet: Polarity-consistent Deep Attention Network for Fine-grained\n  Visual Emotion Regression", "comments": "Accepted by ACM Multimedia 2019", "journal-ref": null, "doi": "10.1145/3343031.3351062", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods on visual emotion analysis mainly focus on coarse-grained\nemotion classification, i.e. assigning an image with a dominant discrete\nemotion category. However, these methods cannot well reflect the complexity and\nsubtlety of emotions. In this paper, we study the fine-grained regression\nproblem of visual emotions based on convolutional neural networks (CNNs).\nSpecifically, we develop a Polarity-consistent Deep Attention Network (PDANet),\na novel network architecture that integrates attention into a CNN with an\nemotion polarity constraint. First, we propose to incorporate both spatial and\nchannel-wise attentions into a CNN for visual emotion regression, which jointly\nconsiders the local spatial connectivity patterns along each channel and the\ninterdependency between different channels. Second, we design a novel\nregression loss, i.e. polarity-consistent regression (PCR) loss, based on the\nweakly supervised emotion polarity to guide the attention generation. By\noptimizing the PCR loss, PDANet can generate a polarity preserved attention map\nand thus improve the emotion regression performance. Extensive experiments are\nconducted on the IAPS, NAPS, and EMOTIC datasets, and the results demonstrate\nthat the proposed PDANet outperforms the state-of-the-art approaches by a large\nmargin for fine-grained visual emotion regression. Our source code is released\nat: https://github.com/ZizhouJia/PDANet.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:16:36 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zhao", "Sicheng", ""], ["Jia", "Zizhou", ""], ["Chen", "Hui", ""], ["Li", "Leida", ""], ["Ding", "Guiguang", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1909.07088", "submitter": "Chieh Yu Chen", "authors": "Hsin-Ying Hsieh, Chieh-Yu Chen, Yu-Shuen Wang, Jung-Hong Chuang", "title": "BasketballGAN: Generating Basketball Play Simulation Through Sketching", "comments": "9 pages, Accepted paper at ACMMM 2019, code is available at\n  https://github.com/chychen/BasketballGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven basketball set play simulation. Given an offensive\nset play sketch, our method simulates potential scenarios that may occur in the\ngame. The simulation provides coaches and players with insights on how a given\nset play can be executed. To achieve the goal, we train a conditional\nadversarial network on NBA movement data to imitate the behaviors of how\nplayers move around the court through two major components: a generator that\nlearns to generate natural player movements based on a latent noise and a user\nsketched set play; and a discriminator that is used to evaluate the realism of\nthe basketball play. To improve the quality of simulation, we minimize 1.) a\ndribbler loss to prevent the ball from drifting away from the dribbler; 2.) a\ndefender loss to prevent the dribbler from not being defended; 3.) a ball\npassing loss to ensure the straightness of passing trajectories; and 4) an\nacceleration loss to minimize unnecessary players' movements. To evaluate our\nsystem, we objectively compared real and simulated basketball set plays.\nBesides, a subjective test was conducted to judge whether a set play was real\nor generated by our network. On average, the mean correct rates to the binary\ntests were 56.17 \\%. Experiment results and the evaluations demonstrated the\neffectiveness of our system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:45:49 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 02:33:51 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Hsieh", "Hsin-Ying", ""], ["Chen", "Chieh-Yu", ""], ["Wang", "Yu-Shuen", ""], ["Chuang", "Jung-Hong", ""]]}, {"id": "1909.07526", "submitter": "Dmitry Konovalov", "authors": "Dina B. Efremova and Mangalam Sankupellay and Dmitry A. Konovalov", "title": "Data-Efficient Classification of Birdcall Through Convolutional Neural\n  Networks Transfer Learning", "comments": "Accepted for IEEE Digital Image Computing: Techniques and\n  Applications, 2019 (DICTA 2019), 2-4 December 2019 in Perth, Australia,\n  http://dicta2019.dictaconference.org/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning Convolutional Neural Network (CNN) models are powerful\nclassification models but require a large amount of training data. In niche\ndomains such as bird acoustics, it is expensive and difficult to obtain a large\nnumber of training samples. One method of classifying data with a limited\nnumber of training samples is to employ transfer learning. In this research, we\nevaluated the effectiveness of birdcall classification using transfer learning\nfrom a larger base dataset (2814 samples in 46 classes) to a smaller target\ndataset (351 samples in 10 classes) using the ResNet-50 CNN. We obtained 79%\naverage validation accuracy on the target dataset in 5-fold cross-validation.\nThe methodology of transfer learning from an ImageNet-trained CNN to a\nproject-specific and a much smaller set of classes and images was extended to\nthe domain of spectrogram images, where the base dataset effectively played the\nrole of the ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 00:16:16 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Efremova", "Dina B.", ""], ["Sankupellay", "Mangalam", ""], ["Konovalov", "Dmitry A.", ""]]}, {"id": "1909.07556", "submitter": "Bolin Chen", "authors": "Huaxiao Mo, Tingting Song, Bolin Chen, Weiqi Luo, Jiwu Huang", "title": "Enhancing JPEG Steganography using Iterative Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) based methods have significantly improved\nthe performance of image steganalysis compared with conventional ones based on\nhand-crafted features. However, many existing literatures on computer vision\nhave pointed out that those effective CNN-based methods can be easily fooled by\nadversarial examples. In this paper, we propose a novel steganography framework\nbased on adversarial example in an iterative manner. The proposed framework\nfirst starts from an existing embedding cost, such as J-UNIWARD in this work,\nand then updates the cost iteratively based on adversarial examples derived\nfrom a series of steganalytic networks until achieving satisfactory results. We\ncarefully analyze two important factors that would affect the security\nperformance of the proposed framework, i.e. the percentage of selected\ngradients with larger amplitude and the adversarial intensity to modify\nembedding cost. The experimental results evaluated on three modern steganalytic\nmodels, including GFR, SCA-GFR and SRNet, show that the proposed framework is\nvery promising to enhance the security performances of JPEG steganography.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 02:34:21 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 02:55:01 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 01:31:03 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mo", "Huaxiao", ""], ["Song", "Tingting", ""], ["Chen", "Bolin", ""], ["Luo", "Weiqi", ""], ["Huang", "Jiwu", ""]]}, {"id": "1909.07615", "submitter": "Jingjing Li", "authors": "Jingjing Li and Mengmeng Jing and Ke Lu and Lei Zhu and Yang Yang and\n  Zi Huang", "title": "Alleviating Feature Confusion for Generative Zero-shot Learning", "comments": "Our codes can be found at github.com/lijin118/AFC-GAN", "journal-ref": "ACM Multimedia 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lately, generative adversarial networks (GANs) have been successfully applied\nto zero-shot learning (ZSL) and achieved state-of-the-art performance. By\nsynthesizing virtual unseen visual features, GAN-based methods convert the\nchallenging ZSL task into a supervised learning problem. However, GAN-based ZSL\nmethods have to train the generator on the seen categories and further apply it\nto unseen instances. An inevitable issue of such a paradigm is that the\nsynthesized unseen features are prone to seen references and incapable to\nreflect the novelty and diversity of real unseen instances. In a nutshell, the\nsynthesized features are confusing. One cannot tell unseen categories from seen\nones using the synthesized features. As a result, the synthesized features are\ntoo subtle to be classified in generalized zero-shot learning (GZSL) which\ninvolves both seen and unseen categories at the test stage. In this paper, we\nfirst introduce the feature confusion issue. Then, we propose a new feature\ngenerating network, named alleviating feature confusion GAN (AFC-GAN), to\nchallenge the issue. Specifically, we present a boundary loss which maximizes\nthe decision boundary of seen categories and unseen ones. Furthermore, a novel\nmetric named feature confusion score (FCS) is proposed to quantify the feature\nconfusion. Extensive experiments on five widely used datasets verify that our\nmethod is able to outperform previous state-of-the-arts under both ZSL and GZSL\nprotocols.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:59:48 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Li", "Jingjing", ""], ["Jing", "Mengmeng", ""], ["Lu", "Ke", ""], ["Zhu", "Lei", ""], ["Yang", "Yang", ""], ["Huang", "Zi", ""]]}, {"id": "1909.07618", "submitter": "Jingjing Li", "authors": "Jingjing Li and Erpeng Chen and Zhengming Ding and Lei Zhu and Ke Lu\n  and Zi Huang", "title": "Cycle-consistent Conditional Adversarial Transfer Networks", "comments": "Codes at github.com/lijin118/3CATN", "journal-ref": "ACM Multimedia 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation investigates the problem of cross-domain knowledge transfer\nwhere the labeled source domain and unlabeled target domain have distinctive\ndata distributions. Recently, adversarial training have been successfully\napplied to domain adaptation and achieved state-of-the-art performance.\nHowever, there is still a fatal weakness existing in current adversarial models\nwhich is raised from the equilibrium challenge of adversarial training.\nSpecifically, although most of existing methods are able to confuse the domain\ndiscriminator, they cannot guarantee that the source domain and target domain\nare sufficiently similar. In this paper, we propose a novel approach named {\\it\ncycle-consistent conditional adversarial transfer networks} (3CATN) to handle\nthis issue. Our approach takes care of the domain alignment by leveraging\nadversarial training. Specifically, we condition the adversarial networks with\nthe cross-covariance of learned features and classifier predictions to capture\nthe multimodal structures of data distributions. However, since the classifier\npredictions are not certainty information, a strong condition with the\npredictions is risky when the predictions are not accurate. We, therefore,\nfurther propose that the truly domain-invariant features should be able to be\ntranslated from one domain to the other. To this end, we introduce two feature\ntranslation losses and one cycle-consistent loss into the conditional\nadversarial domain adaptation networks. Extensive experiments on both classical\nand large-scale datasets verify that our model is able to outperform previous\nstate-of-the-arts with significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 07:14:26 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Li", "Jingjing", ""], ["Chen", "Erpeng", ""], ["Ding", "Zhengming", ""], ["Zhu", "Lei", ""], ["Lu", "Ke", ""], ["Huang", "Zi", ""]]}, {"id": "1909.07730", "submitter": "Alexander Schindler", "authors": "Alexander Schindler and Peter Knees", "title": "Multi-Task Music Representation Learning from Multi-Label Embeddings", "comments": "Best Student Paper award", "journal-ref": "Proceedings of the International Conference on Content-Based\n  Multimedia Indexing (CBMI2019)", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to music representation learning.\nTriplet loss based networks have become popular for representation learning in\nvarious multimedia retrieval domains. Yet, one of the most crucial parts of\nthis approach is the appropriate selection of triplets, which is indispensable,\nconsidering that the number of possible triplets grows cubically. We present an\napproach to harness multi-tag annotations for triplet selection, by using\nLatent Semantic Indexing to project the tags onto a high-dimensional space.\nFrom this we estimate tag-relatedness to select hard triplets. The approach is\nevaluated in a multi-task scenario for which we introduce four large multi-tag\nannotations for the Million Song Dataset for the music properties genres,\nstyles, moods, and themes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:43:57 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Schindler", "Alexander", ""], ["Knees", "Peter", ""]]}, {"id": "1909.07741", "submitter": "Yipeng Sun", "authors": "Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun\n  Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng\n  Chan, Lianwen Jin", "title": "ICDAR 2019 Competition on Large-scale Street View Text with Partial\n  Labeling -- RRC-LSVT", "comments": "ICDAR 2019 Robust Reading Challenge in IAPR International Conference\n  on Document Analysis and Recognition (ICDAR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust text reading from street view images provides valuable information for\nvarious applications. Performance improvement of existing methods in such a\nchallenging scenario heavily relies on the amount of fully annotated training\ndata, which is costly and in-efficient to obtain. To scale up the amount of\ntraining data while keeping the labeling procedure cost-effective, this\ncompetition introduces a new challenge on Large-scale Street View Text with\nPartial Labeling (LSVT), providing 50, 000 and 400, 000 images in full and weak\nannotations, respectively. This competition aims to explore the abilities of\nstate-of-the-art methods to detect and recognize text instances from\nlarge-scale street view images, closing the gap between research benchmarks and\nreal applications. During the competition period, a total of 41 teams\nparticipated in the two proposed tasks with 132 valid submissions, i.e., text\ndetection and end-to-end text spotting. This paper includes dataset\ndescriptions, task definitions, evaluation protocols and results summaries of\nthe ICDAR 2019-LSVT challenge.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 12:09:33 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Sun", "Yipeng", ""], ["Ni", "Zihan", ""], ["Chng", "Chee-Kheng", ""], ["Liu", "Yuliang", ""], ["Luo", "Canjie", ""], ["Ng", "Chun Chet", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Liu", "Jingtuo", ""], ["Karatzas", "Dimosthenis", ""], ["Chan", "Chee Seng", ""], ["Jin", "Lianwen", ""]]}, {"id": "1909.07808", "submitter": "Yipeng Sun", "authors": "Yipeng Sun, Jiaming Liu, Wei Liu, Junyu Han, Errui Ding, Jingtuo Liu", "title": "Chinese Street View Text: Large-scale Chinese Text Reading with\n  Partially Supervised Learning", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing text reading benchmarks make it difficult to evaluate the\nperformance of more advanced deep learning models in large vocabularies due to\nthe limited amount of training data. To address this issue, we introduce a new\nlarge-scale text reading benchmark dataset named Chinese Street View Text\n(C-SVT) with 430,000 street view images, which is at least 14 times as large as\nthe existing Chinese text reading benchmarks. To recognize Chinese text in the\nwild while keeping large-scale datasets labeling cost-effective, we propose to\nannotate one part of the CSVT dataset (30,000 images) in locations and text\nlabels as full annotations and add 400,000 more images, where only the\ncorresponding text-of-interest in the regions is given as weak annotations. To\nexploit the rich information from the weakly annotated data, we design a text\nreading network in a partially supervised learning framework, which enables to\nlocalize and recognize text, learn from fully and weakly annotated data\nsimultaneously. To localize the best matched text proposals from weakly labeled\nimages, we propose an online proposal matching module incorporated in the whole\nmodel, spotting the keyword regions by sharing parameters for end-to-end\ntraining. Compared with fully supervised training algorithms, this model can\nimprove the end-to-end recognition performance remarkably by 4.03% in F-score\nat the same labeling cost. The proposed model can also achieve state-of-the-art\nresults on the ICDAR 2017-RCTW dataset, which demonstrates the effectiveness of\nthe proposed partially supervised learning framework.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:54:24 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 04:50:38 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Sun", "Yipeng", ""], ["Liu", "Jiaming", ""], ["Liu", "Wei", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Liu", "Jingtuo", ""]]}, {"id": "1909.08148", "submitter": "Hongshan Li", "authors": "Hongshan Li, Yu Guo, Zhi Wang, Shutao Xia, Wenwu Zhu", "title": "AdaCompress: Adaptive Compression for Online Computer Vision Services", "comments": "ACM Multimedia", "journal-ref": null, "doi": "10.1145/3343031.3350874", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of computer vision based applications and services, an\nexplosive amount of images have been uploaded to cloud servers which host such\ncomputer vision algorithms, usually in the form of deep learning models. JPEG\nhas been used as the {\\em de facto} compression and encapsulation method before\none uploads the images, due to its wide adaptation. However, standard JPEG\nconfiguration does not always perform well for compressing images that are to\nbe processed by a deep learning model, e.g., the standard quality level of JPEG\nleads to 50\\% of size overhead (compared with the best quality level selection)\non ImageNet under the same inference accuracy in popular computer vision models\nincluding InceptionNet, ResNet, etc. Knowing this, designing a better JPEG\nconfiguration for online computer vision services is still extremely\nchallenging: 1) Cloud-based computer vision models are usually a black box to\nend-users; thus it is difficult to design JPEG configuration without knowing\ntheir model structures. 2) JPEG configuration has to change when different\nusers use it. In this paper, we propose a reinforcement learning based JPEG\nconfiguration framework. In particular, we design an agent that adaptively\nchooses the compression level according to the input image's features and\nbackend deep learning models. Then we train the agent in a reinforcement\nlearning way to adapt it for different deep learning cloud services that act as\nthe {\\em interactive training environment} and feeding a reward with\ncomprehensive consideration of accuracy and data size. In our real-world\nevaluation on Amazon Rekognition, Face++ and Baidu Vision, our approach can\nreduce the size of images by 1/2 -- 1/3 while the overall classification\naccuracy only decreases slightly.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 23:45:28 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Li", "Hongshan", ""], ["Guo", "Yu", ""], ["Wang", "Zhi", ""], ["Xia", "Shutao", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1909.09529", "submitter": "Mansi Sharma", "authors": "Kumar Mridul, M. Ramanathan, Kunal Ahirwar, Mansi Sharma", "title": "Multi-user Augmented Reality Application for Video Communication in\n  Virtual Space", "comments": "European Light Field Imaging Workshop (ELFI 2019), Borovets, Bulgaria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Communication is the most useful tool to impart knowledge, understand ideas,\nclarify thoughts and expressions, organize plan and manage every single\nday-to-day activity. Although there are different modes of communication,\nphysical barrier always affects the clarity of the message due to the absence\nof body language and facial expressions. These barriers are overcome by video\ncalling, which is technically the most advance mode of communication at\npresent. The proposed work concentrates around the concept of video calling in\na more natural and seamless way using Augmented Reality (AR). AR can be helpful\nin giving the users an experience of physical presence in each other's\nenvironment. Our work provides an entirely new platform for video calling,\nwherein the users can enjoy the privilege of their own virtual space to\ninteract with the individual's environment. Moreover, there is no limitation of\nsharing the same screen space. Any number of participants can be accommodated\nover a single conference without having to compromise the screen size.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:32:54 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Mridul", "Kumar", ""], ["Ramanathan", "M.", ""], ["Ahirwar", "Kunal", ""], ["Sharma", "Mansi", ""]]}, {"id": "1909.09677", "submitter": "Zhe Huang", "authors": "Zhe Huang, Weijiang Yu, Wayne Zhang, Litong Feng, Nong Xiao", "title": "Gradual Network for Single Image De-raining", "comments": "In Proceedings of the 27th ACM International Conference on Multimedia\n  (MM 2019)", "journal-ref": null, "doi": "10.1145/3343031.3350883", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most advances in single image de-raining meet a key challenge, which is\nremoving rain streaks with different scales and shapes while preserving image\ndetails. Existing single image de-raining approaches treat rain-streak removal\nas a process of pixel-wise regression directly. However, they are lacking in\nmining the balance between over-de-raining (e.g. removing texture details in\nrain-free regions) and under-de-raining (e.g. leaving rain streaks). In this\npaper, we firstly propose a coarse-to-fine network called Gradual Network\n(GraNet) consisting of coarse stage and fine stage for delving into single\nimage de-raining with different granularities. Specifically, to reveal\ncoarse-grained rain-streak characteristics (e.g. long and thick rain\nstreaks/raindrops), we propose a coarse stage by utilizing local-global spatial\ndependencies via a local-global subnetwork composed of region-aware blocks.\nTaking the residual result (the coarse de-rained result) between the rainy\nimage sample (i.e. the input data) and the output of coarse stage (i.e. the\nlearnt rain mask) as input, the fine stage continues to de-rain by removing the\nfine-grained rain streaks (e.g. light rain streaks and water mist) to get a\nrain-free and well-reconstructed output image via a unified contextual merging\nsub-network with dense blocks and a merging block. Solid and comprehensive\nexperiments on synthetic and real data demonstrate that our GraNet can\nsignificantly outperform the state-of-the-art methods by removing rain streaks\nwith various densities, scales and shapes while keeping the image details of\nrain-free regions well-preserved.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:56:08 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Huang", "Zhe", ""], ["Yu", "Weijiang", ""], ["Zhang", "Wayne", ""], ["Feng", "Litong", ""], ["Xiao", "Nong", ""]]}, {"id": "1909.09822", "submitter": "Zhi Chen", "authors": "Zhi Chen, Jingjing Li, Yadan Luo, Zi Huang, Yang Yang", "title": "CANZSL: Cycle-Consistent Adversarial Networks for Zero-Shot Learning\n  from Natural Language", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods using generative adversarial approaches for Zero-Shot\nLearning (ZSL) aim to generate realistic visual features from class semantics\nby a single generative network, which is highly under-constrained. As a result,\nthe previous methods cannot guarantee that the generated visual features can\ntruthfully reflect the corresponding semantics. To address this issue, we\npropose a novel method named Cycle-consistent Adversarial Networks for\nZero-Shot Learning (CANZSL). It encourages a visual feature generator to\nsynthesize realistic visual features from semantics, and then inversely\ntranslate back synthesized the visual feature to corresponding semantic space\nby a semantic feature generator. Furthermore, in this paper a more challenging\nand practical ZSL problem is considered where the original semantics are from\nnatural language with irrelevant words instead of clean semantics that are\nwidely used in previous work. Specifically, a multi-modal consistent\nbidirectional generative adversarial network is trained to handle unseen\ninstances by leveraging noise in the natural language. A forward one-to-many\nmapping from one text description to multiple visual features is coupled with\nan inverse many-to-one mapping from the visual space to the semantic space.\nThus, a multi-modal cycle-consistency loss between the synthesized semantic\nrepresentations and the ground truth can be learned and leveraged to enforce\nthe generated semantic features to approximate to the real distribution in\nsemantic space. Extensive experiments are conducted to demonstrate that our\nmethod consistently outperforms state-of-the-art approaches on natural\nlanguage-based zero-shot learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 13:19:15 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Chen", "Zhi", ""], ["Li", "Jingjing", ""], ["Luo", "Yadan", ""], ["Huang", "Zi", ""], ["Yang", "Yang", ""]]}, {"id": "1909.10164", "submitter": "Mukesh Saini", "authors": "Mukesh Saini and Benjamin Guthier and Hao Kuang and Dwarikanath\n  Mahapatra and Abdulmotaleb El Saddik", "title": "sZoom: A Framework for Automatic Zoom into High Resolution Surveillance\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current cameras are capable of recording high resolution video. While viewing\non a mobile device, a user can manually zoom into this high resolution video to\nget more detailed view of objects and activities. However, manual zooming is\nnot suitable for surveillance and monitoring. It is tiring to continuously keep\nzooming into various regions of the video. Also, while viewing one region, the\noperator may miss activities in other regions. In this paper, we propose sZoom,\na framework to automatically zoom into a high resolution surveillance video.\nThe proposed framework selectively zooms into the sensitive regions of the\nvideo to present details of the scene, while still preserving the overall\ncontext required for situation assessment. A multi-variate Gaussian penalty is\nintroduced to ensure full coverage of the scene. The method achieves near\nreal-time performance through a number of timing optimizations. An extensive\nuser study shows that, while watching a full HD video on a mobile device, the\nsystem enhances the security operator's efficiency in understanding the details\nof the scene by 99% on the average compared to a scaled version of the original\nhigh resolution video. The produced video achieved 46% higher ratings for\nusefulness in a surveillance task.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 05:20:48 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Saini", "Mukesh", ""], ["Guthier", "Benjamin", ""], ["Kuang", "Hao", ""], ["Mahapatra", "Dwarikanath", ""], ["Saddik", "Abdulmotaleb El", ""]]}, {"id": "1909.10914", "submitter": "Wei Wang Dr.", "authors": "Xuedou Xiao, Wei Wang, Taobin Chen, Yang Cao, Tao Jiang, Qian Zhang", "title": "Sensor-Augmented Neural Adaptive Bitrate Video Streaming on UAVs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in unmanned aerial vehicle (UAV) technology have\nrevolutionized a broad class of civil and military applications. However, the\ndesigns of wireless technologies that enable real-time streaming of\nhigh-definition video between UAVs and ground clients present a conundrum. Most\nexisting adaptive bitrate (ABR) algorithms are not optimized for the\nair-to-ground links, which usually fluctuate dramatically due to the dynamic\nflight states of the UAV. In this paper, we present SA-ABR, a new\nsensor-augmented system that generates ABR video streaming algorithms with the\nassistance of various kinds of inherent sensor data that are used to pilot\nUAVs. By incorporating the inherent sensor data with network observations,\nSA-ABR trains a deep reinforcement learning (DRL) model to extract salient\nfeatures from the flight state information and automatically learn an ABR\nalgorithm to adapt to the varying UAV channel capacity through the training\nprocess. SA-ABR does not rely on any assumptions or models about UAV's flight\nstates or the environment, but instead, it makes decisions by exploiting\ntemporal properties of past throughput through the long short-term memory\n(LSTM) to adapt itself to a wide range of highly dynamic environments. We have\nimplemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare\nSA-ABR with a variety of existing state-of-the-art ABR algorithms, and the\nresults show that our system outperforms the best known existing ABR algorithm\nby 21.4% in terms of the average quality of experience (QoE) reward.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 09:20:44 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Xiao", "Xuedou", ""], ["Wang", "Wei", ""], ["Chen", "Taobin", ""], ["Cao", "Yang", ""], ["Jiang", "Tao", ""], ["Zhang", "Qian", ""]]}, {"id": "1909.11023", "submitter": "Tanwi Mallick", "authors": "Tanwi Mallick, Partha Pratim Das, and Arun Kumar Majumdar", "title": "Posture and sequence recognition for Bharatanatyam dance performances\n  using machine learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the underlying semantics of performing arts like dance is a\nchallenging task. Dance is multimedia in nature and spans over time as well as\nspace. Capturing and analyzing the multimedia content of the dance is useful\nfor the preservation of cultural heritage, to build video recommendation\nsystems, to assist learners to use tutoring systems. To develop an application\nfor dance, three aspects of dance analysis need to be addressed: 1)\nSegmentation of the dance video to find the representative action elements, 2)\nMatching or recognition of the detected action elements, and 3) Recognition of\nthe dance sequences formed by combining a number of action elements under\ncertain rules. This paper attempts to solve three fundamental problems of dance\nanalysis for understanding the underlying semantics of dance forms. Our focus\nis on an Indian Classical Dance (ICD) form known as Bharatanatyam. As dance is\ndriven by music, we use the music as well as motion information for key posture\nextraction. Next, we recognize the key postures using machine learning as well\nas deep learning techniques. Finally, the dance sequence is recognized using\nthe Hidden Markov Model (HMM). We capture the multi-modal data of Bharatanatyam\ndance using Kinect and build an annotated data set for research in ICD.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 16:18:01 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Mallick", "Tanwi", ""], ["Das", "Partha Pratim", ""], ["Majumdar", "Arun Kumar", ""]]}, {"id": "1909.11416", "submitter": "Chunxiao Liu", "authors": "Chunxiao Liu, Zhendong Mao, An-An Liu, Tianzhu Zhang, Bin Wang,\n  Yongdong Zhang", "title": "Focus Your Attention: A Bidirectional Focal Attention Network for\n  Image-Text Matching", "comments": "Accepted by ACMMM2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning semantic correspondence between image and text is significant as it\nbridges the semantic gap between vision and language. The key challenge is to\naccurately find and correlate shared semantics in image and text. Most existing\nmethods achieve this goal by representing the shared semantic as a weighted\ncombination of all the fragments (image regions or text words), where fragments\nrelevant to the shared semantic obtain more attention, otherwise less. However,\ndespite relevant ones contribute more to the shared semantic, irrelevant ones\nwill more or less disturb it, and thus will lead to semantic misalignment in\nthe correlation phase. To address this issue, we present a novel Bidirectional\nFocal Attention Network (BFAN), which not only allows to attend to relevant\nfragments but also diverts all the attention into these relevant fragments to\nconcentrate on them. The main difference with existing works is they mostly\nfocus on learning attention weight while our BFAN focus on eliminating\nirrelevant fragments from the shared semantic. The focal attention is achieved\nby pre-assigning attention based on inter-modality relation, identifying\nrelevant fragments based on intra-modality relation and reassigning attention.\nFurthermore, the focal attention is jointly applied in both image-to-text and\ntext-to-image directions, which enables to avoid preference to long text or\ncomplex image. Experiments show our simple but effective framework\nsignificantly outperforms state-of-the-art, with relative Recall@1 gains of\n2.2% on both Flicr30K and MSCOCO benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 11:47:47 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Liu", "Chunxiao", ""], ["Mao", "Zhendong", ""], ["Liu", "An-An", ""], ["Zhang", "Tianzhu", ""], ["Wang", "Bin", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1909.11856", "submitter": "Zheng Hui", "authors": "Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang", "title": "Lightweight Image Super-Resolution with Information Multi-distillation\n  Network", "comments": "To be appear in ACM Multimedia 2019, https://github.com/Zheng222/IMDN", "journal-ref": null, "doi": "10.1145/3343031.3351084", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, single image super-resolution (SISR) methods using deep\nconvolution neural network (CNN) have achieved impressive results. Thanks to\nthe powerful representation capabilities of the deep networks, numerous\nprevious ways can learn the complex non-linear mapping between low-resolution\n(LR) image patches and their high-resolution (HR) versions. However, excessive\nconvolutions will limit the application of super-resolution technology in low\ncomputing power devices. Besides, super-resolution of any arbitrary scale\nfactor is a critical issue in practical applications, which has not been well\nsolved in the previous approaches. To address these issues, we propose a\nlightweight information multi-distillation network (IMDN) by constructing the\ncascaded information multi-distillation blocks (IMDB), which contains\ndistillation and selective fusion parts. Specifically, the distillation module\nextracts hierarchical features step-by-step, and fusion module aggregates them\naccording to the importance of candidate features, which is evaluated by the\nproposed contrast-aware channel attention mechanism. To process real images\nwith any sizes, we develop an adaptive cropping strategy (ACS) to super-resolve\nblock-wise image patches using the same well-trained model. Extensive\nexperiments suggest that the proposed method performs favorably against the\nstate-of-the-art SR algorithms in term of visual quality, memory footprint, and\ninference time. Code is available at \\url{https://github.com/Zheng222/IMDN}.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 02:40:32 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Hui", "Zheng", ""], ["Gao", "Xinbo", ""], ["Yang", "Yunchu", ""], ["Wang", "Xiumei", ""]]}, {"id": "1909.11946", "submitter": "Palakorn Achananuparp", "authors": "Doyen Sahoo, Wang Hao, Shu Ke, Wu Xiongwei, Hung Le, Palakorn\n  Achananuparp, Ee-Peng Lim, Steven C. H. Hoi", "title": "FoodAI: Food Image Recognition via Deep Learning for Smart Food Logging", "comments": "Published at KDD 2019 (Applied Data Science track). Demo is\n  accessible at https://foodai.org/", "journal-ref": null, "doi": "10.1145/3292500.3330734", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of health monitoring is effective logging of food\nconsumption. This can help management of diet-related diseases like obesity,\ndiabetes, and even cardiovascular diseases. Moreover, food logging can help\nfitness enthusiasts, and people who wanting to achieve a target weight.\nHowever, food-logging is cumbersome, and requires not only taking additional\neffort to note down the food item consumed regularly, but also sufficient\nknowledge of the food item consumed (which is difficult due to the availability\nof a wide variety of cuisines). With increasing reliance on smart devices, we\nexploit the convenience offered through the use of smart phones and propose a\nsmart-food logging system: FoodAI, which offers state-of-the-art deep-learning\nbased image recognition capabilities. FoodAI has been developed in Singapore\nand is particularly focused on food items commonly consumed in Singapore.\nFoodAI models were trained on a corpus of 400,000 food images from 756\ndifferent classes. In this paper we present extensive analysis and insights\ninto the development of this system. FoodAI has been deployed as an API service\nand is one of the components powering Healthy 365, a mobile app developed by\nSingapore's Heath Promotion Board. We have over 100 registered organizations\n(universities, companies, start-ups) subscribing to this service and actively\nreceive several API requests a day. FoodAI has made food logging convenient,\naiding smart consumption and a healthy lifestyle.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:15:46 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sahoo", "Doyen", ""], ["Hao", "Wang", ""], ["Ke", "Shu", ""], ["Xiongwei", "Wu", ""], ["Le", "Hung", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-Peng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1909.11947", "submitter": "Xi Cheng", "authors": "Xi Cheng, Zhenyong Fu, Jian Yang", "title": "Multi-scale Dynamic Feature Encoding Network for Image Demoireing", "comments": "Accepted in Advances in Image Manipulation workshop and challenges at\n  ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalence of digital sensors, such as digital cameras and mobile phones,\nsimplifies the acquisition of photos. Digital sensors, however, suffer from\nproducing Moire when photographing objects having complex textures, which\ndeteriorates the quality of photos. Moire spreads across various frequency\nbands of images and is a dynamic texture with varying colors and shapes, which\npose two main challenges in demoireing---an important task in image\nrestoration. In this paper, towards addressing the first challenge, we design a\nmulti-scale network to process images at different spatial resolutions,\nobtaining features in different frequency bands, and thus our method can\njointly remove moire in different frequency bands. Towards solving the second\nchallenge, we propose a dynamic feature encoding module (DFE), embedded in each\nscale, for dynamic texture. Moire pattern can be eliminated more effectively\nvia DFE.Our proposed method, termed Multi-scale convolutional network with\nDynamic feature encoding for image DeMoireing (MDDM), can outperform the state\nof the arts in fidelity as well as perceptual on benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:23:43 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Cheng", "Xi", ""], ["Fu", "Zhenyong", ""], ["Yang", "Jian", ""]]}, {"id": "1909.12526", "submitter": "Ralph Gasser", "authors": "Luca Rossetto, Ralph Gasser, Heiko Schuldt", "title": "Query by Semantic Sketch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based query formulation is very common in image and video retrieval as\nthese techniques often complement textual retrieval methods that are based on\neither manual or machine generated annotations. In this paper, we present a\nretrieval approach that allows to query visual media collections by sketching\nconcept maps, thereby merging sketch-based retrieval with the search for\nsemantic labels. Users can draw a spatial distribution of different concept\nlabels, such as \"sky\", \"sea\" or \"person\" and then use these sketches to find\nimages or video scenes that exhibit a similar distribution of these concepts.\nHence, this approach does not only take the semantic concepts themselves into\naccount, but also their semantic relations as well as their spatial context.\nThe efficient vector representation enables efficient retrieval even in large\nmultimedia collections. We have integrated the semantic sketch query mode into\nour retrieval engine vitrivr and demonstrated its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:27:04 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Rossetto", "Luca", ""], ["Gasser", "Ralph", ""], ["Schuldt", "Heiko", ""]]}, {"id": "1909.12601", "submitter": "Kashif Ahmad Dr", "authors": "Naina Said, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha", "title": "Active Learning for Event Detection in Support of Disaster Analysis\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disaster analysis in social media content is one of the interesting research\ndomains having abundance of data. However, there is a lack of labeled data that\ncan be used to train machine learning models for disaster analysis\napplications. Active learning is one of the possible solutions to such problem.\nTo this aim, in this paper we propose and assess the efficacy of an active\nlearning based framework for disaster analysis using images shared on social\nmedia outlets. Specifically, we analyze the performance of different active\nlearning techniques employing several sampling and disagreement strategies.\nMoreover, we collect a large-scale dataset covering images from eight common\ntypes of natural disasters. The experimental results show that the use of\nactive learning techniques for disaster analysis using images results in a\nperformance comparable to that obtained using human annotated images, and could\nbe used in frameworks for disaster analysis in images without tedious job of\nmanual annotation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 10:28:10 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Said", "Naina", ""], ["Ahmad", "Kashif", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1909.12909", "submitter": "Yubao Liu", "authors": "Yubao Liu, Kai Lin", "title": "Deeply Matting-based Dual Generative Adversarial Network for Image and\n  Document Label Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many methods have been proposed to deal with nature image\nsuper-resolution (SR) and get impressive performance, the text images SR is not\ngood due to their ignorance of document images. In this paper, we propose a\nmatting-based dual generative adversarial network (mdGAN) for document image\nSR. Firstly, the input image is decomposed into document text, foreground and\nbackground layers using deep image matting. Then two parallel branches are\nconstructed to recover text boundary information and color information\nrespectively. Furthermore, in order to improve the restoration accuracy of\ncharacters in output image, we use the input image's corresponding ground truth\ntext label as extra supervise information to refine the two-branch networks\nduring training. Experiments on real text images demonstrate that our method\noutperforms several state-of-the-art methods quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 18:15:10 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Liu", "Yubao", ""], ["Lin", "Kai", ""]]}, {"id": "1909.12916", "submitter": "Lucas Pascal", "authors": "Lucas Pascal, Xavier Bost (LIA), Beno\\^it Huet", "title": "Semantic and Visual Similarities for Efficient Knowledge Transfer in CNN\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, representation learning approaches have disrupted many\nmultimedia computing tasks. Among those approaches, deep convolutional neural\nnetworks (CNNs) have notably reached human level expertise on some constrained\nimage classification tasks. Nonetheless, training CNNs from scratch for new\ntask or simply new data turns out to be complex and time-consuming. Recently,\ntransfer learning has emerged as an effective methodology for adapting\npre-trained CNNs to new data and classes, by only retraining the last\nclassification layer. This paper focuses on improving this process, in order to\nbetter transfer knowledge between CNN architectures for faster trainings in the\ncase of fine tuning for image classification. This is achieved by combining and\ntransfering supplementary weights, based on similarity considerations between\nsource and target classes. The study includes a comparison between semantic and\ncontent-based similarities, and highlights increased initial performances and\ntraining speed, along with superior long term performances when limited\ntraining samples are available.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:49:38 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Pascal", "Lucas", "", "LIA"], ["Bost", "Xavier", "", "LIA"], ["Huet", "Beno\u00eet", ""]]}, {"id": "1909.12921", "submitter": "Benjamin Renoust", "authors": "Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Noa Garcia, Van\n  Le, Ayaka Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka\n  Fujioka", "title": "Historical and Modern Features for Buddha Statue Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Buddhism has spread along the Silk Roads, many pieces of art have been\ndisplaced. Only a few experts may identify these works, subjectively to their\nexperience. The construction of Buddha statues was taught through the\ndefinition of canon rules, but the applications of those rules greatly varies\nacross time and space. Automatic art analysis aims at supporting these\nchallenges. We propose to automatically recover the proportions induced by the\nconstruction guidelines, in order to use them and compare between different\ndeep learning features for several classification tasks, in a medium size but\nrich dataset of Buddha statues, collected with experts of Buddhism art history.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:22:32 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 14:07:58 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Renoust", "Benjamin", ""], ["Franca", "Matheus Oliveira", ""], ["Chan", "Jacob", ""], ["Garcia", "Noa", ""], ["Le", "Van", ""], ["Uesaka", "Ayaka", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""], ["Wang", "Jueren", ""], ["Fujioka", "Yutaka", ""]]}, {"id": "1909.12932", "submitter": "Benjamin Renoust", "authors": "Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Van Le, Ayaka\n  Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka Fujioka", "title": "BUDA.ART: A Multimodal Content-Based Analysis and Retrieval System for\n  Buddha Statues", "comments": "Demo video at: https://www.youtube.com/watch?v=3XJvLjSWieY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BUDA.ART, a system designed to assist researchers in Art\nHistory, to explore and analyze an archive of pictures of Buddha statues. The\nsystem combines different CBIR and classical retrieval techniques to assemble\n2D pictures, 3D statue scans and meta-data, that is focused on the Buddha\nfacial characteristics. We build the system from an archive of 50,000 Buddhism\npictures, identify unique Buddha statues, extract contextual information, and\nprovide specific facial embedding to first index the archive. The system allows\nfor mobile, on-site search, and to explore similarities of statues in the\narchive. In addition, we provide search visualization and 3D analysis of the\nstatues\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:35:24 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Renoust", "Benjamin", ""], ["Franca", "Matheus Oliveira", ""], ["Chan", "Jacob", ""], ["Le", "Van", ""], ["Uesaka", "Ayaka", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""], ["Wang", "Jueren", ""], ["Fujioka", "Yutaka", ""]]}, {"id": "1909.13287", "submitter": "Jing Luo", "authors": "Jing Luo, Xinyu Yang, Shulei Ji, Juan Li", "title": "MG-VAE: Deep Chinese Folk Songs Generation with Specific Regional Style", "comments": "Accepted by the 7th Conference on Sound and Music Technology, 2019,\n  Harbin, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regional style in Chinese folk songs is a rich treasure that can be used for\nethnic music creation and folk culture research. In this paper, we propose\nMG-VAE, a music generative model based on VAE (Variational Auto-Encoder) that\nis capable of capturing specific music style and generating novel tunes for\nChinese folk songs (Min Ge) in a manipulatable way. Specifically, we\ndisentangle the latent space of VAE into four parts in an adversarial training\nway to control the information of pitch and rhythm sequence, as well as of\nmusic style and content. In detail, two classifiers are used to separate style\nand content latent space, and temporal supervision is utilized to disentangle\nthe pitch and rhythm sequence. The experimental results show that the\ndisentanglement is successful and our model is able to create novel folk songs\nwith controllable regional styles. To our best knowledge, this is the first\nstudy on applying deep generative model and adversarial training for Chinese\nmusic generation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 14:01:36 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Luo", "Jing", ""], ["Yang", "Xinyu", ""], ["Ji", "Shulei", ""], ["Li", "Juan", ""]]}, {"id": "1909.13689", "submitter": "David Semedo", "authors": "David Semedo and Jo\\~ao Magalh\\~aes", "title": "Diachronic Cross-modal Embeddings", "comments": "To appear in ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3351036", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the semantic shifts of multimodal information is only possible\nwith models that capture cross-modal interactions over time. Under this\nparadigm, a new embedding is needed that structures visual-textual interactions\naccording to the temporal dimension, thus, preserving data's original temporal\norganisation. This paper introduces a novel diachronic cross-modal embedding\n(DCM), where cross-modal correlations are represented in embedding space,\nthroughout the temporal dimension, preserving semantic similarity at each\ninstant t. To achieve this, we trained a neural cross-modal architecture, under\na novel ranking loss strategy, that for each multimodal instance, enforces\nneighbour instances' temporal alignment, through subspace structuring\nconstraints based on a temporal alignment window. Experimental results show\nthat our DCM embedding successfully organises instances over time. Quantitative\nexperiments, confirm that DCM is able to preserve semantic cross-modal\ncorrelations at each instant t while also providing better alignment\ncapabilities. Qualitative experiments unveil new ways to browse multimodal\ncontent and hint that multimodal understanding tasks can benefit from this new\nembedding.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 13:34:58 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Semedo", "David", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""]]}, {"id": "1909.13714", "submitter": "Eda Okur", "authors": "Eda Okur, Shachi H Kumar, Saurav Sahay, Lama Nachman", "title": "Towards Multimodal Understanding of Passenger-Vehicle Interactions in\n  Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data", "comments": "Presented as a short-paper at the 23rd Workshop on the Semantics and\n  Pragmatics of Dialogue (SemDial 2019 - LondonLogue), Sep 4-6, 2019, London,\n  UK", "journal-ref": "Proceedings of the 23rd Workshop on the Semantics and Pragmatics\n  of Dialogue (SEMDIAL), pp. 213-215, London, United Kingdom, September 2019", "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding passenger intents from spoken interactions and car's vision\n(both inside and outside the vehicle) are important building blocks towards\ndeveloping contextual dialog systems for natural interactions in autonomous\nvehicles (AV). In this study, we continued exploring AMIE (Automated-vehicle\nMultimodal In-cabin Experience), the in-cabin agent responsible for handling\ncertain multimodal passenger-vehicle interactions. When the passengers give\ninstructions to AMIE, the agent should parse such commands properly considering\navailable three modalities (language/text, audio, video) and trigger the\nappropriate functionality of the AV system. We had collected a multimodal\nin-cabin dataset with multi-turn dialogues between the passengers and AMIE\nusing a Wizard-of-Oz scheme via realistic scavenger hunt game. In our previous\nexplorations, we experimented with various RNN-based models to detect\nutterance-level intents (set destination, change route, go faster, go slower,\nstop, park, pull over, drop off, open door, and others) along with intent\nkeywords and relevant slots (location, position/direction, object,\ngesture/gaze, time-guidance, person) associated with the action to be performed\nin our AV scenarios. In this recent work, we propose to discuss the benefits of\nmultimodal understanding of in-cabin utterances by incorporating\nverbal/language input (text and speech embeddings) together with the\nnon-verbal/acoustic and visual input from inside and outside the vehicle (i.e.,\npassenger gestures and gaze from in-cabin video stream, referred objects\noutside of the vehicle from the road view camera stream). Our experimental\nresults outperformed text-only baselines and with multimodality, we achieved\nimproved performances for utterance-level intent detection and slot filling.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 00:00:41 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Okur", "Eda", ""], ["Kumar", "Shachi H", ""], ["Sahay", "Saurav", ""], ["Nachman", "Lama", ""]]}, {"id": "1909.13733", "submitter": "David Semedo", "authors": "David Semedo and Jo\\~ao Magalh\\~aes", "title": "Cross-Modal Subspace Learning with Scheduled Adaptive Margin Constraints", "comments": "To appear in ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3351030", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal embeddings, between textual and visual modalities, aim to\norganise multimodal instances by their semantic correlations. State-of-the-art\napproaches use maximum-margin methods, based on the hinge-loss, to enforce a\nconstant margin m, to separate projections of multimodal instances from\ndifferent categories. In this paper, we propose a novel scheduled adaptive\nmaximum-margin (SAM) formulation that infers triplet-specific constraints\nduring training, therefore organising instances by adaptively enforcing\ninter-category and inter-modality correlations. This is supported by a\nscheduled adaptive margin function, that is smoothly activated, replacing a\nstatic margin by an adaptively inferred one reflecting triplet-specific\nsemantic correlations while accounting for the incremental learning behaviour\nof neural networks to enforce category cluster formation and enforcement.\nExperiments on widely used datasets show that our model improved upon\nstate-of-the-art approaches, by achieving a relative improvement of up to\n~12.5% over the second best method, thus confirming the effectiveness of our\nscheduled adaptive margin formulation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:20:06 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Semedo", "David", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""]]}]