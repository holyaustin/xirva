[{"id": "1905.00161", "submitter": "Chen Li", "authors": "Chen Li, Mai Xu, Shanyi Zhang, Patrick Le Callet", "title": "State-of-the-art in 360{\\deg} Video/Image Processing: Perception,\n  Assessment and Compression", "comments": "Submitted to IEEE J-STSP SI of Perception-driven 360-degree video\n  processing as an Invited Overview Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, 360{\\deg} video/image has been increasingly popular and drawn great\nattention. The spherical viewing range of 360{\\deg} video/image accounts for\nhuge data, which pose the challenges to 360{\\deg} video/image processing in\nsolving the bottleneck of storage, transmission, etc. Accordingly, the recent\nyears have witnessed the explosive emergence of works on 360{\\deg} video/image\nprocessing. In this paper, we review the state-of-the-art works on 360{\\deg}\nvideo/image processing from the aspects of perception, assessment and\ncompression. First, this paper reviews both datasets and visual attention\nmodelling approaches for 360{\\deg} video/image. Second, we survey the related\nworks on both subjective and objective visual quality assessment (VQA) of\n360{\\deg} video/image. Third, we overview the compression approaches for\n360{\\deg} video/image, which either utilize the spherical characteristics or\nvisual attention models. Finally, we summarize this overview paper and outlook\nthe future research trends on 360{\\deg} video/image processing.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 02:19:38 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 08:32:46 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Chen", ""], ["Xu", "Mai", ""], ["Zhang", "Shanyi", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1905.00190", "submitter": "David Alexandre", "authors": "David Alexandre, Chih-Peng Chang, Wen-Hsiao Peng, Hsueh-Ming Hang", "title": "Learned Image Compression with Soft Bit-based Rate-Distortion\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the notion of soft bits to address the rate-distortion\noptimization for learning-based image compression. Recent methods for such\ncompression train an autoencoder end-to-end with an objective to strike a\nbalance between distortion and rate. They are faced with the zero gradient\nissue due to quantization and the difficulty of estimating the rate accurately.\nInspired by soft quantization, we represent quantization indices of feature\nmaps with differentiable soft bits. This allows us to couple tightly the rate\nestimation with context-adaptive binary arithmetic coding. It also provides a\ndifferentiable distortion objective function. Experimental results show that\nour approach achieves the state-of-the-art compression performance among the\nlearning-based schemes in terms of MS-SSIM and PSNR.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 05:30:55 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Alexandre", "David", ""], ["Chang", "Chih-Peng", ""], ["Peng", "Wen-Hsiao", ""], ["Hang", "Hsueh-Ming", ""]]}, {"id": "1905.00469", "submitter": "Tao Wang", "authors": "Tao Wang, Irene Cheng and Anup Basu", "title": "Fully Automatic Brain Tumor Segmentation using a Normalized Gaussian\n  Bayesian Classifier and 3D Fluid Vector Flow", "comments": "ICIP 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tumor segmentation from Magnetic Resonance Images (MRIs) is an\nimportant task to measure tumor responses to treatments. However, automatic\nsegmentation is very challenging. This paper presents an automatic brain tumor\nsegmentation method based on a Normalized Gaussian Bayesian classification and\na new 3D Fluid Vector Flow (FVF) algorithm. In our method, a Normalized\nGaussian Mixture Model (NGMM) is proposed and used to model the healthy brain\ntissues. Gaussian Bayesian Classifier is exploited to acquire a Gaussian\nBayesian Brain Map (GBBM) from the test brain MR images. GBBM is further\nprocessed to initialize the 3D FVF algorithm, which segments the brain tumor.\nThis algorithm has two major contributions. First, we present a NGMM to model\nhealthy brains. Second, we extend our 2D FVF algorithm to 3D space and use it\nfor brain tumor segmentation. The proposed method is validated on a publicly\navailable dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:49:40 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Wang", "Tao", ""], ["Cheng", "Irene", ""], ["Basu", "Anup", ""]]}, {"id": "1905.00579", "submitter": "Wenmian Yang", "authors": "Wenmian Yang, Wenyuan Gao, Xiaojie Zhou, Weijia Jia, Shaohua Zhang,\n  Yutao Luo", "title": "Herding Effect based Attention for Personalized Time-Sync Video\n  Recommendation", "comments": "ACCEPTED for ORAL presentation at IEEE ICME 2019", "journal-ref": null, "doi": "10.1109/ICME.2019.00085", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-sync comment (TSC) is a new form of user-interaction review associated\nwith real-time video contents, which contains a user's preferences for videos\nand therefore well suited as the data source for video recommendations.\nHowever, existing review-based recommendation methods ignore the\ncontext-dependent (generated by user-interaction), real-time, and\ntime-sensitive properties of TSC data. To bridge the above gaps, in this paper,\nwe use video images and users' TSCs to design an Image-Text Fusion model with a\nnovel Herding Effect Attention mechanism (called ITF-HEA), which can predict\nusers' favorite videos with model-based collaborative filtering. Specifically,\nin the HEA mechanism, we weight the context information based on the semantic\nsimilarities and time intervals between each TSC and its context, thereby\nconsidering influences of the herding effect in the model. Experiments show\nthat ITF-HEA is on average 3.78\\% higher than the state-of-the-art method upon\nF1-score in baselines.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 05:55:53 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Yang", "Wenmian", ""], ["Gao", "Wenyuan", ""], ["Zhou", "Xiaojie", ""], ["Jia", "Weijia", ""], ["Zhang", "Shaohua", ""], ["Luo", "Yutao", ""]]}, {"id": "1905.01053", "submitter": "Wenmian Yang", "authors": "Wenmian Yang, Kun Wang, Na Ruan, Wenyuan Gao, Weijia Jia, Wei Zhao,\n  Nan Liu, Yunyong Zhang", "title": "Time-sync Video Tag Extraction Using Semantic Association Graph", "comments": "Accepted by ACM TKDD 2019", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD), Volume\n  13 Issue 4, July 2019", "doi": "10.1145/3332932", "report-no": "TKDD-1304-37", "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-sync comments reveal a new way of extracting the online video tags.\nHowever, such time-sync comments have lots of noises due to users' diverse\ncomments, introducing great challenges for accurate and fast video tag\nextractions. In this paper, we propose an unsupervised video tag extraction\nalgorithm named Semantic Weight-Inverse Document Frequency (SW-IDF).\nSpecifically, we first generate corresponding semantic association graph (SAG)\nusing semantic similarities and timestamps of the time-sync comments. Second,\nwe propose two graph cluster algorithms, i.e., dialogue-based algorithm and\ntopic center-based algorithm, to deal with the videos with different density of\ncomments. Third, we design a graph iteration algorithm to assign the weight to\neach comment based on the degrees of the clustered subgraphs, which can\ndifferentiate the meaningful comments from the noises. Finally, we gain the\nweight of each word by combining Semantic Weight (SW) and Inverse Document\nFrequency (IDF). In this way, the video tags are extracted automatically in an\nunsupervised way. Extensive experiments have shown that SW-IDF (dialogue-based\nalgorithm) achieves 0.4210 F1-score and 0.4932 MAP (Mean Average Precision) in\nhigh-density comments, 0.4267 F1-score and 0.3623 MAP in low-density comments;\nwhile SW-IDF (topic center-based algorithm) achieves 0.4444 F1-score and 0.5122\nMAP in high-density comments, 0.4207 F1-score and 0.3522 MAP in low-density\ncomments. It has a better performance than the state-of-the-art unsupervised\nalgorithms in both F1-score and MAP.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 07:30:14 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Yang", "Wenmian", ""], ["Wang", "Kun", ""], ["Ruan", "Na", ""], ["Gao", "Wenyuan", ""], ["Jia", "Weijia", ""], ["Zhao", "Wei", ""], ["Liu", "Nan", ""], ["Zhang", "Yunyong", ""]]}, {"id": "1905.01273", "submitter": "Hao Wang", "authors": "Hao Wang, Doyen Sahoo, Chenghao Liu, Ee-peng Lim, Steven C. H. Hoi", "title": "Learning Cross-Modal Embeddings with Adversarial Networks for Cooking\n  Recipes and Food Images", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food computing is playing an increasingly important role in human daily life,\nand has found tremendous applications in guiding human behavior towards smart\nfood consumption and healthy lifestyle. An important task under the\nfood-computing umbrella is retrieval, which is particularly helpful for health\nrelated applications, where we are interested in retrieving important\ninformation about food (e.g., ingredients, nutrition, etc.). In this paper, we\ninvestigate an open research task of cross-modal retrieval between cooking\nrecipes and food images, and propose a novel framework Adversarial Cross-Modal\nEmbedding (ACME) to resolve the cross-modal retrieval task in food domains.\nSpecifically, the goal is to learn a common embedding feature space between the\ntwo modalities, in which our approach consists of several novel ideas: (i)\nlearning by using a new triplet loss scheme together with an effective sampling\nstrategy, (ii) imposing modality alignment using an adversarial learning\nstrategy, and (iii) imposing cross-modal translation consistency such that the\nembedding of one modality is able to recover some important information of\ncorresponding instances in the other modality. ACME achieves the\nstate-of-the-art performance on the benchmark Recipe1M dataset, validating the\nefficacy of the proposed technique.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 17:08:01 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wang", "Hao", ""], ["Sahoo", "Doyen", ""], ["Liu", "Chenghao", ""], ["Lim", "Ee-peng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1905.01304", "submitter": "Tao Yao", "authors": "Tao Yao, Xiangwei Kong, Lianshan Yan, Wenjing Tang and Qi Tian", "title": "Efficient Discrete Supervised Hashing for Large-scale Cross-modal\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised cross-modal hashing has gained increasing research interest on\nlarge-scale retrieval task owning to its satisfactory performance and\nefficiency. However, it still has some challenging issues to be further\nstudied: 1) most of them fail to well preserve the semantic correlations in\nhash codes because of the large heterogenous gap; 2) most of them relax the\ndiscrete constraint on hash codes, leading to large quantization error and\nconsequent low performance; 3) most of them suffer from relatively high memory\ncost and computational complexity during training procedure, which makes them\nunscalable. In this paper, to address above issues, we propose a supervised\ncross-modal hashing method based on matrix factorization dubbed Efficient\nDiscrete Supervised Hashing (EDSH). Specifically, collective matrix\nfactorization on heterogenous features and semantic embedding with class labels\nare seamlessly integrated to learn hash codes. Therefore, the feature based\nsimilarities and semantic correlations can be both preserved in hash codes,\nwhich makes the learned hash codes more discriminative. Then an efficient\ndiscrete optimal algorithm is proposed to handle the scalable issue. Instead of\nlearning hash codes bit-by-bit, hash codes matrix can be obtained directly\nwhich is more efficient. Extensive experimental results on three public\nreal-world datasets demonstrate that EDSH produces a superior performance in\nboth accuracy and scalability over some existing cross-modal hashing methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 02:34:09 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Yao", "Tao", ""], ["Kong", "Xiangwei", ""], ["Yan", "Lianshan", ""], ["Tang", "Wenjing", ""], ["Tian", "Qi", ""]]}, {"id": "1905.01674", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu and Xinpeng Zhang", "title": "Game-theoretic Analysis to Content-adaptive Reversible Watermarking", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many games were designed for steganography and robust watermarking, few\nfocused on reversible watermarking. We present a two-encoder game related to\nthe rate-distortion optimization of content-adaptive reversible watermarking.\nIn the game, Alice first hides a payload into a cover. Then, Bob hides another\npayload into the modified cover. The embedding strategy of Alice affects the\nembedding capacity of Bob. The embedding strategy of Bob may produce\ndata-extraction errors to Alice. Both want to embed as many pure secret bits as\npossible, subjected to an upper-bounded distortion. We investigate\nnon-cooperative game and cooperative game between Alice and Bob. When they\ncooperate with each other, one may consider them as a whole, i.e., an encoder\nuses a cover for data embedding with two times. When they do not cooperate with\neach other, the game corresponds to a separable system, i.e., both want to\nindependently hide a payload within the cover, but recovering the cover may\nneed cooperation. We find equilibrium strategies for both players under\nconstraints.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 12:51:36 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wu", "Hanzhou", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "1905.01723", "submitter": "Ming-Yu Liu", "authors": "Ming-Yu Liu and Xun Huang and Arun Mallya and Tero Karras and Timo\n  Aila and Jaakko Lehtinen and Jan Kautz", "title": "Few-Shot Unsupervised Image-to-Image Translation", "comments": "The paper will be presented at the International Conference on\n  Computer Vision (ICCV) 2019", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation methods learn to map images in a\ngiven class to an analogous image in a different class, drawing on unstructured\n(non-registered) datasets of images. While remarkably successful, current\nmethods require access to many images in both source and destination classes at\ntraining time. We argue this greatly limits their use. Drawing inspiration from\nthe human capability of picking up the essence of a novel object from a small\nnumber of examples and generalizing from there, we seek a few-shot,\nunsupervised image-to-image translation algorithm that works on previously\nunseen target classes that are specified, at test time, only by a few example\nimages. Our model achieves this few-shot generation capability by coupling an\nadversarial training scheme with a novel network design. Through extensive\nexperimental validation and comparisons to several baseline methods on\nbenchmark datasets, we verify the effectiveness of the proposed framework. Our\nimplementation and datasets are available at https://github.com/NVlabs/FUNIT .\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 17:41:31 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 06:11:56 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Huang", "Xun", ""], ["Mallya", "Arun", ""], ["Karras", "Tero", ""], ["Aila", "Timo", ""], ["Lehtinen", "Jaakko", ""], ["Kautz", "Jan", ""]]}, {"id": "1905.01790", "submitter": "Mingzhou Liu", "authors": "Mingzhou Liu, Xiaoyi He, Weiyao Lin, Xintong Han, Yanmin Zhu, Hongtao\n  Lu, Hongkai Xiong", "title": "A multimodal lossless coding method for skeletons in videos", "comments": "This manuscript is the accepted version for ICMEW (IEEE Intl. Conf.\n  Multimedia & Expo Workshop), IEEE Intl. Conf. Multimedia & Expo Workshop\n  (ICME), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, skeleton information in videos plays an important role in\nhuman-centric video analysis but effective coding such massive skeleton\ninformation has never been addressed in previous work. In this paper, we make\nthe first attempt to solve this problem by proposing a multimodal skeleton\ncoding tool containing three different coding schemes, namely, spatial\ndifferential-coding scheme, motionvector-based differential-coding scheme and\ninter prediction scheme, thus utilizing both spatial and temporal redundancy to\nlosslessly compress skeleton data. More importantly, these schemes are switched\nproperly for different types of skeletons in video frames, hence achieving\nfurther improvement of compression rate. Experimental results show that our\napproach leads to 74.4% and 54.7% size reduction on our surveillance sequences\nand overall test sequences respectively, which demonstrates the effectiveness\nof our skeleton coding tool.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 02:08:46 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 00:11:38 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Liu", "Mingzhou", ""], ["He", "Xiaoyi", ""], ["Lin", "Weiyao", ""], ["Han", "Xintong", ""], ["Zhu", "Yanmin", ""], ["Lu", "Hongtao", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1905.02001", "submitter": "Xinfeng Zhang", "authors": "Xinfeng Zhang, Sam Kwong, C.-C. Jay Kuo", "title": "Compressed Image Quality Assessment Based on Saak Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed image quality assessment plays an important role in image\nservices, especially in image compression applications, which can be utilized\nas a guidance to optimize image processing algorithms. In this paper, we\npropose an objective image quality assessment algorithm to measure the quality\nof compressed images. The proposed method utilizes a data-driven transform,\nSaak (Subspace approximation with augmented kernels), to decompose images into\nhierarchical structural feature space. We measure the distortions of Saak\nfeatures and accumulate these distortions according to the feature importance\nto human visual system. Compared with the state-of-the-art image quality\nassessment methods on widely utilized datasets, the proposed method correlates\nbetter with the subjective results. In addition, the proposed methods achieves\nmore robust results on different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 12:49:13 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 11:30:47 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zhang", "Xinfeng", ""], ["Kwong", "Sam", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1905.02540", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng and Kris Kitani", "title": "Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for\n  Lipreading", "comments": "camera ready version for BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the word-level visual lipreading, which requires recognizing the\nword being spoken, given only the video but not the audio. State-of-the-art\nmethods explore the use of end-to-end neural networks, including a shallow (up\nto three layers) 3D convolutional neural network (CNN) + a deep 2D CNN (e.g.,\nResNet) as the front-end to extract visual features, and a recurrent neural\nnetwork (e.g., bidirectional LSTM) as the back-end for classification. In this\nwork, we propose to replace the shallow 3D CNNs + deep 2D CNNs front-end with\nrecent successful deep 3D CNNs --- two-stream (i.e., grayscale video and\noptical flow streams) I3D. We evaluate different combinations of front-end and\nback-end modules with the grayscale video and optical flow inputs on the LRW\ndataset. The experiments show that, compared to the shallow 3D CNNs + deep 2D\nCNNs front-end, the deep 3D CNNs front-end with pre-training on the large-scale\nimage and video datasets (e.g., ImageNet and Kinetics) can improve the\nclassification accuracy. Also, we demonstrate that using the optical flow input\nalone can achieve comparable performance as using the grayscale video as input.\nMoreover, the two-stream network using both the grayscale video and optical\nflow inputs can further improve the performance. Overall, our two-stream I3D\nfront-end with a Bi-LSTM back-end results in an absolute improvement of 5.3%\nover the previous art on the LRW dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 02:32:06 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 03:19:21 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Weng", "Xinshuo", ""], ["Kitani", "Kris", ""]]}, {"id": "1905.02857", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Ruyue Yuan, Liyi Xiao, Fei Wang", "title": "Learning Cascaded Siamese Networks for High Performance Visual Tracking", "comments": "Accepted for IEEE 26th International Conference on Image Processing\n  (ICIP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is one of the most challenging computer vision problems. In\norder to achieve high performance visual tracking in various negative\nscenarios, a novel cascaded Siamese network is proposed and developed based on\ntwo different deep learning networks: a matching subnetwork and a\nclassification subnetwork. The matching subnetwork is a fully convolutional\nSiamese network. According to the similarity score between the exemplar image\nand the candidate image, it aims to search possible object positions and crop\nscaled candidate patches. The classification subnetwork is designed to further\nevaluate the cropped candidate patches and determine the optimal tracking\nresults based on the classification score. The matching subnetwork is trained\noffline and fixed online, while the classification subnetwork performs\nstochastic gradient descent online to learn more target-specific information.\nTo improve the tracking performance further, an effective classification\nsubnetwork update method based on both similarity and classification scores is\nutilized for updating the classification subnetwork. Extensive experimental\nresults demonstrate that our proposed approach achieves state-of-the-art\nperformance in recent benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 01:06:23 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Yuan", "Ruyue", ""], ["Xiao", "Liyi", ""], ["Wang", "Fei", ""]]}, {"id": "1905.02872", "submitter": "Fuqiang Di", "authors": "Zhuo Zhang, Guangyuan Fu, Fuqiang Di, Changlong Li, Jia Liu", "title": "Generative Reversible Data Hiding by Image to Image Translation via GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional reversible data hiding technique is based on cover image\nmodification which inevitably leaves some traces of rewriting that can be more\neasily analyzed and attacked by the warder. Inspired by the cover synthesis\nsteganography based generative adversarial networks, in this paper, a novel\ngenerative reversible data hiding scheme (GRDH) by image translation is\nproposed. First, an image generator is used to obtain a realistic image, which\nis used as an input to the image-to-image translation model with CycleGAN.\nAfter image translation, a stego image with different semantic information will\nbe obtained. The secret message and the original input image can be recovered\nseparately by a well-trained message extractor and the inverse transform of the\nimage translation. Experimental results have verified the effectiveness of the\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 02:24:18 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 13:58:20 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 04:41:40 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 16:29:55 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Zhang", "Zhuo", ""], ["Fu", "Guangyuan", ""], ["Di", "Fuqiang", ""], ["Li", "Changlong", ""], ["Liu", "Jia", ""]]}, {"id": "1905.02899", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Convolutional Neural Networks Considering Local and Global features for\n  Image Enhancement", "comments": "To appear in Proc. ICIP2019. arXiv admin note: text overlap with\n  arXiv:1901.05686", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel convolutional neural network (CNN)\narchitecture considering both local and global features for image enhancement.\nMost conventional image enhancement methods, including Retinex-based methods,\ncannot restore lost pixel values caused by clipping and quantizing. CNN-based\nmethods have recently been proposed to solve the problem, but they still have a\nlimited performance due to network architectures not handling global features.\nTo handle both local and global features, the proposed architecture consists of\nthree networks: a local encoder, a global encoder, and a decoder. In addition,\nhigh dynamic range (HDR) images are used for generating training data for our\nnetworks. The use of HDR images makes it possible to train CNNs with\nbetter-quality images than images directly captured with cameras. Experimental\nresults show that the proposed method can produce higher-quality images than\nconventional image enhancement methods including CNN-based methods, in terms of\nvarious objective quality metrics: TMQI, entropy, NIQE, and BRISQUE.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:20:30 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1905.03508", "submitter": "Lara Mu\\~noz", "authors": "Lara Mu\\~noz, C\\'esar D\\'iaz, Marta Orduna, Jos\\'e Ignacio Ronda,\n  Pablo P\\'erez, Ignacio Benito and Narciso Garc\\'ia", "title": "Methodology for accurately assessing the quality perceived by users on\n  360VR contents", "comments": null, "journal-ref": "Digital Signal Processing, vol. 100, article 102706, pp. 1-16, May\n  2020", "doi": "10.1016/j.dsp.2020.102706", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To properly evaluate the performance of 360VR-specific encoding and\ntransmission schemes, and particularly of the solutions based on viewport\nadaptation, it is necessary to consider not only the bandwidth saved, but also\nthe quality of the portion of the scene actually seen by users over time. With\nthis motivation, we propose a robust, yet flexible methodology for accurately\nassessing the quality within the viewport along the visualization session. This\nprocedure is based on a complete analysis of the geometric relations involved.\nMoreover, the designed methodology allows for both offline and online usage\nthanks to the use of different approximations. In this way, our methodology can\nbe used regardless of the approach to properly evaluate the implemented\nstrategy, obtaining a fairer comparison between them.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 09:54:15 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Mu\u00f1oz", "Lara", ""], ["D\u00edaz", "C\u00e9sar", ""], ["Orduna", "Marta", ""], ["Ronda", "Jos\u00e9 Ignacio", ""], ["P\u00e9rez", "Pablo", ""], ["Benito", "Ignacio", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "1905.03533", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Yuan Ji, Bin Luo", "title": "Reversible Data Hiding in JPEG Images with Multi-objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among various methods of reversible data hiding (RDH) in JPEG images, the\nconsideration in designing is only the image quality, but the image quality and\nthe file size expansion are equally important in JPEG images. Based on this\nsituation, we propose a RDH scheme in JPEG images considering both the image\nquality and the file size expansion while designing the algorithm. The\nmulti-objective optimization strategy is utilized to realize the balance of the\ntwo objectives. Specifically, the cover is divided into several non-overlapping\nsignals firstly, and after that, the embedding costs of signals are calculated\nusing the knowledge of the JPEG compression. Next, the optimized combination of\nsignals for embedding data is gained by the multi-objective optimization.\nExperimental results show the better performance of our proposed RDH compared\nwith state-of-the-art RDH in JPEG images.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 11:13:16 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Ji", "Yuan", ""], ["Luo", "Bin", ""]]}, {"id": "1905.03691", "submitter": "Wei Yan", "authors": "Wei Yan, Yiting shao, Shan Liu, Thomas H Li, Zhu Li, Ge Li", "title": "Deep AutoEncoder-based Lossy Geometry Compression for Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is a fundamental 3D representation which is widely used in real\nworld applications such as autonomous driving. As a newly-developed media\nformat which is characterized by complexity and irregularity, point cloud\ncreates a need for compression algorithms which are more flexible than existing\ncodecs. Recently, autoencoders(AEs) have shown their effectiveness in many\nvisual analysis tasks as well as image compression, which inspires us to employ\nit in point cloud compression. In this paper, we propose a general\nautoencoder-based architecture for lossy geometry point cloud compression. To\nthe best of our knowledge, it is the first autoencoder-based geometry\ncompression codec that directly takes point clouds as input rather than voxel\ngrids or collections of images. Compared with handcrafted codecs, this approach\nadapts much more quickly to previously unseen media contents and media formats,\nmeanwhile achieving competitive performance. Our architecture consists of a\npointnet-based encoder, a uniform quantizer, an entropy estimation block and a\nnonlinear synthesis transformation module. In lossy geometry compression of\npoint cloud, results show that the proposed method outperforms the test model\nfor categories 1 and 3 (TMC13) published by MPEG-3DG group on the 125th\nmeeting, and on average a 73.15\\% BD-rate gain is achieved.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 02:44:50 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yan", "Wei", ""], ["shao", "Yiting", ""], ["Liu", "Shan", ""], ["Li", "Thomas H", ""], ["Li", "Zhu", ""], ["Li", "Ge", ""]]}, {"id": "1905.03694", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Hong Liu, Yongjian Liu", "title": "Supervised Online Hashing via Hadamard Codebook Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, binary code learning, a.k.a hashing, has received extensive\nattention in large-scale multimedia retrieval. It aims to encode\nhigh-dimensional data points to binary codes, hence the original\nhigh-dimensional metric space can be efficiently approximated via Hamming\nspace. However, most existing hashing methods adopted offline batch learning,\nwhich is not suitable to handle incremental datasets with streaming data or new\ninstances. In contrast, the robustness of the existing online hashing remains\nas an open problem, while the embedding of supervised/semantic information\nhardly boosts the performance of the online hashing, mainly due to the defect\nof unknown category numbers in supervised learning. In this paper, we proposed\nan online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH),\nwhich aims to solve the above problems towards robust and supervised online\nhashing. In particular, we first assign an appropriate high-dimensional binary\ncodes to each class label, which is generated randomly by Hadamard codes to\neach class label, which is generated randomly by Hadamard codes. Subsequently,\nLSH is adopted to reduce the length of such Hadamard codes in accordance with\nthe hash bits, which can adapt the predefined binary codes online, and\ntheoretically guarantee the semantic similarity. Finally, we consider the\nsetting of stochastic data acquisition, which facilitates our method to\nefficiently learn the corresponding hashing functions via stochastic gradient\ndescend (SGD) online. Notably, the proposed HCOH can be embedded with\nsupervised labels and it not limited to a predefined category number. Extensive\nexperiments on three widely-used benchmarks demonstrate the merits of the\nproposed scheme over the state-of-the-art methods. The code is available at\nhttps://github.com/lmbxmu/mycode/tree/master/2018ACMMM_HCOH.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 02:33:53 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 18:02:53 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""], ["Liu", "Yongjian", ""]]}, {"id": "1905.03823", "submitter": "Afshin Taghavi Nasrabadi", "authors": "Afshin Taghavi Nasrabadi, Aliehsan Samiei, Anahita Mahzari, Mylene\n  C.Q. Farias, Marcelo M. Carvalho, Ryan P. McMahan, Ravi Prakash", "title": "A Taxonomy and Dataset for 360{\\deg} Videos", "comments": null, "journal-ref": null, "doi": "10.1145/3304109.3325812", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a taxonomy for 360{\\deg} videos that categorizes\nvideos based on moving objects and camera motion. We gathered and produced 28\nvideos based on the taxonomy, and recorded viewport traces from 60 participants\nwatching the videos. In addition to the viewport traces, we provide the\nviewers' feedback on their experience watching the videos, and we also analyze\nviewport patterns on each category.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 19:21:45 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Nasrabadi", "Afshin Taghavi", ""], ["Samiei", "Aliehsan", ""], ["Mahzari", "Anahita", ""], ["Farias", "Mylene C. Q.", ""], ["Carvalho", "Marcelo M.", ""], ["McMahan", "Ryan P.", ""], ["Prakash", "Ravi", ""]]}, {"id": "1905.03908", "submitter": "Dawei Wang Mr", "authors": "Xin Yang, Wenbo Hu, Dawei Wang, Lijing Zhao, Baocai Yin, Qiang Zhang,\n  Xiaopeng Wei, Hongbo Fu", "title": "DEMC: A Deep Dual-Encoder Network for Denoising Monte Carlo Rendering", "comments": "Published in Journal of Computer Science and Technology. The final\n  publication is available at springerlink.com", "journal-ref": "Journal of Computer Science and Technology, 2019, 34.5: 1123-1135", "doi": "10.1007/s11390-019-1964-2", "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present DEMC, a deep Dual-Encoder network to remove Monte\nCarlo noise efficiently while preserving details. Denoising Monte Carlo\nrendering is different from natural image denoising since inexpensive\nby-products (feature buffers) can be extracted in the rendering stage. Most of\nthem are noise-free and can provide sufficient details for image\nreconstruction. However, these feature buffers also contain redundant\ninformation, which makes Monte Carlo denoising different from natural image\ndenoising. Hence, the main challenge of this topic is how to extract useful\ninformation and reconstruct clean images. To address this problem, we propose a\nnovel network structure, Dual-Encoder network with a feature fusion\nsub-network, to fuse feature buffers firstly, then encode the fused feature\nbuffers and a noisy image simultaneously, and finally reconstruct a clean image\nby a decoder network. Compared with the state-of-the-art methods, our model is\nmore robust on a wide range of scenes and is able to generate satisfactory\nresults in a significantly faster way.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 01:42:06 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 08:16:45 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yang", "Xin", ""], ["Hu", "Wenbo", ""], ["Wang", "Dawei", ""], ["Zhao", "Lijing", ""], ["Yin", "Baocai", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""], ["Fu", "Hongbo", ""]]}, {"id": "1905.04079", "submitter": "Yat Hong Lam", "authors": "Yat Hong Lam, Alireza Zare, Caglar Aytekin, Francesco Cricri, Jani\n  Lainema, Emre Aksu, Miska Hannuksela", "title": "Compressing Weight-updates for Image Artifacts Removal Neural Networks", "comments": "Submission for CHALLENGE ON LEARNED IMAGE COMPRESSION (CLIC) 2019\n  (updated on 14 June 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for fine-tuning a decoder-side\nneural network in the context of image compression, such that the\nweight-updates are better compressible. At encoder side, we fine-tune a\npre-trained artifact removal network on target data by using a compression\nobjective applied on the weight-update. In particular, the compression\nobjective encourages weight-updates which are sparse and closer to quantized\nvalues. This way, the final weight-update can be compressed more efficiently by\npruning and quantization, and can be included into the encoded bitstream\ntogether with the image bitstream of a traditional codec. We show that this\napproach achieves reconstruction quality which is on-par or slightly superior\nto a traditional codec, at comparable bitrates. To our knowledge, this is the\nfirst attempt to combine image compression and neural network's weight update\ncompression.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:36:36 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 12:30:34 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Lam", "Yat Hong", ""], ["Zare", "Alireza", ""], ["Aytekin", "Caglar", ""], ["Cricri", "Francesco", ""], ["Lainema", "Jani", ""], ["Aksu", "Emre", ""], ["Hannuksela", "Miska", ""]]}, {"id": "1905.04709", "submitter": "Gang Min", "authors": "Gang Min, Changqing Zhang, Xiongwei Zhang, Wei Tan", "title": "Deep Vocoder: Low Bit Rate Compression of Speech with Deep Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT cs.SD eess.AS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of deep neural networks (DNNs) in speech processing,\nthis paper presents Deep Vocoder, a direct end-to-end low bit rate speech\ncompression method with deep autoencoder (DAE). In Deep Vocoder, DAE is used\nfor extracting the latent representing features (LRFs) of speech, which are\nthen efficiently quantized by an analysis-by-synthesis vector quantization (AbS\nVQ) method. AbS VQ aims to minimize the perceptual spectral reconstruction\ndistortion rather than the distortion of LRFs vector itself. Also, a suboptimal\ncodebook searching technique is proposed to further reduce the computational\ncomplexity. Experimental results demonstrate that Deep Vocoder yields\nsubstantial improvements in terms of frequency-weighted segmental SNR, STOI and\nPESQ score when compared to the output of the conventional SQ- or VQ-based\ncodec. The yielded PESQ score over the TIMIT corpus is 3.34 and 3.08 for speech\ncoding at 2400 bit/s and 1200 bit/s, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 12:24:27 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 09:06:34 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Min", "Gang", ""], ["Zhang", "Changqing", ""], ["Zhang", "Xiongwei", ""], ["Tan", "Wei", ""]]}, {"id": "1905.04854", "submitter": "Ziling Huang", "authors": "Ziling Huang, Zheng Wang, Chung-Chi Tsai, Shin'ichi Satoh, Chia-Wen\n  Lin", "title": "DotSCN: Group Re-identification via Domain-Transferred Single and Couple\n  Representation Learning", "comments": "accepted in IEEE Transctions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group re-identification (G-ReID) is an important yet less-studied task. Its\nchallenges not only lie in appearance changes of individuals which have been\nwell-investigated in general person re-identification (ReID), but also derive\nfrom group layout and membership changes. So the key task of G-ReID is to learn\nrepresentations robust to such changes. To address this issue, we propose a\nTransferred Single and Couple Representation Learning Network (TSCN). Its\nmerits are two aspects: 1) Due to the lack of labelled training samples,\nexisting G-ReID methods mainly rely on unsatisfactory hand-crafted features. To\ngain the superiority of deep learning models, we treat a group as multiple\npersons and transfer the domain of a labeled ReID dataset to a G-ReID target\ndataset style to learn single representations. 2) Taking into account the\nneighborhood relationship in a group, we further propose learning a novel\ncouple representation between two group members, that achieves more\ndiscriminative power in G-ReID tasks. In addition, an unsupervised weight\nlearning method is exploited to adaptively fuse the results of different views\ntogether according to result patterns. Extensive experimental results\ndemonstrate the effectiveness of our approach that significantly outperforms\nstate-of-the-art methods by 11.7\\% CMC-1 on the Road Group dataset and by\n39.0\\% CMC-1 on the DukeMCMT dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 04:19:10 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 06:34:26 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Huang", "Ziling", ""], ["Wang", "Zheng", ""], ["Tsai", "Chung-Chi", ""], ["Satoh", "Shin'ichi", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "1905.04890", "submitter": "Peng Gao", "authors": "Qi Ni, Fei Wang, Ziwei Zhao, Peng Gao", "title": "FPGA-based Binocular Image Feature Extraction and Matching System", "comments": "Accepted for the 4th International Conference on Multimedia Systems\n  and Signal Processing (ICMSSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature extraction and matching is a fundamental but computation\nintensive task in machine vision. This paper proposes a novel FPGA-based\nembedded system to accelerate feature extraction and matching. It implements\nSURF feature point detection and BRIEF feature descriptor construction and\nmatching. For binocular stereo vision, feature matching includes both tracking\nmatching and stereo matching, which simultaneously provide feature point\ncorrespondences and parallax information. Our system is evaluated on a ZYNQ\nXC7Z045 FPGA. The result demonstrates that it can process binocular video data\nat a high frame rate (640$\\times$480 @ 162fps). Moreover, an extensive test\nproves our system has robustness for image compression, blurring and\nillumination.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 07:31:26 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 01:03:34 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ni", "Qi", ""], ["Wang", "Fei", ""], ["Zhao", "Ziwei", ""], ["Gao", "Peng", ""]]}, {"id": "1905.05365", "submitter": "Zhaoxia Yin", "authors": "Yujie Jia, Zhaoxia Yin, Xinpeng Zhang, Yonglong Luo", "title": "Reversible data hiding based on reducing invalid shifting of pixels in\n  histogram shifting", "comments": "11 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, reversible data hiding (RDH), a new research hotspot in the\nfield of information security, has been paid more and more attention by\nresearchers. Most of the existing RDH schemes do not fully take it into account\nthat natural image's texture has influence on embedding distortion. The image\ndistortion caused by embedding data in the image's smooth region is much\nsmaller than that in the unsmooth region, essentially, it is because embedding\nadditional data in the smooth region corresponds to fewer invalid shifting\npixels (ISPs) in histogram shifting. Thus, we propose a RDH scheme based on the\nimages texture to reduce invalid shifting of pixels in histogram shifting.\nSpecifically, first, a cover image is divided into two sub-images by the\ncheckerboard pattern, and then each sub-image's fluctuation values are\ncalculated. Finally, additional data can be embedded into the region of\nsub-images with smaller fluctuation value preferentially. The experimental\nresults demonstrate that the proposed method has higher capacity and better\nstego-image quality than some existing RDH schemes.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 02:51:32 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Jia", "Yujie", ""], ["Yin", "Zhaoxia", ""], ["Zhang", "Xinpeng", ""], ["Luo", "Yonglong", ""]]}, {"id": "1905.05416", "submitter": "Hao Tang", "authors": "Hao Tang, Wei Wang, Songsong Wu, Xinya Chen, Dan Xu, Nicu Sebe, Yan\n  Yan", "title": "Expression Conditional GAN for Facial Expression-to-Expression\n  Translation", "comments": "5 pages, 5 figures, accepted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the facial expression translation task and propose\na novel Expression Conditional GAN (ECGAN) which can learn the mapping from one\nimage domain to another one based on an additional expression attribute. The\nproposed ECGAN is a generic framework and is applicable to different expression\ngeneration tasks where specific facial expression can be easily controlled by\nthe conditional attribute label. Besides, we introduce a novel face mask loss\nto reduce the influence of background changing. Moreover, we propose an entire\nframework for facial expression generation and recognition in the wild, which\nconsists of two modules, i.e., generation and recognition. Finally, we evaluate\nour framework on several public face datasets in which the subjects have\ndifferent races, illumination, occlusion, pose, color, content and background\nconditions. Even though these datasets are very diverse, both the qualitative\nand quantitative results demonstrate that our approach is able to generate\nfacial expressions accurately and robustly.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 06:52:03 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Wei", ""], ["Wu", "Songsong", ""], ["Chen", "Xinya", ""], ["Xu", "Dan", ""], ["Sebe", "Nicu", ""], ["Yan", "Yan", ""]]}, {"id": "1905.05627", "submitter": "Zhaoxia Yin", "authors": "Yang Du, Zhaoxia Yin, Xinpeng Zhang", "title": "High Capacity Lossless Data Hiding in JPEG Bitstream Based on General\n  VLC Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG is the most popular image format, which is widely used in our daily\nlife. Therefore, reversible data hiding (RDH) for JPEG images is important.\nMost of the RDH schemes for JPEG images will cause significant distortions and\nlarge file size increments in the marked JPEG image. As a special case of RDH,\nthe lossless data hiding (LDH) technique can keep the visual quality of the\nmarked images no degradation. In this paper, a novel high capacity LDH scheme\nis proposed. In the JPEG bitstream, not all the variable length codes (VLC) are\nused to encode image data. By constructing the mapping between the used and\nunused VLCs, the secret data can be embedded by replacing the used VLC with the\nunused VLC. Different from the previous schemes, our mapping strategy allows\nthe lengths of unused and used VLCs in a mapping set to be unequal. We present\nsome basic insights into the construction of the mapping relationship.\nExperimental results show that most of the JPEG images using the proposed\nscheme obtain smaller file size increments than previous RDH schemes.\nFurthermore, the proposed scheme can obtain high embedding capacity while\nkeeping the marked JPEG image with no distortion.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:12:12 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 02:13:05 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Du", "Yang", ""], ["Yin", "Zhaoxia", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "1905.05925", "submitter": "Jiangnan Li", "authors": "Haoran Niu, Jiangnan Li, Yu Zhao", "title": "SmartBullets: A Cloud-Assisted Bullet Screen Filter based on Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bullet-screen is a technique that enables the website users to send real-time\ncomment `bullet' cross the screen. Compared with the traditional review of a\nvideo, bullet-screen provides new features of feeling expression to video\nwatching and more iterations between video viewers. However, since all the\ncomments from the viewers are shown on the screen publicly and simultaneously,\nsome low-quality bullets will reduce the watching enjoyment of the users.\nAlthough the bullet-screen video websites have provided filter functions based\non regular expression, bad bullets can still easily pass the filter through\nmaking a small modification.\n  In this paper, we present SmartBullets, a user-centered bullet-screen filter\nbased on deep learning techniques. A convolutional neural network is trained as\nthe classifier to determine whether a bullet need to be removed according to\nits quality. Moreover, to increase the scalability of the filter, we employ a\ncloud-assisted framework by developing a backend cloud server and a front-end\nbrowser extension. The evaluation of 40 volunteers shows that SmartBullets can\neffectively remove the low-quality bullets and improve the overall watching\nexperience of viewers.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:17:18 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Niu", "Haoran", ""], ["Li", "Jiangnan", ""], ["Zhao", "Yu", ""]]}, {"id": "1905.05998", "submitter": "Tongyu Dai", "authors": "Tongyu Dai, Xinggong Zhang, Yihang Zhang, Zongming Guo", "title": "Statistical Learning Based Congestion Control for Real-time Video\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demands on interactive video applications, how to adapt\nvideo bit rate to avoid network congestion has become critical, since\ncongestion results in self-inflicted delay and packet loss which deteriorate\nthe quality of real-time video service. The existing congestion control is hard\nto simultaneously achieve low latency, high throughput, good adaptability and\nfair bandwidth allocation, mainly because of the hardwired control strategy and\negocentric convergence objective. To address these issues, we propose an\nend-to-end statistical learning based congestion control, named Iris. By\nexploring the underlying principles of self-inflicted delay, we reveal that\ncongestion delay is determined by sending rate, receiving rate and network\nstatus, which inspires us to control video bit rate using a\nstatistical-learning congestion control model. The key idea of Iris is to force\nall flows to converge to the same queue load, and adjust the bit rate by the\nmodel. All flows keep a small and fixed number of packets queuing in the\nnetwork, thus the fair bandwidth allocation and low latency are both achieved.\nBesides, the adjustment step size of sending rate is updated by online\nlearning, to better adapt to dynamically changing networks. We carried out\nextensive experiments to evaluate the performance of Iris, with the\nimplementations of transport layer (UDP) and application layer (QUIC)\nrespectively. The testing environment includes emulated network, real-world\nInternet and commercial LTE networks. Compared against TCP flavors and\nstate-of-the-art protocols, Iris is able to achieve high bandwidth utilization,\nlow latency and good fairness concurrently. Especially over QUIC, Iris is able\nto increase the video bitrate up to 25%, and PSNR up to 1dB.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 07:39:43 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 01:49:09 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Dai", "Tongyu", ""], ["Zhang", "Xinggong", ""], ["Zhang", "Yihang", ""], ["Guo", "Zongming", ""]]}, {"id": "1905.06118", "submitter": "Jon Gillick", "authors": "Jon Gillick, Adam Roberts, Jesse Engel, Douglas Eck, David Bamman", "title": "Learning to Groove with Inverse Sequence Transformations", "comments": "Blog post and links: https://g.co/magenta/groovae", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:2269-2279, 2019", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore models for translating abstract musical ideas (scores, rhythms)\ninto expressive performances using Seq2Seq and recurrent Variational\nInformation Bottleneck (VIB) models. Though Seq2Seq models usually require\npainstakingly aligned corpora, we show that it is possible to adapt an approach\nfrom the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola\net al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large\nvolumes of paired data by performing simple transformations and training\ngenerative models to plausibly invert these transformations. Music, and\ndrumming in particular, provides a strong test case for this approach because\nmany common transformations (quantization, removing voices) have clear\nsemantics, and models for learning to invert them have real-world applications.\nFocusing on the case of drum set players, we create and release a new dataset\nfor this purpose, containing over 13 hours of recordings by professional\ndrummers aligned with fine-grained timing and dynamics information. We also\nexplore some of the creative potential of these models, including demonstrating\nimprovements on state-of-the-art methods for Humanization (instantiating a\nperformance from a musical score).\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 17:25:50 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 17:24:12 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Gillick", "Jon", ""], ["Roberts", "Adam", ""], ["Engel", "Jesse", ""], ["Eck", "Douglas", ""], ["Bamman", "David", ""]]}, {"id": "1905.06269", "submitter": "Weiqing Min", "authors": "Weiqing Min, Shuqiang Jiang, Ramesh Jain", "title": "Food Recommendation: Framework, Existing Solutions and Challenges", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing proportion of the global population is becoming overweight or\nobese, leading to various diseases (e.g., diabetes, ischemic heart disease and\neven cancer) due to unhealthy eating patterns, such as increased intake of food\nwith high energy and high fat. Food recommendation is of paramount importance\nto alleviate this problem. Unfortunately, modern multimedia research has\nenhanced the performance and experience of multimedia recommendation in many\nfields such as movies and POI, yet largely lags in the food domain. This\narticle proposes a unified framework for food recommendation, and identifies\nmain issues affecting food recommendation including building the personal\nmodel, analyzing unique food characteristics, incorporating various context and\ndomain knowledge. We then review existing solutions for these issues, and\nfinally elaborate research challenges and future directions in this field. To\nour knowledge, this is the first survey that targets the study of food\nrecommendation in the multimedia field and offers a collection of research\nstudies and technologies to benefit researchers in this field.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 16:12:05 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 06:45:01 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Min", "Weiqing", ""], ["Jiang", "Shuqiang", ""], ["Jain", "Ramesh", ""]]}, {"id": "1905.06500", "submitter": "Kyoungjun Park", "authors": "Kyoungjun Park and Myungchul Kim", "title": "EVSO: Environment-aware Video Streaming Optimization of Power\n  Consumption", "comments": "9 pages, 9 figures, IEEE International Conference on Computer\n  Communications (INFOCOM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming services gradually support high-quality videos for better user\nexperience. However, streaming high-quality video on mobile devices consumes a\nconsiderable amount of energy. This paper presents the design and prototype of\nEVSO, which achieves power saving by applying adaptive frame rates to parts of\nvideos with a little degradation of the user experience. EVSO utilizes a novel\nperceptual similarity measurement method based on human visual perception\nspecialized for a video encoder. We also extend the media presentation\ndescription, in which the video content is selected based only on the network\nbandwidth, to allow for additional consideration of the user's battery status.\nEVSO's streaming server preprocesses the video into several processed videos\naccording to the similarity intensity of each part of the video and then\nprovides the client with the processed video suitable for the network bandwidth\nand the battery status of the client's mobile device. The EVSO system was\nimplemented on the commonly used H.264/AVC encoder. We conduct various\nexperiments and a user study with nine videos. Our experimental results show\nthat EVSO effectively reduces energy consumption when mobile devices use\nstreaming services by 22% on average and up to 27% while maintaining the\nquality of the user experience.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:16:19 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Park", "Kyoungjun", ""], ["Kim", "Myungchul", ""]]}, {"id": "1905.06650", "submitter": "Rui-Xiao Zhang", "authors": "Rui-Xiao Zhang, Tianchi Huang, Chenglei Wu, Lifeng Sun", "title": "Reactive Video Caching via long-short-term fusion approach", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video caching has been a basic network functionality in today's network\narchitectures. Although the abundance of caching replacement algorithms has\nbeen proposed recently, these methods all suffer from a key limitation: due to\ntheir immature rules, inaccurate feature engineering or unresponsive model\nupdate, they cannot strike a balance between the long-term history and\nshort-term sudden events. To address this concern, we propose LA-E2, a\nlong-short-term fusion caching replacement approach, which is based on a\nlearning-aided exploration-exploitation process. Specifically, by effectively\ncombining the deep neural network (DNN) based prediction with the online\nexploitation-exploration process through a \\emph{top-k} method, LA-E2 can both\nmake use of the historical information and adapt to the constantly changing\npopularity responsively. Through the extensive experiments in two real-world\ndatasets, we show that LA-E2 can achieve state-of-the-art performance and\ngeneralize well. Especially when the cache size is small, our approach can\noutperform the baselines by 17.5\\%-68.7\\% higher in total hit rate.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 10:45:06 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zhang", "Rui-Xiao", ""], ["Huang", "Tianchi", ""], ["Wu", "Chenglei", ""], ["Sun", "Lifeng", ""]]}, {"id": "1905.07432", "submitter": "David Barina", "authors": "David Barina and Tomas Chlubna and Marek Solony and Drahomir Dlabaja\n  and Pavel Zemcik", "title": "Evaluation of 4D Light Field Compression Methods", "comments": "accepted for publication and presentation at the WSCG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field data records the amount of light at multiple points in space,\ncaptured e.g. by an array of cameras or by a light-field camera that uses\nmicrolenses. Since the storage and transmission requirements for such data are\ntremendous, compression techniques for light fields are gaining momentum in\nrecent years. Although plenty of efficient compression formats do exist for\nstill and moving images, only a little research on the impact of these methods\non light field imagery is performed. In this paper, we evaluate the impact of\nstate-of-the-art image and video compression methods on quality of images\nrendered from light field data. The methods include recent video compression\nstandards, especially AV1 and XVC finalised in 2018. To fully exploit the\npotential of common image compression methods on four-dimensional light field\nimagery, we have extended these methods into three and four dimensions. In this\npaper, we show that the four-dimensional light field data can be compressed\nmuch more than independent still images while maintaining the same visual\nquality of a perceived picture. We gradually compare the compression\nperformance of all image and video compression methods, and eventually answer\nthe question, \"What is the best compression method for light field data?\".\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 18:37:04 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Barina", "David", ""], ["Chlubna", "Tomas", ""], ["Solony", "Marek", ""], ["Dlabaja", "Drahomir", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1905.08765", "submitter": "Tiejun Lv", "authors": "Xuewei Zhang, Tiejun Lv, Yuan Ren, Wei Ni, Norman C. Beaulieu, and Y.\n  Jay Guo", "title": "Economical Caching for Scalable Videos in Cache-enabled Heterogeneous\n  Networks", "comments": "IEEE Journal on Selected Areas in Communications, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the optimal economical caching schemes in cache-enabled\nheterogeneous networks, while delivering multimedia video services with\npersonalized viewing qualities to mobile users. By applying scalable video\ncoding (SVC), each video file to be requested is divided into one base layer\n(BL) and several enhancement layers (ELs). In order to assign different\ntransmission tasks, the serving small-cell base stations (SBSs) are grouped\ninto K clusters. The SBSs are able to cache and cooperatively transmit BL and\nEL contents to the user. We analytically derive the expressions for successful\ntransmission probability and ergodic service rate, and then the closed form of\nEConomical Efficiency (ECE) is obtained. In order to enhance the ECE\nperformance, we formulate the ECE optimization problems for two cases. In the\nfirst case, with equal cache size equipped at each SBS, the layer caching\nindicator is determined. Since this problem is NP-hard, after the l0-norm\napproximation, the discrete optimization variables are relaxed to be\ncontinuous, and this relaxed problem is convex. Next, based on the optimal\nsolution derived from the relaxed problem, we devise a greedystrategy based\nheuristic algorithm to achieve the near-optimal layer caching indicators. In\nthe second case, the cache size for each SBS, the layer size and the layer\ncaching indicator are jointly optimized. This problem is a mixed integer\nprogramming problem, which is more challenging. To effectively solve this\nproblem, the original ECE maximization problem is divided into two subproblems.\nThese two subproblems are iteratively solved until the original optimization\nproblem is convergent. Numerical results verify the correctness of theoretical\nderivations. Additionally, compared to the most popular layer placement\nstrategy, the performance superiority of the proposed SVC-based caching schemes\nis testified.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 10:32:19 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Zhang", "Xuewei", ""], ["Lv", "Tiejun", ""], ["Ren", "Yuan", ""], ["Ni", "Wei", ""], ["Beaulieu", "Norman C.", ""], ["Guo", "Y. Jay", ""]]}, {"id": "1905.08967", "submitter": "Zhaoxia Yin", "authors": "Zhiqing Lu, Zhaoxia Yin and Bin Luo", "title": "Multiple reconstruction compression framework based on PNG image", "comments": "The experimental results cannot reproduced", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that neural networks (NNs) achieve excellent performances in\nimage compression and reconstruction. However, there are still many\nshortcomings in the practical application, which eventually lead to the loss of\nneural network image processing ability. Based on this, this paper proposes a\njoint framework based on neural network and zoom compression. The framework\nfirst encodes the incoming PNG or JPEG image information, and then the image is\nconverted into binary input decoder to reconstruct the intermediate state\nimage, next we import the intermediate state image into the zooming compressor\nand re-pressurize it, and reconstruct the final image. From the experimental\nresults, this method can better process the digital image and suppress the\nreverse expansion problem, and the compression effect can be improved by 4 to\n10 times as much as that of using RNN alone, showing better ability in\napplication. In this paper, the method is transmitted over a digital image, the\neffect is far better than the existing compression method alone, the Human\nvisual system can not feel the change of the effect.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 06:13:19 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 00:43:11 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 14:02:28 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Lu", "Zhiqing", ""], ["Yin", "Zhaoxia", ""], ["Luo", "Bin", ""]]}, {"id": "1905.09625", "submitter": "Zhaoxia Yin", "authors": "Youqing Wu, Youzhi Xiang, Yutang Guo, Jin Tang, and Zhaoxia Yin", "title": "An Improved Reversible Data Hiding in Encrypted Images using Parametric\n  Binary Tree Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes an improved reversible data hiding scheme in encrypted\nimages using parametric binary tree labeling(IPBTL-RDHEI), which takes\nadvantage of the spatial correlation in the entire original image but not in\nsmall image blocks to reserve room for hiding data. Then the original image is\nencrypted with an encryption key and the parametric binary tree is used to\nlabel encrypted pixels into two different categories. Finally, one of the two\ncategories of encrypted pixels can embed secret information by bit replacement.\nAccording to the experimental results, compared with several state-of-the-art\nmethods, the proposed IPBTL-RDHEI method achieves higher embedding rate and\noutperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the\noriginal plaintext image and the secret information can be restored and\nextracted losslessly and separately.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 12:53:07 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 08:37:52 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wu", "Youqing", ""], ["Xiang", "Youzhi", ""], ["Guo", "Yutang", ""], ["Tang", "Jin", ""], ["Yin", "Zhaoxia", ""]]}, {"id": "1905.09773", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T.\n  Freeman, Michael Rubinstein, Wojciech Matusik", "title": "Speech2Face: Learning the Face Behind a Voice", "comments": "To appear in CVPR2019. Project page: http://speech2face.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much can we infer about a person's looks from the way they speak? In this\npaper, we study the task of reconstructing a facial image of a person from a\nshort audio recording of that person speaking. We design and train a deep\nneural network to perform this task using millions of natural Internet/YouTube\nvideos of people speaking. During training, our model learns voice-face\ncorrelations that allow it to produce images that capture various physical\nattributes of the speakers such as age, gender and ethnicity. This is done in a\nself-supervised manner, by utilizing the natural co-occurrence of faces and\nspeech in Internet videos, without the need to model attributes explicitly. We\nevaluate and numerically quantify how--and in what manner--our Speech2Face\nreconstructions, obtained directly from audio, resemble the true face images of\nthe speakers.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:54:17 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Dekel", "Tali", ""], ["Kim", "Changil", ""], ["Mosseri", "Inbar", ""], ["Freeman", "William T.", ""], ["Rubinstein", "Michael", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1905.10150", "submitter": "Yang Li", "authors": "Yang Li and Xuanqin Mou", "title": "Saliency detection based on structural dissimilarity induced by image\n  quality assessment model", "comments": "For associated source code, see https://github.com/yangli-xjtu/SDS", "journal-ref": "J. Electron. Imag. 28(2) 023025 (3 April 2019)", "doi": "10.1117/1.JEI.28.2.023025", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distinctiveness of image regions is widely used as the cue of saliency.\nGenerally, the distinctiveness is computed according to the absolute difference\nof features. However, according to the image quality assessment (IQA) studies,\nthe human visual system is highly sensitive to structural changes rather than\nabsolute difference. Accordingly, we propose the computation of the structural\ndissimilarity between image patches as the distinctiveness measure for saliency\ndetection. Similar to IQA models, the structural dissimilarity is computed\nbased on the correlation of the structural features. The global structural\ndissimilarity of a patch to all the other patches represents saliency of the\npatch. We adopt two widely used structural features, namely the local contrast\nand gradient magnitude, into the structural dissimilarity computation in the\nproposed model. Without any postprocessing, the proposed model based on the\ncorrelation of either of the two structural features outperforms 11\nstate-of-the-art saliency models on three saliency databases.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 11:13:14 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Li", "Yang", ""], ["Mou", "Xuanqin", ""]]}, {"id": "1905.10861", "submitter": "Min-Hung Chen", "authors": "Min-Hung Chen, Zsolt Kira, Ghassan AlRegib", "title": "Temporal Attentive Alignment for Video Domain Adaptation", "comments": "CVPR2019 Workshop (Learning from Unlabeled Videos). Source code:\n  http://github.com/cmhungsteve/TA3N", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various image-based domain adaptation (DA) techniques have been\nproposed in recent years, domain shift in videos is still not well-explored.\nMost previous works only evaluate performance on small-scale datasets which are\nsaturated. Therefore, we first propose a larger-scale dataset with larger\ndomain discrepancy: UCF-HMDB_full. Second, we investigate different DA\nintegration methods for videos, and show that simultaneously aligning and\nlearning temporal dynamics achieves effective alignment even without\nsophisticated DA methods. Finally, we propose Temporal Attentive Adversarial\nAdaptation Network (TA3N), which explicitly attends to the temporal dynamics\nusing domain discrepancy for more effective domain alignment, achieving\nstate-of-the-art performance on three video DA datasets. The code and data are\nreleased at http://github.com/cmhungsteve/TA3N.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 19:15:23 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 01:24:44 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 17:17:38 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 05:24:54 GMT"}, {"version": "v5", "created": "Fri, 7 Jun 2019 03:45:29 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Chen", "Min-Hung", ""], ["Kira", "Zsolt", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1905.11240", "submitter": "Shang-Yu Su", "authors": "Shang-Yu Su, Yun-Nung Chen", "title": "Bridging Dialogue Generation and Facial Expression Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken dialogue systems that assist users to solve complex tasks such as\nmovie ticket booking have become an emerging research topic in artificial\nintelligence and natural language processing areas. With a well-designed\ndialogue system as an intelligent personal assistant, people can accomplish\ncertain tasks more easily via natural language interactions. Today there are\nseveral virtual intelligent assistants in the market; however, most systems\nonly focus on single modality, such as textual or vocal interaction. A\nmultimodal interface has various advantages: (1) allowing human to communicate\nwith machines in a natural and concise form using the mixture of modalities\nthat most precisely convey the intention to satisfy communication needs, and\n(2) providing more engaging experience by natural and human-like feedback. This\npaper explores a brand new research direction, which aims at bridging dialogue\ngeneration and facial expression synthesis for better multimodal interaction.\nThe goal is to generate dialogue responses and simultaneously synthesize\ncorresponding visual expressions on faces, which is also an ultimate step\ntoward more human-like virtual assistants.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 10:22:16 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 05:32:21 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Su", "Shang-Yu", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1905.11582", "submitter": "Ziqiang Zheng", "authors": "Ziqiang Zheng, Hongzhi Liu, Zhibin Yu, Haiyong Zheng, Yang Wu, Yang\n  Yang, Jianbo Shi", "title": "EncryptGAN: Image Steganography with Domain Transform", "comments": "11pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image steganographic algorithm called EncryptGAN, which\ndisguises private image communication in an open communication channel. The\ninsight is that content transform between two very different domains (e.g.,\nface to flower) allows one to hide image messages in one domain (face) and\ncommunicate using its counterpart in another domain (flower). The key\ningredient in our method, unlike related approaches, is a specially trained\nnetwork to extract transformed images from both domains and use them as the\npublic and private keys. We ensure the image communication remain secret except\nfor the intended recipient even when the content transformation networks are\nexposed.\n  To communicate, one directly pastes the `message' image onto a larger public\nkey image (face). Depending on the location and content of the message image,\nthe `disguise' image (flower) alters its appearance and shape while maintaining\nits overall objectiveness (flower). The recipient decodes the alternated image\nto uncover the original image message using its message image key. We implement\nthe entire procedure as a constrained Cycle-GAN, where the public and the\nprivate key generating network is used as an additional constraint to the cycle\nconsistency. Comprehensive experimental results show our EncryptGAN outperforms\nthe state-of-arts in terms of both encryption and security measures.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 02:57:55 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 01:59:13 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zheng", "Ziqiang", ""], ["Liu", "Hongzhi", ""], ["Yu", "Zhibin", ""], ["Zheng", "Haiyong", ""], ["Wu", "Yang", ""], ["Yang", "Yang", ""], ["Shi", "Jianbo", ""]]}, {"id": "1905.11705", "submitter": "Theodoros Karagkioules Mr", "authors": "Theodoros Karagkioules, Georgios S. Paschos, Nikolaos Liakopoulos,\n  Attilio Fiandrotti, Dimitrios Tsilimantos and Marco Cagnazzo", "title": "Optimizing Adaptive Video Streaming in Mobile Networks via Online\n  Learning", "comments": "9 pages, 3 figures, submitted to IEEE Transactions on Multimedia\n  (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel algorithm for video rate adaptation in HTTP\nAdaptive Streaming (HAS), based on online learning. The proposed algorithm,\nnamed Learn2Adapt (L2A), is shown to provide a robust rate adaptation strategy\nwhich, unlike most of the state-of-the-art techniques, does not require\nparameter tuning, channel model assumptions or application-specific\nadjustments. These properties make it very suitable for mobile users, who\ntypically experience fast variations in channel characteristics. Simulations\nshow that L2A improves on the overall Quality of Experience (QoE) and in\nparticular the average streaming rate, a result obtained independently of the\nchannel and application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 09:37:19 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 14:36:21 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Karagkioules", "Theodoros", ""], ["Paschos", "Georgios S.", ""], ["Liakopoulos", "Nikolaos", ""], ["Fiandrotti", "Attilio", ""], ["Tsilimantos", "Dimitrios", ""], ["Cagnazzo", "Marco", ""]]}, {"id": "1905.12245", "submitter": "Xingxing Wei", "authors": "Sarah Gross, Xingxing Wei, Jun Zhu", "title": "Automatic Realistic Music Video Generation from Segments of Youtube\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Music Video (MV) is a video aiming at visually illustrating or extending\nthe meaning of its background music. This paper proposes a novel method to\nautomatically generate, from an input music track, a music video made of\nsegments of Youtube music videos which would fit this music. The system\nanalyzes the input music to find its genre (pop, rock, ...) and finds segmented\nMVs with the same genre in the database. Then, a K-Means clustering is done to\ngroup video segments by color histogram, meaning segments of MVs having the\nsame global distribution of colors. A few clusters are randomly selected, then\nare assembled around music boundaries, which are moments where a significant\nchange in the music occurs (for instance, transitioning from verse to chorus).\nThis way, when the music changes, the video color mood changes as well. This\nwork aims at generating high-quality realistic MVs, which could be mistaken for\nman-made MVs. By asking users to identify, in a batch of music videos\ncontaining professional MVs, amateur-made MVs and generated MVs by our\nalgorithm, we show that our algorithm gives satisfying results, as 45% of\ngenerated videos are mistaken for professional MVs and 21.6% are mistaken for\namateur-made MVs. More information can be found in the project website:\nhttp://ml.cs.tsinghua.edu.cn/~sarah/\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 06:57:09 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Gross", "Sarah", ""], ["Wei", "Xingxing", ""], ["Zhu", "Jun", ""]]}, {"id": "1905.12439", "submitter": "Dorien Herremans", "authors": "Balamurali BT, Kin Wah Edward Lin, Simon Lui, Jer-Ming Chen, Dorien\n  Herremans", "title": "Towards robust audio spoofing detection: a detailed comparison of\n  traditional and learned features", "comments": null, "journal-ref": "IEEE Access. 2019", "doi": null, "report-no": null, "categories": "cs.SD cs.CR cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speaker verification, like every other biometric system, is\nvulnerable to spoofing attacks. Using only a few minutes of recorded voice of a\ngenuine client of a speaker verification system, attackers can develop a\nvariety of spoofing attacks that might trick such systems. Detecting these\nattacks using the audio cues present in the recordings is an important\nchallenge. Most existing spoofing detection systems depend on knowing the used\nspoofing technique. With this research, we aim at overcoming this limitation,\nby examining robust audio features, both traditional and those learned through\nan autoencoder, that are generalizable over different types of replay spoofing.\nFurthermore, we provide a detailed account of all the steps necessary in\nsetting up state-of-the-art audio feature detection, pre-, and postprocessing,\nsuch that the (non-audio expert) machine learning researcher can implement such\nsystems. Finally, we evaluate the performance of our robust replay speaker\ndetection system with a wide variety and different combinations of both\nextracted and machine learned audio features on the `out in the wild' ASVspoof\n2017 dataset. This dataset contains a variety of new spoofing configurations.\nSince our focus is on examining which features will ensure robustness, we base\nour system on a traditional Gaussian Mixture Model-Universal Background Model.\nWe then systematically investigate the relative contribution of each feature\nset. The fused models, based on both the known audio features and the machine\nlearned features respectively, have a comparable performance with an Equal\nError Rate (EER) of 12. The final best performing model, which obtains an EER\nof 10.8, is a hybrid model that contains both known and machine learned\nfeatures, thus revealing the importance of incorporating both types of features\nwhen developing a robust spoofing prediction model.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 06:51:18 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 01:27:28 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["BT", "Balamurali", ""], ["Lin", "Kin Wah Edward", ""], ["Lui", "Simon", ""], ["Chen", "Jer-Ming", ""], ["Herremans", "Dorien", ""]]}, {"id": "1905.13087", "submitter": "Zhongliang Yang", "authors": "Zhongliang Yang, Ke Wang, Jian Li, Yongfeng Huang and Yu-Jin Zhang", "title": "TS-RNN: Text Steganalysis Based on Recurrent Neural Networks", "comments": "IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2019.2920452", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of natural language processing technologies, more\nand more text steganographic methods based on automatic text generation\ntechnology have appeared in recent years. These models use the powerful\nself-learning and feature extraction ability of the neural networks to learn\nthe feature expression of massive normal texts. Then they can automatically\ngenerate dense steganographic texts which conform to such statistical\ndistribution based on the learned statistical patterns. In this paper, we\nobserve that the conditional probability distribution of each word in the\nautomatically generated steganographic texts will be distorted after embedded\nwith hidden information. We use Recurrent Neural Networks (RNNs) to extract\nthese feature distribution differences and then classify those features into\ncover text and stego text categories. Experimental results show that the\nproposed model can achieve high detection accuracy. Besides, the proposed model\ncan even make use of the subtle differences of the feature distribution of\ntexts to estimate the amount of hidden information embedded in the generated\nsteganographic text.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 15:00:11 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Yang", "Zhongliang", ""], ["Wang", "Ke", ""], ["Li", "Jian", ""], ["Huang", "Yongfeng", ""], ["Zhang", "Yu-Jin", ""]]}, {"id": "1905.13313", "submitter": "Junwei Liang", "authors": "Junwei Liang and Jay D. Aronson and Alexander Hauptmann", "title": "Technical Report of the Video Event Reconstruction and Analysis (VERA)\n  System -- Shooter Localization, Models, Interface, and Beyond", "comments": "The code and models are available at\n  https://github.com/JunweiLiang/VERA_Shooter_Localization . Our system is live\n  at https://vera.cs.cmu.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every minute, hundreds of hours of video are uploaded to social media sites\nand the Internet from around the world. This material creates a visual record\nof the experiences of a significant percentage of humanity and can help\nilluminate how we live in the present moment. When properly analyzed, this\nvideo can also help analysts to reconstruct events of interest, including war\ncrimes, human rights violations, and terrorist acts. Machine learning and\ncomputer vision can play a crucial role in this process. In this technical\nreport, we describe the Video Event Reconstruction and Analysis (VERA) system.\nThis new tool brings together a variety of capabilities we have developed over\nthe past few years (including video synchronization and geolocation to order\nunstructured videos lacking metadata over time and space, and sound recognition\nalgorithms) to enable the reconstruction and analysis of events captured on\nvideo. Among other uses, VERA enables the localization of a shooter from just a\nfew videos that include the sound of gunshots. To demonstrate the efficacy of\nthis suite of tools, we present the results of estimating the shooter's\nlocation of the Las Vegas Shooting in 2017 and show that VERA accurately\npredicts the shooter's location using only the first few gunshots. We then\npoint out future directions that can help improve the system and further reduce\nunnecessary human labor in the process. All of the components of VERA run\nthrough a web interface that enables human-in-the-loop verification to ensure\naccurate estimations. All relevant source code, including the web interface and\nmachine learning models, is freely available on Github. We hope that\nresearchers and software developers will be inspired to improve and expand this\nsystem moving forward to better meet the needs of human rights and public\nsafety.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 17:55:50 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 21:12:12 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 16:04:11 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 06:15:49 GMT"}, {"version": "v5", "created": "Fri, 5 Jul 2019 05:23:13 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Liang", "Junwei", ""], ["Aronson", "Jay D.", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1905.13382", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Shen Chen, Feng Zheng, Xiaoshuai Sun,\n  Baochang Zhang, Liujuan Cao, Guodong Guo, Feiyue Huang", "title": "Supervised Online Hashing via Similarity Distribution Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online hashing has attracted extensive research attention when facing\nstreaming data. Most online hashing methods, learning binary codes based on\npairwise similarities of training instances, fail to capture the semantic\nrelationship, and suffer from a poor generalization in large-scale applications\ndue to large variations. In this paper, we propose to model the similarity\ndistributions between the input data and the hashing codes, upon which a novel\nsupervised online hashing method, dubbed as Similarity Distribution based\nOnline Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship\nin the produced Hamming space. Specifically, we first transform the discrete\nsimilarity matrix into a probability matrix via a Gaussian-based normalization\nto address the extremely imbalanced distribution issue. And then, we introduce\na scaling Student t-distribution to solve the challenging initialization\nproblem, and efficiently bridge the gap between the known and unknown\ndistributions. Lastly, we align the two distributions via minimizing the\nKullback-Leibler divergence (KL-diverence) with stochastic gradient descent\n(SGD), by which an intuitive similarity constraint is imposed to update hashing\nmodel on the new streaming data with a powerful generalizing ability to the\npast data. Extensive experiments on three widely-used benchmarks validate the\nsuperiority of the proposed SDOH over the state-of-the-art methods in the\nonline retrieval task.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 02:12:41 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Chen", "Shen", ""], ["Zheng", "Feng", ""], ["Sun", "Xiaoshuai", ""], ["Zhang", "Baochang", ""], ["Cao", "Liujuan", ""], ["Guo", "Guodong", ""], ["Huang", "Feiyue", ""]]}]