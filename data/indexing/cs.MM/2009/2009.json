[{"id": "2009.00100", "submitter": "Young-Min Song", "authors": "Young-min Song, Young-chul Yoon, Kwangjin Yoon, Moongu Jeon,\n  Seong-Whan Lee, Witold Pedrycz", "title": "Online Multi-Object Tracking and Segmentation with GMPHD Filter and\n  Mask-based Affinity Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a highly practical fully online multi-object\ntracking and segmentation (MOTS) method that uses instance segmentation results\nas an input. The proposed method is based on the Gaussian mixture probability\nhypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a\nmask-based affinity fusion (MAF) model to achieve high-performance online\ntracking. The HDA consists of two associations: segment-to-track and\ntrack-to-track associations. One affinity, for position and motion, is computed\nby using the GMPHD filter, and the other affinity, for appearance is computed\nby using the responses from a single object tracker such as a kernalized\ncorrelation filter. These two affinities are simply fused by using a\nscore-level fusion method such as min-max normalization referred to as MAF. In\naddition, to reduce the number of false positive segments, we adopt mask\nIoU-based merging (mask merging). The proposed MOTS framework with the key\nmodules: HDA, MAF, and mask merging, is easily extensible to simultaneously\ntrack multiple types of objects with CPU only execution in parallel processing.\nIn addition, the developed framework only requires simple parameter tuning\nunlike many existing MOTS methods that need intensive hyperparameter\noptimization. In the experiments on the two popular MOTS datasets, the key\nmodules show some improvements. For instance, ID-switch decreases by more than\nhalf compared to a baseline method in the training sets. In conclusion, our\ntracker achieves state-of-the-art MOTS performance in the test sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:06:22 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 10:55:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Song", "Young-min", ""], ["Yoon", "Young-chul", ""], ["Yoon", "Kwangjin", ""], ["Jeon", "Moongu", ""], ["Lee", "Seong-Whan", ""], ["Pedrycz", "Witold", ""]]}, {"id": "2009.00702", "submitter": "SeyedHamed RahmaniKhezri", "authors": "Suhong Kim, Hamed RahmaniKhezri, Seyed Mohammad Nourbakhsh and Mohamed\n  Hefeeda", "title": "Unsupervised Single-Image Reflection Separation Using Perceptual Deep\n  Image Priors", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflections often degrade the quality of the image by obstructing the\nbackground scene. This is not desirable for everyday users, and it negatively\nimpacts the performance of multimedia applications that process images with\nreflections. Most current methods for removing reflections utilize\nsupervised-learning models. However, these models require an extensive number\nof image pairs to perform well, especially on natural images with reflection,\nwhich is difficult to achieve in practice. In this paper, we propose a novel\nunsupervised framework for single-image reflection separation. Instead of\nlearning from a large dataset, we optimize the parameters of two cross-coupled\ndeep convolutional networks on a target image to generate two exclusive\nbackground and reflection layers. In particular, we design a new architecture\nof the network to embed semantic features extracted from a pre-trained deep\nclassification network, which gives more meaningful separation similar to human\nperception. Quantitative and qualitative results on commonly used datasets in\nthe literature show that our method's performance is at least on par with the\nstate-of-the-art supervised methods and, occasionally, better without requiring\nlarge training datasets. Our results also show that our method significantly\noutperforms the closest unsupervised method in the literature for removing\nreflections from single images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:08:30 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Kim", "Suhong", ""], ["RahmaniKhezri", "Hamed", ""], ["Nourbakhsh", "Seyed Mohammad", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "2009.00763", "submitter": "Tyler Bell", "authors": "Matthew G. Finley and Tyler Bell", "title": "Depth Range Reduction for 3D Range Geometry Compression", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.optlaseng.2020.106457", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) shape measurement devices and techniques are being\nrapidly adopted within a variety of industries and applications. As acquiring\n3D range data becomes faster and more accurate it becomes more challenging to\nefficiently store, transmit, or stream this data. One prevailing approach to\ncompressing 3D range data is to encode it within the color channels of regular\n2D images. This paper presents a novel method for reducing the depth range of a\n3D geometry such that it can be stored within a 2D image using lower encoding\nfrequencies (or a fewer number of encoding periods). This allows for smaller\ncompressed file sizes to be achieved without a proportional increase in\nreconstruction errors. Further, as the proposed method occurs prior to\nencoding, it is readily compatible with a variety of existing image-based 3D\nrange geometry compression methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 00:51:05 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Finley", "Matthew G.", ""], ["Bell", "Tyler", ""]]}, {"id": "2009.00951", "submitter": "Samuel Blake T", "authors": "Sam Blake", "title": "Embedded Blockchains: A Synthesis of Blockchains, Spread Spectrum\n  Watermarking, Perceptual Hashing & Digital Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a scheme for detecting manipulated audio and\nvideo. The scheme is a synthesis of blockchains, encrypted spread spectrum\nwatermarks, perceptual hashing and digital signatures, which we call an\nEmbedded Blockchain. Within this scheme, we use the blockchain for its data\nstructure of a cryptographically linked list, cryptographic hashing for\nabsolute comparisons, perceptual hashing for flexible comparisons, digital\nsignatures for proof of ownership, and encrypted spread spectrum watermarking\nto embed the blockchain into the background noise of the media. So each media\nrecording has its own unique blockchain, with each block holding information\ndescribing the media segment. The problem of verifying the integrity of the\nmedia is recast to traversing the blockchain, block-by-block, and\nsegment-by-segment of the media. If any chain is broken, the difference in the\ncomputed and extracted perceptual hash is used to estimate the level of\nmanipulation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:08:43 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 00:46:47 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 01:55:59 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Blake", "Sam", ""]]}, {"id": "2009.01449", "submitter": "Long Chen", "authors": "Long Chen, Wenbo Ma, Jun Xiao, Hanwang Zhang, Shih-Fu Chang", "title": "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression\n  Grounding", "comments": "Camera ready version at AAAI 2021, Codes are available at:\n  https://github.com/ChopinSharp/ref-nms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevailing framework for solving referring expression grounding is based\non a two-stage process: 1) detecting proposals with an object detector and 2)\ngrounding the referent to one of the proposals. Existing two-stage solutions\nmostly focus on the grounding step, which aims to align the expressions with\nthe proposals. In this paper, we argue that these methods overlook an obvious\nmismatch between the roles of proposals in the two stages: they generate\nproposals solely based on the detection confidence (i.e., expression-agnostic),\nhoping that the proposals contain all right instances in the expression (i.e.,\nexpression-aware). Due to this mismatch, current two-stage methods suffer from\na severe performance drop between detected and ground-truth proposals. To this\nend, we propose Ref-NMS, which is the first method to yield expression-aware\nproposals at the first stage. Ref-NMS regards all nouns in the expression as\ncritical objects, and introduces a lightweight module to predict a score for\naligning each box with a critical object. These scores can guide the NMS\noperation to filter out the boxes irrelevant to the expression, increasing the\nrecall of critical objects, resulting in a significantly improved grounding\nperformance. Since Ref- NMS is agnostic to the grounding step, it can be easily\nintegrated into any state-of-the-art two-stage method. Extensive ablation\nstudies on several backbones, benchmarks, and tasks consistently demonstrate\nthe superiority of Ref-NMS. Codes are available at:\nhttps://github.com/ChopinSharp/ref-nms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 05:04:12 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 08:19:22 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 01:25:59 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Chen", "Long", ""], ["Ma", "Wenbo", ""], ["Xiao", "Jun", ""], ["Zhang", "Hanwang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2009.01887", "submitter": "Priyanka Singh", "authors": "Priyanka Singh", "title": "Robust Homomorphic Video Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has been weaponized to carry out cybercriminal activities at an\nunprecedented pace. The rising concerns for preserving the privacy of personal\ndata while availing modern tools and technologies is alarming. End-to-end\nencrypted solutions are in demand for almost all commercial platforms. On one\nside, it seems imperative to provide such solutions and give people trust to\nreliably use these platforms. On the other side, this creates a huge\nopportunity to carry out unchecked cybercrimes. This paper proposes a robust\nvideo hashing technique, scalable and efficient in chalking out matches from an\nenormous bulk of videos floating on these commercial platforms. The video hash\nis validated to be robust to common manipulations like scaling, corruptions by\nnoise, compression, and contrast changes that are most probable to happen\nduring transmission. It can also be transformed into the encrypted domain and\nwork on top of encrypted videos without deciphering. Thus, it can serve as a\npotential forensic tool that can trace the illegal sharing of videos without\nknowing the underlying content. Hence, it can help preserve privacy and combat\ncybercrimes such as revenge porn, hateful content, child abuse, or illegal\nmaterial propagated in a video.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 19:09:44 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Singh", "Priyanka", ""]]}, {"id": "2009.01934", "submitter": "Arun Kumar Singh", "authors": "Arun Kumar Singh (1), Priyanka Singh (2) ((1) Indian Institute of\n  Technology Jammu, (2) Dhirubhai Ambani Institute of Information and\n  Communication Technology)", "title": "Detection of AI-Synthesized Speech Using Cepstral & Bispectral\n  Statistics", "comments": "6 Pages, 6 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital technology has made possible unimaginable applications come true. It\nseems exciting to have a handful of tools for easy editing and manipulation,\nbut it raises alarming concerns that can propagate as speech clones,\nduplicates, or maybe deep fakes. Validating the authenticity of a speech is one\nof the primary problems of digital audio forensics. We propose an approach to\ndistinguish human speech from AI synthesized speech exploiting the Bi-spectral\nand Cepstral analysis. Higher-order statistics have less correlation for human\nspeech in comparison to a synthesized speech. Also, Cepstral analysis revealed\na durable power component in human speech that is missing for a synthesized\nspeech. We integrate both these analyses and propose a machine learning model\nto detect AI synthesized speech.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 21:29:41 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 11:41:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Singh", "Arun Kumar", ""], ["Singh", "Priyanka", ""]]}, {"id": "2009.02016", "submitter": "Huan Lin", "authors": "Huan Lin and Fandong Meng and Jinsong Su and Yongjing Yin and\n  Zhengyuan Yang and Yubin Ge and Jie Zhou and Jiebo Luo", "title": "Dynamic Context-guided Capsule Network for Multimodal Machine\n  Translation", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413715", "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal machine translation (MMT), which mainly focuses on enhancing\ntext-only translation with visual features, has attracted considerable\nattention from both computer vision and natural language processing\ncommunities. Most current MMT models resort to attention mechanism, global\ncontext modeling or multimodal joint representation learning to utilize visual\nfeatures. However, the attention mechanism lacks sufficient semantic\ninteractions between modalities while the other two provide fixed visual\ncontext, which is unsuitable for modeling the observed variability when\ngenerating translation. To address the above issues, in this paper, we propose\na novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at\neach timestep of decoding, we first employ the conventional source-target\nattention to produce a timestep-specific source-side context vector. Next, DCCN\ntakes this vector as input and uses it to guide the iterative extraction of\nrelated visual features via a context-guided dynamic routing mechanism.\nParticularly, we represent the input image with global and regional visual\nfeatures, we introduce two parallel DCCNs to model multimodal context vectors\nwith visual features at different granularities. Finally, we obtain two\nmultimodal context vectors, which are fused and incorporated into the decoder\nfor the prediction of the target word. Experimental results on the Multi30K\ndataset of English-to-German and English-to-French translation demonstrate the\nsuperiority of DCCN. Our code is available on\nhttps://github.com/DeepLearnXMU/MM-DCCN.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 06:18:24 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Lin", "Huan", ""], ["Meng", "Fandong", ""], ["Su", "Jinsong", ""], ["Yin", "Yongjing", ""], ["Yang", "Zhengyuan", ""], ["Ge", "Yubin", ""], ["Zhou", "Jie", ""], ["Luo", "Jiebo", ""]]}, {"id": "2009.02598", "submitter": "Ruichen Li", "authors": "Jingjun Liang, Ruichen Li and Qin Jin", "title": "Semi-supervised Multi-modal Emotion Recognition with Cross-Modal\n  Distribution Matching", "comments": "10 pages, 5 figures, to be published on ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413579", "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic emotion recognition is an active research topic with wide range of\napplications. Due to the high manual annotation cost and inevitable label\nambiguity, the development of emotion recognition dataset is limited in both\nscale and quality. Therefore, one of the key challenges is how to build\neffective models with limited data resource. Previous works have explored\ndifferent approaches to tackle this challenge including data enhancement,\ntransfer learning, and semi-supervised learning etc. However, the weakness of\nthese existing approaches includes such as training instability, large\nperformance loss during transfer, or marginal improvement.\n  In this work, we propose a novel semi-supervised multi-modal emotion\nrecognition model based on cross-modality distribution matching, which\nleverages abundant unlabeled data to enhance the model training under the\nassumption that the inner emotional status is consistent at the utterance level\nacross modalities.\n  We conduct extensive experiments to evaluate the proposed model on two\nbenchmark datasets, IEMOCAP and MELD. The experiment results prove that the\nproposed semi-supervised learning model can effectively utilize unlabeled data\nand combine multi-modalities to boost the emotion recognition performance,\nwhich outperforms other state-of-the-art approaches under the same condition.\nThe proposed model also achieves competitive capacity compared with existing\napproaches which take advantage of additional auxiliary information such as\nspeaker and interaction context.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 20:51:01 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liang", "Jingjun", ""], ["Li", "Ruichen", ""], ["Jin", "Qin", ""]]}, {"id": "2009.02733", "submitter": "Chao Liu", "authors": "Chao Liu, Heming Sun, Jiro Katto, Xiaoyang Zeng, and Yibo Fan", "title": "A Convolutional Neural Network-Based Low Complexity Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN)-based filters have achieved significant\nperformance in video artifacts reduction. However, the high complexity of\nexisting methods makes it difficult to be applied in real usage. In this paper,\na CNN-based low complexity filter is proposed. We utilize depth separable\nconvolution (DSC) merged with the batch normalization (BN) as the backbone of\nour proposed CNN-based network. Besides, a weight initialization method is\nproposed to enhance the training performance. To solve the well known over\nsmoothing problem for the inter frames, a frame-level residual mapping (RM) is\npresented. We analyze some of the mainstream methods like frame-level and\nblock-level based filters quantitatively and build our CNN-based filter with\nframe-level control to avoid the extra complexity and artificial boundaries\ncaused by block-level control. In addition, a novel module called RM is\ndesigned to restore the distortion from the learned residuals. As a result, we\ncan effectively improve the generalization ability of the learning-based filter\nand reach an adaptive filtering effect. Moreover, this module is flexible and\ncan be combined with other learning-based filters. The experimental results\nshow that our proposed method achieves significant BD-rate reduction than\nH.265/HEVC. It achieves about 1.2% BD-rate reduction and 79.1% decrease in\nFLOPs than VR-CNN. Finally, the measurement on H.266/VVC and ablation studies\nare also conducted to ensure the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 13:42:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liu", "Chao", ""], ["Sun", "Heming", ""], ["Katto", "Jiro", ""], ["Zeng", "Xiaoyang", ""], ["Fan", "Yibo", ""]]}, {"id": "2009.03051", "submitter": "Kashif Ahmad Dr", "authors": "Syed Zohaib Hassan, Kashif Ahmad, Steven Hicks, Paal Halvorsen, Ala\n  Al-Fuqaha, Nicola Conci, Michael Riegler", "title": "Visual Sentiment Analysis from Disaster Images in Social Media", "comments": "10 pages, 6 figures, 6 tables. arXiv admin note: substantial text\n  overlap with arXiv:2002.03773", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of social networks and users' tendency towards\nsharing their feelings, expressions, and opinions in text, visual, and audio\ncontent, have opened new opportunities and challenges in sentiment analysis.\nWhile sentiment analysis of text streams has been widely explored in\nliterature, sentiment analysis from images and videos is relatively new. This\narticle focuses on visual sentiment analysis in a societal important domain,\nnamely disaster analysis in social media. To this aim, we propose a deep visual\nsentiment analyzer for disaster related images, covering different aspects of\nvisual sentiment analysis starting from data collection, annotation, model\nselection, implementation, and evaluations. For data annotation, and analyzing\npeoples' sentiments towards natural disasters and associated images in social\nmedia, a crowd-sourcing study has been conducted with a large number of\nparticipants worldwide. The crowd-sourcing study resulted in a large-scale\nbenchmark dataset with four different sets of annotations, each aiming a\nseparate task. The presented analysis and the associated dataset will provide a\nbaseline/benchmark for future research in the domain. We believe the proposed\nsystem can contribute toward more livable communities by helping different\nstakeholders, such as news broadcasters, humanitarian organizations, as well as\nthe general public.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:29:52 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Hassan", "Syed Zohaib", ""], ["Ahmad", "Kashif", ""], ["Hicks", "Steven", ""], ["Halvorsen", "Paal", ""], ["Al-Fuqaha", "Ala", ""], ["Conci", "Nicola", ""], ["Riegler", "Michael", ""]]}, {"id": "2009.03155", "submitter": "Pavel Korshunov", "authors": "Pavel Korshunov and S\\'ebastien Marcel", "title": "Deepfake detection: humans vs. machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake videos, where a person's face is automatically swapped with a face\nof someone else, are becoming easier to generate with more realistic results.\nIn response to the threat such manipulations can pose to our trust in video\nevidence, several large datasets of deepfake videos and many methods to detect\nthem were proposed recently. However, it is still unclear how realistic\ndeepfake videos are for an average person and whether the algorithms are\nsignificantly better than humans at detecting them. In this paper, we present a\nsubjective study conducted in a crowdsourcing-like scenario, which\nsystematically evaluates how hard it is for humans to see if the video is\ndeepfake or not. For the evaluation, we used 120 different videos (60 deepfakes\nand 60 originals) manually pre-selected from the Facebook deepfake database,\nwhich was provided in the Kaggle's Deepfake Detection Challenge 2020. For each\nvideo, a simple question: \"Is face of the person in the video real of fake?\"\nwas answered on average by 19 na\\\"ive subjects. The results of the subjective\nevaluation were compared with the performance of two different state of the art\ndeepfake detection methods, based on Xception and EfficientNets (B4 variant)\nneural networks, which were pre-trained on two other large public databases:\nthe Google's subset from FaceForensics++ and the recent Celeb-DF dataset. The\nevaluation demonstrates that while the human perception is very different from\nthe perception of a machine, both successfully but in different ways are fooled\nby deepfakes. Specifically, algorithms struggle to detect those deepfake\nvideos, which human subjects found to be very easy to spot.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:20:37 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Korshunov", "Pavel", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "2009.03281", "submitter": "Mohamed Hefeeda", "authors": "Amgad Ahmed, Suhong Kim, Mohamed Elgharib, Mohamed Hefeeda", "title": "User-assisted Video Reflection Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflections in videos are obstructions that often occur when videos are taken\nbehind reflective surfaces like glass. These reflections reduce the quality of\nsuch videos, lead to information loss and degrade the accuracy of many computer\nvision algorithms. A video containing reflections is a combination of\nbackground and reflection layers. Thus, reflection removal is equivalent to\ndecomposing the video into two layers. This, however, is a challenging and\nill-posed problem as there is an infinite number of valid decompositions. To\naddress this problem, we propose a user-assisted method for video reflection\nremoval. We rely on both spatial and temporal information and utilize sparse\nuser hints to help improve separation. The key idea of the proposed method is\nto use motion cues to separate the background layer from the reflection layer\nwith minimal user assistance. We show that user-assistance significantly\nimproves the layer separation results. We implement and evaluate the proposed\nmethod through quantitative and qualitative results on real and synthetic\nvideos. Our experiments show that the proposed method successfully removes\nreflection from video sequences, does not introduce visual distortions, and\nsignificantly outperforms the state-of-the-art reflection removal methods in\nthe literature.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:42:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ahmed", "Amgad", ""], ["Kim", "Suhong", ""], ["Elgharib", "Mohamed", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "2009.03411", "submitter": "Wei Zhou", "authors": "Wei Zhou and Zhibo Chen", "title": "Deep Local and Global Spatiotemporal Feature Aggregation for Blind Video\n  Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has achieved promising success for multimedia\nquality assessment, especially for image quality assessment (IQA). However,\nsince there exist more complex temporal characteristics in videos, very little\nwork has been done on video quality assessment (VQA) by exploiting powerful\ndeep convolutional neural networks (DCNNs). In this paper, we propose an\nefficient VQA method named Deep SpatioTemporal video Quality assessor (DeepSTQ)\nto predict the perceptual quality of various distorted videos in a no-reference\nmanner. In the proposed DeepSTQ, we first extract local and global\nspatiotemporal features by pre-trained deep learning models without fine-tuning\nor training from scratch. The composited features consider distorted video\nframes as well as frame difference maps from both global and local views. Then,\nthe feature aggregation is conducted by the regression model to predict the\nperceptual video quality. Finally, experimental results demonstrate that our\nproposed DeepSTQ outperforms state-of-the-art quality assessment algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 20:39:48 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2009.03523", "submitter": "Thyagharajan K K", "authors": "L. Balaji, K.K. Thyagharajan, C. Raja, A. Dhanalakshmi", "title": "An optimal mode selection algorithm for scalable video coding", "comments": "14 pages, 1 figure", "journal-ref": "International Journal of Computational Vision and Robotics\n  (IJCVR), Vol. 10, No. 2, 2020", "doi": "10.1504/IJCVR.2020.105685", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable video coding (SVC) is extended from its predecessor advanced video\ncoding (AVC) because of its flexible transmission to all type of gadgets.\nHowever, SVC is more flexible and scalable than AVC, but it is more complex in\ndetermining the computations than AVC. The traditional full search method in\nthe standard H.264 SVC consumes more encoding time for computation. This\ncomplexity in computation need to be reduced and many fast mode decision (FMD)\nalgorithms were developed, but many fail to balance in all the three measures\nsuch as peak signal to noise ratio (PSNR), encoding time and bit rate. In this\npaper, the proposed optimal mode selection algorithm based on the orientation\nof pixels achieves better time saving, good PSNR and coding efficiency. The\nproposed algorithm is compared with the standard H.264 JSVM reference software\nand found to be 57.44% time saving, 0.43 dB increments in PSNR and 0.23%\ncompression in bit rate.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 05:26:16 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Balaji", "L.", ""], ["Thyagharajan", "K. K.", ""], ["Raja", "C.", ""], ["Dhanalakshmi", "A.", ""]]}, {"id": "2009.04107", "submitter": "Zexu Pan", "authors": "Zexu Pan, Zhaojie Luo, Jichen Yang, Haizhou Li", "title": "Multi-modal Attention for Speech Emotion Recognition", "comments": "Accepted by Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion represents an essential aspect of human speech that is manifested in\nspeech prosody. Speech, visual, and textual cues are complementary in human\ncommunication. In this paper, we study a hybrid fusion method, referred to as\nmulti-modal attention network (MMAN) to make use of visual and textual cues in\nspeech emotion recognition. We propose a novel multi-modal attention mechanism,\ncLSTM-MMA, which facilitates the attention across three modalities and\nselectively fuse the information. cLSTM-MMA is fused with other uni-modal\nsub-networks in the late fusion. The experiments show that speech emotion\nrecognition benefits significantly from visual and textual cues, and the\nproposed cLSTM-MMA alone is as competitive as other fusion methods in terms of\naccuracy, but with a much more compact network structure. The proposed hybrid\nnetwork MMAN achieves state-of-the-art performance on IEMOCAP database for\nemotion recognition.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 05:06:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Pan", "Zexu", ""], ["Luo", "Zhaojie", ""], ["Yang", "Jichen", ""], ["Li", "Haizhou", ""]]}, {"id": "2009.04646", "submitter": "Xiaoyi He", "authors": "Weiyao Lin, Xiaoyi He, Wenrui Dai, John See, Tushar Shinde, Hongkai\n  Xiong, Lingyu Duan", "title": "Key-Point Sequence Lossless Compression for Intelligent Video Analysis", "comments": "Accepted version for IEEE MultiMedia Journal", "journal-ref": "IEEE MultiMedia, vol. 27, no. 3, pp. 12-22, 2020", "doi": "10.1109/MMUL.2020.2990863", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature coding has been recently considered to facilitate intelligent video\nanalysis for urban computing. Instead of raw videos, extracted features in the\nfront-end are encoded and transmitted to the back-end for further processing.\nIn this article, we present a lossless key-point sequence compression approach\nfor efficient feature coding. The essence of this predict-and-encode strategy\nis to eliminate the spatial and temporal redundancies of key points in videos.\nMultiple prediction modes with an adaptive mode selection method are proposed\nto handle key-point sequences with various structures and motion. Experimental\nresults validate the effectiveness of the proposed scheme on four types of\nwidely used key-point sequences in video analysis.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 02:41:57 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Lin", "Weiyao", ""], ["He", "Xiaoyi", ""], ["Dai", "Wenrui", ""], ["See", "John", ""], ["Shinde", "Tushar", ""], ["Xiong", "Hongkai", ""], ["Duan", "Lingyu", ""]]}, {"id": "2009.05103", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Yaxian Li, Xingxu Yao, Weizhi Nie, Pengfei Xu, Jufeng\n  Yang, Kurt Keutzer", "title": "Emotion-Based End-to-End Matching Between Image and Music in\n  Valence-Arousal Space", "comments": "Accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both images and music can convey rich semantics and are widely used to induce\nspecific emotions. Matching images and music with similar emotions might help\nto make emotion perceptions more vivid and stronger. Existing emotion-based\nimage and music matching methods either employ limited categorical emotion\nstates which cannot well reflect the complexity and subtlety of emotions, or\ntrain the matching model using an impractical multi-stage pipeline. In this\npaper, we study end-to-end matching between image and music based on emotions\nin the continuous valence-arousal (VA) space. First, we construct a large-scale\ndataset, termed Image-Music-Emotion-Matching-Net (IMEMNet), with over 140K\nimage-music pairs. Second, we propose cross-modal deep continuous metric\nlearning (CDCML) to learn a shared latent embedding space which preserves the\ncross-modal similarity relationship in the continuous matching space. Finally,\nwe refine the embedding space by further preserving the single-modal emotion\nrelationship in the VA spaces of both images and music. The metric learning in\nthe embedding space and task regression in the label space are jointly\noptimized for both cross-modal matching and single-modal VA prediction. The\nextensive experiments conducted on IMEMNet demonstrate the superiority of CDCML\nfor emotion-based image and music matching as compared to the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 20:12:23 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Zhao", "Sicheng", ""], ["Li", "Yaxian", ""], ["Yao", "Xingxu", ""], ["Nie", "Weizhi", ""], ["Xu", "Pengfei", ""], ["Yang", "Jufeng", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2009.05158", "submitter": "Hailey James", "authors": "Hailey James, Otkrist Gupta, Dan Raviv", "title": "OCR Graph Features for Manipulation Detection in Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting manipulations in digital documents is becoming increasingly\nimportant for information verification purposes. Due to the proliferation of\nimage editing software, altering key information in documents has become widely\naccessible. Nearly all approaches in this domain rely on a procedural approach,\nusing carefully generated features and a hand-tuned scoring system, rather than\na data-driven and generalizable approach. We frame this issue as a graph\ncomparison problem using the character bounding boxes, and propose a model that\nleverages graph features using OCR (Optical Character Recognition). Our model\nrelies on a data-driven approach to detect alterations by training a random\nforest classifier on the graph-based OCR features. We evaluate our algorithm's\nforgery detection performance on dataset constructed from real business\ndocuments with slight forgery imperfections. Our proposed model dramatically\noutperforms the most closely-related document manipulation detection model on\nthis task.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 21:50:45 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 15:52:09 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["James", "Hailey", ""], ["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""]]}, {"id": "2009.05381", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang,\n  Meng Wang", "title": "Dual Encoding for Video Retrieval by Text", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. Code and data will be available at\n  https://github.com/danieljf24/hybrid_space. Conference version:\n  arXiv:1809.06181", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3059295", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attacks the challenging problem of video retrieval by text. In\nsuch a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc\nqueries described exclusively in the form of a natural-language sentence, with\nno visual example provided. Given videos as sequences of frames and queries as\nsequences of words, an effective sequence-to-sequence cross-modal matching is\ncrucial. To that end, the two modalities need to be first encoded into\nreal-valued vectors and then projected into a common space. In this paper we\nachieve this by proposing a dual deep encoding network that encodes videos and\nqueries into powerful dense representations of their own. Our novelty is\ntwo-fold. First, different from prior art that resorts to a specific\nsingle-level encoder, the proposed network performs multi-level encoding that\nrepresents the rich content of both modalities in a coarse-to-fine fashion.\nSecond, different from a conventional common space learning algorithm which is\neither concept based or latent space based, we introduce hybrid space learning\nwhich combines the high performance of the latent space and the good\ninterpretability of the concept space. Dual encoding is conceptually simple,\npractically effective and end-to-end trained with hybrid space learning.\nExtensive experiments on four challenging video datasets show the viability of\nthe new method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:49:39 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 09:26:20 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Dong", "Jianfeng", ""], ["Li", "Xirong", ""], ["Xu", "Chaoxi", ""], ["Yang", "Xun", ""], ["Yang", "Gang", ""], ["Wang", "Xun", ""], ["Wang", "Meng", ""]]}, {"id": "2009.05695", "submitter": "Niluthpol Chowdhury Mithun", "authors": "Niluthpol Chowdhury Mithun, Karan Sikka, Han-Pang Chiu, Supun\n  Samarasekera, Rakesh Kumar", "title": "RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization", "comments": "ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413647", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an important, yet largely unexplored problem of large-scale\ncross-modal visual localization by matching ground RGB images to a\ngeo-referenced aerial LIDAR 3D point cloud (rendered as depth images). Prior\nworks were demonstrated on small datasets and did not lend themselves to\nscaling up for large-scale applications. To enable large-scale evaluation, we\nintroduce a new dataset containing over 550K pairs (covering 143 km^2 area) of\nRGB and aerial LIDAR depth images. We propose a novel joint embedding based\nmethod that effectively combines the appearance and semantic cues from both\nmodalities to handle drastic cross-modal variations. Experiments on the\nproposed dataset show that our model achieves a strong result of a median rank\nof 5 in matching across a large test set of 50K location pairs collected from a\n14km^2 area. This represents a significant advancement over prior works in\nperformance and scale. We conclude with qualitative results to highlight the\nchallenging nature of this task and the benefits of the proposed model. Our\nwork provides a foundation for further research in cross-modal visual\nlocalization.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 01:18:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Mithun", "Niluthpol Chowdhury", ""], ["Sikka", "Karan", ""], ["Chiu", "Han-Pang", ""], ["Samarasekera", "Supun", ""], ["Kumar", "Rakesh", ""]]}, {"id": "2009.05778", "submitter": "Thyagharajan K K", "authors": "S. D. Lalitha, K. K. Thyagharajan", "title": "Micro-Facial Expression Recognition Based on Deep-Rooted Learning\n  Algorithm", "comments": "20 pages, 7 figures, \"for the published version of the article, see\n  https://www.atlantis-press.com/journals/ijcis/125915627\"", "journal-ref": "12 (2) 903 - 913 2019/08 International Journal of Computational\n  Intelligence Systems", "doi": "10.2991/IJCIS.D.190801.001", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are important cues to observe human emotions. Facial\nexpression recognition has attracted many researchers for years, but it is\nstill a challenging topic since expression features vary greatly with the head\nposes, environments, and variations in the different persons involved. In this\nwork, three major steps are involved to improve the performance of micro-facial\nexpression recognition. First, an Adaptive Homomorphic Filtering is used for\nface detection and rotation rectification processes. Secondly, Micro-facial\nfeatures were used to extract the appearance variations of a testing\nimage-spatial analysis. The features of motion information are used for\nexpression recognition in a sequence of facial images. An effective\nMicro-Facial Expression Based Deep-Rooted Learning (MFEDRL) classifier is\nproposed in this paper to better recognize spontaneous micro-expressions by\nlearning parameters on the optimal features. This proposed method includes two\nloss functions such as cross entropy loss function and centre loss function.\nThen the performance of the algorithm will be evaluated using recognition rate\nand false measures. Simulation results show that the predictive performance of\nthe proposed method outperforms that of the existing classifiers such as\nConvolutional Neural Network (CNN), Deep Neural Network (DNN), Artificial\nNeural Network (ANN), Support Vector Machine (SVM), and k-Nearest Neighbours\n(KNN) in terms of accuracy and Mean Absolute Error (MAE).\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 12:23:27 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lalitha", "S. D.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.05784", "submitter": "Weicong Chen", "authors": "Weicong Chen, Xu Tan, Yingce Xia, Tao Qin, Yu Wang, Tie-Yan Liu", "title": "DualLip: A System for Joint Lip Reading and Generation", "comments": "Accepted by ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413623", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading aims to recognize text from talking lip, while lip generation\naims to synthesize talking lip according to text, which is a key component in\ntalking face generation and is a dual task of lip reading. In this paper, we\ndevelop DualLip, a system that jointly improves lip reading and generation by\nleveraging the task duality and using unlabeled text and lip video data. The\nkey ideas of the DualLip include: 1) Generate lip video from unlabeled text\nwith a lip generation model, and use the pseudo pairs to improve lip reading;\n2) Generate text from unlabeled lip video with a lip reading model, and use the\npseudo pairs to improve lip generation. We further extend DualLip to talking\nface generation with two additionally introduced components: lip to face\ngeneration and text to speech generation. Experiments on GRID and TCD-TIMIT\ndemonstrate the effectiveness of DualLip on improving lip reading, lip\ngeneration, and talking face generation by utilizing unlabeled data.\nSpecifically, the lip generation model in our DualLip system trained with\nonly10% paired data surpasses the performance of that trained with the whole\npaired data. And on the GRID benchmark of lip reading, we achieve 1.16%\ncharacter error rate and 2.71% word error rate, outperforming the\nstate-of-the-art models using the same amount of paired data.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 13:13:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Weicong", ""], ["Tan", "Xu", ""], ["Xia", "Yingce", ""], ["Qin", "Tao", ""], ["Wang", "Yu", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2009.05907", "submitter": "Yucheng Hang", "authors": "Yucheng Hang, Qingmin Liao, Wenming Yang, Yupeng Chen, Jie Zhou", "title": "Attention Cube Network for Image Restoration", "comments": "Accepted by the 28th ACM International Conference on Multimedia (ACM\n  MM 2020); Code is available at https://github.com/YCHang686/A-CubeNet", "journal-ref": null, "doi": "10.1145/3394171.3413564", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural network (CNN) have been widely used in\nimage restoration and obtained great success. However, most of existing methods\nare limited to local receptive field and equal treatment of different types of\ninformation. Besides, existing methods always use a multi-supervised method to\naggregate different feature maps, which can not effectively aggregate\nhierarchical feature information. To address these issues, we propose an\nattention cube network (A-CubeNet) for image restoration for more powerful\nfeature expression and feature correlation learning. Specifically, we design a\nnovel attention mechanism from three dimensions, namely spatial dimension,\nchannel-wise dimension and hierarchical dimension. The adaptive spatial\nattention branch (ASAB) and the adaptive channel attention branch (ACAB)\nconstitute the adaptive dual attention module (ADAM), which can capture the\nlong-range spatial and channel-wise contextual information to expand the\nreceptive field and distinguish different types of information for more\neffective feature representations. Furthermore, the adaptive hierarchical\nattention module (AHAM) can capture the long-range hierarchical contextual\ninformation to flexibly aggregate different feature maps by weights depending\non the global context. The ADAM and AHAM cooperate to form an \"attention in\nattention\" structure, which means AHAM's inputs are enhanced by ASAB and ACAB.\nExperiments demonstrate the superiority of our method over state-of-the-art\nimage restoration methods in both quantitative comparison and visual analysis.\nCode is available at https://github.com/YCHang686/A-CubeNet.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 03:42:14 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 03:35:45 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 11:32:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hang", "Yucheng", ""], ["Liao", "Qingmin", ""], ["Yang", "Wenming", ""], ["Chen", "Yupeng", ""], ["Zhou", "Jie", ""]]}, {"id": "2009.06001", "submitter": "Thyagharajan K K", "authors": "K. K. Thyagharajan, I. Kiruba Raji", "title": "A Review of Visual Descriptors and Classification Techniques Used in\n  Leaf Species Identification", "comments": "44 pages, 7 figures, \"for final published version, see\n  https://link.springer.com/article/10.1007/s11831-018-9266-3\"", "journal-ref": "Sept. 2019", "doi": "10.1007/s11831-018-9266-3", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plants are fundamentally important to life. Key research areas in plant\nscience include plant species identification, weed classification using hyper\nspectral images, monitoring plant health and tracing leaf growth, and the\nsemantic interpretation of leaf information. Botanists easily identify plant\nspecies by discriminating between the shape of the leaf, tip, base, leaf margin\nand leaf vein, as well as the texture of the leaf and the arrangement of\nleaflets of compound leaves. Because of the increasing demand for experts and\ncalls for biodiversity, there is a need for intelligent systems that recognize\nand characterize leaves so as to scrutinize a particular species, the diseases\nthat affect them, the pattern of leaf growth, and so on. We review several\nimage processing methods in the feature extraction of leaves, given that\nfeature extraction is a crucial technique in computer vision. As computers\ncannot comprehend images, they are required to be converted into features by\nindividually analysing image shapes, colours, textures and moments. Images that\nlook the same may deviate in terms of geometric and photometric variations. In\nour study, we also discuss certain machine learning classifiers for an analysis\nof different species of leaves.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 14:11:00 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Thyagharajan", "K. K.", ""], ["Raji", "I. Kiruba", ""]]}, {"id": "2009.06573", "submitter": "Runze Su", "authors": "Runze Su, Fei Tao, Xudong Liu, Haoran Wei, Xiaorong Mei, Zhiyao Duan,\n  Lei Yuan, Ji Liu, Yuying Xie", "title": "Themes Informed Audio-visual Correspondence Learning", "comments": "Submitting to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The applications of short-term user-generated video (UGV), such as Snapchat,\nand Youtube short-term videos, booms recently, raising lots of multimodal\nmachine learning tasks. Among them, learning the correspondence between audio\nand visual information from videos is a challenging one. Most previous work of\nthe audio-visual correspondence(AVC) learning only investigated constrained\nvideos or simple settings, which may not fit the application of UGV. In this\npaper, we proposed new principles for AVC and introduced a new framework to set\nsight of videos' themes to facilitate AVC learning. We also released the\nKWAI-AD-AudVis corpus which contained 85432 short advertisement videos (around\n913 hours) made by users. We evaluated our proposed approach on this corpus,\nand it was able to outperform the baseline by 23.15% absolute difference.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:03:04 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 06:40:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Su", "Runze", ""], ["Tao", "Fei", ""], ["Liu", "Xudong", ""], ["Wei", "Haoran", ""], ["Mei", "Xiaorong", ""], ["Duan", "Zhiyao", ""], ["Yuan", "Lei", ""], ["Liu", "Ji", ""], ["Xie", "Yuying", ""]]}, {"id": "2009.07480", "submitter": "Shahroz Tariq", "authors": "Shahroz Tariq, Sangyup Lee and Simon S. Woo", "title": "A Convolutional LSTM based Residual Network for Deepfake Video Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning-based video manipulation methods have become\nwidely accessible to masses. With little to no effort, people can easily learn\nhow to generate deepfake videos with only a few victims or target images. This\ncreates a significant social problem for everyone whose photos are publicly\navailable on the Internet, especially on social media websites. Several deep\nlearning-based detection methods have been developed to identify these\ndeepfakes. However, these methods lack generalizability, because they perform\nwell only for a specific type of deepfake method. Therefore, those methods are\nnot transferable to detect other deepfake methods. Also, they do not take\nadvantage of the temporal information of the video. In this paper, we addressed\nthese limitations. We developed a Convolutional LSTM based Residual Network\n(CLRNet), which takes a sequence of consecutive images as an input from a video\nto learn the temporal information that helps in detecting unnatural looking\nartifacts that are present between frames of deepfake videos. We also propose a\ntransfer learning-based approach to generalize different deepfake methods.\nThrough rigorous experimentations using the FaceForensics++ dataset, we showed\nthat our method outperforms five of the previously proposed state-of-the-art\ndeepfake detection methods by better generalizing at detecting different\ndeepfake methods using the same model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 05:57:06 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tariq", "Shahroz", ""], ["Lee", "Sangyup", ""], ["Woo", "Simon S.", ""]]}, {"id": "2009.07504", "submitter": "Kexin Feng", "authors": "Kexin Feng, Preeti Zanwar, Amir H. Behzadan, Theodora Chaspari", "title": "Exploring Speech Cues in Web-mined COVID-19 Conversational Vlogs", "comments": "accepted in ACM Multimedia workshop on Media Analytics for Societal\n  Trends (MAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic caused by the novel SARS-Coronavirus-2 (n-SARS-CoV-2)\nhas impacted people's lives in unprecedented ways. During the time of the\npandemic, social vloggers have used social media to actively share their\nopinions or experiences in quarantine. This paper collected videos from YouTube\nto track emotional responses in conversational vlogs and their potential\nassociations with events related to the pandemic. In particular, vlogs uploaded\nfrom locations in New York City were analyzed given that this was one of the\nfirst epicenters of the pandemic in the United States. We observed some common\npatterns in vloggers' acoustic and linguistic features across the time span of\nthe quarantine, which is indicative of changes in emotional reactivity.\nAdditionally, we investigated fluctuations of acoustic and linguistic patterns\nin relation to COVID-19 events in the New York area (e.g. the number of daily\nnew cases, number of deaths, and extension of stay-at-home order and state of\nemergency). Our results indicate that acoustic features, such as\nzero-crossing-rate, jitter, and shimmer, can be valuable for analyzing\nemotional reactivity in social media videos. Our findings further indicate that\nsome of the peaks of the acoustic and linguistic indices align with COVID-19\nevents, such as the peak in the number of deaths and emergency declaration.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 06:57:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Feng", "Kexin", ""], ["Zanwar", "Preeti", ""], ["Behzadan", "Amir H.", ""], ["Chaspari", "Theodora", ""]]}, {"id": "2009.07526", "submitter": "Yuan Chai", "authors": "Jing Yu, Yuan Chai, Yujing Wang, Yue Hu, Qi Wu", "title": "CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation", "comments": "Accepted by IJCAI 2021. SOLE copyright holder is IJCAI (International\n  Joint Conferences on Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graphs are semantic abstraction of images that encourage visual\nunderstanding and reasoning. However, the performance of Scene Graph Generation\n(SGG) is unsatisfactory when faced with biased data in real-world scenarios.\nConventional debiasing research mainly studies from the view of balancing data\ndistribution or learning unbiased models and representations, ignoring the\ncorrelations among the biased classes. In this work, we analyze this problem\nfrom a novel cognition perspective: automatically building a hierarchical\ncognitive structure from the biased predictions and navigating that hierarchy\nto locate the relationships, making the tail relationships receive more\nattention in a coarse-to-fine mode. To this end, we propose a novel debiasing\nCognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive\nstructure CogTree to organize the relationships based on the prediction of a\nbiased SGG model. The CogTree distinguishes remarkably different relationships\nat first and then focuses on a small portion of easily confused ones. Then, we\npropose a debiasing loss specially for this cognitive structure, which supports\ncoarse-to-fine distinction for the correct relationships. The loss is\nmodel-agnostic and consistently boosting the performance of several\nstate-of-the-art models. The code is available at:\nhttps://github.com/CYVincent/Scene-Graph-Transformer-CogTree.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:47:26 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 06:27:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yu", "Jing", ""], ["Chai", "Yuan", ""], ["Wang", "Yujing", ""], ["Hu", "Yue", ""], ["Wu", "Qi", ""]]}, {"id": "2009.07557", "submitter": "Daichi Horita", "authors": "Daichi Horita and Kiyoharu Aizawa", "title": "SLGAN: Style- and Latent-guided Generative Adversarial Network for\n  Desirable Makeup Transfer and Removal", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are five features to consider when using generative adversarial\nnetworks to apply makeup to photos of the human face. These features include\n(1) facial components, (2) interactive color adjustments, (3) makeup\nvariations, (4) robustness to poses and expressions, and the (5) use of\nmultiple reference images. Several related works have been proposed, mainly\nusing generative adversarial networks (GAN). Unfortunately, none of them have\naddressed all five features simultaneously. This paper closes the gap with an\ninnovative style- and latent-guided GAN (SLGAN). We provide a novel, perceptual\nmakeup loss and a style-invariant decoder that can transfer makeup styles based\non histogram matching to avoid the identity-shift problem. In our experiments,\nwe show that our SLGAN is better than or comparable to state-of-the-art\nmethods. Furthermore, we show that our proposal can interpolate facial makeup\nimages to determine the unique features, compare existing methods, and help\nusers find desirable makeup configurations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 08:54:20 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 01:58:37 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 13:08:51 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Horita", "Daichi", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2009.07632", "submitter": "Christian von der Weth", "authors": "Christian von der Weth, Ashraf Abdul, Shaojing Fan, Mohan Kankanhalli", "title": "Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia\n  Research Agenda", "comments": "This work has been accepted to the \"Brave New Ideas\" track of the ACM\n  Multimedia Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participation on social media platforms has many benefits but also poses\nsubstantial threats. Users often face an unintended loss of privacy, are\nbombarded with mis-/disinformation, or are trapped in filter bubbles due to\nover-personalized content. These threats are further exacerbated by the rise of\nhidden AI-driven algorithms working behind the scenes to shape users' thoughts,\nattitudes, and behavior. We investigate how multimedia researchers can help\ntackle these problems to level the playing field for social media users. We\nperform a comprehensive survey of algorithmic threats on social media and use\nit as a lens to set a challenging but important research agenda for effective\nand real-time user nudging. We further implement a conceptual prototype and\nevaluate it with experts to supplement our research agenda. This paper calls\nfor solutions that combat the algorithmic threats on social media by utilizing\nmachine learning and multimedia content analysis techniques but in a\ntransparent manner and for the benefit of the users.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 08:58:29 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["von der Weth", "Christian", ""], ["Abdul", "Ashraf", ""], ["Fan", "Shaojing", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "2009.07637", "submitter": "Zijie Ye", "authors": "Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, Yanfeng\n  Wang", "title": "ChoreoNet: Towards Music to Dance Synthesis with Choreographic Action\n  Unit", "comments": "10 pages, 5 figures, Accepted by ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3414005", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dance and music are two highly correlated artistic forms. Synthesizing dance\nmotions has attracted much attention recently. Most previous works conduct\nmusic-to-dance synthesis via directly music to human skeleton keypoints\nmapping. Meanwhile, human choreographers design dance motions from music in a\ntwo-stage manner: they firstly devise multiple choreographic dance units\n(CAUs), each with a series of dance motions, and then arrange the CAU sequence\naccording to the rhythm, melody and emotion of the music. Inspired by these, we\nsystematically study such two-stage choreography approach and construct a\ndataset to incorporate such choreography knowledge. Based on the constructed\ndataset, we design a two-stage music-to-dance synthesis framework ChoreoNet to\nimitate human choreography procedure. Our framework firstly devises a CAU\nprediction model to learn the mapping relationship between music and CAU\nsequences. Afterwards, we devise a spatial-temporal inpainting model to convert\nthe CAU sequence into continuous dance motions. Experimental results\ndemonstrate that the proposed ChoreoNet outperforms baseline methods (0.622 in\nterms of CAU BLEU score and 1.59 in terms of user study score).\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 12:38:19 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ye", "Zijie", ""], ["Wu", "Haozhe", ""], ["Jia", "Jia", ""], ["Bu", "Yaohua", ""], ["Chen", "Wei", ""], ["Meng", "Fanbo", ""], ["Wang", "Yanfeng", ""]]}, {"id": "2009.07816", "submitter": "Li Su", "authors": "Yuen-Jen Lin, Hsuan-Kai Kao, Yih-Chih Tseng, Ming Tsai, Li Su", "title": "A Human-Computer Duet System for Music Performance", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413921", "report-no": null, "categories": "cs.MM cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual musicians have become a remarkable phenomenon in the contemporary\nmultimedia arts. However, most of the virtual musicians nowadays have not been\nendowed with abilities to create their own behaviors, or to perform music with\nhuman musicians. In this paper, we firstly create a virtual violinist, who can\ncollaborate with a human pianist to perform chamber music automatically without\nany intervention. The system incorporates the techniques from various fields,\nincluding real-time music tracking, pose estimation, and body movement\ngeneration. In our system, the virtual musician's behavior is generated based\non the given music audio alone, and such a system results in a low-cost,\nefficient and scalable way to produce human and virtual musicians'\nco-performance. The proposed system has been validated in public concerts.\nObjective quality assessment approaches and possible ways to systematically\nimprove the system are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:19:23 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lin", "Yuen-Jen", ""], ["Kao", "Hsuan-Kai", ""], ["Tseng", "Yih-Chih", ""], ["Tsai", "Ming", ""], ["Su", "Li", ""]]}, {"id": "2009.07879", "submitter": "Qiong Liu", "authors": "Qiong Liu, Yanxia Zhang", "title": "Using Sensory Time-cue to enable Unsupervised Multimodal Meta-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data from IoT (Internet of Things) sensors become ubiquitous,\nstate-of-the-art machine learning algorithms face many challenges on directly\nusing sensor data. To overcome these challenges, methods must be designed to\nlearn directly from sensors without manual annotations. This paper introduces\nSensory Time-cue for Unsupervised Meta-learning (STUM). Different from\ntraditional learning approaches that either heavily depend on labels or on\ntime-independent feature extraction assumptions, such as Gaussian distribution\nfeatures, the STUM system uses time relation of inputs to guide the feature\nspace formation within and across modalities. The fact that STUM learns from a\nvariety of small tasks may put this method in the camp of Meta-Learning.\nDifferent from existing Meta-Learning approaches, STUM learning tasks are\ncomposed within and across multiple modalities based on time-cue co-exist with\nthe IoT streaming data. In an audiovisual learning example, because consecutive\nvisual frames usually comprise the same object, this approach provides a unique\nway to organize features from the same object together. The same method can\nalso organize visual object features with the object's spoken-name features\ntogether if the spoken name is presented with the object at about the same\ntime. This cross-modality feature organization may further help the\norganization of visual features that belong to similar objects but acquired at\ndifferent location and time. Promising results are achieved through\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:18:49 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Liu", "Qiong", ""], ["Zhang", "Yanxia", ""]]}, {"id": "2009.08015", "submitter": "Li Su", "authors": "Hsuan-Kai Kao and Li Su", "title": "Temporally Guided Music-to-Body-Movement Generation", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413848", "report-no": null, "categories": "cs.MM cs.AI cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a neural network model to generate virtual violinist's\n3-D skeleton movements from music audio. Improved from the conventional\nrecurrent neural network models for generating 2-D skeleton data in previous\nworks, the proposed model incorporates an encoder-decoder architecture, as well\nas the self-attention mechanism to model the complicated dynamics in body\nmovement sequences. To facilitate the optimization of self-attention model,\nbeat tracking is applied to determine effective sizes and boundaries of the\ntraining examples. The decoder is accompanied with a refining network and a\nbowing attack inference mechanism to emphasize the right-hand behavior and\nbowing attack timing. Both objective and subjective evaluations reveal that the\nproposed model outperforms the state-of-the-art methods. To the best of our\nknowledge, this work represents the first attempt to generate 3-D violinists'\nbody movements considering key features in musical body movement.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:10:05 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Kao", "Hsuan-Kai", ""], ["Su", "Li", ""]]}, {"id": "2009.08037", "submitter": "Pawan Kumar Singh Dr.", "authors": "Pawan Kumar Singh, Shubham Sinha, Sagnik Pal Chowdhury, Ram Sarkar,\n  Mita Nasipuri", "title": "Word Segmentation from Unconstrained Handwritten Bangla Document Images\n  using Distance Transform", "comments": "12 pages, 5 figures, conference", "journal-ref": "7th International Conference on Advances in Communication, Network\n  and Computing (CNC),pp. 271-282, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 03:14:27 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Singh", "Pawan Kumar", ""], ["Sinha", "Shubham", ""], ["Chowdhury", "Sagnik Pal", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2009.08083", "submitter": "Li Su", "authors": "Cheng-Che Lee, Wan-Yi Lin, Yen-Ting Shih, Pei-Yi Patricia Kuo, Li Su", "title": "Crossing You in Style: Cross-modal Style Transfer from Music to Visual\n  Arts", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413624", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Music-to-visual style transfer is a challenging yet important cross-modal\nlearning problem in the practice of creativity. Its major difference from the\ntraditional image style transfer problem is that the style information is\nprovided by music rather than images. Assuming that musical features can be\nproperly mapped to visual contents through semantic links between the two\ndomains, we solve the music-to-visual style transfer problem in two steps:\nmusic visualization and style transfer. The music visualization network\nutilizes an encoder-generator architecture with a conditional generative\nadversarial network to generate image-based music representations from music\ndata. This network is integrated with an image style transfer method to\naccomplish the style transfer process. Experiments are conducted on\nWikiArt-IMSLP, a newly compiled dataset including Western music recordings and\npaintings listed by decades. By utilizing such a label to learn the semantic\nconnection between paintings and music, we demonstrate that the proposed\nframework can generate diverse image style representations from a music piece,\nand these representations can unveil certain art forms of the same era.\nSubjective testing results also emphasize the role of the era label in\nimproving the perceptual quality on the compatibility between music and visual\ncontent.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 05:58:13 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lee", "Cheng-Che", ""], ["Lin", "Wan-Yi", ""], ["Shih", "Yen-Ting", ""], ["Kuo", "Pei-Yi Patricia", ""], ["Su", "Li", ""]]}, {"id": "2009.08395", "submitter": "Tariq Habib Afridi Mr.", "authors": "Tariq Habib Afridi, Aftab Alam, Muhammad Numan Khan, Jawad Khan,\n  Young-Koo Lee", "title": "A Multimodal Memes Classification: A Survey and Open Research Issues", "comments": "This is a survey paper on recent state of the art VL models that can\n  be used for memes classification. it has 15 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memes are graphics and text overlapped so that together they present concepts\nthat become dubious if one of them is absent. It is spread mostly on social\nmedia platforms, in the form of jokes, sarcasm, motivating, etc. After the\nsuccess of BERT in Natural Language Processing (NLP), researchers inclined to\nVisual-Linguistic (VL) multimodal problems like memes classification, image\ncaptioning, Visual Question Answering (VQA), and many more. Unfortunately, many\nmemes get uploaded each day on social media platforms that need automatic\ncensoring to curb misinformation and hate. Recently, this issue has attracted\nthe attention of researchers and practitioners. State-of-the-art methods that\nperformed significantly on other VL dataset, tends to fail on memes\nclassification. In this context, this work aims to conduct a comprehensive\nstudy on memes classification, generally on the VL multimodal problems and\ncutting edge solutions. We propose a generalized framework for VL problems. We\ncover the early and next-generation works on VL problems. Finally, we identify\nand articulate several open research issues and challenges. This is the first\nstudy that presents the generalized view of the advanced classification\ntechniques concerning memes classification to the best of our knowledge. We\nbelieve this study presents a clear road-map for the Machine Learning (ML)\nresearch community to implement and enhance memes classification techniques.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 16:13:21 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Afridi", "Tariq Habib", ""], ["Alam", "Aftab", ""], ["Khan", "Muhammad Numan", ""], ["Khan", "Jawad", ""], ["Lee", "Young-Koo", ""]]}, {"id": "2009.09300", "submitter": "Thyagharajan K K", "authors": "S. Kavitha, K.K. Thyagharajan", "title": "Features based Mammogram Image Classification using Weighted Feature\n  Support Vector Machine", "comments": "9 pages, 3 figures, \"submitted to International Conference on\n  Computing and Communication Systems\"", "journal-ref": "Vol. 270, 2012, 320-329", "doi": "10.1007/978-3-642-29216-3_35", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the existing research of mammogram image classification, either clinical\ndata or image features of a specific type is considered along with the\nsupervised classifiers such as Neural Network (NN) and Support Vector Machine\n(SVM). This paper considers automated classification of breast tissue type as\nbenign or malignant using Weighted Feature Support Vector Machine (WFSVM)\nthrough constructing the precomputed kernel function by assigning more weight\nto relevant features using the principle of maximizing deviations. Initially,\nMIAS dataset of mammogram images is divided into training and test set, then\nthe preprocessing techniques such as noise removal and background removal are\napplied to the input images and the Region of Interest (ROI) is identified. The\nstatistical features and texture features are extracted from the ROI and the\nclinical features are obtained directly from the dataset. The extracted\nfeatures of the training dataset are used to construct the weighted features\nand precomputed linear kernel for training the WFSVM, from which the training\nmodel file is created. Using this model file the kernel matrix of test samples\nis classified as benign or malignant. This analysis shows that the texture\nfeatures have resulted in better accuracy than the other features with WFSVM\nand SVM. However, the number of support vectors created in WFSVM is less than\nthe SVM classifier.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 21:28:31 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kavitha", "S.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.09428", "submitter": "Thyagharajan K K", "authors": "L. Balaji, K. K. Thyagharajan", "title": "An enhanced performance for H.265/SHVC based on combined AEGBM3D filter\n  and back-propagation neural network", "comments": "14 pages, 9 figures, \"for final published version, see\n  https://doi.org/10.1007/s11760-018-1265-1\"", "journal-ref": "12 (2018) 809-817", "doi": "10.1007/s11760-018-1265-1", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the latest video coding standard H265 SHVC, a scalable\nextension to High Efficiency Video Coding (HEVC). HEVC introduces new coding\ntools compared to its predecessor and is backward compatible with all types of\nelectronic gadgets. The gadgets with different display capabilities cannot be\noffered the same quality video due to the constraints in transmission bandwidth\nis a major problem. One solution to this problem will be the compression of the\nvideo sequence which is focused in this paper to preserve or increase PSNR\nwhile reducing bit-rate besides a novel method implemented in SHVC encoder. The\nnovel method undergoes a combined AEGBM3D (adaptive edge guided block-matching\nand 3D) filtering and back-propagation technique. The technique includes an\nAEGBM3D filter which avoids spatial redundancy and de-noise frames; hence\nenhancement in PSNR is achieved. The obtained PSNR of the video is compared\nwith the set threshold PSNR to maintain PSNR above the threshold by repeated\nAEGBM3D filtering. The BP technique based on the neural network machine\nlearning approach continually restrains the output if the input block does not\ncontain a feature they were trained to recognize. This frequent control over\nthe output produces few bits; hence reduction in bit-rate is achieved. The\nsimulation results show that the proposed technique delivers an average\nincrement of 0.16 and 0.25dB in PSNR and an average decrement of 28 and 37% in\nbit-rate for 1.5 and 2 times spatial ratios respectively, compared with the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 13:30:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Balaji", "L.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.10315", "submitter": "Aneesh Vartakavi", "authors": "Aneesh Vartakavi and Amanmeet Garg", "title": "PodSumm -- Podcast Audio Summarization", "comments": "For PodRecs: Workshop on Podcast Recommendations at RecSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The diverse nature, scale, and specificity of podcasts present a unique\nchallenge to content discovery systems. Listeners often rely on text\ndescriptions of episodes provided by the podcast creators to discover new\ncontent. Some factors like the presentation style of the narrator and\nproduction quality are significant indicators of subjective user preference but\nare difficult to quantify and not reflected in the text descriptions provided\nby the podcast creators. We propose the automated creation of podcast audio\nsummaries to aid in content discovery and help listeners to quickly preview\npodcast content before investing time in listening to an entire episode. In\nthis paper, we present a method to automatically construct a podcast summary\nvia guidance from the text-domain. Our method performs two key steps, namely,\naudio to text transcription and text summary generation. Motivated by a lack of\ndatasets for this task, we curate an internal dataset, find an effective scheme\nfor data augmentation, and design a protocol to gather summaries from\nannotators. We fine-tune a PreSumm[10] model with our augmented dataset and\nperform an ablation study. Our method achieves ROUGE-F(1/2/L) scores of\n0.63/0.53/0.63 on our dataset. We hope these results may inspire future\nresearch in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 04:49:33 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Vartakavi", "Aneesh", ""], ["Garg", "Amanmeet", ""]]}, {"id": "2009.10370", "submitter": "Bassem Seddik", "authors": "Bassem Seddik and Najoua Essoukri Ben Amara", "title": "Visual Methods for Sign Language Recognition: A Modality-Based Review", "comments": "This survey paper is accepted as Springer book chapter, currently\n  under edition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language visual recognition from continuous multi-modal streams is still\none of the most challenging fields.\n  Recent advances in human actions recognition are exploiting the ascension of\nGPU-based learning from massive data, and are getting closer to human-like\nperformances.\n  They are then prone to creating interactive services for the deaf and\nhearing-impaired communities.\n  A population that is expected to grow considerably in the years to come.\n  This paper aims at reviewing the human actions recognition literature with\nthe sign-language visual understanding as a scope.\n  The methods analyzed will be mainly organized according to the different\ntypes of unimodal inputs exploited, their relative multi-modal combinations and\npipeline steps.\n  In each section, we will detail and compare the related datasets, approaches\nthen distinguish the still open contribution paths suitable for the creation of\nsign language related services.\n  Special attention will be paid to the approaches and commercial solutions\nhandling facial expressions and continuous signing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 07:56:02 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Seddik", "Bassem", ""], ["Amara", "Najoua Essoukri Ben", ""]]}, {"id": "2009.10434", "submitter": "Haoyu Tang", "authors": "Haoyu Tang, Jihua Zhu, Meng Liu, Member, IEEE, Zan Gao, and Zhiyong\n  Cheng", "title": "Frame-wise Cross-modal Matching for Video Moment Retrieval", "comments": "12 pages; accepted by IEEE TMM", "journal-ref": "IEEE Transactions on Multimedia 2021", "doi": "10.1109/TMM.2021.3063631", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video moment retrieval targets at retrieving a moment in a video for a given\nlanguage query. The challenges of this task include 1) the requirement of\nlocalizing the relevant moment in an untrimmed video, and 2) bridging the\nsemantic gap between textual query and video contents. To tackle those\nproblems, early approaches adopt the sliding window or uniform sampling to\ncollect video clips first and then match each clip with the query. Obviously,\nthese strategies are time-consuming and often lead to unsatisfied accuracy in\nlocalization due to the unpredictable length of the golden moment. To avoid the\nlimitations, researchers recently attempt to directly predict the relevant\nmoment boundaries without the requirement to generate video clips first. One\nmainstream approach is to generate a multimodal feature vector for the target\nquery and video frames (e.g., concatenation) and then use a regression approach\nupon the multimodal feature vector for boundary detection. Although some\nprogress has been achieved by this approach, we argue that those methods have\nnot well captured the cross-modal interactions between the query and video\nframes.\n  In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)\nmodel which predicts the temporal boundaries based on an interaction modeling.\nIn addition, an attention module is introduced to assign higher weights to\nquery words with richer semantic cues, which are considered to be more\nimportant for finding relevant video contents. Another contribution is that we\npropose an additional predictor to utilize the internal frames in the model\ntraining to improve the localization accuracy. Extensive experiments on two\ndatasets TACoS and Charades-STA demonstrate the superiority of our method over\nseveral state-of-the-art methods. Ablation studies have been also conducted to\nexamine the effectiveness of different modules in our ACRM model.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:25:41 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:32:20 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Tang", "Haoyu", ""], ["Zhu", "Jihua", ""], ["Liu", "Meng", ""], ["Member", "", ""], ["IEEE", "", ""], ["Gao", "Zan", ""], ["Cheng", "Zhiyong", ""]]}, {"id": "2009.10708", "submitter": "Thyagharajan K K", "authors": "L. Balaji, K. K. Thyagharajan", "title": "H.264/SVC Mode Decision Based on Mode Correlation and Desired Mode List", "comments": "12 pages, 13 figures", "journal-ref": "11(5), October 2014, 510-516", "doi": "10.1007/s11633-014-0830-5", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of video encoders involves the implementation of fast mode\ndecision (FMD) algorithm to reduce computation complexity while maintaining the\nperformance of the coding. Although H.264/scalable video coding (SVC) achieves\nhigh scalability and coding efficiency, it also has high complexity in\nimplementing its exhaustive computation. In this paper, a novel algorithm is\nproposed to reduce the redundant candidate modes by making use of the\ncorrelation among layers. The desired mode list is created based on the\nprobability to be the best mode for each block in the base layer and a\ncandidate mode selection in the enhancement layer by the correlations of modes\namong the reference frame and current frame. Our algorithm is implemented in\njoint scalable video model (JSVM) 9.19.15 reference software and the\nperformance is evaluated based on the average encoding time, peak signal to\nnoise ratio (PSNR) and bit rate. The experimental results show 41.89%\nimprovement in encoding time with minimal loss of 0.02dB in PSNR and 0.05%\nincrease in bit rate.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:53:15 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Balaji", "L.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.10760", "submitter": "Taras Kucherenko", "authors": "Patrik Jonell, Taras Kucherenko, Ilaria Torre, Jonas Beskow", "title": "Can we trust online crowdworkers? Comparing online and offline\n  participants in a preference test of virtual agents", "comments": "Patrik Jonell and Taras Kucherenko contributed equally to this work.\n  Published at the Proceedings of the 20th ACM International Conference on\n  Intelligent Virtual Agent. 8 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3383652.3423860", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting user studies is a crucial component in many scientific fields.\nWhile some studies require participants to be physically present, other studies\ncan be conducted both physically (e.g. in-lab) and online (e.g. via\ncrowdsourcing). Inviting participants to the lab can be a time-consuming and\nlogistically difficult endeavor, not to mention that sometimes research groups\nmight not be able to run in-lab experiments, because of, for example, a\npandemic. Crowdsourcing platforms such as Amazon Mechanical Turk (AMT) or\nProlific can therefore be a suitable alternative to run certain experiments,\nsuch as evaluating virtual agents. Although previous studies investigated the\nuse of crowdsourcing platforms for running experiments, there is still\nuncertainty as to whether the results are reliable for perceptual studies. Here\nwe replicate a previous experiment where participants evaluated a gesture\ngeneration model for virtual agents. The experiment is conducted across three\nparticipant pools -- in-lab, Prolific, and AMT -- having similar demographics\nacross the in-lab participants and the Prolific platform. Our results show no\ndifference between the three participant pools in regards to their evaluations\nof the gesture generation models and their reliability scores. The results\nindicate that online platforms can successfully be used for perceptual\nevaluations of this kind.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 18:43:28 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 09:01:24 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Jonell", "Patrik", ""], ["Kucherenko", "Taras", ""], ["Torre", "Ilaria", ""], ["Beskow", "Jonas", ""]]}, {"id": "2009.10942", "submitter": "Ping Li PhD", "authors": "Ping Li, Qinghao Ye, Luming Zhang, Li Yuan, Xianghua Xu, Ling Shao", "title": "Exploring global diverse attention via pairwise temporal relation for\n  video summarization", "comments": "12 pages, 8 figures", "journal-ref": "Pattern Recognition, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization is an effective way to facilitate video searching and\nbrowsing. Most of existing systems employ encoder-decoder based recurrent\nneural networks, which fail to explicitly diversify the system-generated\nsummary frames while requiring intensive computations. In this paper, we\npropose an efficient convolutional neural network architecture for video\nSUMmarization via Global Diverse Attention called SUM-GDA, which adapts\nattention mechanism in a global perspective to consider pairwise temporal\nrelations of video frames. Particularly, the GDA module has two advantages: 1)\nit models the relations within paired frames as well as the relations among all\npairs, thus capturing the global attention across all frames of one video; 2)\nit reflects the importance of each frame to the whole video, leading to diverse\nattention on these frames. Thus, SUM-GDA is beneficial for generating diverse\nframes to form satisfactory video summary. Extensive experiments on three data\nsets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its\nextension outperform other competing state-of-the-art methods with remarkable\nimprovements. In addition, the proposed models can be run in parallel with\nsignificantly less computational costs, which helps the deployment in highly\ndemanding applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:29:09 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Li", "Ping", ""], ["Ye", "Qinghao", ""], ["Zhang", "Luming", ""], ["Yuan", "Li", ""], ["Xu", "Xianghua", ""], ["Shao", "Ling", ""]]}, {"id": "2009.11129", "submitter": "Saba Nazir", "authors": "Saba Nazir, Taner Cagali, Chris Newell, Mehrnoosh Sadrzadeh", "title": "Cosine Similarity of Multimodal Content Vectors for TV Programmes", "comments": "3 pages, 1 figure, Machine Learning for Media Discovery (ML4MD)\n  Workshop at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal information originates from a variety of sources: audiovisual\nfiles, textual descriptions, and metadata. We show how one can represent the\ncontent encoded by each individual source using vectors, how to combine the\nvectors via middle and late fusion techniques, and how to compute the semantic\nsimilarities between the contents. Our vectorial representations are built from\nspectral features and Bags of Audio Words, for audio, LSI topics and Doc2vec\nembeddings for subtitles, and the categorical features, for metadata. We\nimplement our model on a dataset of BBC TV programmes and evaluate the fused\nrepresentations to provide recommendations. The late fused similarity matrices\nsignificantly improve the precision and diversity of recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 13:12:30 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Nazir", "Saba", ""], ["Cagali", "Taner", ""], ["Newell", "Chris", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "2009.11455", "submitter": "Scott Howard", "authors": "Scott Howard, Grant Barthelmes, Cara Ravasio, Lisa Huang, Benjamin\n  Poag, Varun Mannam", "title": "Packet Compressed Sensing Imaging (PCSI): Robust Image Transmission over\n  Noisy Channels", "comments": "15 pages, 3 figures, 3 tables, presented at ARRL/TAPR Digital\n  Communications Conference 2020, for associated software tool see\n  https://github.com/maqifrnswa/PCSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Packet Compressed Sensing Imaging (PCSI) is digital unconnected image\ntransmission method resilient to packet loss. The goal is to develop a robust\nimage transmission method that is computationally trivial to transmit (e.g.,\ncompatible with low-power 8-bit microcontrollers) and well suited for weak\nsignal environments where packets are likely to be lost. In other image\ntransmission techniques, noise and packet loss leads to parts of the image\nbeing distorted or missing. In PCSI, every packet contains random pixel\ninformation from the entire image, and each additional packet received (in any\norder) simply enhances image quality. Satisfactory SSTV resolution (320x240\npixel) images can be received in ~1-2 minutes when transmitted at 1200 baud\nAFSK, which is on par with analog SSTV transmission time. Image transmission\nand reception can occur simultaneously on a computer, and multiple images can\nbe received from multiple stations simultaneously - allowing for the creation\nof \"image nets.\" This paper presents a simple computer application for Windows,\nMac, and Linux that implements PCSI transmission and reception on any KISS\ncompatible hardware or software modem on any band and digital mode.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 02:32:46 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Howard", "Scott", ""], ["Barthelmes", "Grant", ""], ["Ravasio", "Cara", ""], ["Huang", "Lisa", ""], ["Poag", "Benjamin", ""], ["Mannam", "Varun", ""]]}, {"id": "2009.11939", "submitter": "Ali Karaali", "authors": "Ali Karaali, Naomi Harte, Claudio Rosito Jung", "title": "Deep Multi-Scale Feature Learning for Defocus Blur Estimation", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an edge-based defocus blur estimation method from a\nsingle defocused image. We first distinguish edges that lie at depth\ndiscontinuities (called depth edges, for which the blur estimate is ambiguous)\nfrom edges that lie at approximately constant depth regions (called pattern\nedges, for which the blur estimate is well-defined). Then, we estimate the\ndefocus blur amount at pattern edges only, and explore an interpolation scheme\nbased on guided filters that prevents data propagation across the detected\ndepth edges to obtain a dense blur map with well-defined object boundaries.\nBoth tasks (edge classification and blur estimation) are performed by deep\nconvolutional neural networks (CNNs) that share weights to learn meaningful\nlocal features from multi-scale patches centered at edge locations. Experiments\non naturally defocused images show that the proposed method presents\nqualitative and quantitative results that outperform state-of-the-art (SOTA)\nmethods, with a good compromise between running time and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 20:36:40 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Karaali", "Ali", ""], ["Harte", "Naomi", ""], ["Jung", "Claudio Rosito", ""]]}, {"id": "2009.12088", "submitter": "Nicol\\`o Bonettini", "authors": "Sara Mandelli, Nicol\\`o Bonettini, Paolo Bestagini, Stefano Tubaro", "title": "Training CNNs in Presence of JPEG Compression: Multimedia Forensics vs\n  Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proved very accurate in multiple\ncomputer vision image classification tasks that required visual inspection in\nthe past (e.g., object recognition, face detection, etc.). Motivated by these\nastonishing results, researchers have also started using CNNs to cope with\nimage forensic problems (e.g., camera model identification, tampering\ndetection, etc.). However, in computer vision, image classification methods\ntypically rely on visual cues easily detectable by human eyes. Conversely,\nforensic solutions rely on almost invisible traces that are often very subtle\nand lie in the fine details of the image under analysis. For this reason,\ntraining a CNN to solve a forensic task requires some special care, as common\nprocessing operations (e.g., resampling, compression, etc.) can strongly hinder\nforensic traces. In this work, we focus on the effect that JPEG has on CNN\ntraining considering different computer vision and forensic image\nclassification problems. Specifically, we consider the issues that rise from\nJPEG compression and misalignment of the JPEG grid. We show that it is\nnecessary to consider these effects when generating a training dataset in order\nto properly train a forensic detector not losing generalization capability,\nwhereas it is almost possible to ignore these effects for computer vision\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 08:47:21 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Mandelli", "Sara", ""], ["Bonettini", "Nicol\u00f2", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2009.12148", "submitter": "Jun Yu", "authors": "Jun Yu, Donglin Zhang, Zhenqiu Shu", "title": "Adaptive Multi-modal Fusion Hashing via Hadamard Matrix", "comments": "There are theoretical errors in our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing plays an important role in information retrieval, due to its low\nstorage and high speed of processing. Among the techniques available in the\nliterature, multi-modal hashing, which can encode heterogeneous multi-modal\nfeatures into compact hash codes, has received particular attention. Most of\nthe existing multi-modal hashing methods adopt the fixed weighting factors to\nfuse multiple modalities for any query data, which cannot capture the variation\nof different queries. Besides, many methods introduce hyper-parameters to\nbalance many regularization terms that make the optimization harder. Meanwhile,\nit is time-consuming and labor-intensive to set proper parameter values. The\nlimitations may significantly hinder their promotion in real applications. In\nthis paper, we propose a simple, yet effective method that is inspired by the\nHadamard matrix. The proposed method captures the multi-modal feature\ninformation in an adaptive manner and preserves the discriminative semantic\ninformation in the hash codes. Our framework is flexible and involves a very\nfew hyper-parameters. Extensive experimental results show the method is\neffective and achieves superior performance compared to state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 11:58:07 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 22:16:43 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 13:58:18 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Yu", "Jun", ""], ["Zhang", "Donglin", ""], ["Shu", "Zhenqiu", ""]]}, {"id": "2009.12153", "submitter": "Franziska Boenisch", "authors": "Franziska Boenisch", "title": "A Survey on Model Watermarking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are applied in an increasing variety of domains.\nThe availability of large amounts of data and computational resources\nencourages the development of ever more complex and valuable models. These\nmodels are considered intellectual property of the legitimate parties who have\ntrained them, which makes their protection against stealing, illegitimate\nredistribution, and unauthorized application an urgent need. Digital\nwatermarking presents a strong mechanism for marking model ownership and,\nthereby, offers protection against those threats. The emergence of numerous\nwatermarking schemes and attacks against them is pushed forward by both\nacademia and industry, which motivates a comprehensive survey on this field.\nThis document at hand provides the first extensive literature review on ML\nmodel watermarking schemes and attacks against them. It offers a taxonomy of\nexisting approaches and systemizes general knowledge around them. Furthermore,\nit assembles the security requirements to watermarking approaches and evaluates\nschemes published by the scientific community according to them in order to\npresent systematic shortcomings and vulnerabilities. Thus, it can not only\nserve as valuable guidance in choosing the appropriate scheme for specific\nscenarios, but also act as an entry point into developing new mechanisms that\novercome presented shortcomings, and thereby contribute in advancing the field.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:03:02 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Boenisch", "Franziska", ""]]}, {"id": "2009.12763", "submitter": "Tianyang Shi", "authors": "Yinglin Duan (1), Tianyang Shi (1), Zhengxia Zou (2), Jia Qin (1 and\n  3), Yifei Zhao (1), Yi Yuan (1), Jie Hou (1), Xiang Wen (1 and 3), Changjie\n  Fan (1) ((1) NetEase Fuxi AI Lab, (2) University of Michigan, Ann Arbor, (3)\n  Zhejiang University)", "title": "Semi-Supervised Learning for In-Game Expert-Level Music-to-Dance\n  Translation", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music-to-dance translation is a brand-new and powerful feature in recent\nrole-playing games. Players can now let their characters dance along with\nspecified music clips and even generate fan-made dance videos. Previous works\nof this topic consider music-to-dance as a supervised motion generation problem\nbased on time-series data. However, these methods suffer from limited training\ndata pairs and the degradation of movements. This paper provides a new\nperspective for this task where we re-formulate the translation problem as a\npiece-wise dance phrase retrieval problem based on the choreography theory.\nWith such a design, players are allowed to further edit the dance movements on\ntop of our generation while other regression based methods ignore such user\ninteractivity. Considering that the dance motion capture is an expensive and\ntime-consuming procedure which requires the assistance of professional dancers,\nwe train our method under a semi-supervised learning framework with a large\nunlabeled dataset (20x than labeled data) collected. A co-ascent mechanism is\nintroduced to improve the robustness of our network. Using this unlabeled\ndataset, we also introduce self-supervised pre-training so that the translator\ncan understand the melody, rhythm, and other components of music phrases. We\nshow that the pre-training significantly improves the translation accuracy than\nthat of training from scratch. Experimental results suggest that our method not\nonly generalizes well over various styles of music but also succeeds in\nexpert-level choreography for game players.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 07:08:04 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Duan", "Yinglin", "", "1 and\n  3"], ["Shi", "Tianyang", "", "1 and\n  3"], ["Zou", "Zhengxia", "", "1 and\n  3"], ["Qin", "Jia", "", "1 and\n  3"], ["Zhao", "Yifei", "", "1 and 3"], ["Yuan", "Yi", "", "1 and 3"], ["Hou", "Jie", "", "1 and 3"], ["Wen", "Xiang", "", "1 and 3"], ["Fan", "Changjie", ""]]}, {"id": "2009.13216", "submitter": "Zahurul Haque", "authors": "Md. Zahurul Haque, Md. Rafiqul Isla", "title": "Traffic model of LTE using maximum flow algorithm with binary search\n  technique", "comments": null, "journal-ref": "IJCSIS September 2020, Vol. 18 No. 9 Publication", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inrecent time a rapid increase in the number of smart devices and user\napplications have generated an intensity volume of data traffic from/to a\ncellular network. So the Long Term Evaluation(LTE)network is facing some\nissuesdifficulties ofthebase station and infrastructure in terms of upgrade and\nconfiguration becausethere is no concept of BSC (Base Station Controller) of 2G\nand RNC (Radio Network Controller) of 3G to control several BTS/NB. Only 4G\n(LTE) all the eNBs areinterconnected for traffic flow from UE (user equipment)\nto core switch. Determination of capacity of a linkof such a network is a\nchallenging job since each node offers its own traffic andat the same time\nconveys traffic of other nodes.In this paper, we apply maximum flow algorithm\nincluding the binary search techniqueto solve the traffic flow of radio\nnetworkandinterconnected eNBs of the LTE network. The throughput of the LTE\nnetwork shown graphically under the QPSK and 16-QAM\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 11:10:27 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Haque", "Md. Zahurul", ""], ["Isla", "Md. Rafiqul", ""]]}, {"id": "2009.13304", "submitter": "Lucie Leveque", "authors": "Lucie L\\'ev\\^eque (UNIV GUSTAVE EIFFEL), Ji Yang, Xiaohan Yang,\n  Pengfei Guo, Kenneth Dasalla, Leida Li, Yingying Wu, Hantao Liu", "title": "Cuid: A new study of perceived image quality and its subjective\n  assessment", "comments": null, "journal-ref": "27th IEEE International Conference on Image Processing (ICIP), Oct\n  2020, Abu Dhabi, United Arab Emirates", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on image quality assessment (IQA) remains limited mainly due to our\nincomplete knowledge about human visual perception. Existing IQA algorithms\nhave been designed or trained with insufficient subjective data with a small\ndegree of stimulus variability. This has led to challenges for those algorithms\nto handle complexity and diversity of real-world digital content. Perceptual\nevidence from human subjects serves as a grounding for the development of\nadvanced IQA algorithms. It is thus critical to acquire reliable subjective\ndata with controlled perception experiments that faithfully reflect human\nbehavioural responses to distortions in visual signals. In this paper, we\npresent a new study of image quality perception where subjective ratings were\ncollected in a controlled lab environment. We investigate how quality\nperception is affected by a combination of different categories of images and\ndifferent types and levels of distortions. The database will be made publicly\navailable to facilitate calibration and validation of IQA algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:14:45 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["L\u00e9v\u00eaque", "Lucie", "", "UNIV GUSTAVE EIFFEL"], ["Yang", "Ji", ""], ["Yang", "Xiaohan", ""], ["Guo", "Pengfei", ""], ["Dasalla", "Kenneth", ""], ["Li", "Leida", ""], ["Wu", "Yingying", ""], ["Liu", "Hantao", ""]]}, {"id": "2009.13372", "submitter": "Jakub Nawa{\\l}a", "authors": "Jakub Nawa{\\l}a (1), Lucjan Janowski (1), Bogdan \\'Cmiel (2),\n  Krzysztof Rusek (1) ((1) AGH University of Science and Technology, Department\n  of Telecommunications, (2) AGH University of Science and Technology,\n  Department of Mathematical Analysis, Computational Mathematics and\n  Probability Methods)", "title": "Describing Subjective Experiment Consistency by $p$-Value P-P Plot", "comments": "11 pages, 3 figures. Accepted to 28th ACM International Conference on\n  Multimedia (MM '20). For associated data sets, source codes and\n  documentation, see https://github.com/Qub3k/subjective-exp-consistency-check", "journal-ref": null, "doi": "10.1145/3394171.3413749", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are phenomena that cannot be measured without subjective testing.\nHowever, subjective testing is a complex issue with many influencing factors.\nThese interplay to yield either precise or incorrect results. Researchers\nrequire a tool to classify results of subjective experiment as either\nconsistent or inconsistent. This is necessary in order to decide whether to\ntreat the gathered scores as quality ground truth data. Knowing if subjective\nscores can be trusted is key to drawing valid conclusions and building\nfunctional tools based on those scores (e.g., algorithms assessing the\nperceived quality of multimedia materials). We provide a tool to classify\nsubjective experiment (and all its results) as either consistent or\ninconsistent. Additionally, the tool identifies stimuli having irregular score\ndistribution. The approach is based on treating subjective scores as a random\nvariable coming from the discrete Generalized Score Distribution (GSD). The\nGSD, in combination with a bootstrapped G-test of goodness-of-fit, allows to\nconstruct $p$-value P-P plot that visualizes experiment's consistency. The tool\nsafeguards researchers from using inconsistent subjective data. In this way, it\nmakes sure that conclusions they draw and tools they build are more precise and\ntrustworthy. The proposed approach works in line with expectations drawn solely\non experiment design descriptions of 21 real-life multimedia quality subjective\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:40:45 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nawa\u0142a", "Jakub", ""], ["Janowski", "Lucjan", ""], ["\u0106miel", "Bogdan", ""], ["Rusek", "Krzysztof", ""]]}, {"id": "2009.13685", "submitter": "Yash Goyal", "authors": "Aayush Surana, Yash Goyal, Vinoo Alluri", "title": "Static and Dynamic Measures of Active Music Listening as Indicators of\n  Depression Risk", "comments": "Appearing in the proceedings of the Speech, Music and Mind Workshop\n  2020, a satellite workshop of INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music, an integral part of our lives, which is not only a source of\nentertainment but plays an important role in mental well-being by impacting\nmoods, emotions and other affective states. Music preferences and listening\nstrategies have been shown to be associated with the psychological well-being\nof listeners including internalized symptomatology and depression. However,\ntill date no studies exist that examine time-varying music consumption, in\nterms of acoustic content, and its association with users' well-being. In the\ncurrent study, we aim at unearthing static and dynamic patterns prevalent in\nactive listening behavior of individuals which may be used as indicators of\nrisk for depression. Mental well-being scores and listening histories of 541\nLast.fm users were examined. Static and dynamic acoustic and emotion-related\nfeatures were extracted from each user's listening history and correlated with\ntheir mental well-being scores. Results revealed that individuals with greater\ndepression risk resort to higher dependency on music with greater\nrepetitiveness in their listening activity. Furthermore, the affinity of\ndepressed individuals towards music that can be perceived as sad was found to\nbe resistant to change over time. This study has large implications for future\nwork in the area of assessing mental illness risk by exploiting digital\nfootprints of users via online music streaming platforms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 23:29:53 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Surana", "Aayush", ""], ["Goyal", "Yash", ""], ["Alluri", "Vinoo", ""]]}, {"id": "2009.13737", "submitter": "Jun Fu", "authors": "Jun Fu, Zhibo Chen, Xiaoming Chen, Weiping Li", "title": "Sequential Reinforced 360-Degree Video Adaptive Streaming with\n  Cross-user Attentive Network", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the tile-based 360-degree video streaming, predicting user's future\nviewpoints and developing adaptive bitrate (ABR) algorithms are essential for\noptimizing user's quality of experience (QoE). Traditional single-user based\nviewpoint prediction methods fail to achieve good performance in long-term\nprediction, and the recently proposed reinforcement learning (RL) based ABR\nschemes applied in traditional video streaming can not be directly applied in\nthe tile-based 360-degree video streaming due to the exponential action space.\nTherefore, we propose a sequential reinforced 360-degree video streaming scheme\nwith cross-user attentive network. Firstly, considering different users may\nhave the similar viewing preference on the same video, we propose a cross-user\nattentive network (CUAN), boosting the performance of long-term viewpoint\nprediction by selectively utilizing cross-user information. Secondly, we\npropose a sequential RL-based (360SRL) ABR approach, transforming action space\nsize of each decision step from exponential to linear via introducing a\nsequential decision structure. We evaluate the proposed CUAN and 360SRL using\ntrace-driven experiments and experimental results demonstrate that CUAN and\n360SRL outperform existing viewpoint prediction and ABR approaches with a\nnoticeable margin.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 02:54:52 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Fu", "Jun", ""], ["Chen", "Zhibo", ""], ["Chen", "Xiaoming", ""], ["Li", "Weiping", ""]]}, {"id": "2009.13792", "submitter": "K K Thyagharajan", "authors": "S. D. Lalitha, K. K. Thyagharajan", "title": "Micro-Facial Expression Recognition in Video Based on Optimal\n  Convolutional Neural Network (MFEOCNN) Algorithm", "comments": "19 pages, 10 figures, \"for published version see\n  https://www.ijeat.org/wp-content/uploads/papers/v9i1/A9802109119.pdf\"", "journal-ref": null, "doi": "10.35940/ijeat.A9802.109119", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression is a standout amongst the most imperative features of human\nemotion recognition. For demonstrating the emotional states facial expressions\nare utilized by the people. In any case, recognition of facial expressions has\npersisted a testing and intriguing issue with regards to PC vision. Recognizing\nthe Micro-Facial expression in video sequence is the main objective of the\nproposed approach. For efficient recognition, the proposed method utilizes the\noptimal convolution neural network. Here the proposed method considering the\ninput dataset is the CK+ dataset. At first, by means of Adaptive median\nfiltering preprocessing is performed in the input image. From the preprocessed\noutput, the extracted features are Geometric features, Histogram of Oriented\nGradients features and Local binary pattern features. The novelty of the\nproposed method is, with the help of Modified Lion Optimization (MLO)\nalgorithm, the optimal features are selected from the extracted features. In a\nshorter computational time, it has the benefits of rapidly focalizing and\neffectively acknowledging with the aim of getting an overall arrangement or\nidea. Finally, the recognition is done by Convolution Neural network (CNN).\nThen the performance of the proposed MFEOCNN method is analysed in terms of\nfalse measures and recognition accuracy. This kind of emotion recognition is\nmainly used in medicine, marketing, E-learning, entertainment, law and\nmonitoring. From the simulation, we know that the proposed approach achieves\nmaximum recognition accuracy of 99.2% with minimum Mean Absolute Error (MAE)\nvalue. These results are compared with the existing for MicroFacial Expression\nBased Deep-Rooted Learning (MFEDRL), Convolutional Neural Network with Lion\nOptimization (CNN+LO) and Convolutional Neural Network (CNN) without\noptimization. The simulation of the proposed method is done in the working\nplatform of MATLAB.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 05:56:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Lalitha", "S. D.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.13862", "submitter": "Jiuniu Wang", "authors": "Wenjia Xu, Jiuniu Wang, Yang Wang, Guangluan Xu, Wei Dai, Yirong Wu", "title": "Where is the Model Looking At?--Concentrate and Explain the Network\n  Attention", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 14, no.\n  3, pp. 506-516, March 2020", "doi": "10.1109/JSTSP.2020.2987729", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification models have achieved satisfactory performance on many\ndatasets, sometimes even better than human. However, The model attention is\nunclear since the lack of interpretability. This paper investigates the\nfidelity and interpretability of model attention. We propose an Explainable\nAttribute-based Multi-task (EAT) framework to concentrate the model attention\non the discriminative image area and make the attention interpretable. We\nintroduce attributes prediction to the multi-task learning network, helping the\nnetwork to concentrate attention on the foreground objects. We generate\nattribute-based textual explanations for the network and ground the attributes\non the image to show visual explanations. The multi-model explanation can not\nonly improve user trust but also help to find the weakness of network and\ndataset. Our framework can be generalized to any basic model. We perform\nexperiments on three datasets and five basic models. Results indicate that the\nEAT framework can give multi-modal explanations that interpret the network\ndecision. The performance of several recognition approaches is improved by\nguiding network attention.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:36:18 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Xu", "Wenjia", ""], ["Wang", "Jiuniu", ""], ["Wang", "Yang", ""], ["Xu", "Guangluan", ""], ["Dai", "Wei", ""], ["Wu", "Yirong", ""]]}, {"id": "2009.13931", "submitter": "Yanhong Leng", "authors": "Xinquan Zhou, Yanhong Leng", "title": "Residual acoustic echo suppression based on efficient multi-task\n  convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic echo degrades the user experience in voice communication systems\nthus needs to be suppressed completely. We propose a real-time residual\nacoustic echo suppression (RAES) method using an efficient convolutional neural\nnetwork. The double talk detector is used as an auxiliary task to improve the\nperformance of RAES in the context of multi-task learning. The training\ncriterion is based on a novel loss function, which we call as the suppression\nloss, to balance the suppression of residual echo and the distortion of\nnear-end signals. The experimental results show that the proposed method can\nefficiently suppress the residual echo under different circumstances.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 11:26:25 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 03:33:36 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Zhou", "Xinquan", ""], ["Leng", "Yanhong", ""]]}, {"id": "2009.14059", "submitter": "Ruichen Li", "authors": "Ruichen Li, JingWen Hu, Shuai Guo, Jinming Zhao", "title": "MUSE2020 challenge report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a brief report for MUSE2020 challenge. We present our solution\nfor Muse-Wild sub challenge. The aim of this challenge is to investigate\nsentiment analysis method in real-world situation. Our solutions achieve the\nbest CCC performance of 0.4670, 0.3571 for arousal, and valence respectively on\nthe challenge validation set, which outperforms the baseline system with\ncorresponding CCC of 0.3078 and 1506.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 04:39:45 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Ruichen", ""], ["Hu", "JingWen", ""], ["Guo", "Shuai", ""], ["Zhao", "Jinming", ""]]}, {"id": "2009.14165", "submitter": "Ludovic Roux", "authors": "Ludovic Roux and Alexandre Gouaillard", "title": "Performance of AV1 Real-Time Mode", "comments": "8 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With COVID-19, the interest for digital interactions has raised, putting in\nturn real-time (or low-latency) codecs into a new light. Most of the codec\nresearch has been traditionally focusing on coding efficiency, while very\nlittle literature exist on real-time codecs. It is shown how the speed at which\ncontent is made available impacts both latency and throughput. The authors\nintroduce a new test set up, integrating a paced reader, which allows to run\ncodec in the same condition as real-time media capture. Quality measurements\nusing VMAF, as well as multiple speed measurements are made on encoding of HD\nand full HD video sequences, both at 25 fps and 50 fps to compare the\nrespective performances of several implementations of the H.264, H.265, VP8,\nVP9 and AV1 codecs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 17:28:15 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 01:33:21 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Roux", "Ludovic", ""], ["Gouaillard", "Alexandre", ""]]}, {"id": "2009.14405", "submitter": "Yiqing Huang", "authors": "Yiqing Huang, Jiansheng Chen", "title": "Teacher-Critical Training Strategies for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image captioning models are usually trained by cross-entropy (XE)\nloss and reinforcement learning (RL), which set ground-truth words as hard\ntargets and force the captioning model to learn from them. However, the widely\nadopted training strategies suffer from misalignment in XE training and\ninappropriate reward assignment in RL training. To tackle these problems, we\nintroduce a teacher model that serves as a bridge between the ground-truth\ncaption and the caption model by generating some easier-to-learn word proposals\nas soft targets. The teacher model is constructed by incorporating the\nground-truth image attributes into the baseline caption model. To effectively\nlearn from the teacher model, we propose Teacher-Critical Training Strategies\n(TCTS) for both XE and RL training to facilitate better learning processes for\nthe caption model. Experimental evaluations of several widely adopted caption\nmodels on the benchmark MSCOCO dataset show the proposed TCTS comprehensively\nenhances most evaluation metrics, especially the Bleu and Rouge-L scores, in\nboth training stages. TCTS is able to achieve to-date the best published single\nmodel Bleu-4 and Rouge-L performances of 40.2% and 59.4% on the MSCOCO Karpathy\ntest split. Our codes and pre-trained models will be open-sourced.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 03:15:12 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Huang", "Yiqing", ""], ["Chen", "Jiansheng", ""]]}, {"id": "2009.14525", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Edward Curry", "title": "Visual Semantic Multimedia Event Model for Complex Event Detection in\n  Video Streams", "comments": "15 pages, 14 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia data is highly expressive and has traditionally been very\ndifficult for a machine to interpret. Middleware systems such as complex event\nprocessing (CEP) mine patterns from data streams and send notifications to\nusers in a timely fashion. Presently, CEP systems have inherent limitations to\nprocess multimedia streams due to its data complexity and the lack of an\nunderlying structured data model. In this work, we present a visual event\nspecification method to enable complex multimedia event processing by creating\na semantic knowledge representation derived from low-level media streams. The\nmethod enables the detection of high-level semantic concepts from the media\nstreams using an ensemble of pattern detection capabilities. The semantic model\nis aligned with a multimedia CEP engine deep learning models to give\nflexibility to end-users to build rules using spatiotemporal event calculus.\nThis enhances CEP capability to detect patterns from media streams and bridge\nthe semantic gap between highly expressive knowledge-centric user queries to\nthe low-level features of the multi-media data. We have built a small traffic\nevent ontology prototype to validate the approach and performance. The paper\ncontribution is threefold: i) we present a knowledge graph representation for\nmultimedia streams, ii) a hierarchical event network to detect visual patterns\nfrom media streams and iii) define complex pattern rules for complex multimedia\nevent reasoning using event calculus\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 09:22:23 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Yadav", "Piyush", ""], ["Curry", "Edward", ""]]}]