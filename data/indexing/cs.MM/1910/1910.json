[{"id": "1910.01221", "submitter": "Bingyang Wen", "authors": "Bingyang Wen and Sergul Aydore", "title": "ROMark: A Robust Watermarking System Using Adversarial Training", "comments": "5 pages, 1 figure, Machine Learning with Guarantees workshop at\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability and easy access to digital communication increase the risk\nof copyrighted material piracy. In order to detect illegal use or distribution\nof data, digital watermarking has been proposed as a suitable tool. It protects\nthe copyright of digital content by embedding imperceptible information into\nthe data in the presence of an adversary. The goal of the adversary is to\nremove the copyrighted content of the data. Therefore, an efficient\nwatermarking framework must be robust to multiple image-processing operations\nknown as attacks that can alter embedded copyright information. Another line of\nresearch \\textit{adversarial machine learning} also tackles with similar\nproblems to guarantee robustness to imperceptible perturbations of the input.\nIn this work, we propose to apply robust optimization from adversarial machine\nlearning to improve the robustness of a CNN-based watermarking framework. Our\nexperimental results on the COCO dataset show that the robustness of a\nwatermarking framework can be improved by utilizing robust optimization in\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 21:05:05 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Wen", "Bingyang", ""], ["Aydore", "Sergul", ""]]}, {"id": "1910.01546", "submitter": "Chih Han Chung", "authors": "Yi-Ting Chen, Chi-Hsuan Hsu, Chih-Han Chung, Yu-Shuen Wang, Sabarish\n  V. Babu", "title": "iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking\n  Interface for Study and Reflection in VR Learning Environments", "comments": "9 pages, IEEE VR 2019,\n  https://people.cs.nctu.edu.tw/~yushuen/data/iVRNote19.mp4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we design, implement and evaluate the pedagogical\nbenefits of a novel interactive note taking interface (iVRNote) in VR for the\npurpose of learning and reflection lectures. In future VR learning\nenvironments, students would have challenges in taking notes when they wear a\nhead mounted display (HMD). To solve this problem, we installed a digital\ntablet on the desk and provided several tools in VR to facilitate the learning\nexperience. Specifically, we track the stylus position and orientation in the\nphysical world and then render a virtual stylus in VR. In other words, when\nstudents see a virtual stylus somewhere on the desk, they can reach out with\ntheir hand for the physical stylus. The information provided will also enable\nthem to know where they will draw or write before the stylus touches the\ntablet. Since the presented iVRNote featuring our note taking system is a\ndigital environment, we also enable students save efforts in taking extensive\nnotes by providing several functions, such as post-editing and picture taking,\nso that they can pay more attention to lectures in VR. We also record the time\nof each stroke on the note to help students review a lecture. They can select a\npart of their note to revisit the corresponding segment in a virtual online\nlecture. Figures and the accompanying video demonstrate the feasibility of the\npresented iVRNote system. To evaluate the system, we conducted a user study\nwith 20 participants to assess the preference and pedagogical benefits of the\niVRNote interface.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 15:11:32 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Chen", "Yi-Ting", ""], ["Hsu", "Chi-Hsuan", ""], ["Chung", "Chih-Han", ""], ["Wang", "Yu-Shuen", ""], ["Babu", "Sabarish V.", ""]]}, {"id": "1910.01586", "submitter": "David Murphy", "authors": "David Murphy and Conor Higgins", "title": "Secondary Inputs for Measuring User Engagement in Immersive VR Education\n  Environments", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an experiment to assess the feasibility of using\nsecondary input data as a method of determining user engagement in immersive\nvirtual reality (VR). The work investigates whether secondary data (biosignals)\nacquired from users are useful as a method of detecting levels of\nconcentration, stress, relaxation etc. in immersive environments, and if they\ncould be used to create an affective feedback loop in immersive VR\nenvironments, including educational contexts. A VR Experience was developed in\nthe Unity game engine, with three different levels, each designed to expose the\nuser in one of three different states (relaxation, concentration, stress).\nWhile in the VR Experience users physiological responses were measured using\nECG and EEG sensors. After the experience users completed questionnaires to\nestablish their perceived state during the levels, and to established the\nusability of the system. Next a comparison between the reported levels of\nemotion and the measured signals is presented, which show a strong\ncorrespondence between the two measures indicating that biosignals are a useful\nindicator of emotional state while in VR. Finally we make some recommendations\non the practicalities of using biosensors, and design considerations for their\nincorporation in to a VR system, with particular focus on their integration in\nto task-based training and educational virtual environments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:39:28 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Murphy", "David", ""], ["Higgins", "Conor", ""]]}, {"id": "1910.01795", "submitter": "Bo Wu", "authors": "Bo Wu, Wen-Huang Cheng, Peiye Liu, Bei Liu, Zhaoyang Zeng, Jiebo Luo", "title": "SMP Challenge: An Overview of Social Media Prediction Challenge 2019", "comments": "ACM MM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"SMP Challenge\" aims to discover novel prediction tasks for numerous data on\nsocial multimedia and seek excellent research teams. Making predictions via\nsocial multimedia data (e.g. photos, videos or news) is not only helps us to\nmake better strategic decisions for the future, but also explores advanced\npredictive learning and analytic methods on various problems and scenarios,\nsuch as multimedia recommendation, advertising system, fashion analysis etc.\n  In the SMP Challenge at ACM Multimedia 2019, we introduce a novel prediction\ntask Temporal Popularity Prediction, which focuses on predicting future\ninteraction or attractiveness (in terms of clicks, views or likes etc.) of new\nonline posts in social media feeds before uploading. We also collected and\nreleased a large-scale SMPD benchmark with over 480K posts from 69K users. In\nthis paper, we define the challenge problem, give an overview of the dataset,\npresent statistics of rich information for data and annotation and design the\naccuracy and correlation evaluation metrics for temporal popularity prediction\nto the challenge.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 04:10:43 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 05:06:20 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wu", "Bo", ""], ["Cheng", "Wen-Huang", ""], ["Liu", "Peiye", ""], ["Liu", "Bei", ""], ["Zeng", "Zhaoyang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1910.01933", "submitter": "Pavel Korshunov", "authors": "Pavel Korshunov and S\\'ebastien Marcel", "title": "Vulnerability of Face Recognition to Deep Morphing", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.08685", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly easy to automatically swap faces in images and video or\nmorph two faces into one using generative adversarial networks (GANs). The high\nquality of the resulted deep-morph raises the question of how vulnerable the\ncurrent face recognition systems are to such fake images and videos. It also\ncalls for automated ways to detect these GAN-generated faces. In this paper, we\npresent the publicly available dataset of the Deepfake videos with faces\nmorphed with a GAN-based algorithm. To generate these videos, we used open\nsource software based on GANs, and we emphasize that training and blending\nparameters can significantly impact the quality of the resulted videos. We show\nthat the state of the art face recognition systems based on VGG and Facenet\nneural networks are vulnerable to the deep morph videos, with 85.62 and 95.00\nfalse acceptance rates, respectively, which means methods for detecting these\nvideos are necessary. We consider several baseline approaches for detecting\ndeep morphs and find that the method based on visual quality metrics (often\nused in presentation attack detection domain) leads to the best performance\nwith 8.97 equal error rate. Our experiments demonstrate that GAN-generated deep\nmorph videos are challenging for both face recognition systems and existing\ndetection methods, and the further development of deep morphing technologies\nwill make it even more so.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 12:34:08 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Korshunov", "Pavel", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "1910.02334", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Benet Oriol Sabat, Cristian Canton Ferrer, Xavier Giro-i-Nieto", "title": "Hate Speech in Pixels: Detection of Offensive Memes towards Automatic\n  Moderation", "comments": "AI for Social Good Workshop at NeurIPS 2019 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the challenge of hate speech detection in Internet memes,\nand attempts using visual information to automatically detect hate speech,\nunlike any previous work of our knowledge. Memes are pixel-based multimedia\ndocuments that contain photos or illustrations together with phrases which,\nwhen combined, usually adopt a funny meaning. However, hate memes are also used\nto spread hate through social networks, so their automatic detection would help\nreduce their harmful societal impact. Our results indicate that the model can\nlearn to detect some of the memes, but that the task is far from being solved\nwith this simple architecture. While previous work focuses on linguistic hate\nspeech, our experiments indicate how the visual modality can be much more\ninformative for hate speech detection than the linguistic one in memes. In our\nexperiments, we built a dataset of 5,020 memes to train and evaluate a\nmulti-layer perceptron over the visual and language representations, whether\nindependently or fused. The source code and mode and models are available\nhttps://github.com/imatge-upc/hate-speech-detection .\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 22:05:43 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Sabat", "Benet Oriol", ""], ["Ferrer", "Cristian Canton", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1910.02932", "submitter": "Kashif Ahmad Dr", "authors": "Kashif Ahmad, Konstantin Pogorelov, Mohib Ullah, Michael Riegler,\n  Nicola Conci, Johannes Langguth, Ala Al-Fuqaha", "title": "Multi-Modal Machine Learning for Flood Detection in News, Social Media\n  and Satellite Sequences", "comments": null, "journal-ref": "MediaEval 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our methods for the MediaEval 2019 Mul-timedia\nSatellite Task, which is aiming to extract complementaryinformation associated\nwith adverse events from Social Media andsatellites. For the first challenge,\nwe propose a framework jointly uti-lizing colour, object and scene-level\ninformation to predict whetherthe topic of an article containing an image is a\nflood event or not.Visual features are combined using early and late fusion\ntechniquesachieving an average F1-score of82.63,82.40,81.40and76.77. Forthe\nmulti-modal flood level estimation, we rely on both visualand textual\ninformation achieving an average F1-score of58.48and46.03, respectively.\nFinally, for the flooding detection in time-based satellite image sequences we\nused a combination of classicalcomputer-vision and machine learning approaches\nachieving anaverage F1-score of58.82%\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:49:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ahmad", "Kashif", ""], ["Pogorelov", "Konstantin", ""], ["Ullah", "Mohib", ""], ["Riegler", "Michael", ""], ["Conci", "Nicola", ""], ["Langguth", "Johannes", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1910.03281", "submitter": "Gabriele D'Angelo", "authors": "Gyordan Caminati, Sara Kiade, Gabriele D'Angelo, Stefano Ferretti,\n  Vittorio Ghini", "title": "Fast Session Resumption in DTLS for Mobile Communications", "comments": "Proceedings of the IEEE Consumer Communications and Networking\n  Conference 2020 (CCNC 2020)", "journal-ref": null, "doi": "10.1109/CCNC46108.2020.9045119", "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DTLS is a protocol that provides security guarantees to Internet\ncommunications. It can operate on top of both TCP and UDP transport protocols.\nThus, it is particularly suited for peer-to-peer and distributed multimedia\napplications. The same holds if the endpoints are mobile devices. In this\nscenario, mechanisms are needed to surmount possible network disconnections,\noften arising due to the mobility or the scarce resources of devices, that can\njeopardize the quality of the communications. Session resumption is thus a main\nissue to deal with. To this aim, we propose a fast reconnection scheme that\nemploys non-connected sockets to quickly resume DTLS communication sessions.\nThe proposed scheme is assessed in a performance evaluation that confirms its\nviability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 08:56:05 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:22:48 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Caminati", "Gyordan", ""], ["Kiade", "Sara", ""], ["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Ghini", "Vittorio", ""]]}, {"id": "1910.04416", "submitter": "Kashif Ahmad", "authors": "Syed Zohaib, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha", "title": "Sentiment Analysis from Images of Natural Disasters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media have been widely exploited to detect and gather relevant\ninformation about opinions and events. However, the relevance of the\ninformation is very subjective and rather depends on the application and the\nend-users. In this article, we tackle a specific facet of social media data\nprocessing, namely the sentiment analysis of disaster-related images by\nconsidering people's opinions, attitudes, feelings and emotions. We analyze how\nvisual sentiment analysis can improve the results for the\nend-users/beneficiaries in terms of mining information from social media. We\nalso identify the challenges and related applications, which could help\ndefining a benchmark for future research efforts in visual sentiment analysis.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:59:11 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Zohaib", "Syed", ""], ["Ahmad", "Kashif", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1910.04433", "submitter": "Yang Hao", "authors": "Hao Yang, Zhongliang Yang, YongJian Bao, and Yongfeng Huang", "title": "Hierarchical Representation Network for Steganalysis of QIM\n  Steganography in Low-Bit-Rate Speech Signals", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the Volume of Voice over IP (VoIP) traffic rises shapely, more and more\nVoIP-based steganography methods have emerged in recent years, which poses a\ngreat threat to the security of cyberspace. Low bit-rate speech codecs are\nwidely used in the VoIP application due to its powerful compression capability.\nQIM steganography makes it possible to hide secret information in VoIP streams.\nPrevious research mostly focus on capturing the inter-frame correlation or\ninner-frame correlation features in code-words but ignore the hierarchical\nstructure which exists in speech frame. In this paper, motivated by the complex\nmulti-scale structure, we design a Hierarchical Representation Network to\ntackle the steganalysis of QIM steganography in low-bit-rate speech signal. In\nthe proposed model, Convolution Neural Network (CNN) is used to model the\nhierarchical structure in the speech frame, and three level of attention\nmechanisms are applied at different convolution block, enabling it to attend\ndifferentially to more and less important content in speech frame. Experiments\ndemonstrated that the steganalysis performance of the proposed method can\noutperforms the state-of-the-art methods especially in detecting both short and\nlow embeded speech samples. Moreover, our model needs less computation and has\nhigher time efficiency to be applied to real online services.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 08:40:15 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Yang", "Hao", ""], ["Yang", "Zhongliang", ""], ["Bao", "YongJian", ""], ["Huang", "Yongfeng", ""]]}, {"id": "1910.04697", "submitter": "Tommy Nilsson", "authors": "Glyn Lawson, Emily Shaw, Tessa Roper, Tommy Nilsson, Laura\n  Bajorunaite, Ayesha Batool", "title": "Immersive virtual worlds: Multi-sensory virtual environments for health\n  and safety training", "comments": "IOSH, (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual environments (VEs) offer potential benefits to health and safety\ntraining: exposure to dangerous (virtual) environments; the opportunity for\nexperiential learning; and a high level of control over the training, in that\naspects can be repeated or reviewed based on the trainee's performance.\nHowever, VEs are typically presented as audiovisual (AV) systems, whereas\nengagement of other senses could increase the immersion in the virtual\nexperience. Moreover, other senses play a key role in certain health and safety\ncontexts, for example the feel of heat and smell in a fire or smell in a fuel\nleak. A multisensory (MS) VE was developed, which provided simulated heat and\nsmell in accordance with events in a virtual world. As users approached a\nvirtual fire, they felt heat from three 2 kW heaters and smelled smoke from a\nscent diffuser. Behaviours in the MS VE demonstrated higher validity than those\nin a comparable AV VE, which ratings and verbatim responses indicated was down\nto a greater belief that participants were in a real fire. However, a study of\nthe effectiveness of the MS VE as a training tool demonstrated that it did not\noffer benefits over AV as measured by a written knowledge test and subjective\nratings of engagement, attitude towards health and safety and desire to repeat.\nHowever, the study found further evidence for the use of AV VEs in health and\nsafety training, particularly as the subjective ratings were generally better\nthan for PowerPoint based training. Despite the lack of evidence for MS\nsimulation on traditional measures of training, the different attitudes and\nexperiences of users suggest that it may have value as a system for changing\ntrainees' attitudes towards their personal safety and awareness. This view was\nsupported by feedback from industrial partners.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 18:26:33 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Lawson", "Glyn", ""], ["Shaw", "Emily", ""], ["Roper", "Tessa", ""], ["Nilsson", "Tommy", ""], ["Bajorunaite", "Laura", ""], ["Batool", "Ayesha", ""]]}, {"id": "1910.04944", "submitter": "Alireza Tavakkoli", "authors": "Lee Easson and Alireza Tavakkoli and Jonathan Greenberg", "title": "An Automatic Digital Terrain Generation Technique for Terrestrial\n  Sensing and Virtual Reality Applications", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-33720-9_48", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and modeling of the terrain from point cloud data is an\nimportant component of Terrestrial Remote Sensing (TRS) applications. The main\nfocus in terrain modeling is capturing details of complex geological features\nof landforms. Traditional terrain modeling approaches rely on the user to exert\ncontrol over terrain features. However, relying on the user input to manually\ndevelop the digital terrain becomes intractable when considering the amount of\ndata generated by new remote sensing systems capable of producing massive\naerial and ground-based point clouds from scanned environments. This article\nprovides a novel terrain modeling technique capable of automatically generating\naccurate and physically realistic Digital Terrain Models (DTM) from a variety\nof point cloud data. The proposed method runs efficiently on large-scale point\ncloud data with real-time performance over large segments of terrestrial\nlandforms. Moreover, generated digital models are designed to effectively\nrender within a Virtual Reality (VR) environment in real time. The paper\nconcludes with an in-depth discussion of possible research directions and\noutstanding technical and scientific challenges to improve the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 02:26:01 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Easson", "Lee", ""], ["Tavakkoli", "Alireza", ""], ["Greenberg", "Jonathan", ""]]}, {"id": "1910.04964", "submitter": "Xin Wang", "authors": "Wenwu Zhu, Xin Wang, Hongzhi Li", "title": "Multi-modal Deep Analysis for Multimedia", "comments": "25 pages, 39 figures, IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2940647", "report-no": null, "categories": "cs.MM cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of Internet and multimedia services in the past\ndecade, a huge amount of user-generated and service provider-generated\nmultimedia data become available. These data are heterogeneous and multi-modal\nin nature, imposing great challenges for processing and analyzing them.\nMulti-modal data consist of a mixture of various types of data from different\nmodalities such as texts, images, videos, audios etc. In this article, we\npresent a deep and comprehensive overview for multi-modal analysis in\nmultimedia. We introduce two scientific research problems, data-driven\ncorrelational representation and knowledge-guided fusion for multimedia\nanalysis. To address the two scientific problems, we investigate them from the\nfollowing aspects: 1) multi-modal correlational representation: multi-modal\nfusion of data across different modalities, and 2) multi-modal data and\nknowledge fusion: multi-modal fusion of data with domain knowledge. More\nspecifically, on data-driven correlational representation, we highlight three\nimportant categories of methods, such as multi-modal deep representation,\nmulti-modal transfer learning, and multi-modal hashing. On knowledge-guided\nfusion, we discuss the approaches for fusing knowledge with data and four\nexemplar applications that require various kinds of domain knowledge, including\nmulti-modal visual question answering, multi-modal video summarization,\nmulti-modal visual pattern mining and multi-modal recommendation. Finally, we\nbring forward our insights and future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:21:36 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 08:42:13 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhu", "Wenwu", ""], ["Wang", "Xin", ""], ["Li", "Hongzhi", ""]]}, {"id": "1910.05304", "submitter": "Soumen Kanrar", "authors": "Soumen Kanrar, Soamdeep Singha", "title": "Content Delivery Through Hybrid Architecture in Video on Demand System", "comments": "13 pages, 13 figures", "journal-ref": "Ingenierie des Systemes d'Information, ISSN: 1633-1311, Vol.24,\n  No.3, pp.289-301,2019", "doi": "10.18280/isi.240309", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer-to-Peer (P2P) network needs architectural modification for smooth and\nfast transportation of video content. The viewer imports chunk video objects\nthrough the proxy server. The enormous growth of user requests in a small\nsession of time creates huge load on the VOD system. The situation requires\neither the proxy server streamed video-content fully or partly to the viewers.\nThe missing chunk at the proxy server is imported from the connected peer\nnodes. Peers exchange chunks among themselves according to some chunk selection\npolicy. Peer node randomly contacts another peer to download a missing chunk\nfrom the buffers during each time slot. In video streaming, when the relevant\nframe is required at the viewer ends that should be available at the respective\nproxy server. The video watcher also initiates various types of interactive\noperations like a move forward or skips some finite number of frames that\ncreate congestion inside the VOD system. To elevate the situation it needs an\neffective content delivery mechanism for smooth transportation of content. The\nproposed hybrid architecture is composed of P2P and mesh architecture that\neffectively enhances the search mechanism and content transportation in the VOD\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:59:23 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Kanrar", "Soumen", ""], ["Singha", "Soamdeep", ""]]}, {"id": "1910.05770", "submitter": "Lamberto Ballan", "authors": "Tobia Tesan, Pasquale Coscia, Lamberto Ballan", "title": "A CNN-RNN Framework for Image Annotation from Visual Cues and Social\n  Network Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images represent a commonly used form of visual communication among people.\nNevertheless, image classification may be a challenging task when dealing with\nunclear or non-common images needing more context to be correctly annotated.\nMetadata accompanying images on social-media represent an ideal source of\nadditional information for retrieving proper neighborhoods easing image\nannotation task. To this end, we blend visual features extracted from neighbors\nand their metadata to jointly leverage context and visual cues. Our models use\nmultiple semantic embeddings to achieve the dual objective of being robust to\nvocabulary changes between train and test sets and decoupling the architecture\nfrom the low-level metadata representation. Convolutional and recurrent neural\nnetworks (CNNs-RNNs) are jointly adopted to infer similarity among neighbors\nand query images. We perform comprehensive experiments on the NUS-WIDE dataset\nshowing that our models outperform state-of-the-art architectures based on\nimages and metadata, and decrease both sensory and semantic gaps to better\nannotate images.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 15:24:48 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 17:19:26 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Tesan", "Tobia", ""], ["Coscia", "Pasquale", ""], ["Ballan", "Lamberto", ""]]}, {"id": "1910.06005", "submitter": "Konstantin Schall", "authors": "Kai Uwe Barthel, Nico Hezel, Konstantin Schall, Klaus Jung", "title": "Real-Time Visual Navigation in Huge Image Sets Using Similarity Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays stock photo agencies often have millions of images. Non-stop viewing\nof 20 million images at a speed of 10 images per second would take more than\nthree weeks. This demonstrates the impossibility to inspect all images and the\ndifficulty to get an overview of the entire collection. Although there has been\na lot of effort to improve visual image search, there is little research and\nsupport for visual image exploration. Typically, users start \"exploring\" an\nimage collection with a keyword search or an example image for a similarity\nsearch. Both searches lead to long unstructured lists of result images. In\nearlier publications, we introduced the idea of graph-based image navigation\nand proposed an efficient algorithm for building hierarchical image similarity\ngraphs for dynamically changing image collections. In this demo we showcase\nreal-time visual exploration of millions of images with a standard web browser.\nSubsets of images are successively retrieved from the graph and displayed as a\nvisually sorted 2D image map, which can be zoomed and dragged to explore\nrelated concepts. Maintaining the positions of previously shown images creates\nthe impression of an \"endless map\". This approach allows an easy visual\nimage-based navigation, while preserving the complex image relationships of the\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:24:46 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Barthel", "Kai Uwe", ""], ["Hezel", "Nico", ""], ["Schall", "Konstantin", ""], ["Jung", "Klaus", ""]]}, {"id": "1910.06043", "submitter": "Huan Peng", "authors": "Huan Peng, Yuan Zhang, Yongbei Yang, Jinyao Yan", "title": "A Hybrid Control Scheme for Adaptive Live Streaming", "comments": "5 pages, ACMMM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The live streaming is more challenging than on-demand streaming, because the\nlow latency is also a strong requirement in addition to the trade-off between\nvideo quality and jitters in playback. To balance several inherently\nconflicting performance metrics and improve the overall quality of experience\n(QoE), many adaptation schemes have been proposed. Bitrate adaptation is one of\nthe major solutions for video streaming under time-varying network conditions,\nwhich works even better combining with some latency control methods, such as\nadaptive playback rate control and frame dropping. However, it still remains a\nchallenging problem to design an algorithm to combine these adaptation schemes\ntogether. To tackle this problem, we propose a hybrid control scheme for\nadaptive live streaming, namely HYSA, based on heuristic playback rate control,\nlatency-constrained bitrate control and QoE-oriented adaptive frame dropping.\nThe proposed scheme utilizes Kaufman's Adaptive Moving Average (KAMA) to\npredict segment bitrates for better rate decisions. Extensive simulations\ndemonstrate that HYSA outperforms most of the existing adaptation schemes on\noverall QoE.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 11:26:08 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Peng", "Huan", ""], ["Zhang", "Yuan", ""], ["Yang", "Yongbei", ""], ["Yan", "Jinyao", ""]]}, {"id": "1910.06180", "submitter": "Vlad Hosu", "authors": "Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe", "title": "KonIQ-10k: An ecologically valid database for deep learning of blind\n  image quality assessment", "comments": "Published", "journal-ref": "Trans. Image Proc. 29 (2020) 4041-4056", "doi": "10.1109/TIP.2020.2967829", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods for image quality assessment (IQA) are limited due to\nthe small size of existing datasets. Extensive datasets require substantial\nresources both for generating publishable content and annotating it accurately.\nWe present a systematic and scalable approach to creating KonIQ-10k, the\nlargest IQA dataset to date, consisting of 10,073 quality scored images. It is\nthe first in-the-wild database aiming for ecological validity, concerning the\nauthenticity of distortions, the diversity of content, and quality-related\nindicators. Through the use of crowdsourcing, we obtained 1.2 million reliable\nquality ratings from 1,459 crowd workers, paving the way for more general IQA\nmodels. We propose a novel, deep learning model (KonCept512), to show an\nexcellent generalization beyond the test set (0.921 SROCC), to the current\nstate-of-the-art database LIVE-in-the-Wild (0.825 SROCC). The model derives its\ncore performance from the InceptionResNet architecture, being trained at a\nhigher resolution than previous models (512x384). Correlation analysis shows\nthat KonCept512 performs similar to having 9 subjective scores for each test\nimage.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:38:48 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 09:40:51 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Hosu", "Vlad", ""], ["Lin", "Hanhe", ""], ["Sziranyi", "Tamas", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1910.06514", "submitter": "Takashi Matsubara", "authors": "Takashi Matsubara", "title": "Target-Oriented Deformation of Visual-Semantic Embedding Space", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal embedding is a crucial research topic for cross-modal\nunderstanding, data mining, and translation. Many studies have attempted to\nextract representations from given entities and align them in a shared\nembedding space. However, because entities in different modalities exhibit\ndifferent abstraction levels and modality-specific information, it is\ninsufficient to embed related entities close to each other. In this study, we\npropose the Target-Oriented Deformation Network (TOD-Net), a novel module that\ncontinuously deforms the embedding space into a new space under a given\ncondition, thereby adjusting similarities between entities. Unlike methods\nbased on cross-modal attention, TOD-Net is a post-process applied to the\nembedding space learned by existing embedding systems and improves their\nperformances of retrieval. In particular, when combined with cutting-edge\nmodels, TOD-Net gains the state-of-the-art cross-modal retrieval model\nassociated with the MSCOCO dataset. Qualitative analysis reveals that TOD-Net\nsuccessfully emphasizes entity-specific concepts and retrieves diverse targets\nvia handling higher levels of diversity than existing models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:54:27 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Matsubara", "Takashi", ""]]}, {"id": "1910.06573", "submitter": "Cheng-En Wu", "authors": "Cheng-En Wu, Yi-Ming Chan, Chien-Hung Chen, Wen-Cheng Chen, Chu-Song\n  Chen", "title": "IMMVP: An Efficient Daytime and Nighttime On-Road Object Detector", "comments": "Accepted at IEEE 21st International Workshop on Multimedia Signal\n  Processing (MMSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hard to detect on-road objects under various lighting conditions. To\nimprove the quality of the classifier, three techniques are used. We define\nsubclasses to separate daytime and nighttime samples. Then we skip similar\nsamples in the training set to prevent overfitting. With the help of the\noutside training samples, the detection accuracy is also improved. To detect\nobjects in an edge device, Nvidia Jetson TX2 platform, we exert the lightweight\nmodel ResNet-18 FPN as the backbone feature extractor. The FPN (Feature Pyramid\nNetwork) generates good features for detecting objects over various scales.\nWith Cascade R-CNN technique, the bounding boxes are iteratively refined for\nbetter results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 07:46:03 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 02:16:59 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 05:00:49 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wu", "Cheng-En", ""], ["Chan", "Yi-Ming", ""], ["Chen", "Chien-Hung", ""], ["Chen", "Wen-Cheng", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1910.06699", "submitter": "Cesar Roberto De Souza", "authors": "C\\'esar Roberto de Souza, Adrien Gaidon, Yohann Cabon, Naila Murray,\n  Antonio Manuel L\\'opez", "title": "Generating Human Action Videos by Coupling 3D Game Engines and\n  Probabilistic Graphical Models", "comments": "Pre-print of the article accepted for publication in the Special\n  Issue on Generating Realistic Visual Data of Human Behavior of the\n  International Journal of Computer Vision (IJCV). arXiv admin note:\n  substantial text overlap with arXiv:1612.00881", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep video action recognition models have been highly successful in recent\nyears but require large quantities of manually annotated data, which are\nexpensive and laborious to obtain. In this work, we investigate the generation\nof synthetic training data for video action recognition, as synthetic data have\nbeen successfully used to supervise models for a variety of other computer\nvision tasks. We propose an interpretable parametric generative model of human\naction videos that relies on procedural generation, physics models and other\ncomponents of modern game engines. With this model we generate a diverse,\nrealistic, and physically plausible dataset of human action videos, called PHAV\nfor \"Procedural Human Action Videos\". PHAV contains a total of 39,982 videos,\nwith more than 1,000 examples for each of 35 action categories. Our video\ngeneration approach is not limited to existing motion capture sequences: 14 of\nthese 35 categories are procedurally defined synthetic actions. In addition,\neach video is represented with 6 different data modalities, including RGB,\noptical flow and pixel-level semantic labels. These modalities are generated\nalmost simultaneously using the Multiple Render Targets feature of modern GPUs.\nIn order to leverage PHAV, we introduce a deep multi-task (i.e. that considers\naction classes from multiple datasets) representation learning architecture\nthat is able to simultaneously learn from synthetic and real video datasets,\neven when their action categories differ. Our experiments on the UCF-101 and\nHMDB-51 benchmarks suggest that combining our large set of synthetic videos\nwith small real-world datasets can boost recognition performance. Our approach\nalso significantly outperforms video representations produced by fine-tuning\nstate-of-the-art unsupervised generative models of videos.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 11:51:24 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["de Souza", "C\u00e9sar Roberto", ""], ["Gaidon", "Adrien", ""], ["Cabon", "Yohann", ""], ["Murray", "Naila", ""], ["L\u00f3pez", "Antonio Manuel", ""]]}, {"id": "1910.07193", "submitter": "Yiqiang Sheng", "authors": "Yiqiang Sheng", "title": "Scalable Intelligence-Enabled Networking with Traffic Engineering in 5G\n  Scenarios for Future Audio-Visual-Tactile Internet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to improve future network performance, this paper proposes scalable\nintelligence-enabled networking (SIEN) with eliminating traffic redundancy for\naudio-visual-tactile Internet in 5G scenarios such as enhanced mobile\nbroadband, ultra-reliable and low latency communication, and massive\nmachine-type communication. The SIEN consists of an intelligent management\nplane (ImP), an intelligence-enabled plane (IeP), a control plane and a user\nplane. For the ImP, the containers with decision execution are constructed by a\nnovel graph algorithm to organize objects such as network elements and resource\npartitions. For the IeP, a novel learning system is designed with decision\nmaking using a congruity function for generalization and personalization in the\npresence of imbalanced, conflicting and partial data. For the control plane, a\nscheme of identifier-locator mapping is designed by referring to\ninformation-centric networking and software-defined networking. For the user\nplane, the registrations, requests and data are forwarded to implement the SIEN\nand test its performance. The evaluation shows the SIEN outperforms four\nstate-of-the-art techniques for redundant traffic reduction by up to 46.04%\nbased on a mix of assumption, simulation and proof-of-concept implementation\nfor audio-visual-tactile Internet multimedia service. To confirm the validity,\nthe best case and the worst case for traffic offloading are tested with the\ndata rate, the latency and the density. The evaluation only focused on the\nscalability issue, while the SIEN would be beneficial to improve more issues\nsuch as inter-domain security, ultra-low latency, on-demand mobility,\nmulti-homing routing, and cross-layer feature incongruity.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 07:22:16 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Sheng", "Yiqiang", ""]]}, {"id": "1910.07383", "submitter": "Anier Soria Lorente", "authors": "Anier Soria-Lorente, Stefan Berres, and Ernesto Avila-Domenech", "title": "Hiding data inside images using orthogonal moments", "comments": "30 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we propose a novel steganographic method based on\nseveral orthogonal polynomials and their combinations. The steganographic\nalgorithm embeds a secrete message at the first eight coefficients of high\nfrequency image. Moreover, this embedding method uses the Beta chaotic map to\ndetermine the order of the blocks where the secret bits will be inserted. In\naddition, from a 128-bit private key and the steps of a cryptography algorithm\naccording to the Advanced Encryption Standard (AES) to generate the key\nexpansion, the proposed method generates a key expansion of 2560 bits, with the\npurpose to permute the first eight coefficients of high frequency before the\ninsertion. The insertion takes eventually place at the first eight high\nfrequency coefficients in the transformed orthogonal moments domain. Before the\ninsertion of the message the image undergoes a series of transformations. After\nthe insertion the inverse transformations are applied to the original\ntransformations in reverse order. The experimental work on the validation of\nthe algorithm consists of the calculation of the Peak Signal-to-Noise Ratio\n(PSNR), the Universal Image Quality Index (UIQI), the Image Fidelity (IF), and\nthe Relative Entropy (RE), comparing the same characteristics for the cover and\nstego image. The proposed algorithm improves the level of imperceptibility and\nsecurity analyzed through the PSNR and RE values, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:48:00 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Soria-Lorente", "Anier", ""], ["Berres", "Stefan", ""], ["Avila-Domenech", "Ernesto", ""]]}, {"id": "1910.07394", "submitter": "Thassilo Gadermaier", "authors": "Thassilo Gadermaier and Gerhard Widmer", "title": "A Study of Annotation and Alignment Accuracy for Performance Comparison\n  in Complex Orchestral Music", "comments": null, "journal-ref": "Proceedings of the 20th International Society for Music\n  Information Retrieval Conference, (ISMIR) 2019, Delft, The Netherlands,\n  November 4-8, 2019, pages: 769--775", "doi": null, "report-no": null, "categories": "cs.MM cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative analysis of commonalities and differences between recorded music\nperformances is an increasingly common task in computational musicology. A\ntypical scenario involves manual annotation of different recordings of the same\npiece along the time dimension, for comparative analysis of, e.g., the musical\ntempo, or for mapping other performance-related information between\nperformances. This can be done by manually annotating one reference\nperformance, and then automatically synchronizing other performances, using\naudio-to-audio alignment algorithms. In this paper we address several questions\nrelated to those tasks. First, we analyze different annotations of the same\nmusical piece, quantifying timing deviations between the respective human\nannotators. A statistical evaluation of the marker time stamps will provide (a)\nan estimate of the expected timing precision of human annotations and (b) a\nground truth for subsequent automatic alignment experiments. We then carry out\na systematic evaluation of different audio features for audio-to-audio\nalignment, quantifying the degree of alignment accuracy that can be achieved,\nand relate this to the results from the annotation study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:59:59 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Gadermaier", "Thassilo", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1910.07992", "submitter": "Pengpeng Yang", "authors": "Pengpeng Yang, Rongrong Ni, Yao Zhao, Gang Cao and Wei Zhao", "title": "Dual-Domain Fusion Convolutional Neural Network for Contrast Enhancement\n  Forensics", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast enhancement (CE) forensics techniques have always been of great\ninterest for image forensics community, as they can be an effective tool for\nrecovering image history and identifying tampered images. Although several CE\nforensic algorithms have been proposed, their accuracy and robustness against\nsome kinds of processing are still unsatisfactory. In order to attenuate such\ndeficiency, in this paper we propose a new framework based on dual-domain\nfusion convolutional neural network to fuse the features of pixel and histogram\ndomains for CE forensics. Specifically, we first present a pixel-domain\nconvolutional neural network (P-CNN) to automatically capture the patterns of\ncontrast-enhanced images in the pixel domain. Then, we present a\nhistogram-domain convolutional neural network (H-CNN) to extract the features\nin the histogram domain. The feature representations of pixel and histogram\ndomains are fused and fed into two fully connected layers for the\nclassification of contrast-enhanced images. Experimental results show that the\nproposed method achieve better performance and is robust against pre-JPEG\ncompression and anti-forensics attacks. In addition, a strategy for performance\nimprovement of CNN-based forensics is explored, which could provide guidance\nfor the design of CNN-based forensics tools.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:06:12 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Yang", "Pengpeng", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""], ["Cao", "Gang", ""], ["Zhao", "Wei", ""]]}, {"id": "1910.08341", "submitter": "Liu Nao", "authors": "Duan Xintao, Liu Nao", "title": "Hide the Image in FC-DenseNets to another Image", "comments": "12 pages,7 figures,conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past, steganography was to embed text in a carrier, the sender Alice\nand the recipient Bob share the key, and the text is extracted by Bob through\nthe key. If more information is embedded, the image is easily distorted. In\ncontrast, if there is less embedded information, the image maintains good\nvisual integrity, but does not meet our requirements for steganographic\ncapacity. In this paper, we focus on tackling these challenges and limitations\nto improve steganographic capacity. An image steganography method based on\nFully Convolutional Dense Network(FC-DenseNet) was proposed by us. The hidden\nnetwork and the extracted network are trained at the same time. The dataset of\nthe deep neural network is derived from various natural images of ImageNet. The\nexperimental results show that the stego-image after steganography and the\nsecret image extracted from stego-imge have a visually good effect, and the\nstego-image has high capacity and high peak signal to noise ratio.\nImage-to-image full size hiding is implemented.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 10:53:46 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Xintao", "Duan", ""], ["Nao", "Liu", ""]]}, {"id": "1910.08732", "submitter": "Kranti Kumar Parida", "authors": "Kranti Kumar Parida, Neeraj Matiyali, Tanaya Guha, Gaurav Sharma", "title": "Coordinated Joint Multimodal Embeddings for Generalized Audio-Visual\n  Zeroshot Classification and Retrieval of Videos", "comments": "To appear in WACV 2020, Project Page:\n  https://cse.iitk.ac.in/users/kranti/avzsl.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an audio-visual multimodal approach for the task of zeroshot\nlearning (ZSL) for classification and retrieval of videos. ZSL has been studied\nextensively in the recent past but has primarily been limited to visual\nmodality and to images. We demonstrate that both audio and visual modalities\nare important for ZSL for videos. Since a dataset to study the task is\ncurrently not available, we also construct an appropriate multimodal dataset\nwith 33 classes containing 156,416 videos, from an existing large scale audio\nevent dataset. We empirically show that the performance improves by adding\naudio modality for both tasks of zeroshot classification and retrieval, when\nusing multimodal extensions of embedding learning methods. We also propose a\nnovel method to predict the `dominant' modality using a jointly learned\nmodality attention network. We learn the attention in a semi-supervised setting\nand thus do not require any additional explicit labelling for the modalities.\nWe provide qualitative validation of the modality specific attention, which\nalso successfully generalizes to unseen test classes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 09:39:28 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Parida", "Kranti Kumar", ""], ["Matiyali", "Neeraj", ""], ["Guha", "Tanaya", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1910.08773", "submitter": "Dave Cliff", "authors": "Vansh Dassani, Jon Bird, and Dave Cliff", "title": "Automated Composition of Picture-Synched Music Soundtracks for Movies", "comments": "To be presented at the 16th ACM SIGGRAPH European Conference on\n  Visual Media Production. London, England: 17th-18th December 2019. 10 pages,\n  9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the implementation of and early results from a system that\nautomatically composes picture-synched musical soundtracks for videos and\nmovies. We use the phrase \"picture-synched\" to mean that the structure of the\nautomatically composed music is determined by visual events in the input movie,\ni.e. the final music is synchronised to visual events and features such as cut\ntransitions or within-shot key-frame events. Our system combines automated\nvideo analysis and computer-generated music-composition techniques to create\nunique soundtracks in response to the video input, and can be thought of as an\ninitial step in creating a computerised replacement for a human composer\nwriting music to fit the picture-locked edit of a movie. Working only from the\nvideo information in the movie, key features are extracted from the input\nvideo, using video analysis techniques, which are then fed into a\nmachine-learning-based music generation tool, to compose a piece of music from\nscratch. The resulting soundtrack is tied to video features, such as scene\ntransition markers and scene-level energy values, and is unique to the input\nvideo. Although the system we describe here is only a preliminary\nproof-of-concept, user evaluations of the output of the system have been\npositive.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 13:51:57 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Dassani", "Vansh", ""], ["Bird", "Jon", ""], ["Cliff", "Dave", ""]]}, {"id": "1910.09021", "submitter": "Zehao Wang", "authors": "Zehao Wang, Jingru Li, Xiaoou Chen, Zijin Li, Shicheng Zhang, Baoqiang\n  Han, Deshun Yang", "title": "Musical Instrument Playing Technique Detection Based on FCN: Using\n  Chinese Bowed-Stringed Instrument as an Example", "comments": "Submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike melody extraction and other aspects of music transcription, research\non playing technique detection is still in its early stages. Compared to\nexisting work mostly focused on playing technique detection for individual\nsingle notes, we propose a general end-to-end method based on Sound Event\nDetection by FCN for musical instrument playing technique detection. In our\ncase, we choose Erhu, a well-known Chinese bowed-stringed instrument, to\nexperiment with our method. Because of the limitation of FCN, we present an\nalgorithm to detect on variable length audio. The effectiveness of the proposed\nframework is tested on a new dataset, its categorization of techniques is\nsimilar to our training dataset. The highest accuracy of our 3 experiments on\nthe new test set is 87.31%. Furthermore, we also evaluate the performance of\nthe proposed framework on 10 real-world studio music (produced by midi) and 7\nreal-world recording samples to address the ability of generalization on our\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:50:49 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wang", "Zehao", ""], ["Li", "Jingru", ""], ["Chen", "Xiaoou", ""], ["Li", "Zijin", ""], ["Zhang", "Shicheng", ""], ["Han", "Baoqiang", ""], ["Yang", "Deshun", ""]]}, {"id": "1910.09307", "submitter": "Xueting Wang", "authors": "Xueting Wang, Yiwei Zhang, Toshihiko Yamasaki", "title": "User-Aware Folk Popularity Rank: User-Popularity-Based Tag\n  Recommendation That Can Enhance Social Popularity", "comments": "Accepted to ACM Multimedia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method that can enhance the social popularity of a\npost (i.e., the number of views or likes) by recommending appropriate hash tags\nconsidering both content popularity and user popularity. A previous approach\ncalled FolkPopularityRank (FP-Rank) considered only the relationship among\nimages, tags, and their popularity. However, the popularity of an image/video\nis strongly affected by who uploaded it. Therefore, we develop an algorithm\nthat can incorporate user popularity and users' tag usage tendency into the\nFP-Rank algorithm. The experimental results using 60,000 training images with\ntheir accompanying tags and 1,000 test data, which were actually uploaded to a\nreal social network service (SNS), show that, in ten days, our proposed\nalgorithm can achieve 1.2 times more views than the FP-Rank algorithm. This\ntechnology would be critical to individual users and companies/brands who want\nto promote themselves in SNSs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:42:07 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wang", "Xueting", ""], ["Zhang", "Yiwei", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "1910.09770", "submitter": "Yongfei Zhang", "authors": "Yongfei Zhang, Chao Zhang, Rui Fan, Siwei Ma, Zhibo Chen, C.-C. Jay\n  Kuo", "title": "Recent Advances on HEVC Inter-frame Coding: From Optimization to\n  Implementation and Beyond", "comments": "accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (T-CSVT) as transactions paper", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2954474", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Efficiency Video Coding (HEVC) has doubled the video compression ratio\nwith equivalent subjective quality as compared to its predecessor H.264/AVC.\nThe significant coding efficiency improvement is attributed to many new\ntechniques. Inter-frame coding is one of the most powerful yet complicated\ntechniques therein and has posed high computational burden thus main obstacle\nin HEVC-based real-time applications. Recently, plenty of research has been\ndone to optimize the inter-frame coding, either to reduce the complexity for\nreal-time applications, or to further enhance the encoding efficiency. In this\npaper, we provide a comprehensive review of the state-of-the-art techniques for\nHEVC inter-frame coding from three aspects, namely fast inter coding solutions,\nimplementation on different hardware platforms as well as advanced inter coding\ntechniques. More specifically, different algorithms in each aspect are further\nsubdivided into sub-categories and compared in terms of pros, cons, coding\nefficiency and coding complexity. To the best of our knowledge, this is the\nfirst such comprehensive review of the recent advances of the inter-frame\ncoding for HEVC and hopefully it would help the improvement, implementation and\napplications of HEVC as well as the ongoing development of the next generation\nvideo coding standard.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:04:33 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 13:15:23 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 04:14:15 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhang", "Yongfei", ""], ["Zhang", "Chao", ""], ["Fan", "Rui", ""], ["Ma", "Siwei", ""], ["Chen", "Zhibo", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1910.10106", "submitter": "Alessandro Lameiras Koerich", "authors": "Karl Michel Koerich, Mohammad Esmaeilpour, Sajjad Abdoli, Alceu de\n  Souza Britto Jr., Alessandro Lameiras Koerich", "title": "Cross-Representation Transferability of Adversarial Attacks: From\n  Spectrograms to Audio Waveforms", "comments": "8 pages", "journal-ref": "IEEE International Joint Conference on Neural Networks (IJCNN\n  2020), Glasgow, UK", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows the susceptibility of spectrogram-based audio classifiers to\nadversarial attacks and the transferability of such attacks to audio waveforms.\nSome commonly used adversarial attacks to images have been applied to\nMel-frequency and short-time Fourier transform spectrograms, and such perturbed\nspectrograms are able to fool a 2D convolutional neural network (CNN). Such\nattacks produce perturbed spectrograms that are visually imperceptible by\nhumans. Furthermore, the audio waveforms reconstructed from the perturbed\nspectrograms are also able to fool a 1D CNN trained on the original audio.\nExperimental results on a dataset of western music have shown that the 2D CNN\nachieves up to 81.87% of mean accuracy on legitimate examples and such\nperformance drops to 12.09% on adversarial examples. Likewise, the 1D CNN\nachieves up to 78.29% of mean accuracy on original audio samples and such\nperformance drops to 27.91% on adversarial audio waveforms reconstructed from\nthe perturbed spectrograms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:46:37 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 21:18:07 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 16:38:44 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 11:16:58 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Koerich", "Karl Michel", ""], ["Esmaeilpour", "Mohammad", ""], ["Abdoli", "Sajjad", ""], ["Britto", "Alceu de Souza", "Jr."], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1910.11006", "submitter": "Dongxu Li", "authors": "Dongxu Li, Cristian Rodriguez Opazo, Xin Yu, Hongdong Li", "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale\n  Dataset and Methods Comparison", "comments": "Accepted by WACV2020, First Round, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based sign language recognition aims at helping deaf people to\ncommunicate with others. However, most existing sign language datasets are\nlimited to a small number of words. Due to the limited vocabulary size, models\nlearned from those datasets cannot be applied in practice. In this paper, we\nintroduce a new large-scale Word-Level American Sign Language (WLASL) video\ndataset, containing more than 2000 words performed by over 100 signers. This\ndataset will be made publicly available to the research community. To our\nknowledge, it is by far the largest public ASL dataset to facilitate word-level\nsign recognition research.\n  Based on this new large-scale dataset, we are able to experiment with several\ndeep learning methods for word-level sign recognition and evaluate their\nperformances in large scale scenarios. Specifically we implement and compare\ntwo different models,i.e., (i) holistic visual appearance-based approach, and\n(ii) 2D human pose based approach. Both models are valuable baselines that will\nbenefit the community for method benchmarking. Moreover, we also propose a\nnovel pose-based temporal graph convolution networks (Pose-TGCN) that models\nspatial and temporal dependencies in human pose trajectories simultaneously,\nwhich has further boosted the performance of the pose-based method. Our results\nshow that pose-based and appearance-based models achieve comparable\nperformances up to 66% at top-10 accuracy on 2,000 words/glosses, demonstrating\nthe validity and challenges of our dataset. Our dataset and baseline deep\nmodels are available at \\url{https://dxli94.github.io/WLASL/}.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:04:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 00:24:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Li", "Dongxu", ""], ["Opazo", "Cristian Rodriguez", ""], ["Yu", "Xin", ""], ["Li", "Hongdong", ""]]}, {"id": "1910.11102", "submitter": "Yao Peng", "authors": "Xinxin Zhu, Longteng Guo, Peng Yao, Shichen Lu, Wei Liu, Jing Liu", "title": "Vatex Video Captioning Challenge 2020: Multi-View Features and Hybrid\n  Reward Strategies for Video Captioning", "comments": "4 pages,2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our solution for the VATEX Captioning Challenge 2020,\nwhich requires generating descriptions for the videos in both English and\nChinese languages. We identified three crucial factors that improve the\nperformance, namely: multi-view features, hybrid reward, and diverse ensemble.\nBased on our method of VATEX 2019 challenge, we achieved significant\nimprovements this year with more advanced model architectures, combination of\nappearance and motion features, and careful hyper-parameters tuning. Our method\nachieves very competitive results on both of the Chinese and English video\ncaptioning tracks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 13:52:49 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 01:47:18 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 04:13:30 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 03:42:09 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Zhu", "Xinxin", ""], ["Guo", "Longteng", ""], ["Yao", "Peng", ""], ["Lu", "Shichen", ""], ["Liu", "Wei", ""], ["Liu", "Jing", ""]]}, {"id": "1910.11677", "submitter": "Imad El Hanouti", "authors": "Imad El Hanouti, Hakim El Fadili", "title": "Security analysis of an audio data encryption scheme based on key\n  chaining and DNA encoding", "comments": "19 pages, 7 figures, 2 tables", "journal-ref": null, "doi": "10.1007/s11042-020-10153-8", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairly recently, a new encryption scheme for audio data encryption has been\nproposed by Naskar, P.K., et al. The cryptosystem is based on\nsubstitution-permutation encryption structure using DNA encoding at the\nsubstitution stage, in which the key generation is based on a key chaining\nalgorithm that generates new key block for every plain block using a logistic\nchaotic map. After some several statistical tests done by the authors of the\nscheme, they claimed that their cryptosystem is robust and can resist\nconventional cryptanalysis attacks. Negatively, in this paper we show the\nopposite: the scheme is extremely weak against chosen ciphertext and plaintext\nattacks thus only two chosen plaintexts of 32 byte size are sufficient to\nrecover the equivalent key used for encryption. The cryptosystem's shuffling\nprocess design is vulnerable which allow us recovering the unknown original\nplaintext by applying repeated encryptions. Our study proves that the scheme is\nextremely weak and should not be used for any information security or\ncryptographic concern. Lessons learned from this cryptanalytic paper are then\noutlined in order to be considered in further designs and proposals.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 10:39:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Hanouti", "Imad El", ""], ["Fadili", "Hakim El", ""]]}, {"id": "1910.11678", "submitter": "Imad El Hanouti", "authors": "Imad El Hanouti, Hakim El Fadili, Khalid Zenkouar", "title": "Breaking an image encryption scheme based on Arnold map and Lucas series", "comments": "13 pages, 4 figures, 2 tables", "journal-ref": "Multimed Tools Appl (2020)", "doi": "10.1007/s11042-020-09815-4", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairly recently, a novel image encryption based on Arnold scrambling and\nLucas series has been proposed in the literature. The scheme design is based on\npermutation-substitution operations, where Arnold map is used to permute pixels\nfor some T rounds, and Lucas sequence is used to mask the image and substitute\npixel's values. The authors of the cryptosystem have claimed, after several\nstatistical analyses, that their system is \"with high efficiency\" and resists\nchosen and known plaintext attacks. Negatively, in this paper we showed the\nopposite. The key space of the scheme under study could be reduced considerably\nafter our equivalent keys analysis, and thus the system is breakable under\nreasonable brute force attack. After all, the design of the scheme has several\nweaknesses that make it weak against chosen and known plaintext attacks.\nConsequently, we do not recommend the use of this system for any cryptographic\nconcern or security purpose.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 10:37:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hanouti", "Imad El", ""], ["Fadili", "Hakim El", ""], ["Zenkouar", "Khalid", ""]]}, {"id": "1910.11679", "submitter": "Imad El Hanouti", "authors": "Imad El Hanouti, Hakim El Fadili, Khalid Zenkouar", "title": "Cryptanalysis of a Chaos-Based Fast Image Encryption Algorithm for\n  Embedded Systems", "comments": "11 pages", "journal-ref": null, "doi": "10.1007/s11042-020-10289-7", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairly recently, a new encryption scheme for embedded systems based on\ncontinuous third-order hyperbolic sine chaotic system was proposed by Z. Lin et\nal. The cryptosystem's main objective is to provide a faster algorithm with\nlowest computational time in order to be qualified for use in embedded systems\nespecially on a program of UAV (unmanned aerial vehicle). In this paper, we\nscrutinize the design architecture of this recently proposed scheme against\nconventional attacks e.g., chosen plaintext attack, differential attack, known\nplaintext attack. We prove in this paper that, negatively, the studied system\nis vulnerable. For differential attack, only two chosen plain images are\nrequired to recover the full equivalent key. Moreover, only one 3x400 size\nimage is sufficient to break the cryptosystem under chosen plaintext attack\nconsidering stability of sort algorithm. Therefore, the proposed scheme is not\nrecommended for security purposes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 10:37:35 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Hanouti", "Imad El", ""], ["Fadili", "Hakim El", ""], ["Zenkouar", "Khalid", ""]]}, {"id": "1910.11949", "submitter": "Mariona Car\\'os Roca", "authors": "Mariona Caros, Maite Garolera, Petia Radeva and Xavier Giro-i-Nieto", "title": "Automatic Reminiscence Therapy for Dementia", "comments": "MSc thesis at TelecomBCN, Universitat Politecnica de Catalunya 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With people living longer than ever, the number of cases with dementia such\nas Alzheimer's disease increases steadily. It affects more than 46 million\npeople worldwide, and it is estimated that in 2050 more than 100 million will\nbe affected. While there are not effective treatments for these terminal\ndiseases, therapies such as reminiscence, that stimulate memories from the past\nare recommended. Currently, reminiscence therapy takes place in care homes and\nis guided by a therapist or a carer. In this work, we present an AI-based\nsolution to automatize the reminiscence therapy, which consists in a dialogue\nsystem that uses photos as input to generate questions. We run a usability case\nstudy with patients diagnosed of mild cognitive impairment that shows they\nfound the system very entertaining and challenging. Overall, this paper\npresents how reminiscence therapy can be automatized by using machine learning,\nand deployed to smartphones and laptops, making the therapy more accessible to\nevery person affected by dementia.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 21:47:52 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 12:26:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Caros", "Mariona", ""], ["Garolera", "Maite", ""], ["Radeva", "Petia", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1910.12828", "submitter": "Mohamed Hamidi", "authors": "Mohamed Hamidi and Aladine Chetouani and Mohamed El Haziti and\n  Mohammed El Hassouni and and Hocine Cherifi", "title": "Blind Robust 3-D Mesh Watermarking based on Mesh Saliency and QIM\n  quantization for Copyright Protection", "comments": "11 pages, 5 figures, published in IbPRIA 2019: 9th Iberian Conference\n  on Pattern Recognition and Image Analysis. arXiv admin note: substantial text\n  overlap with arXiv:1910.11211", "journal-ref": null, "doi": "10.1007/978-3-030-31332-6_15", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent demand of 3-D models in several applications like medical\nimaging, video games, among others, the necessity of implementing 3-D mesh\nwatermarking schemes aiming to protect copyright has increased considerably.\nThe majority of robust 3-D watermarking techniques have essentially focused on\nthe robustness against attacks while the imperceptibility of these techniques\nis still a real issue. In this context, a blind robust 3-D mesh watermarking\nmethod based on mesh saliency and Quantization Index Modulation (QIM) for\nCopyright protection is proposed. The watermark is embedded by quantifying the\nvertex norms of the 3-D mesh using QIM scheme since it offers a good\nrobustness-capacity tradeoff. The choice of the vertices is adjusted by the\nmesh saliency to achieve watermark robustness and to avoid visual distortions.\nThe experimental results show the high imperceptibility of the proposed scheme\nwhile ensuring a good robustness against a wide range of attacks including\nadditive noise, similarity transformations, smoothing, quantization, etc.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:39:09 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Hamidi", "Mohamed", ""], ["Chetouani", "Aladine", ""], ["Haziti", "Mohamed El", ""], ["Hassouni", "Mohammed El", ""], ["Cherifi", "and Hocine", ""]]}, {"id": "1910.13523", "submitter": "Fan Yang", "authors": "Fan Yang, Jaymar Soriano, Takatomi Kubo, Kazushi Ikeda", "title": "A Hierarchical Mixture Density Network", "comments": "8 pages, 5 figures, conference", "journal-ref": "The 24th International Conference on Neural Information\n  Processing, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship among three correlated variables could be very\nsophisticated, as a result, we may not be able to find their hidden causality\nand model their relationship explicitly. However, we still can make our best\nguess for possible mappings among these variables, based on the observed\nrelationship. One of the complicated relationships among three correlated\nvariables could be a two-layer hierarchical many-to-many mapping. In this\npaper, we proposed a Hierarchical Mixture Density Network (HMDN) to model the\ntwo-layer hierarchical many-to-many mapping. We apply HMDN on an indoor\npositioning problem and show its benefit.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:33:07 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Yang", "Fan", ""], ["Soriano", "Jaymar", ""], ["Kubo", "Takatomi", ""], ["Ikeda", "Kazushi", ""]]}, {"id": "1910.13801", "submitter": "Subham Banga", "authors": "Subham Banga, Ujjwal Upadhyay, Piyush Agarwal, Aniket Sharma and\n  Prerana Mukherjee", "title": "Indian EmoSpeech Command Dataset: A dataset for emotion based speech\n  recognition in the wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech emotion analysis is an important task which further enables several\napplication use cases. The non-verbal sounds within speech utterances also play\na pivotal role in emotion analysis in speech. Due to the widespread use of\nsmartphones, it becomes viable to analyze speech commands captured using\nmicrophones for emotion understanding by utilizing on-device machine learning\nmodels. The non-verbal information includes the environment background sounds\ndescribing the type of surroundings, current situation and activities being\nperformed. In this work, we consider both verbal (speech commands) and\nnon-verbal sounds (background noises) within an utterance for emotion analysis\nin real-life scenarios. We create an indigenous dataset for this task namely\n\"Indian EmoSpeech Command Dataset\". It contains keywords with diverse emotions\nand background sounds, presented to explore new challenges in audio analysis.\nWe exhaustively compare with various baseline models for emotion analysis on\nspeech commands on several performance metrics. We demonstrate that we achieve\na significant average gain of 3.3% in top-one score over a subset of speech\ncommand dataset for keyword spotting.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 06:55:18 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Banga", "Subham", ""], ["Upadhyay", "Ujjwal", ""], ["Agarwal", "Piyush", ""], ["Sharma", "Aniket", ""], ["Mukherjee", "Prerana", ""]]}, {"id": "1910.13884", "submitter": "Xing Wei", "authors": "Xing Wei, Chenyang Yang, and Shengqian Han", "title": "Prediction, Communication, and Computing Duration Optimization for VR\n  Video Streaming", "comments": "32 pages, one column, 7 figures, submitted to IEEE for possible\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactive tile-based video streaming can avoid motion-to-photon latency of\nwireless virtual reality (VR) by computing and delivering the predicted tiles\nto be requested before playback. All existing works either focus on the task of\ntile prediction or on the tasks of computing and communications, overlooking\nthe facts that these successively executed tasks have to share the same\nduration to avoid the latency and the quality of experience (QoE) of proactive\nVR streaming depends on the worst performance of the three tasks. In this\npaper, we jointly optimize the duration of the observation window for\npredicting tiles and the durations for computing and transmitting the predicted\ntiles to maximize the QoE given arbitrary predictor and configured resources.\nWe obtain the global optimal solution with closed-form expression by\ndecomposing the formulated problem equivalently into two subproblems. With the\noptimized durations, we find a resource-limited region where the QoE can be\nimproved effectively by configuring more resources, and a prediction-limited\nregion where the QoE can be improved with a better predictor. Simulation\nresults using three existing tile predictors with a real dataset demonstrate\nthe gain of the joint optimization over the non-optimized counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:27:58 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 11:35:15 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 13:01:39 GMT"}, {"version": "v4", "created": "Fri, 14 Aug 2020 10:44:02 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Wei", "Xing", ""], ["Yang", "Chenyang", ""], ["Han", "Shengqian", ""]]}, {"id": "1910.14571", "submitter": "Yang Hao", "authors": "Hao Yang, ZhongLiang Yang, YongJian Bao and YongFeng Huang", "title": "Fast Steganalysis Method for VoIP Streams", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": "10.1109/LSP.2019.2961610", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we present a novel and extremely fast steganalysis method of\nVoice over IP (VoIP) streams, driven by the need for a quick and accurate\ndetection of possible steganography in VoIP streams. We firstly analyzed the\ncorrelations in carriers. To better exploit the correlation in code-words, we\nmapped vector quantization code-words into a semantic space. In order to\nachieve high detection efficiency, only one hidden layer is utilized to extract\nthe correlations between these code-words. Finally, based on the extracted\ncorrelation features, we used the softmax classifier to categorize the input\nstream carriers. To boost the performance of this proposed model, we\nincorporate a simple knowledge distillation framework into the training\nprocess. Experimental results show that the proposed method achieves\nstate-of-the-art performance both in detection accuracy and efficiency. In\nparticular, the processing time of this method on average is only about 0.05\\%\nwhen sample length is as short as 0.1s, attaching strong practical value to\nonline serving of steganography monitor.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:25:49 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Yang", "Hao", ""], ["Yang", "ZhongLiang", ""], ["Bao", "YongJian", ""], ["Huang", "YongFeng", ""]]}]