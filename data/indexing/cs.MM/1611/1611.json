[{"id": "1611.00869", "submitter": "Tianyi Xu", "authors": "Tianyi Xu, Liangping Ma and Gregory Sternberg", "title": "QoE-based MAC Layer Optimization for Video Teleconferencing over WiFi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In IEEE 802.11, the retry limit is set the same value for all packets. In\nthis paper, we dynamically classify video teleconferencing packets based on the\ntype of the video frame that a packet carries and the packet loss events that\nhave happened in the network, and assign them different retry limits. We\nconsider the IPPP video encoding structure with instantaneous decoder refresh\n(IDR) frame insertion based on packet loss feedback. The loss of a single frame\ncauses error propagation for a period of time equal to the packet loss feedback\ndelay. To optimize the video quality, we propose a method to concentrate the\npacket losses to small segments of the entire video sequence, and study the\nperformance by an analytic model. Our proposed method is implemented only on\nthe stations interested in enhanced video quality, and is compatible with\nunmodified IEEE 802.11 stations and access points in terms of performance.\nSimulation results show that the performance gain can be significant compared\nto the IEEE 802.11 standard without negatively affecting cross traffic.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 03:19:15 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Xu", "Tianyi", ""], ["Ma", "Liangping", ""], ["Sternberg", "Gregory", ""]]}, {"id": "1611.01009", "submitter": "Christoph Rachinger", "authors": "Christoph Rachinger, Ralf R. M\\\"uller, Johannes B. Huber", "title": "Phase Shift Keying on the Hypersphere: Peak Power-Efficient MIMO\n  Communications", "comments": "This paper has been submitted to IEEE Transactions on Wireless\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase Shift Keying on the Hypersphere (PSKH), a generalization of\nconventional Phase Shift Keying (PSK) for Multiple-Input Multiple-Output (MIMO)\nsystems, is introduced. In PSKH, constellation points are distributed on a\nmultidimensional hypersphere. The use of such constellations with a\nPeak-To-Average-Sum-Power-Ratio (PASPR) of 1 allows to use load-modulated\ntransmitters which can cope with a small backoff, which in turn results in a\nhigh power efficiency. In this paper, we discuss several methods how to\ngenerate PSKH constellations and compare their performance. After applying\nconventional Pulse-Amplitude Modulation (PAM), the PASPR of the continuous time\nPSKH signal depends on the choice of the pulse shaping method. This choice also\ninfluences bandwidth and power efficiency of a PSKH system. In order to reduce\nthe PASPR of the continuous transmission signal, we use spherical interpolation\nto generate a smooth signal over the hypersphere and present corresponding\nreceiver techniques. Additionally, complexity reduction techniques are proposed\nand compared. Finally, we discuss the methods presented in this paper regarding\ntheir trade-offs with respect to PASPR, bandwidth, power efficiency and\nreceiver complexity.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 13:44:40 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 15:29:08 GMT"}, {"version": "v3", "created": "Tue, 20 Dec 2016 15:56:41 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Rachinger", "Christoph", ""], ["M\u00fcller", "Ralf R.", ""], ["Huber", "Johannes B.", ""]]}, {"id": "1611.01715", "submitter": "Zhi Li", "authors": "Zhi Li and Christos G. Bampis", "title": "Recover Subjective Quality Scores from Noisy Measurements", "comments": "16 pages; abridged version appeared in Data Compression Conference\n  (DCC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple quality metrics such as PSNR are known to not correlate well with\nsubjective quality when tested across a wide spectrum of video content or\nquality regime. Recently, efforts have been made in designing objective quality\nmetrics trained on subjective data (e.g. VMAF), demonstrating better\ncorrelation with video quality perceived by human. Clearly, the accuracy of\nsuch a metric heavily depends on the quality of the subjective data that it is\ntrained on. In this paper, we propose a new approach to recover subjective\nquality scores from noisy raw measurements, using maximum likelihood\nestimation, by jointly estimating the subjective quality of impaired videos,\nthe bias and consistency of test subjects, and the ambiguity of video contents\nall together. We also derive closed-from expression for the confidence interval\nof each estimate. Compared to previous methods which partially exploit the\nsubjective information, our approach is able to exploit the information in\nfull, yielding tighter confidence interval and better handling of outliers\nwithout the need for z-scoring or subject rejection. It also handles missing\ndata more gracefully. Finally, as side information, it provides interesting\ninsights on the test subjects and video contents.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 01:34:47 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 08:05:52 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 23:34:35 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Li", "Zhi", ""], ["Bampis", "Christos G.", ""]]}, {"id": "1611.03233", "submitter": "Shunquan Tan", "authors": "Jishen Zeng and Shunquan Tan and Bin Li and Jiwu Huang", "title": "Large-scale JPEG steganalysis using hybrid deep-learning framework", "comments": "Accepted by IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": "10.1109/TIFS.2017.2779446", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adoption of deep learning in image steganalysis is still in its initial\nstage. In this paper we propose a generic hybrid deep-learning framework for\nJPEG steganalysis incorporating the domain knowledge behind rich steganalytic\nmodels. Our proposed framework involves two main stages. The first stage is\nhand-crafted, corresponding to the convolution phase and the quantization &\ntruncation phase of the rich models. The second stage is a compound deep neural\nnetwork containing multiple deep subnets in which the model parameters are\nlearned in the training procedure. We provided experimental evidences and\ntheoretical reflections to argue that the introduction of threshold quantizers,\nthough disable the gradient-descent-based learning of the bottom convolution\nphase, is indeed cost-effective. We have conducted extensive experiments on a\nlarge-scale dataset extracted from ImageNet. The primary dataset used in our\nexperiments contains 500,000 cover images, while our largest dataset contains\nfive million cover images. Our experiments show that the integration of\nquantization and truncation into deep-learning steganalyzers do boost the\ndetection performance by a clear margin. Furthermore, we demonstrate that our\nframework is insensitive to JPEG blocking artifact alterations, and the learned\nmodel can be easily transferred to a different attacking target and even a\ndifferent dataset. These properties are of critical importance in practical\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 09:33:12 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 11:50:08 GMT"}, {"version": "v3", "created": "Sat, 25 Nov 2017 01:50:14 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Zeng", "Jishen", ""], ["Tan", "Shunquan", ""], ["Li", "Bin", ""], ["Huang", "Jiwu", ""]]}, {"id": "1611.03652", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves", "title": "Show me the material evidence: Initial experiments on evaluating\n  hypotheses from user-generated multimedia data", "comments": "6 pages, 6 figures, 3 tables in Proc. of the 1st Workshop on\n  Multimedia Support for Decision-Making Processes, at IEEE Intl. Symposium on\n  Multimedia (ISM'16), San Jose, CA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective questions such as `does neymar dive', or `is clinton lying', or\n`is trump a fascist', are popular queries to web search engines, as can be seen\nby autocompletion suggestions on Google, Yahoo and Bing. In the era of\ncognitive computing, beyond search, they could be handled as hypotheses issued\nfor evaluation. Our vision is to leverage on unstructured data and metadata of\nthe rich user-generated multimedia that is often shared as material evidence in\nfavor or against hypotheses in social media platforms. In this paper we present\ntwo preliminary experiments along those lines and discuss challenges for a\ncognitive computing system that collects material evidence from user-generated\nmultimedia towards aggregating it into some form of collective decision on the\nhypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 10:46:58 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""]]}, {"id": "1611.04021", "submitter": "Tseng-Hung Chen", "authors": "Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan\n  Carlos Niebles, Min Sun", "title": "Leveraging Video Descriptions to Learn Video Question Answering", "comments": "7 pages, 5 figures. Accepted to AAAI 2017. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable approach to learn video-based question answering (QA):\nanswer a \"free-form natural language question\" about a video content. Our\napproach automatically harvests a large number of videos and descriptions\nfreely available online. Then, a large number of candidate QA pairs are\nautomatically generated from descriptions rather than manually annotated. Next,\nwe use these candidate QA pairs to train a number of video-based QA methods\nextended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et\nal. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect\ncandidate QA pairs, we propose a self-paced learning procedure to iteratively\nidentify them and mitigate their effects in training. Finally, we evaluate\nperformance on manually generated video-based QA pairs. The results show that\nour self-paced learning procedure is effective, and the extended SS model\noutperforms various baselines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 17:15:57 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 16:07:33 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Chen", "Tseng-Hung", ""], ["Chuang", "Ching-Yao", ""], ["Liao", "Yuan-Hong", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1611.04455", "submitter": "Hongyi Liu", "authors": "Vaidehi Dalmia, Hongyi Liu, Shih-Fu Chang", "title": "Columbia MVSO Image Sentiment Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multilingual Visual Sentiment Ontology (MVSO) consists of 15,600 concepts\nin 12 different languages that are strongly related to emotions and sentiments\nexpressed in images. These concepts are defined in the form of Adjective-Noun\nPair (ANP), which are crawled and discovered from online image forum Flickr. In\nthis work, we used Amazon Mechanical Turk as a crowd-sourcing platform to\ncollect human judgments on sentiments expressed in images that are uniformly\nsampled over 3,911 English ANPs extracted from a tag-restricted subset of MVSO.\nOur goal is to use the dataset as a benchmark for the evaluation of systems\nthat automatically predict sentiments in images or ANPs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 16:48:12 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Dalmia", "Vaidehi", ""], ["Liu", "Hongyi", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1611.04503", "submitter": "Hideki Nakayama", "authors": "Hideki Nakayama and Noriki Nishida", "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network\n  with Multimedia Pivot", "comments": "Some error corrections in Sect.2.2 and Table 5, Machine Translation,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to build a neural machine translation system with no\nsupervised resources (i.e., no parallel corpora) using multimodal embedded\nrepresentation over texts and images. Based on the assumption that text\ndocuments are often likely to be described with other multimedia information\n(e.g., images) somewhat related to the content, we try to indirectly estimate\nthe relevance between two languages. Using multimedia as the \"pivot\", we\nproject all modalities into one common hidden space where samples belonging to\nsimilar semantic concepts should come close to each other, whatever the\nobserved space of each sample is. This modality-agnostic representation is the\nkey to bridging the gap between different modalities. Putting a decoder on top\nof it, our network can flexibly draw the outputs from any input modality.\nNotably, in the testing phase, we need only source language texts as the input\nfor translation. In experiments, we tested our method on two benchmarks to show\nthat it can achieve reasonable translation performance. We compared and\ninvestigated several possible implementations and found that an end-to-end\nmodel that simultaneously optimized both rank loss in multimodal encoders and\ncross-entropy loss in decoders performed the best.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 18:07:54 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 17:36:30 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 15:52:08 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Nakayama", "Hideki", ""], ["Nishida", "Noriki", ""]]}, {"id": "1611.05328", "submitter": "Zhiwei Jin", "authors": "Zhiwei Jin, Juan Cao, Jiebo Luo, and Yongdong Zhang", "title": "Image Credibility Analysis with Effective Domain Transferred Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous fake images spread on social media today and can severely jeopardize\nthe credibility of online content to public. In this paper, we employ deep\nnetworks to learn distinct fake image related features. In contrast to\nauthentic images, fake images tend to be eye-catching and visually striking.\nCompared with traditional visual recognition tasks, it is extremely challenging\nto understand these psychologically triggered visual patterns in fake images.\nTraditional general image classification datasets, such as ImageNet set, are\ndesigned for feature learning at the object level but are not suitable for\nlearning the hyper-features that would be required by image credibility\nanalysis. In order to overcome the scarcity of training samples of fake images,\nwe first construct a large-scale auxiliary dataset indirectly related to this\ntask. This auxiliary dataset contains 0.6 million weakly-labeled fake and real\nimages collected automatically from social media. Through an AdaBoost-like\ntransfer learning algorithm, we train a CNN model with a few instances in the\ntarget training set and 0.6 million images in the collected auxiliary set. This\nlearning algorithm is able to leverage knowledge from the auxiliary set and\ngradually transfer it to the target task. Experiments on a real-world testing\nset show that our proposed domain transferred CNN model outperforms several\ncompeting baselines. It obtains superiror results over transfer learning\nmethods based on the general ImageNet set. Moreover, case studies show that our\nproposed method reveals some interesting patterns for distinguishing fake and\nauthentic images.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 15:45:19 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Jin", "Zhiwei", ""], ["Cao", "Juan", ""], ["Luo", "Jiebo", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1611.07156", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Fumin Shen, Xiansheng Hua, Jingsong Xu and\n  Zhenmin Tang", "title": "Exploiting Web Images for Dataset Construction: A Domain Robust Approach", "comments": "Journal", "journal-ref": null, "doi": "10.1109/TMM.2017.2684626", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labelled image datasets have played a critical role in high-level image\nunderstanding. However, the process of manual labelling is both time-consuming\nand labor intensive. To reduce the cost of manual labelling, there has been\nincreased research interest in automatically constructing image datasets by\nexploiting web images. Datasets constructed by existing methods tend to have a\nweak domain adaptation ability, which is known as the \"dataset bias problem\".\nTo address this issue, we present a novel image dataset construction framework\nthat can be generalized well to unseen target domains. Specifically, the given\nqueries are first expanded by searching the Google Books Ngrams Corpus to\nobtain a rich semantic description, from which the visually non-salient and\nless relevant expansions are filtered out. By treating each selected expansion\nas a \"bag\" and the retrieved images as \"instances\", image selection can be\nformulated as a multi-instance learning problem with constrained positive bags.\nWe propose to solve the employed problems by the cutting-plane and\nconcave-convex procedure (CCCP) algorithm. By using this approach, images from\ndifferent distributions can be kept while noisy images are filtered out. To\nverify the effectiveness of our proposed approach, we build an image dataset\nwith 20 categories. Extensive experiments on image classification,\ncross-dataset generalization, diversity comparison and object detection\ndemonstrate the domain robustness of our dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 06:22:19 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 23:53:20 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 22:54:15 GMT"}, {"version": "v4", "created": "Tue, 28 Mar 2017 06:30:41 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Shen", "Fumin", ""], ["Hua", "Xiansheng", ""], ["Xu", "Jingsong", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1611.07233", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Pascal Hager, Luca Benini", "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression\n  Artifact Suppression", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7965927", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression algorithms are pervasively used to reduce the size of\nimages transmitted over the web and recorded on data storage media. However, we\npay for their high compression rate with visual artifacts degrading the user\nexperience. Deep convolutional neural networks have become a widespread tool to\naddress high-level computer vision tasks very successfully. Recently, they have\nfound their way into the areas of low-level computer vision and image\nprocessing to solve regression problems mostly with relatively shallow\nnetworks.\n  We present a novel 12-layer deep convolutional network for image compression\nartifact suppression with hierarchical skip connections and a multi-scale loss\nfunction. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an\nimprovement of up to 0.36 dB over the best previous ConvNet result. We show\nthat a network trained for a specific quality factor (QF) is resilient to the\nQF used to compress the input image - a single network trained for QF 60\nprovides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:11:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Hager", "Pascal", ""], ["Benini", "Luca", ""]]}, {"id": "1611.07351", "submitter": "Manuel Mazzara", "authors": "Munir Makhmutov, Joseph Alexander Brown, Manuel Mazzara, Leonard\n  Johard", "title": "MOMOS-MT: Mobile Monophonic System for Music Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music holds a significant cultural role in social identity and in the\nencouragement of socialization. Technology, by the destruction of physical and\ncultural distance, has lead to many changes in musical themes and the complete\nloss of forms. Yet, it also allows for the preservation and distribution of\nmusic from societies without a history of written sheet music. This paper\npresents early work on a tool for musicians and ethnomusicologists to\ntranscribe sheet music from monophonic voiced pieces for preservation and\ndistribution. Using FFT, the system detects the pitch frequencies, also other\nmethods detect note durations, tempo, time signatures and generates sheet\nmusic. The final system is able to be used in mobile platforms allowing the\nuser to take recordings and produce sheet music in situ to a performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 15:18:31 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Makhmutov", "Munir", ""], ["Brown", "Joseph Alexander", ""], ["Mazzara", "Manuel", ""], ["Johard", "Leonard", ""]]}, {"id": "1611.08397", "submitter": "Christophe Guyeux", "authors": "Jean-Fran\\c{c}ois Couchot, Rapha\\\"el Couturier, Yousra Ahmed Fadil,\n  Christophe Guyeux", "title": "A Second Order Derivatives based Approach for Steganography", "comments": "Accepted to SECRYPT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography schemes are designed with the objective of minimizing a defined\ndistortion function. In most existing state of the art approaches, this\ndistortion function is based on image feature preservation. Since smooth\nregions or clean edges define image core, even a small modification in these\nareas largely modifies image features and is thus easily detectable. On the\ncontrary, textures, noisy or chaotic regions are so difficult to model that the\nfeatures having been modified inside these areas are similar to the initial\nones. These regions are characterized by disturbed level curves. This work\npresents a new distortion function for steganography that is based on second\norder derivatives, which are mathematical tools that usually evaluate level\ncurves. Two methods are explained to compute these partial derivatives and have\nbeen completely implemented. The first experiments show that these approaches\nare promising.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 09:38:10 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Couchot", "Jean-Fran\u00e7ois", ""], ["Couturier", "Rapha\u00ebl", ""], ["Fadil", "Yousra Ahmed", ""], ["Guyeux", "Christophe", ""]]}, {"id": "1611.09083", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa (Yandex, Moscow, Russia), Gleb Gusev (Yandex, Moscow,\n  Russia), Pavel Serdyukov (Yandex, Moscow, Russia)", "title": "Prediction of Video Popularity in the Absence of Reliable Data from\n  Video Hosting Services: Utility of Traces Left by Users on the Web", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of user-generated content, we observe the constant rise of\nthe number of companies, such as search engines, content aggregators, etc.,\nthat operate with tremendous amounts of web content not being the services\nhosting it. Thus, aiming to locate the most important content and promote it to\nthe users, they face the need of estimating the current and predicting the\nfuture content popularity.\n  In this paper, we approach the problem of video popularity prediction not\nfrom the side of a video hosting service, as done in all previous studies, but\nfrom the side of an operating company, which provides a popular video search\nservice that aggregates content from different video hosting websites. We\ninvestigate video popularity prediction based on features from three primary\nsources available for a typical operating company: first, the content hosting\nprovider may deliver its data via its API, second, the operating company makes\nuse of its own search and browsing logs, third, the company crawls information\nabout embeds of a video and links to a video page from publicly available\nresources on the Web. We show that video popularity prediction based on the\nembed and link data coupled with the internal search and browsing data\nsignificantly improves video popularity prediction based only on the data\nprovided by the video hosting and can even adequately replace the API data in\nthe cases when it is partly or completely unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:43:31 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Drutsa", "Alexey", "", "Yandex, Moscow, Russia"], ["Gusev", "Gleb", "", "Yandex, Moscow,\n  Russia"], ["Serdyukov", "Pavel", "", "Yandex, Moscow, Russia"]]}, {"id": "1611.10017", "submitter": "Gou Koutaki", "authors": "Gou Koutaki, Keiichiro Shirai, Mitsuru Ambai", "title": "Fast Supervised Discrete Hashing and its Analysis", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learning-based supervised discrete hashing\nmethod. Binary hashing is widely used for large-scale image retrieval as well\nas video and document searches because the compact representation of binary\ncode is essential for data storage and reasonable for query searches using\nbit-operations. The recently proposed Supervised Discrete Hashing (SDH)\nefficiently solves mixed-integer programming problems by alternating\noptimization and the Discrete Cyclic Coordinate descent (DCC) method. We show\nthat the SDH model can be simplified without performance degradation based on\nsome preliminary experiments; we call the approximate model for this the \"Fast\nSDH\" (FSDH) model. We analyze the FSDH model and provide a mathematically exact\nsolution for it. In contrast to SDH, our model does not require an alternating\noptimization algorithm and does not depend on initial values. FSDH is also\neasier to implement than Iterative Quantization (ITQ). Experimental results\ninvolving a large-scale database showed that FSDH outperforms conventional SDH\nin terms of precision, recall, and computation time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 06:35:39 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Koutaki", "Gou", ""], ["Shirai", "Keiichiro", ""], ["Ambai", "Mitsuru", ""]]}]