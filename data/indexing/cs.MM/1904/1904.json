[{"id": "1904.00150", "submitter": "Gaurav Verma", "authors": "Gaurav Verma, Eeshan Gunesh Dhekane, Tanaya Guha", "title": "Learning Affective Correspondence between Music and Image", "comments": "5 pages, International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of learning affective correspondence between audio\n(music) and visual data (images). For this task, a music clip and an image are\nconsidered similar (having true correspondence) if they have similar emotion\ncontent. In order to estimate this crossmodal, emotion-centric similarity, we\npropose a deep neural network architecture that learns to project the data from\nthe two modalities to a common representation space, and performs a binary\nclassification task of predicting the affective correspondence (true or false).\nTo facilitate the current study, we construct a large scale database containing\nmore than $3,500$ music clips and $85,000$ images with three emotion classes\n(positive, neutral, negative). The proposed approach achieves $61.67\\%$\naccuracy for the affective correspondence prediction task on this database,\noutperforming two relevant and competitive baselines. We also demonstrate that\nour network learns modality-specific representations of emotion (without\nexplicitly being trained with emotion labels), which are useful for emotion\nrecognition in individual modalities.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 05:17:27 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 03:27:35 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Verma", "Gaurav", ""], ["Dhekane", "Eeshan Gunesh", ""], ["Guha", "Tanaya", ""]]}, {"id": "1904.00344", "submitter": "Huili Chen", "authors": "Huili Chen, Bita Darvish Rouhani, and Farinaz Koushanfar", "title": "BlackMarks: Blackbox Multibit Watermarking for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have created a paradigm shift in our ability to\ncomprehend raw data in various important fields ranging from computer vision\nand natural language processing to intelligence warfare and healthcare. While\nDNNs are increasingly deployed either in a white-box setting where the model\ninternal is publicly known, or a black-box setting where only the model outputs\nare known, a practical concern is protecting the models against Intellectual\nProperty (IP) infringement. We propose BlackMarks, the first end-to-end\nmulti-bit watermarking framework that is applicable in the black-box scenario.\nBlackMarks takes the pre-trained unmarked model and the owner's binary\nsignature as inputs and outputs the corresponding marked model with a set of\nwatermark keys. To do so, BlackMarks first designs a model-dependent encoding\nscheme that maps all possible classes in the task to bit '0' and bit '1' by\nclustering the output activations into two groups. Given the owner's watermark\nsignature (a binary string), a set of key image and label pairs are designed\nusing targeted adversarial attacks. The watermark (WM) is then embedded in the\nprediction behavior of the target DNN by fine-tuning the model with generated\nWM key set. To extract the WM, the remote model is queried by the WM key images\nand the owner's signature is decoded from the corresponding predictions\naccording to the designed encoding scheme. We perform a comprehensive\nevaluation of BlackMarks's performance on MNIST, CIFAR10, ImageNet datasets and\ncorroborate its effectiveness and robustness. BlackMarks preserves the\nfunctionality of the original DNN and incurs negligible WM embedding runtime\noverhead as low as 2.054%.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 06:04:50 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chen", "Huili", ""], ["Rouhani", "Bita Darvish", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1904.00553", "submitter": "Chuanmin Jia", "authors": "Chuanmin Jia, Zhaoyi Liu, Yao Wang, Siwei Ma, Wen Gao", "title": "Layered Image Compression using Scalable Auto-encoder", "comments": "accepted by IEEE MIPR 2019 as conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel convolutional neural network (CNN) based image\ncompression framework via scalable auto-encoder (SAE). Specifically, our SAE\nbased deep image codec consists of hierarchical coding layers, each of which is\nan end-to-end optimized auto-encoder. The coarse image content and texture are\nencoded through the first (base) layer while the consecutive (enhance) layers\niteratively code the pixel-level reconstruction errors between the original and\nformer reconstructed images. The proposed SAE structure alleviates the need to\ntrain multiple models for different bit-rate points by recently proposed\nauto-encoder based codecs. The SAE layers can be combined to realize multiple\nrate points, or to produce a scalable stream. The proposed method has similar\nrate-distortion performance in the low-to-medium rate range as the\nstate-of-the-art CNN based image codec (which uses different optimized networks\nto realize different bit rates) over a standard public image dataset.\nFurthermore, the proposed codec generates better perceptual quality in this bit\nrate range.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:12:57 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Jia", "Chuanmin", ""], ["Liu", "Zhaoyi", ""], ["Wang", "Yao", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "1904.00623", "submitter": "Yu-Jung Heo", "authors": "Yu-Jung Heo, Kyoung-Woon On, Seongho Choi, Jaeseo Lim, Jinah Kim,\n  Jeh-Kwang Ryu, Byung-Chull Bae and Byoung-Tak Zhang", "title": "Constructing Hierarchical Q&A Datasets for Video Story Understanding", "comments": "Accepted to AAAI 2019 Spring Symposium Series : Story-Enabled\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding is emerging as a new paradigm for studying human-like AI.\nQuestion-and-Answering (Q&A) is used as a general benchmark to measure the\nlevel of intelligence for video understanding. While several previous studies\nhave suggested datasets for video Q&A tasks, they did not really incorporate\nstory-level understanding, resulting in highly-biased and lack of variance in\ndegree of question difficulty. In this paper, we propose a hierarchical method\nfor building Q&A datasets, i.e. hierarchical difficulty levels. We introduce\nthree criteria for video story understanding, i.e. memory capacity, logical\ncomplexity, and DIKW (Data-Information-Knowledge-Wisdom) pyramid. We discuss\nhow three-dimensional map constructed from these criteria can be used as a\nmetric for evaluating the levels of intelligence relating to video story\nunderstanding.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:05:19 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Heo", "Yu-Jung", ""], ["On", "Kyoung-Woon", ""], ["Choi", "Seongho", ""], ["Lim", "Jaeseo", ""], ["Kim", "Jinah", ""], ["Ryu", "Jeh-Kwang", ""], ["Bae", "Byung-Chull", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1904.00726", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu", "title": "Unsupervised Multi-modal Hashing for Cross-modal retrieval", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advantage of low storage cost and high efficiency, hashing learning\nhas received much attention in the domain of Big Data. In this paper, we\npropose a novel unsupervised hashing learning method to cope with this open\nproblem to directly preserve the manifold structure by hashing. To address this\nproblem, both the semantic correlation in textual space and the locally\ngeometric structure in the visual space are explored simultaneously in our\nframework. Besides, the `2;1-norm constraint is imposed on the projection\nmatrices to learn the discriminative hash function for each modality. Extensive\nexperiments are performed to evaluate the proposed method on the three publicly\navailable datasets and the experimental results show that our method can\nachieve superior performance over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 07:47:13 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 08:05:43 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 09:41:25 GMT"}, {"version": "v4", "created": "Sun, 27 Sep 2020 10:51:56 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1904.00776", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu", "title": "Cross-modal Subspace Learning via Kernel Correlation Maximization and\n  Discriminative Structure Preserving", "comments": "The paper is under consideration at Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measure between heterogeneous data is still an open problem. Many\nresearch works have been developed to learn a common subspace where the\nsimilarity between different modalities can be calculated directly. However,\nmost of existing works focus on learning a latent subspace but the semantically\nstructural information is not well preserved. Thus, these approaches cannot get\ndesired results. In this paper, we propose a novel framework, termed\nCross-modal subspace learning via Kernel correlation maximization and\nDiscriminative structure-preserving (CKD), to solve this problem in two\naspects. Firstly, we construct a shared semantic graph to make each modality\ndata preserve the neighbor relationship semantically. Secondly, we introduce\nthe Hilbert-Schmidt Independence Criteria (HSIC) to ensure the consistency\nbetween feature-similarity and semantic-similarity of samples. Our model not\nonly considers the inter-modality correlation by maximizing the kernel\ncorrelation but also preserves the semantically structural information within\neach modality. The extensive experiments are performed to evaluate the proposed\nframework on the three public datasets. The experimental results demonstrated\nthat the proposed CKD is competitive compared with the classic subspace\nlearning methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 11:29:47 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 04:09:43 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 03:25:14 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1904.01417", "submitter": "Jingwei Guan", "authors": "Jingwei Guan, Yibo Chen and Wai-kuen Cham", "title": "The bilateral solver for quality estimation based multi-focus image\n  fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a fast Bilateral Solver for Quality Estimation Based\nmulti-focus Image Fusion method (BS-QEBIF) is proposed. The all-in-focus image\nis generated by pixel-wise summing up the multi-focus source images with their\nfocus-levels maps as weights. Since the visual quality of an image patch is\nhighly correlated with its focus level, the focus-level maps are preliminarily\nobtained based on visual quality scores, as pre-estimations. However, the\npre-estimations are not ideal. Thus the fast bilateral solver is then adopted\nto smooth the pre-estimations, and edges in the multi-focus source images can\nbe preserved simultaneously. The edge-preserving smoothed results are utilized\nas final focus-level maps. Moreover, this work provides a confidence-map\nsolution for the unstable fusion in the focus-level-changed boundary regions.\nExperiments were conducted on $25$ pairs of source images. The proposed\nBS-QEBIF outperforms the other $13$ fusion methods objectively and\nsubjectively. The all-in-focus image produced by the proposed method can well\nmaintain the details in the multi-focus source images and does not suffer from\nany residual errors. Experimental results show that BS-QEBIF can handle the\nfocus-level-changed boundary regions without any blocking, ringing and blurring\nartifacts.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:15:14 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Guan", "Jingwei", ""], ["Chen", "Yibo", ""], ["Cham", "Wai-kuen", ""]]}, {"id": "1904.01533", "submitter": "Samet Taspinar", "authors": "Samet Taspinar, Manoranjan Mohanty, and Nasir Memon", "title": "Source Camera Attribution of Multi-Format Devices", "comments": "16 pages, 9 figures, 11 tables. \"The paper is under consideration at\n  Pattern Recognition Letters\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo Response Non-Uniformity (PRNU) based source camera attribution is an\neffective method to determine the origin camera of visual media (an image or a\nvideo). However, given that modern devices, especially smartphones, capture\nimages, and videos at different resolutions using the same sensor array, PRNU\nattribution can become ineffective as the camera fingerprint and query visual\nmedia can be misaligned. We examine different resizing techniques such as\nbinning, line-skipping, cropping and scaling that cameras use to downsize the\nraw sensor image to different media. Taking such techniques into account, this\npaper studies the problem of source camera attribution. We define the notion of\nRatio of Alignment, which is a measure of shared sensor elements among\nspatially corresponding pixels within two media objects resized with different\ntechniques. We then compute the Ratio of Alignment between the different\ncombinations of three common resizing methods under simplified conditions and\nexperimentally validate our analysis. Based on the insights drawn from the\ndifferent techniques used by cameras and the RoA analysis, the paper proposes\nan algorithm for matching the source of a video with an image and vice versa.\nWe also present an efficient search method resulting in significantly improved\nperformance in matching as well as computation time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 16:46:12 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 17:25:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Taspinar", "Samet", ""], ["Mohanty", "Manoranjan", ""], ["Memon", "Nasir", ""]]}, {"id": "1904.01739", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Guo-sen Xie, Yang Li, Sheng Li, Zi Huang", "title": "SADIH: Semantic-Aware DIscrete Hashing", "comments": "Accepted by The Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its low storage cost and fast query speed, hashing has been recognized\nto accomplish similarity search in large-scale multimedia retrieval\napplications. Particularly supervised hashing has recently received\nconsiderable research attention by leveraging the label information to preserve\nthe pairwise similarities of data points in the Hamming space. However, there\nstill remain two crucial bottlenecks: 1) the learning process of the full\npairwise similarity preservation is computationally unaffordable and unscalable\nto deal with big data; 2) the available category information of data are not\nwell-explored to learn discriminative hash functions. To overcome these\nchallenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH)\nframework, which aims to directly embed the transformed semantic information\ninto the asymmetric similarity approximation and discriminative hashing\nfunction learning. Specifically, a semantic-aware latent embedding is\nintroduced to asymmetrically preserve the full pairwise similarities while\nskillfully handle the cumbersome n times n pairwise similarity matrix.\nMeanwhile, a semantic-aware autoencoder is developed to jointly preserve the\ndata structures in the discriminative latent semantic space and perform data\nreconstruction. Moreover, an efficient alternating optimization algorithm is\nproposed to solve the resulting discrete optimization problem. Extensive\nexperimental results on multiple large-scale datasets demonstrate that our\nSADIH can clearly outperform the state-of-the-art baselines with the additional\nbenefit of lower computational costs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:45:05 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 04:51:18 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhang", "Zheng", ""], ["Xie", "Guo-sen", ""], ["Li", "Yang", ""], ["Li", "Sheng", ""], ["Huang", "Zi", ""]]}, {"id": "1904.02077", "submitter": "Pengcheng Lin", "authors": "Peng-Cheng Lin and Wan-Lei Zhao", "title": "Graph based Nearest Neighbor Search: Promises and Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DS cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph based nearest neighbor search gets more and more popular on\nlarge-scale retrieval tasks. The attractiveness of this type of approaches lies\nin its superior performance over most of the known nearest neighbor search\napproaches as well as its genericness to various metrics. In this paper, the\nrole of two strategies, namely hierarchical structure and graph diversification\nthat are adopted as the key steps in the graph based approaches, is\ninvestigated. We find the hierarchical structure could not achieve \"much better\nlogarithmic complexity scaling\" as it was claimed in the original paper,\nparticularly on high dimensional cases. Moreover, we find that similar high\nsearch speed efficiency as the one with hierarchical structure could be\nachieved with the support of flat k-NN graph after graph diversification.\nFinally, we point out the difficulty, that is faced by most of the graph based\nsearch approaches, is directly linked to \"curse of dimensionality\".\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:12:55 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 09:51:07 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 09:01:31 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 14:23:49 GMT"}, {"version": "v5", "created": "Tue, 18 Jun 2019 09:07:06 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Lin", "Peng-Cheng", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1904.02348", "submitter": "Yanchao Wang", "authors": "Yan-Chao Wang and Feng Lin and Hock-Soon Seah", "title": "Orthogonal Voronoi Diagram and Treemap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel space partitioning strategy for implicit\nhierarchy visualization such that the new plot not only has a tidy layout\nsimilar to the treemap, but also is flexible to data changes similar to the\nVoronoi treemap. To achieve this, we define a new distance function and\nneighborhood relationship between sites so that space will be divided by\naxis-aligned segments. Then a sweepline+skyline based heuristic algorithm is\nproposed to allocate the partitioned spaces to form an orthogonal Voronoi\ndiagram with orthogonal rectangles. To the best of our knowledge, it is the\nfirst time to use a sweepline-based strategy for the Voronoi treemap. Moreover,\nwe design a novel strategy to initialize the diagram status and modify the\nstatus update procedure so that the generation of our plot is more effective\nand efficient. We show that the proposed algorithm has an O(nlog(n)) complexity\nwhich is the same as the state-of-the-art Voronoi treemap. To this end, we show\nvia experiments on the artificial dataset and real-world dataset the\nperformance of our algorithm in terms of computation time, converge rate, and\naspect ratio. Finally, we discuss the pros and cons of our method and make a\nconclusion.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:05:49 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Yan-Chao", ""], ["Lin", "Feng", ""], ["Seah", "Hock-Soon", ""]]}, {"id": "1904.02354", "submitter": "Zehang Lin", "authors": "Zhenguo Yang, Zehang Lin, Min Cheng, Qing Li and Wenyin Liu", "title": "MMED: A Multi-domain and Multi-modality Event Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we construct and release a multi-domain and multi-modality\nevent dataset (MMED), containing 25,165 textual news articles collected from\nhundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and\n76,516 image posts shared on Flickr social media, which are annotated according\nto 412 real-world events. The dataset is collected to explore the problem of\norganizing heterogeneous data contributed by professionals and amateurs in\ndifferent data domains, and the problem of transferring event knowledge\nobtained from one data domain to heterogeneous data domain, thus summarizing\nthe data with different contributors. We hope that the release of the MMED\ndataset can stimulate innovate research on related challenging problems, such\nas event discovery, cross-modal (event) retrieval, and visual question\nanswering, etc.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:27:10 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 12:05:49 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Yang", "Zhenguo", ""], ["Lin", "Zehang", ""], ["Cheng", "Min", ""], ["Li", "Qing", ""], ["Liu", "Wenyin", ""]]}, {"id": "1904.03024", "submitter": "Melpomeni Dimopoulou", "authors": "Melpomeni Dimopoulou, Marc Antonini, Pascal Barbry, Raja Appuswamy", "title": "A biologically constrained encoding solution for long-term storage of\n  images onto synthetic DNA", "comments": "Submitted to EUSIPCO 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Living in the age of the digital media explosion, the amount of data that is\nbeing stored increases dramatically. However, even if existing storage systems\nsuggest efficiency in capacity, they are lacking in durability. Hard disks,\nflash, tape or even optical storage have limited lifespan in the range of 5 to\n20 years. Interestingly, recent studies have proven that it was possible to use\nsynthetic DNA for the storage of digital data, introducing a strong candidate\nto achieve data longevity. The DNA's biological properties allows the storage\nof a great amount of information into an extraordinary small volume while also\npromising efficient storage for centuries or even longer with no loss of\ninformation. However, encoding digital data onto DNA is not obvious, because\nwhen decoding, we have to face the problem of sequencing noise robustness.\nFurthermore, synthesizing DNA is an expensive process and thus, controlling the\ncompression ratio by optimizing the rate-distortion trade-off is an important\nchallenge we have to deal with. This work proposes a coding solution for the\nstorage of digital images onto synthetic DNA. We developed a new encoding\nalgorithm which generates a DNA code robust to biological errors coming from\nthe synthesis and the sequencing processes. Furthermore, thanks to an optimized\nallocation process the solution is able to control the compression ratio and\nthus the length of the synthesized DNA strand. Results show an improvement in\nterms of coding potential compared to previous state-of-the-art works.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:30:03 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Dimopoulou", "Melpomeni", ""], ["Antonini", "Marc", ""], ["Barbry", "Pascal", ""], ["Appuswamy", "Raja", ""]]}, {"id": "1904.03282", "submitter": "Niluthpol Chowdhury Mithun", "authors": "Niluthpol Chowdhury Mithun, Sujoy Paul, Amit K. Roy-Chowdhury", "title": "Weakly Supervised Video Moment Retrieval From Text Queries", "comments": "Revised Table 1 in Page 6, A small bug related to rounding resulted\n  in a slightly improved score in the previous version. Our conclusion remains\n  the same after the update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been a few recent methods proposed in text to video moment\nretrieval using natural language queries, but requiring full supervision during\ntraining. However, acquiring a large number of training videos with temporal\nboundary annotations for each text description is extremely time-consuming and\noften not scalable. In order to cope with this issue, in this work, we\nintroduce the problem of learning from weak labels for the task of text to\nvideo moment retrieval. The weak nature of the supervision is because, during\ntraining, we only have access to the video-text pairs rather than the temporal\nextent of the video to which different text descriptions relate. We propose a\njoint visual-semantic embedding based framework that learns the notion of\nrelevant segments from video using only video-level sentence descriptions.\nSpecifically, our main idea is to utilize latent alignment between video frames\nand sentence descriptions using Text-Guided Attention (TGA). TGA is then used\nduring the test phase to retrieve relevant moments. Experiments on two\nbenchmark datasets demonstrate that our method achieves comparable performance\nto state-of-the-art fully supervised approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:11:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 23:03:18 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Mithun", "Niluthpol Chowdhury", ""], ["Paul", "Sujoy", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1904.04723", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Michael Wu, Michael N. Chin, Robert N. Chin,\n  Allen Y. Yang", "title": "Affordance Analysis of Virtual and Augmented Reality Mediated\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual and augmented reality communication platforms are seen as promising\nmodalities for next-generation remote face-to-face interactions. Our study\nattempts to explore non-verbal communication features in relation to their\nconversation context for virtual and augmented reality mediated communication\nsettings. We perform a series of user experiments, triggering nine conversation\ntasks in 4 settings, each containing corresponding non-verbal communication\nfeatures. Our results indicate that conversation types which involve less\nemotional engagement are more likely to be acceptable in virtual reality and\naugmented reality settings with low-fidelity avatar representation, compared to\nscenarios that involve high emotional engagement or intellectually difficult\ndiscussions. We further systematically analyze and rank the impact of\nlow-fidelity representation of micro-expressions, body scale, head pose, and\nhand gesture in affecting the user experience in one-on-one conversations, and\nvalidate that preserving micro-expression cues plays the most effective role in\nimproving bi-directional conversations in future virtual and augmented reality\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:07:51 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Wu", "Michael", ""], ["Chin", "Michael N.", ""], ["Chin", "Robert N.", ""], ["Yang", "Allen Y.", ""]]}, {"id": "1904.05073", "submitter": "Prateek Verma", "authors": "Prateek Verma, Chris Chafe, Jonathan Berger", "title": "Neuralogram: A Deep Neural Network Based Representation for Audio\n  Signals", "comments": "Submitted to DAFx 2019, the 22nd International Conference on Digital\n  Audio Effects, Birmingham, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Neuralogram -- a deep neural network based representation for\nunderstanding audio signals which, as the name suggests, transforms an audio\nsignal to a dense, compact representation based upon embeddings learned via a\nneural architecture. Through a series of probing signals, we show how our\nrepresentation can encapsulate pitch, timbre and rhythm-based information, and\nother attributes. This representation suggests a method for revealing\nmeaningful relationships in arbitrarily long audio signals that are not readily\nrepresented by existing algorithms. This has the potential for numerous\napplications in audio understanding, music recommendation, meta-data extraction\nto name a few.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 09:04:18 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Verma", "Prateek", ""], ["Chafe", "Chris", ""], ["Berger", "Jonathan", ""]]}, {"id": "1904.05086", "submitter": "Helena Cuesta", "authors": "Helena Cuesta, Emilia G\\'omez, Pritish Chandna", "title": "A Framework for Multi-f0 Modeling in SATB Choir Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fundamental frequency (f0) modeling is an important but relatively unexplored\naspect of choir singing. Performance evaluation as well as auditory analysis of\nsinging, whether individually or in a choir, often depend on extracting f0\ncontours for the singing voice. However, due to the large number of singers,\nsinging at a similar frequency range, extracting the exact individual pitch\ncontours from choir recordings is a challenging task. In this paper, we address\nthis task and develop a methodology for modeling pitch contours of SATB choir\nrecordings. A typical SATB choir consists of four parts, each covering a\ndistinct range of pitches and often with multiple singers each. We first\nevaluate some state-of-the-art multi-f0 estimation systems for the particular\ncase of choirs with a single singer per part, and observe that the pitch of\nindividual singers can be estimated to a relatively high degree of accuracy. We\nobserve, however, that the scenario of multiple singers for each choir part\n(i.e. unison singing) is far more challenging. In this work we propose a\nmethodology based on combining a multi-f0 estimation methodology based on deep\nlearning followed by a set of traditional DSP techniques to model f0 and its\ndispersion instead of a single f0 trajectory for each choir part. We present\nand discuss our observations and test our framework with different singer\nconfigurations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 09:35:50 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Cuesta", "Helena", ""], ["G\u00f3mez", "Emilia", ""], ["Chandna", "Pritish", ""]]}, {"id": "1904.05181", "submitter": "Linxi Jiang", "authors": "Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, Yu-Gang Jiang", "title": "Black-box Adversarial Attacks on Video Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known for their vulnerability to adversarial\nexamples. These are examples that have undergone small, carefully crafted\nperturbations, and which can easily fool a DNN into making misclassifications\nat test time. Thus far, the field of adversarial research has mainly focused on\nimage models, under either a white-box setting, where an adversary has full\naccess to model parameters, or a black-box setting where an adversary can only\nquery the target model for probabilities or labels. Whilst several white-box\nattacks have been proposed for video models, black-box video attacks are still\nunexplored. To close this gap, we propose the first black-box video attack\nframework, called V-BAD. V-BAD utilizes tentative perturbations transferred\nfrom image models, and partition-based rectifications found by the NES on\npartitions (patches) of tentative perturbations, to obtain good adversarial\ngradient estimates with fewer queries to the target model. V-BAD is equivalent\nto estimating the projection of an adversarial gradient on a selected subspace.\nUsing three benchmark video datasets, we demonstrate that V-BAD can craft both\nuntargeted and targeted attacks to fool two state-of-the-art deep video\nrecognition models. For the targeted attack, it achieves $>$93\\% success rate\nusing only an average of $3.4 \\sim 8.4 \\times 10^4$ queries, a similar number\nof queries to state-of-the-art black-box image attacks. This is despite the\nfact that videos often have two orders of magnitude higher dimensionality than\nstatic images. We believe that V-BAD is a promising new tool to evaluate and\nimprove the robustness of video recognition models to black-box adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:41:02 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 06:22:54 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Jiang", "Linxi", ""], ["Ma", "Xingjun", ""], ["Chen", "Shaoxiang", ""], ["Bailey", "James", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1904.06457", "submitter": "Yilin Wang", "authors": "Yilin Wang, Sasi Inguva, Balu Adsumilli", "title": "YouTube UGC Dataset for Video Compression Research", "comments": null, "journal-ref": "2019 IEEE 21st International Workshop on Multimedia Signal\n  Processing (MMSP)", "doi": "10.1109/MMSP.2019.8901772", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-professional video, commonly known as User Generated Content (UGC) has\nbecome very popular in today's video sharing applications. However, traditional\nmetrics used in compression and quality assessment, like BD-Rate and PSNR, are\ndesigned for pristine originals. Thus, their accuracy drops significantly when\nbeing applied on non-pristine originals (the majority of UGC). Understanding\ndifficulties for compression and quality assessment in the scenario of UGC is\nimportant, but there are few public UGC datasets available for research. This\npaper introduces a large scale UGC dataset (1500 20 sec video clips) sampled\nfrom millions of YouTube videos. The dataset covers popular categories like\nGaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel\nsampling method based on features extracted from encoding, challenges for UGC\ncompression and quality evaluation are also discussed. Shortcomings of\ntraditional reference-based metrics on UGC are addressed. We demonstrate a\npromising way to evaluate UGC quality by no-reference objective quality\nmetrics, and evaluate the current dataset with three no-reference metrics\n(Noise, Banding, and SLEEQ).\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 00:39:18 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 23:24:57 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Wang", "Yilin", ""], ["Inguva", "Sasi", ""], ["Adsumilli", "Balu", ""]]}, {"id": "1904.06505", "submitter": "Kede Ma", "authors": "Kede Ma, Wentao Liu, Tongliang Liu, Zhou Wang, Dacheng Tao", "title": "dipIQ: Blind Image Quality Assessment by Learning-to-Rank Discriminable\n  Image Pairs", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2708503", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective assessment of image quality is fundamentally important in many\nimage processing tasks. In this work, we focus on learning blind image quality\nassessment (BIQA) models which predict the quality of a digital image with no\naccess to its original pristine-quality counterpart as reference. One of the\nbiggest challenges in learning BIQA models is the conflict between the gigantic\nimage space (which is in the dimension of the number of image pixels) and the\nextremely limited reliable ground truth data for training. Such data are\ntypically collected via subjective testing, which is cumbersome, slow, and\nexpensive. Here we first show that a vast amount of reliable training data in\nthe form of quality-discriminable image pairs (DIP) can be obtained\nautomatically at low cost by exploiting large-scale databases with diverse\nimage content. We then learn an opinion-unaware BIQA (OU-BIQA, meaning that no\nsubjective opinions are used for training) model using RankNet, a pairwise\nlearning-to-rank (L2R) algorithm, from millions of DIPs, each associated with a\nperceptual uncertainty level, leading to a DIP inferred quality (dipIQ) index.\nExtensive experiments on four benchmark IQA databases demonstrate that dipIQ\noutperforms state-of-the-art OU-BIQA models. The robustness of dipIQ is also\nsignificantly improved as confirmed by the group MAximum Differentiation (gMAD)\ncompetition method. Furthermore, we extend the proposed framework by learning\nmodels with ListNet (a listwise L2R algorithm) on quality-discriminable image\nlists (DIL). The resulting DIL Inferred Quality (dilIQ) index achieves an\nadditional performance gain.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 08:25:54 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ma", "Kede", ""], ["Liu", "Wentao", ""], ["Liu", "Tongliang", ""], ["Wang", "Zhou", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06744", "submitter": "Won-Yong Shin", "authors": "Adeel Malik, Joongheon Kim, Kwang Soon Kim, Won-Yong Shin", "title": "A Personalized Preference Learning Framework for Caching in Mobile\n  Networks", "comments": "21 pages, 10 figures, 1 table, to appear in the IEEE Transactions on\n  Mobile Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR cs.IT cs.LG cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper comprehensively studies a content-centric mobile network based on\na preference learning framework, where each mobile user is equipped with a\nfinite-size cache. We consider a practical scenario where each user requests a\ncontent file according to its own preferences, which is motivated by the\nexistence of heterogeneity in file preferences among different users. Under our\nmodel, we consider a single-hop-based device-to-device (D2D) content delivery\nprotocol and characterize the average hit ratio for the following two file\npreference cases: the personalized file preferences and the common file\npreferences. By assuming that the model parameters such as user activity\nlevels, user file preferences, and file popularity are unknown and thus need to\nbe inferred, we present a collaborative filtering (CF)-based approach to learn\nthese parameters. Then, we reformulate the hit ratio maximization problems into\na submodular function maximization and propose two computationally efficient\nalgorithms including a greedy approach to efficiently solve the cache\nallocation problems. We analyze the computational complexity of each algorithm.\nMoreover, we analyze the corresponding level of the approximation that our\ngreedy algorithm can achieve compared to the optimal solution. Using a\nreal-world dataset, we demonstrate that the proposed framework employing the\npersonalized file preferences brings substantial gains over its counterpart for\nvarious system parameters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:54:53 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:22:14 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 05:43:21 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Malik", "Adeel", ""], ["Kim", "Joongheon", ""], ["Kim", "Kwang Soon", ""], ["Shin", "Won-Yong", ""]]}, {"id": "1904.06807", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso, Yan Yan", "title": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance\n  for Cross-View Image Translation", "comments": "20 pages, 16 figures, accepted to CVPR 2019 as an oral paper", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view image translation is challenging because it involves images with\ndrastically different views and severe deformation. In this paper, we propose a\nnovel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that\nmakes it possible to generate images of natural scenes in arbitrary viewpoints,\nbased on an image of the scene and a novel semantic map. The proposed\nSelectionGAN explicitly utilizes the semantic information and consists of two\nstages. In the first stage, the condition image and the target semantic map are\nfed into a cycled semantic-guided generation network to produce initial coarse\nresults. In the second stage, we refine the initial results by using a\nmulti-channel attention selection mechanism. Moreover, uncertainty maps\nautomatically learned from attentions are used to guide the pixel loss for\nbetter network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top\ndatasets show that our model is able to generate significantly better results\nthan the state-of-the-art methods. The source code, data and trained models are\navailable at https://github.com/Ha0Tang/SelectionGAN.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:04:15 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 20:36:07 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Sebe", "Nicu", ""], ["Wang", "Yanzhi", ""], ["Corso", "Jason J.", ""], ["Yan", "Yan", ""]]}, {"id": "1904.06851", "submitter": "Masashi Nakatani", "authors": "Shiori Honda, Yuri Ishikawa, Rei Konno, Eiko Imai, Natsumi Nomiyama,\n  Kazuki Sakurada, Takuya Koumura, Hirohito M. Kondo, Shigeto Furukawa, Shinya\n  Fujii, and Masashi Nakatani", "title": "Proximal binaural sound can induce subjective frisson", "comments": "21 pages, 3 figures, 3 tables, 3 supplemental figures, 3 supplemental\n  tables", "journal-ref": "Front Psychol. 2020 Mar 3;11:316", "doi": "10.3389/fpsyg.2020.00316.", "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Auditory frisson is the experience of feeling of cold or shivering related to\nsound in the absence of a physical cold stimulus. Multiple examples of\nfrisson-inducing sounds have been reported, but the mechanism of auditory\nfrisson remains elusive. Typical frisson-inducing sounds may contain a looming\neffect, in which a sound appears to approach the listener's peripersonal space.\nPrevious studies on sound in peripersonal space have provided objective\nmeasurements of sound-inducing effects, but few have investigated the\nsubjective experience of frisson-inducing sounds. Here we explored whether it\nis possible to produce subjective feelings of frisson by moving a noise sound\n(white noise, rolling beads noise, or frictional noise produced by rubbing a\nplastic bag) stimulus around a listener's head. Our results demonstrated that\nsound-induced frisson can be experienced stronger when auditory stimuli are\nrotated around the head (binaural moving sounds) than the one without the\nrotation (monaural static sounds), regardless of the source of the noise sound.\nPearson's correlation analysis showed that several acoustic features of\nauditory stimuli, such as variance of interaural level difference (ILD),\nloudness, and sharpness, were correlated with the magnitude of subjective\nfrisson. We had also observed that the subjective feelings of frisson by moving\na musical sound had increased comparing with a static musical sound.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:19:05 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 07:53:12 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Honda", "Shiori", ""], ["Ishikawa", "Yuri", ""], ["Konno", "Rei", ""], ["Imai", "Eiko", ""], ["Nomiyama", "Natsumi", ""], ["Sakurada", "Kazuki", ""], ["Koumura", "Takuya", ""], ["Kondo", "Hirohito M.", ""], ["Furukawa", "Shigeto", ""], ["Fujii", "Shinya", ""], ["Nakatani", "Masashi", ""]]}, {"id": "1904.07080", "submitter": "Li Yang", "authors": "Mai Xu, Li Yang, Xiaoming Tao, Yiping Duan and Zulin Wang", "title": "Saliency Prediction on Omnidirectional Images with Generative\n  Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When watching omnidirectional images (ODIs), subjects can access different\nviewports by moving their heads. Therefore, it is necessary to predict\nsubjects' head fixations on ODIs. Inspired by generative adversarial imitation\nlearning (GAIL), this paper proposes a novel approach to predict saliency of\nhead fixations on ODIs, named SalGAIL. First, we establish a dataset for\nattention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset\nis large-scale, which contains the head fixations of 30 subjects viewing 600\nODIs. Next, we mine our AOI dataset and determine three findings: (1) The\nconsistency of head fixations are consistent among subjects, and it grows\nalongside the increased subject number; (2) The head fixations exist with a\nfront center bias (FCB); and (3) The magnitude of head movement is similar\nacross subjects. According to these findings, our SalGAIL approach applies deep\nreinforcement learning (DRL) to predict the head fixations of one subject, in\nwhich GAIL learns the reward of DRL, rather than the traditional human-designed\nreward. Then, multi-stream DRL is developed to yield the head fixations of\ndifferent subjects, and the saliency map of an ODI is generated via convoluting\npredicted head fixations. Finally, experiments validate the effectiveness of\nour approach in predicting saliency maps of ODIs, significantly better than 10\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:36:40 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Mai", ""], ["Yang", "Li", ""], ["Tao", "Xiaoming", ""], ["Duan", "Yiping", ""], ["Wang", "Zulin", ""]]}, {"id": "1904.07554", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu", "title": "Steganographer Identification", "comments": "A tutorial with 30 pages", "journal-ref": null, "doi": "10.1016/B978-0-12-819438-6.00021-9", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional steganalysis detects the presence of steganography within single\nobjects. In the real-world, we may face a complex scenario that one or some of\nmultiple users called actors are guilty of using steganography, which is\ntypically defined as the Steganographer Identification Problem (SIP). One might\nuse the conventional steganalysis algorithms to separate stego objects from\ncover objects and then identify the guilty actors. However, the guilty actors\nmay be lost due to a number of false alarms. To deal with the SIP, most of the\nstate-of-the-arts use unsupervised learning based approaches. In their\nsolutions, each actor holds multiple digital objects, from which a set of\nfeature vectors can be extracted. The well-defined distances between these\nfeature sets are determined to measure the similarity between the corresponding\nactors. By applying clustering or outlier detection, the most suspicious\nactor(s) will be judged as the steganographer(s). Though the SIP needs further\nstudy, the existing works have good ability to identify the steganographer(s)\nwhen non-adaptive steganographic embedding was applied. In this chapter, we\nwill present foundational concepts and review advanced methodologies in SIP.\nThis chapter is self-contained and intended as a tutorial introducing the SIP\nin the context of media steganography.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 09:34:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wu", "Hanzhou", ""]]}, {"id": "1904.07750", "submitter": "Ruohan Gao", "authors": "Ruohan Gao and Kristen Grauman", "title": "Co-Separating Sounds of Visual Objects", "comments": "ICCV 2019, Project page:\n  http://vision.cs.utexas.edu/projects/coseparation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how objects sound from video is challenging, since they often\nheavily overlap in a single audio channel. Current methods for visually-guided\naudio source separation sidestep the issue by training with artificially mixed\nvideo clips, but this puts unwieldy restrictions on training data collection\nand may even prevent learning the properties of \"true\" mixed sounds. We\nintroduce a co-separation training paradigm that permits learning object-level\nsounds from unlabeled multi-source videos. Our novel training objective\nrequires that the deep neural network's separated audio for similar-looking\nobjects be consistently identifiable, while simultaneously reproducing accurate\nvideo-level audio tracks for each source training pair. Our approach\ndisentangles sounds in realistic test videos, even in cases where an object was\nnot observed individually during training. We obtain state-of-the-art results\non visually-guided audio source separation and audio denoising for the MUSIC,\nAudioSet, and AV-Bench datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:07:50 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 21:18:03 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Gao", "Ruohan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1904.08042", "submitter": "Xin Wen", "authors": "Xin Wen, Zhizhong Han, Xinyu Yin, Yu-Shen Liu", "title": "Adversarial Cross-Modal Retrieval via Learning and Transferring\n  Single-Modal Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval aims to retrieve relevant data across different\nmodalities (e.g., texts vs. images). The common strategy is to apply\nelement-wise constraints between manually labeled pair-wise items to guide the\ngenerators to learn the semantic relationships between the modalities, so that\nthe similar items can be projected close to each other in the common\nrepresentation subspace. However, such constraints often fail to preserve the\nsemantic structure between unpaired but semantically similar items (e.g. the\nunpaired items with the same class label are more similar than items with\ndifferent labels). To address the above problem, we propose a novel cross-modal\nsimilarity transferring (CMST) method to learn and preserve the semantic\nrelationships between unpaired items in an unsupervised way. The key idea is to\nlearn the quantitative similarities in single-modal representation subspace,\nand then transfer them to the common representation subspace to establish the\nsemantic relationships between unpaired items across modalities. Experiments\nshow that our method outperforms the state-of-the-art approaches both in the\nclass-based and pair-based retrieval tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 01:19:51 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wen", "Xin", ""], ["Han", "Zhizhong", ""], ["Yin", "Xinyu", ""], ["Liu", "Yu-Shen", ""]]}, {"id": "1904.08412", "submitter": "Yongsheng Dong", "authors": "Xuelong Li, Quanmao Lu, Yongsheng Dong, and Dacheng Tao", "title": "SCE: A manifold regularized set-covering method for data partitioning", "comments": "14 pages, 10 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, May,\n  2018", "doi": "10.1109/TNNLS.2017.2682179", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis plays a very important role in data analysis. In these\nyears, cluster ensemble, as a cluster analysis tool, has drawn much attention\nfor its robustness, stability, and accuracy. Many efforts have been done to\ncombine different initial clustering results into a single clustering solution\nwith better performance. However, they neglect the structure information of the\nraw data in performing the cluster ensemble. In this paper, we propose a\nStructural Cluster Ensemble (SCE) algorithm for data partitioning formulated as\na set-covering problem. In particular, we construct a Laplacian regularized\nobjective function to capture the structure information among clusters.\nMoreover, considering the importance of the discriminative information\nunderlying in the initial clustering results, we add a discriminative\nconstraint into our proposed objective function. Finally, we verify the\nperformance of the SCE algorithm on both synthetic and real data sets. The\nexperimental results show the effectiveness of our proposed method SCE\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 04:43:17 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Li", "Xuelong", ""], ["Lu", "Quanmao", ""], ["Dong", "Yongsheng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.08418", "submitter": "Mohamed Hamroun", "authors": "Mohamed Hamroun, Sonia Lajmi", "title": "An efficient multi-language Video Search Engine to facilitate the HADJ\n  and the UMRA", "comments": "arXiv admin note: text overlap with arXiv:1308.3225", "journal-ref": "Conference: 17th Scientific Forum for the Research of Hajj, Umrah\n  and Madinah Visit At: Madinah, Saudi Arabia Volume: 17 , May 2017", "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos clips became the most important and prominent multimedia document to\nillustrate the rituals process of Hajj and Umrah. Therefore, it is necessary to\ndevelop a system to facilitate access to information related to the duties, the\npillars, the stages and the prayers. In this paper present a new project\naccomplishing a search engine in a large video database enabling any pilgrims\nto get the information that he care about as fast, accurate. This project is\nbased on two techniques: (a) the weighting method to determine the degree of\naffiliation of a video clip to a particular topic (b) organizing data using\nseveral layers.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:30:54 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Hamroun", "Mohamed", ""], ["Lajmi", "Sonia", ""]]}, {"id": "1904.08504", "submitter": "Takashi Matsubara", "authors": "Kenta Hama, Takashi Matsubara, Kuniaki Uehara, Jianfei Cai", "title": "Exploring Uncertainty Measures for Image-Caption Embedding-and-Retrieval\n  Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide development of black-box machine learning algorithms,\nparticularly deep neural network (DNN), the practical demand for the\nreliability assessment is rapidly rising. On the basis of the concept that\n`Bayesian deep learning knows what it does not know,' the uncertainty of DNN\noutputs has been investigated as a reliability measure for the classification\nand regression tasks. However, in the image-caption retrieval task, well-known\nsamples are not always easy-to-retrieve samples. This study investigates two\naspects of image-caption embedding-and-retrieval systems. On one hand, we\nquantify feature uncertainty by considering image-caption embedding as a\nregression task, and use it for model averaging, which can improve the\nretrieval performance. On the other hand, we further quantify posterior\nuncertainty by considering the retrieval as a classification task, and use it\nas a reliability measure, which can greatly improve the retrieval performance\nby rejecting uncertain queries. The consistent performance of two uncertainty\nmeasures is observed with different datasets (MS COCO and Flickr30k), different\ndeep learning architectures (dropout and batch normalization), and different\nsimilarity functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:19:09 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Hama", "Kenta", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""], ["Cai", "Jianfei", ""]]}, {"id": "1904.08689", "submitter": "Bj\\\"orn {\\TH}\\'or J\\'onsson", "authors": "Bj\\\"orn {\\TH}\\'or J\\'onsson, Omar Shahbaz Khan, Hanna Ragnarsd\\'ottir,\n  {\\TH}\\'orhildur {\\TH}orleiksd\\'ottir, Jan Zah\\'alka, Stevan Rudinac, Gylfi\n  {\\TH}\\'or Gu{\\dh}mundsson, Laurent Amsaleg, Marcel Worring", "title": "Exquisitor: Interactive Learning at Large", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing scale is a dominant trend in today's multimedia collections, which\nespecially impacts interactive applications. To facilitate interactive\nexploration of large multimedia collections, new approaches are needed that are\ncapable of learning on the fly new analytic categories based on the visual and\ntextual content. To facilitate general use on standard desktops, laptops, and\nmobile devices, they must furthermore work with limited computing resources. We\npresent Exquisitor, a highly scalable interactive learning approach, capable of\nintelligent exploration of the large-scale YFCC100M image collection with\nextremely efficient responses from the interactive classifier. Based on\nrelevance feedback from the user on previously suggested items, Exquisitor uses\nsemantic features, extracted from both visual and text attributes, to suggest\nrelevant media items to the user. Exquisitor builds upon the state of the art\nin large-scale data representation, compression and indexing, introducing a\ncluster-based retrieval mechanism that facilitates the efficient suggestions.\nWith Exquisitor, each interaction round over the full YFCC100M collection is\ncompleted in less than 0.3 seconds using a single CPU core. That is 4x less\ntime using 16x smaller computational resources than the most efficient\nstate-of-the-art method, with a positive impact on result quality. These\nresults open up many interesting research avenues, both for exploration of\nindustry-scale media collections and for media exploration on mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:07:04 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 12:41:49 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 05:57:38 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["J\u00f3nsson", "Bj\u00f6rn \u00de\u00f3r", ""], ["Khan", "Omar Shahbaz", ""], ["Ragnarsd\u00f3ttir", "Hanna", ""], ["\u00deorleiksd\u00f3ttir", "\u00de\u00f3rhildur", ""], ["Zah\u00e1lka", "Jan", ""], ["Rudinac", "Stevan", ""], ["Gu\u00f0mundsson", "Gylfi \u00de\u00f3r", ""], ["Amsaleg", "Laurent", ""], ["Worring", "Marcel", ""]]}, {"id": "1904.08971", "submitter": "Mohamed Mansour", "authors": "Amit Chhetri, Mohamed Mansour, Wontak Kim, Guangdong Pan", "title": "On Acoustic Modeling for Broadband Beamforming", "comments": "5 pages, conference", "journal-ref": "European Signal Processing Conference (EUSIPCO 2019)", "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe limitations of the free-field propagation model for\ndesigning broadband beamformers for microphone arrays on a rigid surface.\nTowards this goal, we describe a general framework for quantifying the\nmicrophone array performance in a general wave-field by directly solving the\nacoustic wave equation. The model utilizes Finite-Element-Method (FEM) for\nevaluating the response of the microphone array surface to background 3D planar\nand spherical waves. The effectiveness of the framework is established by\ndesigning and evaluating a representative broadband beamformer under realistic\nacoustic conditions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 18:38:28 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Chhetri", "Amit", ""], ["Mansour", "Mohamed", ""], ["Kim", "Wontak", ""], ["Pan", "Guangdong", ""]]}, {"id": "1904.09115", "submitter": "Di Hu", "authors": "Di Hu, Dong Wang, Xuelong Li, Feiping Nie, Qi Wang", "title": "Listen to the Image", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-to-auditory sensory substitution devices can assist the blind in\nsensing the visual environment by translating the visual information into a\nsound pattern. To improve the translation quality, the task performances of the\nblind are usually employed to evaluate different encoding schemes. In contrast\nto the toilsome human-based assessment, we argue that machine model can be also\ndeveloped for evaluation, and more efficient. To this end, we firstly propose\ntwo distinct cross-modal perception model w.r.t. the late-blind and\ncongenitally-blind cases, which aim to generate concrete visual contents based\non the translated sound. To validate the functionality of proposed models, two\nnovel optimization strategies w.r.t. the primary encoding scheme are presented.\nFurther, we conduct sets of human-based experiments to evaluate and compare\nthem with the conducted machine-based assessments in the cross-modal generation\ntask. Their highly consistent results w.r.t. different encoding schemes\nindicate that using machine model to accelerate optimization evaluation and\nreduce experimental cost is feasible to some extent, which could dramatically\npromote the upgrading of encoding scheme then help the blind to improve their\nvisual perception ability.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:13:34 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Hu", "Di", ""], ["Wang", "Dong", ""], ["Li", "Xuelong", ""], ["Nie", "Feiping", ""], ["Wang", "Qi", ""]]}, {"id": "1904.09360", "submitter": "Li Lin", "authors": "Jennifer Newman, Li Lin, Wenhao Chen, Stephanie Reinders, Yangxiao\n  Wang, Min Wu, and Yong Guan", "title": "StegoAppDB: a Steganography Apps Forensics Image Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new reference dataset simulating digital evidence\nfor image steganography. Steganography detection is a digital image forensic\ntopic that is relatively unknown in practical forensics, although stego app use\nin the wild is on the rise. This paper introduces the first database consisting\nof mobile phone photographs and stego images produced from mobile stego apps,\nincluding a rich set of side information, offering simulated digital evidence.\nStegoAppDB, a steganography apps forensics image database, contains over\n810,000 innocent and stego images using a minimum of 10 different phone models\nfrom 24 distinct devices, with detailed provenanced data comprising a wide\nrange of ISO and exposure settings, EXIF data, message information, embedding\nrates, etc. We develop a camera app, Cameraw, specifically for data\nacquisition, with multiple images per scene, saving simultaneously in both DNG\nand high-quality JPEG formats. Stego images are created from these original\nimages using selected mobile stego apps through a careful process of reverse\nengineering. StegoAppDB contains cover-stego image pairs including for apps\nthat resize the stego dimensions. We retainthe original devices and continue to\nenlarge the database, and encourage the image forensics community to use\nStegoAppDB. While designed for steganography, we discuss uses of this publicly\navailable database to other digital image forensic topics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 22:44:57 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Newman", "Jennifer", ""], ["Lin", "Li", ""], ["Chen", "Wenhao", ""], ["Reinders", "Stephanie", ""], ["Wang", "Yangxiao", ""], ["Wu", "Min", ""], ["Guan", "Yong", ""]]}, {"id": "1904.09917", "submitter": "Alcardo Alex Barakabitze Aab", "authors": "Alcardo Alex Barakabitze, Lingfen Sun, Is-Haka Mkwawa, Emmanuel\n  Ifeachor", "title": "A Novel QoE-Aware SDN-enabled, NFV-based Management Architecture for\n  Future Multimedia Applications on 5G Systems", "comments": null, "journal-ref": "2016", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes a novel QoE-aware SDN enabled NFV architecture for\ncontrolling and managing Future Multimedia Applications on 5G systems. The aim\nis to improve the QoE of the delivered multimedia services through the\nfulfilment of personalized QoE application requirements. This novel approach\nprovides some new features, functionalities, concepts and opportunities for\novercoming the key QoE provisioning limitations in current 4G systems such as\nincreased network management complexity and inability to adapt dynamically to\nchanging application, network transmission or traffic or end-users demand.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 15:19:59 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Barakabitze", "Alcardo Alex", ""], ["Sun", "Lingfen", ""], ["Mkwawa", "Is-Haka", ""], ["Ifeachor", "Emmanuel", ""]]}, {"id": "1904.10128", "submitter": "Peng Gao", "authors": "Peng Gao, Ruyue Yuan, Fei Wang, Liyi Xiao, Hamido Fujita, Yan Zhang", "title": "Siamese Attentional Keypoint Network for High Performance Visual\n  Tracking", "comments": "Accepted by Knowledge-Based SYSTEMS", "journal-ref": null, "doi": "10.1016/j.knosys.2019.105448", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the impacts of three main aspects of visual\ntracking, i.e., the backbone network, the attentional mechanism, and the\ndetection component, and propose a Siamese Attentional Keypoint Network, dubbed\nSATIN, for efficient tracking and accurate localization. Firstly, a new Siamese\nlightweight hourglass network is specially designed for visual tracking. It\ntakes advantage of the benefits of the repeated bottom-up and top-down\ninference to capture more global and local contextual information at multiple\nscales. Secondly, a novel cross-attentional module is utilized to leverage both\nchannel-wise and spatial intermediate attentional information, which can\nenhance both discriminative and localization capabilities of feature maps.\nThirdly, a keypoints detection approach is invented to trace any target object\nby detecting the top-left corner point, the centroid point, and the\nbottom-right corner point of its bounding box. Therefore, our SATIN tracker not\nonly has a strong capability to learn more effective object representations,\nbut also is computational and memory storage efficiency, either during the\ntraining or testing stages. To the best of our knowledge, we are the first to\npropose this approach. Without bells and whistles, experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nrecent benchmark datasets, at a speed far exceeding 27 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 03:02:34 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 03:03:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gao", "Peng", ""], ["Yuan", "Ruyue", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""], ["Fujita", "Hamido", ""], ["Zhang", "Yan", ""]]}, {"id": "1904.10715", "submitter": "Mohamed Hamroun", "authors": "Mohamed Hamroun, Mohamed Salim Bouhlel", "title": "Syst\\`{e}me d'indexation et de recherche de vid\\'{e}o int\\'{e}grant un\n  syst\\`{e}me gestuel pour les personnes handicap\\'{e}es", "comments": "6 pages, in French. International conference Human-Machine\n  Interaction and Image IHMIM14, May 2014, Hammamet, Tunisia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of audio-visual information has increased dramatically with the\nadvent of High Speed Internet. Furthermore, technological advances in recent\nyears in the field of information technology, have simplified the use of video\ndata in various fields by the general public. This made it possible to store\nlarge collections of video documents into computer systems. To enable efficient\nuse of these collections, it is necessary to develop tools to facilitate access\nto these documents and handling them. In this paper we propose a method for\nindexing and retrieval of video sequences in a video database of large\ndimension, based on a weighting technique to calculate the degree of membership\nof a concept in a video also a structuring of the data of the audio-visual\n(context / concept / video). Finally, we decided to create a search system,\noffering in addition to the usual commands, different types of access to the\nsystem, depending on the disability of the person. Indeed, the application\nconsists of a search system but offers access to commands through voice or\ngestures. Our contribution at the experimental level consists with the\nimplementation of prototype. We integrated the techniques proposed in system to\nevaluate it contributions in terms of effectiveness and precision.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:39:11 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Hamroun", "Mohamed", ""], ["Bouhlel", "Mohamed Salim", ""]]}, {"id": "1904.10961", "submitter": "Yuma Kinoshita", "authors": "Chien-Cheng Chien, Yuma Kinoshita, Hitoshi Kiya", "title": "A Noise-aware Enhancement Method for Underexposed Images", "comments": "arXiv admin note: text overlap with arXiv:1811.03280", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method of contrast enhancement is proposed for underexposed images,\nin which heavy noise is hidden. Under low light conditions, images taken by\ndigital cameras have low contrast in dark or bright regions. This is due to a\nlimited dynamic range that imaging sensors have. For these reasons, various\ncontrast enhancement methods have been proposed so far. These methods, however,\nhave two problems: (1) The loss of details in bright regions due to\nover-enhancement of contrast. (2) The noise is amplified in dark regions\nbecause conventional enhancement methods do not consider noise included in\nimages. The proposed method aims to overcome these problems. In the proposed\nmethod, a shadow-up function is applied to adaptive gamma correction with\nweighting distribution, and a denoising filter is also used to avoid noise\nbeing amplified in dark regions. As a result, the proposed method allows us not\nonly to enhance contrast of dark regions, but also to avoid amplifying noise,\neven under strong noise environments.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:42:33 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Chien", "Chien-Cheng", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1904.11754", "submitter": "Olgierd Stankiewicz Dr", "authors": "Olgierd Stankiewicz", "title": "Video coding technique with parametric modeling of noise", "comments": "16 pages, 10 figures, submitted to Opto-Electronics Review - Journal\n  - Elsevier", "journal-ref": null, "doi": "10.1016/j.opelre.2019.05.006", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a video encoding method in which noise is encoded using a\nnovel parametric model representing spectral envelope and spatial distribution\nof energy. The proposed method has been experimentally assessed using video\ntest sequences in a practical setup consisting of a simple, real-time noise\nreduction technique and High Efficiency Video Codec (HEVC). The attained\nresults show that the use of the proposed parametric modeling of noise can\nimprove the subjective quality of reconstructed video by approximately 1.8 Mean\nOpinion Scope (MOS) points (in 11-point scale) related to the classical video\ncoding. Moreover, the present work confirms results attained in the previous\nworks that the usage of even sole noise reduction prior to the encoding\nprovides quality increase.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 10:39:31 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Stankiewicz", "Olgierd", ""]]}, {"id": "1904.12284", "submitter": "Zehua Wang", "authors": "Wei Hu, Qianjiang Hu, Zehua Wang, and Xiang Gao", "title": "3D Dynamic Point Cloud Denoising via Spatial-Temporal Graph Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of accessible depth sensing and 3D laser scanning techniques\nhas enabled the convenient acquisition of 3D dynamic point clouds, which\nprovide efficient representation of arbitrarily-shaped objects in motion.\nNevertheless, dynamic point clouds are often perturbed by noise due to\nhardware, software or other causes. While a plethora of methods have been\nproposed for static point cloud denoising, few efforts are made for the\ndenoising of dynamic point clouds with varying number of irregularly-sampled\npoints in each frame. In this paper, we represent dynamic point clouds\nnaturally on graphs and address the denoising problem by inferring the\nunderlying graph via spatio-temporal graph learning, exploiting both the\nintra-frame similarity and inter-frame consistency. Firstly, assuming the\navailability of a relevant feature vector per node, we pose spatial-temporal\ngraph learning as optimizing a Mahalanobis distance metric $\\mathbf{M}$, which\nis formulated as the minimization of graph Laplacian regularizer. Secondly, to\nease the optimization of the symmetric and positive definite metric matrix\n$\\mathbf{M}$, we decompose it into $\\mathbf{M}=\\mathbf{R}^{\\top}\\mathbf{R}$ and\nsolve $\\mathbf{R}$ instead via proximal gradient. Finally, based on the\nspatial-temporal graph learning, we formulate dynamic point cloud denoising as\nthe joint optimization of the desired point cloud and underlying\nspatio-temporal graph, which leverages both intra-frame affinities and\ninter-frame consistency and is solved via alternating minimization.\nExperimental results show that the proposed method significantly outperforms\nindependent denoising of each frame from state-of-the-art static point cloud\ndenoising approaches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 09:07:26 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 04:06:17 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Hu", "Wei", ""], ["Hu", "Qianjiang", ""], ["Wang", "Zehua", ""], ["Gao", "Xiang", ""]]}, {"id": "1904.12462", "submitter": "Dong Liu", "authors": "Dong Liu, Yue Li, Jianping Lin, Houqiang Li, Feng Wu", "title": "Deep Learning-Based Video Coding: A Review and A Case Study", "comments": null, "journal-ref": "ACM Computing Surveys, vol 53, no 1, article no 11, Feb 2020", "doi": "10.1145/3368405", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has witnessed great success of deep learning technology in\nmany disciplines, especially in computer vision and image processing. However,\ndeep learning-based video coding remains in its infancy. This paper reviews the\nrepresentative works about using deep learning for image/video coding, which\nhas been an actively developing research area since the year of 2015. We divide\nthe related works into two categories: new coding schemes that are built\nprimarily upon deep networks (deep schemes), and deep network-based coding\ntools (deep tools) that shall be used within traditional coding schemes or\ntogether with traditional coding tools. For deep schemes, pixel probability\nmodeling and auto-encoder are the two approaches, that can be viewed as\npredictive coding scheme and transform coding scheme, respectively. For deep\ntools, there have been several proposed techniques using deep learning to\nperform intra-picture prediction, inter-picture prediction, cross-channel\nprediction, probability distribution prediction, transform, post- or in-loop\nfiltering, down- and up-sampling, as well as encoding optimizations. In the\nhope of advocating the research of deep learning-based video coding, we present\na case study of our developed prototype video codec, namely Deep Learning Video\nCoding (DLVC). DLVC features two deep tools that are both based on\nconvolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF)\nand CNN-based block adaptive resolution coding (CNN-BARC). Both tools help\nimprove the compression efficiency by a significant margin. With the two deep\ntools as well as other non-deep coding tools, DLVC is able to achieve on\naverage 39.6\\% and 33.0\\% bits saving than HEVC, under random-access and\nlow-delay configurations, respectively. The source code of DLVC has been\nreleased for future researches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 06:24:07 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Liu", "Dong", ""], ["Li", "Yue", ""], ["Lin", "Jianping", ""], ["Li", "Houqiang", ""], ["Wu", "Feng", ""]]}, {"id": "1904.12620", "submitter": "Tao Li", "authors": "Tao Li and Lei Lin", "title": "AnonymousNet: Natural Face De-Identification with Measurable Privacy", "comments": "CVPR-19 Workshop on Computer Vision: Challenges and Opportunities for\n  Privacy and Security (CV-COPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With billions of personal images being generated from social media and\ncameras of all sorts on a daily basis, security and privacy are unprecedentedly\nchallenged. Although extensive attempts have been made, existing face image\nde-identification techniques are either insufficient in photo-reality or\nincapable of balancing privacy and usability qualitatively and quantitatively,\ni.e., they fail to answer counterfactual questions such as \"is it private\nnow?\", \"how private is it?\", and \"can it be more private?\" In this paper, we\npropose a novel framework called AnonymousNet, with an effort to address these\nissues systematically, balance usability, and enhance privacy in a natural and\nmeasurable manner. The framework encompasses four stages: facial attribute\nestimation, privacy-metric-oriented face obfuscation, directed natural image\nsynthesis, and adversarial perturbation. Not only do we achieve the\nstate-of-the-arts in terms of image quality and attribute prediction accuracy,\nwe are also the first to show that facial privacy is measurable, can be\nfactorized, and accordingly be manipulated in a photo-realistic fashion to\nfulfill different requirements and application scenarios. Experiments further\ndemonstrate the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:57:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Tao", ""], ["Lin", "Lei", ""]]}, {"id": "1904.12628", "submitter": "Onkar Krishna", "authors": "Onkar Krishna, Kiyoharu Aizawa, Go Irie", "title": "Computational Attention System for Children, Adults and Elderly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing computational visual attention systems have focused on the\nobjective to basically simulate and understand the concept of visual attention\nsystem in adults. Consequently, the impact of observer's age in scene viewing\nbehavior has rarely been considered. This study quantitatively analyzed the\nage-related differences in gaze landings during scene viewing for three\ndifferent class of images: naturals, man-made, and fractals. Observer's of\ndifferent age-group have shown different scene viewing tendencies independent\nto the class of the image viewed. Several interesting observations are drawn\nfrom the results. First, gaze landings for man-made dataset showed that whereas\nchild observers focus more on the scene foreground, i.e., locations that are\nnear, elderly observers tend to explore the scene background, i.e., locations\nfarther in the scene. Considering this result a framework is proposed in this\npaper to quantitatively measure the depth bias tendency across age groups.\nSecond, the quantitative analysis results showed that children exhibit the\nlowest exploratory behavior level but the highest central bias tendency among\nthe age groups and across the different scene categories. Third,\ninter-individual similarity metrics reveal that an adult had significantly\nlower gaze consistency with children and elderly compared to other adults for\nall the scene categories. Finally, these analysis results were consequently\nleveraged to develop a more accurate age-adapted saliency model independent to\nthe image type. The prediction accuracy suggests that our model fits better to\nthe collected eye-gaze data of the observers belonging to different age groups\nthan the existing models do.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 06:22:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Krishna", "Onkar", ""], ["Aizawa", "Kiyoharu", ""], ["Irie", "Go", ""]]}, {"id": "1904.13325", "submitter": "Chih-Yi Chiu", "authors": "Sarawut Markchit and Chih-Yi Chiu", "title": "Effective and Efficient Indexing in Cross-Modal Hashing-Based Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the barrier of storage and computation, the hashing technique has\nbeen widely used for nearest neighbor search in multimedia retrieval\napplications recently. Particularly, cross-modal retrieval that searches across\ndifferent modalities becomes an active but challenging problem. Although dozens\nof cross-modal hashing algorithms are proposed to yield compact binary codes,\nthe exhaustive search is impractical for the real-time purpose, and Hamming\ndistance computation suffers inaccurate results. In this paper, we propose a\nnovel search method that utilizes a probability-based index scheme over binary\nhash codes in cross-modal retrieval. The proposed hash code indexing scheme\nexploits a few binary bits of the hash code as the index code. We construct an\ninverted index table based on index codes and train a neural network to improve\nthe indexing accuracy and efficiency. Experiments are performed on two\nbenchmark datasets for retrieval across image and text modalities, where hash\ncodes are generated by three cross-modal hashing methods. Results show the\nproposed method effectively boost the performance on these hash methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:41:05 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 01:59:24 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Markchit", "Sarawut", ""], ["Chiu", "Chih-Yi", ""]]}]