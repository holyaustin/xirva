[{"id": "1603.00859", "submitter": "Konstantin Miller", "authors": "Konstantin Miller, Abdel-Karim Al-Tamimi, Adam Wolisz", "title": "QoE-Based Low-Delay Live Streaming Using Throughput Predictions", "comments": "Technical Report TKN-16-001, Telecommunication Networks Group,\n  Technische Universitaet Berlin. This TR updated TR TKN-15-001", "journal-ref": null, "doi": null, "report-no": "TKN-16-001", "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, HTTP-based adaptive streaming has become the de facto standard for\nvideo streaming over the Internet. It allows clients to dynamically adapt media\ncharacteristics to network conditions in order to ensure a high quality of\nexperience, that is, minimize playback interruptions, while maximizing video\nquality at a reasonable level of quality changes. In the case of live\nstreaming, this task becomes particularly challenging due to the latency\nconstraints. The challenge further increases if a client uses a wireless\nnetwork, where the throughput is subject to considerable fluctuations.\nConsequently, live streams often exhibit latencies of up to 30 seconds. In the\npresent work, we introduce an adaptation algorithm for HTTP-based live\nstreaming called LOLYPOP (Low-Latency Prediction-Based Adaptation) that is\ndesigned to operate with a transport latency of few seconds. To reach this\ngoal, LOLYPOP leverages TCP throughput predictions on multiple time scales,\nfrom 1 to 10 seconds, along with an estimate of the prediction error\ndistribution. In addition to satisfying the latency constraint, the algorithm\nheuristically maximizes the quality of experience by maximizing the average\nvideo quality as a function of the number of skipped segments and quality\ntransitions. In order to select an efficient prediction method, we studied the\nperformance of several time series prediction methods in IEEE 802.11 wireless\naccess networks. We evaluated LOLYPOP under a large set of experimental\nconditions limiting the transport latency to 3 seconds, against a\nstate-of-the-art adaptation algorithm from the literature, called FESTIVE. We\nobserved that the average video quality is by up to a factor of 3 higher than\nwith FESTIVE. We also observed that LOLYPOP is able to reach a broader region\nin the quality of experience space, and thus it is better adjustable to the\nuser profile or service provider requirements.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 20:38:01 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 10:27:12 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Miller", "Konstantin", ""], ["Al-Tamimi", "Abdel-Karim", ""], ["Wolisz", "Adam", ""]]}, {"id": "1603.01068", "submitter": "Paolo Bestagini", "authors": "Luca Bondi and Luca Baroffio and David G\\\"uera and Paolo Bestagini and\n  Edward J. Delp and Stefano Tubaro", "title": "First Steps Toward Camera Model Identification with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2641006", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the camera model used to shoot a picture enables to solve a wide\nseries of forensic problems, from copyright infringement to ownership\nattribution. For this reason, the forensic community has developed a set of\ncamera model identification algorithms that exploit characteristic traces left\non acquired images by the processing pipelines specific of each camera model.\nIn this paper, we investigate a novel approach to solve camera model\nidentification problem. Specifically, we propose a data-driven algorithm based\non convolutional neural networks, which learns features characterizing each\ncamera model directly from the acquired pictures. Results on a well-known\ndataset of 18 camera models show that: (i) the proposed method outperforms\nup-to-date state-of-the-art algorithms on classification of 64x64 color image\npatches; (ii) features learned by the proposed network generalize to camera\nmodels never used for training.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 12:10:47 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 09:29:28 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Bondi", "Luca", ""], ["Baroffio", "Luca", ""], ["G\u00fcera", "David", ""], ["Bestagini", "Paolo", ""], ["Delp", "Edward J.", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1603.02472", "submitter": "Amaya Nogales Gomez", "authors": "Dimitrios Tsilimantos, Amaya Nogales-G\\'omez, and Stefan Valentin", "title": "Anticipatory Radio Resource Management for Mobile Video Streaming with\n  Linear Programming", "comments": "6 pages, 5 figures, ICC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In anticipatory networking, channel prediction is used to improve\ncommunication performance. This paper describes a new approach for allocating\nresources to video streaming traffic while accounting for quality of service.\nThe proposed method is based on integrating a model of the user's local\nplay-out buffer into the radio access network. The linearity of this model\nallows to formulate a Linear Programming problem that optimizes the trade-off\nbetween the allocated resources and the stalling time of the media stream. Our\nsimulation results demonstrate the full power of anticipatory optimization in a\nsimple, yet representative, scenario. Compared to instantaneous adaptation, our\nanticipatory solution shows impressive gains in spectral efficiency and\nstalling duration at feasible computation time while being robust against\nprediction errors.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 10:45:05 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Tsilimantos", "Dimitrios", ""], ["Nogales-G\u00f3mez", "Amaya", ""], ["Valentin", "Stefan", ""]]}, {"id": "1603.02980", "submitter": "Chau-Wai Wong", "authors": "Chau-Wai Wong, Guan-Ming Su, Min Wu", "title": "Impact Analysis of Baseband Quantizer on Coding Efficiency for HDR Video", "comments": "Accepted for publication in IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2016.2597175", "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitally acquired high dynamic range (HDR) video baseband signal can take 10\nto 12 bits per color channel. It is economically important to be able to reuse\nthe legacy 8 or 10-bit video codecs to efficiently compress the HDR video.\nLinear or nonlinear mapping on the intensity can be applied to the baseband\nsignal to reduce the dynamic range before the signal is sent to the codec, and\nwe refer to this range reduction step as a baseband quantization. We show\nanalytically and verify using test sequences that the use of the baseband\nquantizer lowers the coding efficiency. Experiments show that as the baseband\nquantizer is strengthened by 1.6 bits, the drop of PSNR at a high bitrate is up\nto 1.60dB. Our result suggests that in order to achieve high coding efficiency,\ninformation reduction of videos in terms of quantization error should be\nintroduced in the video codec instead of on the baseband signal.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 17:59:26 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 01:58:59 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 22:20:23 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Wong", "Chau-Wai", ""], ["Su", "Guan-Ming", ""], ["Wu", "Min", ""]]}, {"id": "1603.03129", "submitter": "Jean-Marc Valin", "authors": "Thomas J. Daede, Nathan E. Egge, Jean-Marc Valin, Guillaume Martres,\n  Timothy B. Terriberry", "title": "Daala: A Perceptually-Driven Next Generation Video Codec", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Daala project is a royalty-free video codec that attempts to compete with\nthe best patent-encumbered codecs. Part of our strategy is to replace core\ntools of traditional video codecs with alternative approaches, many of them\ndesigned to take perceptual aspects into account, rather than optimizing for\nsimple metrics like PSNR. This paper documents some of our experiences with\nthese tools, which ones worked and which did not, and what we've learned from\nthem. The result is a codec which compares favorably with HEVC on still images,\nand is on a path to do so for video as well.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 02:40:05 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Daede", "Thomas J.", ""], ["Egge", "Nathan E.", ""], ["Valin", "Jean-Marc", ""], ["Martres", "Guillaume", ""], ["Terriberry", "Timothy B.", ""]]}, {"id": "1603.03482", "submitter": "Nathan Egge", "authors": "Nathan E. Egge and Jean-Marc Valin", "title": "Predicting Chroma from Luma with Frequency Domain Intra Prediction", "comments": "10 pages, 7 figures", "journal-ref": "Proceedings of SPIE 9410, Visual Information Processing and\n  Communication VI, 941009 (March 4, 2015)", "doi": "10.1117/12.2080837", "report-no": null, "categories": "cs.MM cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a technique for performing intra prediction of the\nchroma planes based on the reconstructed luma plane in the frequency domain.\nThis prediction exploits the fact that while RGB to YUV color conversion has\nthe property that it decorrelates the color planes globally across an image,\nthere is still some correlation locally at the block level. Previous proposals\ncompute a linear model of the spatial relationship between the luma plane (Y)\nand the two chroma planes (U and V). In codecs that use lapped transforms this\nis not possible since transform support extends across the block boundaries and\nthus neighboring blocks are unavailable during intra-prediction. We design a\nfrequency domain intra predictor for chroma that exploits the same local\ncorrelation with lower complexity than the spatial predictor and which works\nwith lapped transforms. We then describe a low-complexity algorithm that\ndirectly uses luma coefficients as a chroma predictor based on gain-shape\nquantization and band partitioning. An experiment is performed that compares\nthese two techniques inside the experimental Daala video codec and shows the\nlower complexity algorithm to be a better chroma predictor.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 22:55:36 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Egge", "Nathan E.", ""], ["Valin", "Jean-Marc", ""]]}, {"id": "1603.04930", "submitter": "Michael Iliadis", "authors": "Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos", "title": "Deep Fully-Connected Networks for Video Compressive Sensing", "comments": "14 pages, to appear in Elsevier Digital Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a deep learning framework for video compressive\nsensing. The proposed formulation enables recovery of video frames in a few\nseconds at significantly improved reconstruction quality compared to previous\napproaches. Our investigation starts by learning a linear mapping between video\nsequences and corresponding measured frames which turns out to provide\npromising results. We then extend the linear formulation to deep\nfully-connected networks and explore the performance gains using deeper\narchitectures. Our analysis is always driven by the applicability of the\nproposed framework on existing compressive video architectures. Extensive\nsimulations on several video sequences document the superiority of our approach\nboth quantitatively and qualitatively. Finally, our analysis offers insights\ninto understanding how dataset sizes and number of layers affect reconstruction\nperformance while raising a few points for future investigation.\n  Code is available at Github: https://github.com/miliadis/DeepVideoCS\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 01:15:35 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 23:26:43 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Iliadis", "Michael", ""], ["Spinoulas", "Leonidas", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1603.06083", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini and Gregorij Kurillo and Seyed Rasoul Etesami and\n  Jiang Yu", "title": "Towards Coordinated Bandwidth Adaptations for Hundred-Scale 3D\n  Tele-Immersive Systems", "comments": "Springer Multimedia Systems Journal, 14 pages, March 2016", "journal-ref": null, "doi": "10.1007/s00530-016-0511-z", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D tele-immersion improves the state of collaboration among geographically\ndistributed participants. Unlike the traditional 2D videos, a 3D tele-immersive\nsystem employs multiple 3D cameras based in each physical site to cover a much\nlarger field of view, generating a very large amount of stream data. One of the\nmajor challenges is how to efficiently transmit these bulky 3D streaming data\nto bandwidth-constrained sites. In this paper, we study an adaptive Human\nVisual System (HVS) -compliant bandwidth management framework for efficient\ndelivery of hundred-scale streams produced from distributed 3D tele-immersive\nsites to a receiver site with limited bandwidth budget. Our adaptation\nframework exploits the semantics link of HVS with multiple 3D streams in the 3D\ntele-immersive environment. We developed TELEVIS, a visual simulation tool to\nshowcase a HVS-aware tele-immersive system for realistic cases. Our evaluation\nresults show that the proposed adaptation can improve the total quality per\nunit of bandwidth used to deliver streams in 3D tele-immersive systems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 11:43:01 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 19:30:21 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Kurillo", "Gregorij", ""], ["Etesami", "Seyed Rasoul", ""], ["Yu", "Jiang", ""]]}, {"id": "1603.06123", "submitter": "Ana De Abreu", "authors": "Ana De Abreu, Gene Cheung, Pascal Frossard, Fernando Pereira", "title": "Optimal Lagrange Multipliers for Dependent Rate Allocation in Video\n  Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical video rate allocation problem, the objective is to optimally\ndistribute a source rate budget among a set of (in)dependently coded data units\nto minimize the total distortion of all units. Conventional Lagrangian\napproaches convert the lone rate constraint to a linear rate penalty scaled by\na multiplier in the objective, resulting in a simpler unconstrained\nformulation. However, the search for the \"optimal\" multiplier, one that results\nin a distortion-minimizing solution among all Lagrangian solutions that satisfy\nthe original rate constraint, remains an elusive open problem in the general\nsetting. To address this problem, we propose a computation-efficient search\nstrategy to identify this optimal multiplier numerically. Specifically, we\nfirst formulate a general rate allocation problem where each data unit can be\ndependently coded at different quantization parameters (QP) using a previous\nunit as predictor, or left uncoded at the encoder and subsequently interpolated\nat the decoder using neighboring coded units. After converting the original\nrate constrained problem to the unconstrained Lagrangian counterpart, we design\nan efficient dynamic programming (DP) algorithm that finds the optimal\nLagrangian solution for a fixed multiplier. Finally, within the DP framework,\nwe iteratively compute neighboring singular multiplier values, each resulting\nin multiple simultaneously optimal Lagrangian solutions, to drive the rates of\nthe computed Lagrangian solutions towards the bit budget. We terminate when a\nsingular multiplier value results in two Lagrangian solutions with rates below\nand above the bit budget. In extensive monoview and multiview video coding\nexperiments, we show that our DP algorithm and selection of optimal multipliers\non average outperform comparable rate control solutions used in video\ncompression standards such as HEVC that do not skip frames in Y-PSNR.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:02:17 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["De Abreu", "Ana", ""], ["Cheung", "Gene", ""], ["Frossard", "Pascal", ""], ["Pereira", "Fernando", ""]]}, {"id": "1603.07990", "submitter": "Raj Jain", "authors": "Abdel-Karim Al-Tamimi, Raj Jain, Chakchai So-In", "title": "Modeling and Resource Allocation for HD Videos over WiMAX Broadband\n  Wireless Networks", "comments": null, "journal-ref": "IEEE Communication Society Multimedia Communications Technical\n  Committee, E-letter Vol. 5, No. 3, May 2010", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile video is considered a major upcoming application and revenue generator\nfor broadband wireless networks like WiMAX and LTE. Therefore, it is important\nto design a proper resource allocation scheme for mobile video, since video\ntraffic is both throughput consuming and delay sensitive.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 19:35:59 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Al-Tamimi", "Abdel-Karim", ""], ["Jain", "Raj", ""], ["So-In", "Chakchai", ""]]}, {"id": "1603.09012", "submitter": "Laleh Jalali", "authors": "Laleh Jalali and Ramesh Jain", "title": "A framework for event co-occurrence detection in event streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that characterizing co-occurrence between events is an\nimportant but non-trivial and neglected aspect of discovering potential causal\nrelationships in multimedia event streams. First an introduction to the notion\nof event co-occurrence and its relation to co-occurrence pattern detection is\ngiven. Then a finite state automaton extended with a time model and event\nparameterization is introduced to convert high level co-occurrence pattern\ndefinition to its corresponding pattern matching automaton. Finally a\nprocessing algorithm is applied to count the occurrence frequency of a\ncollection of patterns with only one pass through input event streams. The\nmethod proposed in this paper can be used for detecting co-occurrences between\nboth events of one event stream (Auto co-occurrence), and events from multiple\nevent streams (Cross co-occurrence). Some fundamental results concerning the\ncharacterization of event co-occurrence are presented in form of a visual co-\noccurrence matrix. Reusable causality rules can be extracted easily from\nco-occurrence matrix and fed into various analysis tools, such as\nrecommendation systems and complex event processing systems for further\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 01:16:37 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Jalali", "Laleh", ""], ["Jain", "Ramesh", ""]]}, {"id": "1603.09396", "submitter": "Malihe Mardanpour", "authors": "Malihe Mardanpour, Mohammad Ali Zare Chahooki", "title": "Robust Hybrid Image Watermarking based on Discrete Wavelet and Shearlet\n  Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of digital networks such as the Internet, digital media have\nbeen explosively developed in e-commerce and online services. This causes\nproblems such as illegal copy and fake ownership. Watermarking is proposed as\none of the solutions to such cases. Among different watermarking techniques,\nthe wavelet transform has been used more because of its good ability in\nmodeling the human visual system. Recently, Shearlet transform as an extension\nof Wavelet transform which is based on multi-resolution and multi-directional\nanalysis is introduced. The most important feature of this transform is the\nappropriate representation of image edges. In this paper a hybrid scheme using\nDiscrete Wavelet Transform (DWT) and Discrete Shearlet Transform (DST) is\npresented. In this way, the host image is decomposed using DWT, and then its\nlow frequency sub-band is decomposed by DST. After that, the bidiagonal\nsingular value decomposition (BSVD) is applied on the selected sub-band from\nShearlet transform and the gray-scale watermark image is embedded into its\nbidiagonal singular values. The proposed method is examined on the images with\ndifferent textures and resistance is evaluated against various attacks like\nimage processing and geometric attacks. The results show good transparency and\nhigh robustness in proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 22:03:19 GMT"}], "update_date": "2016-04-02", "authors_parsed": [["Mardanpour", "Malihe", ""], ["Chahooki", "Mohammad Ali Zare", ""]]}]