[{"id": "1903.01277", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Deep Inverse Tone Mapping Using LDR Based Learning for Estimating HDR\n  Images with Absolute Luminance", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.05686", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel inverse tone mapping method using a convolutional\nneural network (CNN) with LDR based learning is proposed. In conventional\ninverse tone mapping with CNNs, generated HDR images cannot have absolute\nluminance, although relative luminance can. Moreover, loss functions suitable\nfor learning HDR images are problematic, so it is difficult to train CNNs by\ndirectly using HDR images. In contrast, the proposed method enables us not only\nto estimate absolute luminance, but also to train a CNN by using LDR images.\nThe CNN used in the proposed method learns a transformation from various input\nLDR images to LDR images mapped by Reinhard's global operator. Experimental\nresults show that HDR images generated by the proposed method have\nhigher-quality than HDR ones generated by conventional inverse tone mapping\nmethods,in terms of HDR-VDP-2.2 and PU encoding + MS-SSIM.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:06:26 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1903.01545", "submitter": "Svebor Karaman", "authors": "Svebor Karaman, Xudong Lin, Xuefeng Hu, Shih-Fu Chang", "title": "Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised hashing method which aims to produce binary codes\nthat preserve the ranking induced by a real-valued representation. Such compact\nhash codes enable the complete elimination of real-valued feature storage and\nallow for significant reduction of the computation complexity and storage cost\nof large-scale image retrieval applications. Specifically, we learn a neural\nnetwork-based model, which transforms the input representation into a binary\nrepresentation. We formalize the training objective of the network in an\nintuitive and effective way, considering each training sample as a query and\naiming to obtain the same retrieval results using the produced hash codes as\nthose obtained with the original features. This training formulation directly\noptimizes the hashing model for the target usage of the hash codes it produces.\nWe further explore the addition of a decoder trained to obtain an approximated\nreconstruction of the original features. At test time, we retrieved the most\npromising database samples with an efficient graph-based search procedure using\nonly our hash codes and perform re-ranking using the reconstructed features,\nthus without needing to access the original features at all. Experiments\nconducted on multiple publicly available large-scale datasets show that our\nmethod consistently outperforms all compared state-of-the-art unsupervised\nhashing methods and that the reconstruction procedure can effectively boost the\nsearch accuracy with a minimal constant additional cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 21:24:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Karaman", "Svebor", ""], ["Lin", "Xudong", ""], ["Hu", "Xuefeng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1903.01743", "submitter": "Zheng Li", "authors": "Zheng Li, Chengyu Hu, Yang Zhang, Shanqing Guo", "title": "How to Prove Your Model Belongs to You: A Blind-Watermark based\n  Framework to Protect Intellectual Property of DNN", "comments": "To be published in ACSAC'19", "journal-ref": null, "doi": "10.1145/3359789.3359801", "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have made tremendous progress in a variety of\nchallenging tasks, such as image recognition and machine translation, during\nthe past decade. Training deep neural networks is computationally expensive and\nrequires both human and intellectual resources. Therefore, it is necessary to\nprotect the intellectual property of the model and externally verify the\nownership of the model. However, previous studies either fail to defend against\nthe evasion attack or have not explicitly dealt with fraudulent claims of\nownership by adversaries. Furthermore, they can not establish a clear\nassociation between the model and the creator's identity.\n  To fill these gaps, in this paper, we propose a novel intellectual property\nprotection (IPP) framework based on blind-watermark for watermarking deep\nneural networks that meet the requirements of security and feasibility. Our\nframework accepts ordinary samples and the exclusive logo as inputs, outputting\nnewly generated samples as watermarks, which are almost indistinguishable from\nthe origin, and infuses these watermarks into DNN models by assigning specific\nlabels, leaving the backdoor as the basis for our copyright claim. We evaluated\nour IPP framework on two benchmark datasets and 15 popular deep learning\nmodels. The results show that our framework successfully verifies the ownership\nof all the models without a noticeable impact on their primary task. Most\nimportantly, we are the first to successfully design and implement a\nblind-watermark based framework, which can achieve state-of-art performances on\nundetectability against evasion attack and unforgeability against fraudulent\nclaims of ownership. Further, our framework shows remarkable robustness and\nestablishes a clear association between the model and the author's identity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 09:32:36 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 08:43:51 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2019 06:54:43 GMT"}, {"version": "v4", "created": "Thu, 7 Nov 2019 13:08:57 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Li", "Zheng", ""], ["Hu", "Chengyu", ""], ["Zhang", "Yang", ""], ["Guo", "Shanqing", ""]]}, {"id": "1903.02165", "submitter": "Jon McCormack", "authors": "Dilpreet Singh, Nina Rajcic, Simon Colton and Jon McCormack", "title": "Camera Obscurer: Generative Art for Design Inspiration", "comments": "Accepted for EvoMUSART 2019: 8th International Conference on\n  Computational Intelligence in Music, Sound, Art and Design. April 2019,\n  Leipzig, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate using generated decorative art as a source of inspiration for\ndesign tasks. Using a visual similarity search for image retrieval, the\n\\emph{Camera Obscurer} app enables rapid searching of tens of thousands of\ngenerated abstract images of various types. The seed for a visual similarity\nsearch is a given image, and the retrieved generated images share some visual\nsimilarity with the seed. Implemented in a hand-held device, the app empowers\nusers to use photos of their surroundings to search through the archive of\ngenerated images and other image archives. Being abstract in nature, the\nretrieved images supplement the seed image rather than replace it, providing\ndifferent visual stimuli including shapes, colours, textures and\njuxtapositions, in addition to affording their own interpretations. This\napproach can therefore be used to provide inspiration for a design task, with\nthe abstract images suggesting new ideas that might give direction to a graphic\ndesign project. We describe a crowdsourcing experiment with the app to estimate\nuser confidence in retrieved images, and we describe a pilot study where Camera\nObscurer provided inspiration for a design task. These experiments have enabled\nus to describe future improvements, and to begin to understand sources of\nvisual inspiration for design tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 04:05:47 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Singh", "Dilpreet", ""], ["Rajcic", "Nina", ""], ["Colton", "Simon", ""], ["McCormack", "Jon", ""]]}, {"id": "1903.02971", "submitter": "Dimitri Podborski", "authors": "Dimitri Podborski, Jangwoo Son, Gurdeep Singh Bhullar, Robert Skupin,\n  Yago Sanchez, Cornelius Hellge, Thomas Schierl", "title": "HTML5 MSE Playback of MPEG 360 VR Tiled Streaming", "comments": "Accepted for the demo track of ACM MMSys'19", "journal-ref": null, "doi": "10.1145/3304109.3323835", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) and 360-degree video streaming have gained significant\nattention in recent years. First standards have been published in order to\navoid market fragmentation. For instance, 3GPP released its first VR\nspecification to enable 360-degree video streaming over 5G networks which\nrelies on several technologies specified in ISO/IEC 23090-2, also known as\nMPEG-OMAF. While some implementations of OMAF-compatible players have already\nbeen demonstrated at several trade shows, so far, no web browser-based\nimplementations have been presented. In this demo paper we describe a\nbrowser-based JavaScript player implementation of the most advanced media\nprofile of OMAF: HEVC-based viewport-dependent OMAF video profile, also known\nas tile-based streaming, with multi-resolution HEVC tiles. We also describe the\napplied workarounds for the implementation challenges we encountered with\nstate-of-the-art HTML5 browsers. The presented implementation was tested in the\nSafari browser with support of HEVC video through the HTML5 Media Source\nExtensions API. In addition, the WebGL API was used for rendering, using\nregion-wise packing metadata as defined in OMAF.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:02:27 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 13:50:44 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Podborski", "Dimitri", ""], ["Son", "Jangwoo", ""], ["Bhullar", "Gurdeep Singh", ""], ["Skupin", "Robert", ""], ["Sanchez", "Yago", ""], ["Hellge", "Cornelius", ""], ["Schierl", "Thomas", ""]]}, {"id": "1903.03247", "submitter": "Toshiaki Koike-Akino", "authors": "Takuya Fujihashi, Toshiaki Koike-Akino, Takashi Watanabe, Philip V.\n  Orlik", "title": "HoloCast: Graph Signal Processing for Graceful Point Cloud Delivery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional point cloud delivery, a sender uses octree-based digital\nvideo compression to stream three-dimensional (3D) points and the corresponding\ncolor attributes over band-limited links, e.g., wireless channels, for 3D scene\nreconstructions. However, the digital-based delivery schemes have an issue\ncalled cliff effect, where the 3D reconstruction quality is a step function in\nterms of wireless channel quality. We propose a novel scheme of point cloud\ndelivery, called HoloCast, to gracefully improve the reconstruction quality\nwith the improvement of wireless channel quality. HoloCast regards the 3D\npoints and color components as graph signals and directly transmits\nlinear-transformed signals based on graph Fourier transform (GFT), without\ndigital quantization and entropy coding operations. One of main contributions\nin HoloCast is that the use of GFT can deal with non-ordered and non-uniformly\ndistributed multi-dimensional signals such as holographic data unlike\nconventional delivery schemes. Performance results with point cloud data show\nthat HoloCast yields better 3D reconstruction quality compared to digital-based\nmethods in noisy wireless environment.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 02:02:51 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Fujihashi", "Takuya", ""], ["Koike-Akino", "Toshiaki", ""], ["Watanabe", "Takashi", ""], ["Orlik", "Philip V.", ""]]}, {"id": "1903.04235", "submitter": "Zhao Kang", "authors": "Zhao Kang, Yiwei Lu, Yuanzhang Su, Changsheng Li, Zenglin Xu", "title": "Similarity Learning via Kernel Preserving Embedding", "comments": "Published in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data similarity is a key concept in many data-driven applications. Many\nalgorithms are sensitive to similarity measures. To tackle this fundamental\nproblem, automatically learning of similarity information from data via\nself-expression has been developed and successfully applied in various models,\nsuch as low-rank representation, sparse subspace learning, semi-supervised\nlearning. However, it just tries to reconstruct the original data and some\nvaluable information, e.g., the manifold structure, is largely ignored. In this\npaper, we argue that it is beneficial to preserve the overall relations when we\nextract similarity information. Specifically, we propose a novel similarity\nlearning framework by minimizing the reconstruction error of kernel matrices,\nrather than the reconstruction error of original data adopted by existing work.\nTaking the clustering task as an example to evaluate our method, we observe\nconsiderable improvements compared to other state-of-the-art methods. More\nimportantly, our proposed framework is very general and provides a novel and\nfundamental building block for many other similarity-based tasks. Besides, our\nproposed kernel preserving opens up a large number of possibilities to embed\nhigh-dimensional data into low-dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 11:58:40 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Yiwei", ""], ["Su", "Yuanzhang", ""], ["Li", "Changsheng", ""], ["Xu", "Zenglin", ""]]}, {"id": "1903.05297", "submitter": "Zheng Li", "authors": "Zheng Li, Ge Han, Yunqing Wei, Shanqing Guo", "title": "Learning Symmetric and Asymmetric Steganography via Adversarial Training", "comments": "Some experiments need to be done", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography refers to the art of concealing secret messages within multiple\nmedia carriers so that an eavesdropper is unable to detect the presence and\ncontent of the hidden messages. In this paper, we firstly propose a novel\nkey-dependent steganographic scheme that achieves steganographic objectives\nwith adversarial training. Symmetric (secret-key) and Asymmetric (public-key)\nsteganographic scheme are separately proposed and each scheme is successfully\ndesigned and implemented. We show that these encodings produced by our scheme\nimprove the invisibility by 20% than previous deep-leanring-based work, and\nfurther that perform competitively remarkable undetectability 25% better than\nclassic steganographic algorithms. Finally, we simulated our scheme in a real\nsituation where the decoder achieved an accuracy of more than 98% of the\noriginal message.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 02:55:09 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 07:36:23 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Li", "Zheng", ""], ["Han", "Ge", ""], ["Wei", "Yunqing", ""], ["Guo", "Shanqing", ""]]}, {"id": "1903.05940", "submitter": "Lucjan Janowski", "authors": "Lucjan Janowski and Jakub Nawa{\\l}a and Werner Robitza and Zhi Li and\n  Luk\\'a\\v{s} Kasula and Krzysztof Rusek", "title": "Notation for Subject Answer Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is believed that consistent notation helps the research community in many\nways. First and foremost, it provides a consistent interface of communication.\nSubjective experiments described according to uniform rules are easier to\nunderstand and analyze. Additionally, a comparison of various results is less\ncomplicated. In this publication we describe notation proposed by VQEG (Video\nQuality Expert Group) working group SAM (Statistical Analysis and Methods).\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:26:23 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Janowski", "Lucjan", ""], ["Nawa\u0142a", "Jakub", ""], ["Robitza", "Werner", ""], ["Li", "Zhi", ""], ["Kasula", "Luk\u00e1\u0161", ""], ["Rusek", "Krzysztof", ""]]}, {"id": "1903.06253", "submitter": "Marijana Kracunov", "authors": "Marijana Kracunov, Milica Bastica, Jovana Tesovic", "title": "Object tracking in video signals using Compressive Sensing", "comments": "Student paper submitted to \"The 8th Mediterranean Conference on\n  Embedded Computing\" - MECO'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the number of pixels in video signals while maintaining quality\nneeded for recovering the trace of an object using Compressive Sensing is main\nsubject of this work. Quality of frames, from video that contains moving\nobject, are gradually reduced by keeping different number of pixels in each\niteration, going from 45% all the way to 1%. Using algorithm for tracing\nobject, results were satisfactory and showed mere changes in trajectory graphs,\nobtained from original and reconstructed videos.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 17:41:02 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Kracunov", "Marijana", ""], ["Bastica", "Milica", ""], ["Tesovic", "Jovana", ""]]}, {"id": "1903.06474", "submitter": "Ioannis Agtzidis", "authors": "Ioannis Agtzidis, Mikhail Startsev, Michael Dorr", "title": "A Ground-Truth Data Set and a Classification Algorithm for Eye Movements\n  in 360-degree Videos", "comments": null, "journal-ref": "Proceedings of the 27th ACM International Conference on\n  Multimedia, (2019), p. 1007-1015", "doi": "10.1145/3343031.3350947", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of a gaze trace into its constituent eye movements has been\nactively researched since the early days of eye tracking. As we move towards\nmore naturalistic viewing conditions, the segmentation becomes even more\nchallenging and convoluted as more complex patterns emerge. The definitions and\nthe well-established methods that were developed for monitor-based eye tracking\nexperiments are often not directly applicable to unrestrained set-ups such as\neye tracking in wearable contexts or with head-mounted displays. The main\ncontributions of this work to the eye movement research for 360-degree content\nare threefold: First, we collect, partially annotate, and make publicly\navailable a new eye tracking data set, which consists of 13 participants\nviewing 15 video clips that are recorded in 360-degree. Second, we propose a\nnew two-stage pipeline for ground truth annotation of the traditional\nfixations, saccades, smooth pursuits, as well as (optokinetic) nystagmus,\nvestibulo-ocular reflex, and pursuit of moving objects performed exclusively\nvia the movement of the head. A flexible user interface for this pipeline is\nimplemented and made freely accessible for use or modification. Lastly, we\ndevelop and test a simple proof-of-concept algorithm for automatic\nclassification of all the eye movement types in our data set based on their\noperational definitions that were used for manual annotation. The data set and\nthe source code for both the annotation tool and the algorithm are publicly\navailable at https://web.gin.g-node.org/ioannis.agtzidis/360_em_dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 11:46:08 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Agtzidis", "Ioannis", ""], ["Startsev", "Mikhail", ""], ["Dorr", "Michael", ""]]}, {"id": "1903.06658", "submitter": "Ayub Gubran", "authors": "Ayub A. Gubran, Felix Huang, Tor M. Aamodt", "title": "Surface Compression Using Dynamic Color Palettes", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-chip memory traffic is a major source of power and energy consumption on\nmobile platforms. A large amount of this off-chip traffic is used to manipulate\ngraphics framebuffer surfaces. To cut down the cost of accessing off-chip\nmemory, framebuffer surfaces are compressed to reduce the bandwidth consumed on\nsurface manipulation when rendering or displaying.\n  In this work, we study the compression properties of framebuffer surfaces and\nhighlight the fact that surfaces from different applications have different\ncompression characteristics. We use the results of our analysis to propose a\nscheme, Dynamic Color Palettes (DCP), which achieves higher compression rates\nwith UI and 2D surfaces.\n  DCP is a hardware mechanism for exploiting inter-frame coherence in lossless\nsurface compression; it implements a scheme that dynamically constructs color\npalettes, which are then used to efficiently compress framebuffer surfaces. To\nevaluate DCP, we created an extensive set of OpenGL workload traces from 124\nAndroid applications. We found that DCP improves compression rates by 91% for\nUI and 20% for 2D applications compared to previous proposals. We also evaluate\na hybrid scheme that combines DCP with a generic compression scheme. We found\nthat compression rates improve over previous proposals by 161%, 124% and 83%\nfor UI, 2D and 3D applications, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 01:35:56 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Gubran", "Ayub A.", ""], ["Huang", "Felix", ""], ["Aamodt", "Tor M.", ""]]}, {"id": "1903.07195", "submitter": "Wieslaw Kopec", "authors": "Jaros{\\l}aw Kowalski, Anna Jaskulska, Kinga Skorupska, Katarzyna\n  Abramczuk, Cezary Biele, Wies{\\l}aw Kope\\'c, Krzysztof Marasek", "title": "Older Adults and Voice Interaction: A Pilot Study with Google Home", "comments": null, "journal-ref": null, "doi": "10.1145/3290607.3312973", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the results of an exploratory study examining the\npotential of voice assistants (VA) for some groups of older adults in the\ncontext of Smart Home Technology (SHT). To research the aspect of older adults'\ninteraction with voice user interfaces (VUI) we organized two workshops and\ngathered insights concerning possible benefits and barriers to the use of VA\ncombined with SHT by older adults. Apart from evaluating the participants'\ninteraction with the devices during the two workshops we also discuss some\nimprovements to the VA interaction paradigm.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 23:12:45 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kowalski", "Jaros\u0142aw", ""], ["Jaskulska", "Anna", ""], ["Skorupska", "Kinga", ""], ["Abramczuk", "Katarzyna", ""], ["Biele", "Cezary", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1903.07428", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Scene Segmentation-Based Luminance Adjustment for Multi-Exposure Image\n  Fusion", "comments": "will be published in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2906501", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for adjusting luminance for multi-exposure image\nfusion. For the adjustment, two novel scene segmentation approaches based on\nluminance distribution are also proposed. Multi-exposure image fusion is a\nmethod for producing images that are expected to be more informative and\nperceptually appealing than any of the input ones, by directly fusing photos\ntaken with different exposures. However, existing fusion methods often produce\nunclear fused images when input images do not have a sufficient number of\ndifferent exposure levels. In this paper, we point out that adjusting the\nluminance of input images makes it possible to improve the quality of the final\nfused images. This insight is the basis of the proposed method. The proposed\nmethod enables us to produce high-quality images, even when undesirable inputs\nare given. Visual comparison results show that the proposed method can produce\nimages that clearly represent a whole scene. In addition, multi-exposure image\nfusion with the proposed method outperforms state-of-the-art fusion methods in\nterms of MEF-SSIM, discrete entropy, tone mapped image quality index, and\nstatistical naturalness.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 13:33:34 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1903.08238", "submitter": "Mohamed Mansour", "authors": "Yuan-Yen Tai, Mohamed F. Mansour", "title": "Audio Watermarking over the Air With Modulated Self-Correlation", "comments": "5 pages, conference; preprint, ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel audio watermarking system that is robust to the distortion\ndue to the indoor acoustic propagation channel between the loudspeaker and the\nreceiving microphone. The system utilizes a set of new algorithms that\neffectively mitigate the impact of room reverberation and interfering sound\nsources without using dereverberation procedures. The decoder has low-latency\nand it operates asynchronously, which alleviates the need for explicit\nsynchronization with the encoder. It is also robust to standard audio\nprocessing operations in legacy watermarking systems, e.g., compression and\nvolume change. The effectiveness of the system is established with a real-time\nsystem under general room conditions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:49:33 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Tai", "Yuan-Yen", ""], ["Mansour", "Mohamed F.", ""]]}, {"id": "1903.09884", "submitter": "Saffet Vatansever", "authors": "Saffet Vatansever, Ahmet Emir Dirik, Nasir Memon", "title": "Detecting the Presence of ENF Signal in Digital Videos: a Superpixel\n  based Approach", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2017.2741440", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ENF (Electrical Network Frequency) instantaneously fluctuates around its\nnominal value (50/60 Hz) due to a continuous disparity between generated power\nand consumed power. Consequently, luminous intensity of a mains-powered light\nsource varies depending on ENF fluctuations in the grid network. Variations in\nthe luminance over time can be captured from video recordings and ENF can be\nestimated through content analysis of these recordings. In ENF based video\nforensics, it is critical to check whether a given video file is appropriate\nfor this type of analysis. That is, if ENF signal is not present in a given\nvideo, it would be useless to apply ENF based forensic analysis. In this work,\nan ENF signal presence detection method is introduced for videos. The proposed\nmethod is based on multiple ENF signal estimations from steady superpixels,\ni.e. pixels that are most likely uniform in color, brightness, and texture, and\nintraclass similarity of the estimated signals. Subsequently, consistency among\nthese estimates is then used to determine the presence or absence of an ENF\nsignal in a given video. The proposed technique can operate on video clips as\nshort as 2 minutes and is independent of the camera sensor type, i.e. CCD or\nCMOS.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 21:28:43 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Vatansever", "Saffet", ""], ["Dirik", "Ahmet Emir", ""], ["Memon", "Nasir", ""]]}, {"id": "1903.09889", "submitter": "Saffet Vatansever", "authors": "Saffet Vatansever, Ahmet Emir Dirik, Nasir Memon", "title": "Analysis of Rolling Shutter Effect on ENF based Video Forensics", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2019.2895540", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ENF is a time-varying signal of the frequency of mains electricity in a power\ngrid. It continuously fluctuates around a nominal value (50/60 Hz) due to\nchanges in supply and demand of power over time. Depending on these ENF\nvariations, the luminous intensity of a mains-powered light source also\nfluctuates. These fluctuations in luminance can be captured by video\nrecordings. Accordingly, ENF can be estimated from such videos by analysis of\nsteady content in the video scene. When videos are captured by using a rolling\nshutter sampling mechanism, as is done mostly with CMOS cameras, there is an\nidle period between successive frames. Consequently, a number of illumination\nsamples of the scene are effectively lost due to the idle period. These missing\nsamples affect ENF estimation, in the sense of the frequency shift caused and\nthe power attenuation that results. This work develops an analytical model for\nvideos captured using a rolling shutter mechanism. The model illustrates how\nthe frequency of the main ENF harmonic varies depending on the idle period\nlength, and how the power of the captured ENF attenuates as idle period\nincreases. Based on this, a novel idle period estimation method for potential\nuse in camera forensics that is able to operate independently of video frame\nrate is proposed. Finally, a novel time-of-recording verification approach\nbased on use of multiple ENF components, idle period assumptions and\ninterpolation of missing ENF samples is also proposed.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 21:46:39 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Vatansever", "Saffet", ""], ["Dirik", "Ahmet Emir", ""], ["Memon", "Nasir", ""]]}, {"id": "1903.10195", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Amanda Duarte, Francisco Roldan, Miquel Tubau, Janna Escur, Santiago\n  Pascual, Amaia Salvador, Eva Mohedano, Kevin McGuinness, Jordi Torres and\n  Xavier Giro-i-Nieto", "title": "Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial\n  Networks", "comments": "ICASSP 2019. Projevct website at\n  https://imatge-upc.github.io/wav2pix/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is a rich biometric signal that contains information about the\nidentity, gender and emotional state of the speaker. In this work, we explore\nits potential to generate face images of a speaker by conditioning a Generative\nAdversarial Network (GAN) with raw speech input. We propose a deep neural\nnetwork that is trained from scratch in an end-to-end fashion, generating a\nface directly from the raw speech waveform without any additional identity\ninformation (e.g reference image or one-hot encoding). Our model is trained in\na self-supervised approach by exploiting the audio and visual signals naturally\naligned in videos. With the purpose of training from video data, we present a\nnovel dataset collected for this work, with high-quality videos of youtubers\nwith notable expressiveness in both the speech and visual signals.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 09:27:44 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Duarte", "Amanda", ""], ["Roldan", "Francisco", ""], ["Tubau", "Miquel", ""], ["Escur", "Janna", ""], ["Pascual", "Santiago", ""], ["Salvador", "Amaia", ""], ["Mohedano", "Eva", ""], ["McGuinness", "Kevin", ""], ["Torres", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1903.10683", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, Pheng-Ann Heng", "title": "Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data", "comments": "Accepted to ICCV 2019", "journal-ref": "IEEE International Conference on Computer Vision (ICCV), pp.\n  2472-2481, 2019", "doi": "10.1109/ICCV.2019.00256", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for shadow removal using unpaired data,\nenabling us to avoid tedious annotations and obtain more diverse training\nsamples. However, directly employing adversarial learning and cycle-consistency\nconstraints is insufficient to learn the underlying relationship between the\nshadow and shadow-free domains, since the mapping between shadow and\nshadow-free images is not simply one-to-one. To address the problem, we\nformulate Mask-ShadowGAN, a new deep framework that automatically learns to\nproduce a shadow mask from the input shadow image and then takes the mask to\nguide the shadow generation via re-formulated cycle-consistency constraints.\nParticularly, the framework simultaneously learns to produce shadow masks and\nlearns to remove shadows, to maximize the overall performance. Also, we\nprepared an unpaired dataset for shadow removal and demonstrated the\neffectiveness of Mask-ShadowGAN on various experiments, even it was trained on\nunpaired data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 05:30:55 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 03:48:11 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 13:25:52 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hu", "Xiaowei", ""], ["Jiang", "Yitong", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1903.11722", "submitter": "Abbas Soltanian", "authors": "Abbas Soltanian, Diala Naboulsi, Roch Glitho, Halima Elbiaze", "title": "Resource Allocation Mechanism for Media Handling Services in Cloud\n  Multimedia Conferencing", "comments": "14 pages, 14 figures, IEEE Journal on Selected Areas in\n  Communications (JSAC)", "journal-ref": null, "doi": "10.1109/JSAC.2019.2906806", "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia conferencing is the conversational exchange of multimedia content\nbetween multiple parties. It has a wide range of applications (e.g., Massively\nMultiplayer Online Games (MMOGs) and distance learning). Media handling\nservices (e.g., video mixing, transcoding, and compressing) are critical to\nmultimedia conferencing. However, efficient resource usage and scalability\nstill remain important challenges. Unfortunately, the cloud-based approaches\nproposed so far have several deficiencies in terms of efficiency in resource\nusage and scaling, while meeting Quality of Service (QoS) requirements. This\npaper proposes a solution which optimizes resource allocation and scales in\nterms of the number of participants while guaranteeing QoS. Moreover, our\nsolution composes different media handling services to support the\nparticipants' demands. We formulate the resource allocation problem\nmathematically as an Integer Linear Programming (ILP) problem and design a\nheuristic for it. We evaluate our proposed solution for different numbers of\nparticipants and different participants' geographical distributions. Simulation\nresults show that our resource allocation mechanism can compose the media\nhandling services and allocate the required resources in an optimal manner\nwhile honoring the QoS in terms of end-to-end delay.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 23:00:07 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Soltanian", "Abbas", ""], ["Naboulsi", "Diala", ""], ["Glitho", "Roch", ""], ["Elbiaze", "Halima", ""]]}, {"id": "1903.11821", "submitter": "Jingwei Guan", "authors": "Jingwei Guan, Cheng Pan, Songnan Li and Dahai Yu", "title": "SRDGAN: learning the noise prior for Super Resolution with Dual\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Image Super Resolution (SISR) is the task of producing a high\nresolution (HR) image from a given low-resolution (LR) image. It is a well\nresearched problem with extensive commercial applications such as digital\ncamera, video compression, medical imaging and so on. Most super resolution\nworks focus on the features learning architecture, which can recover the\ntexture details as close as possible. However, these works suffer from the\nfollowing challenges: (1) The low-resolution (LR) training images are\nartificially synthesized using HR images with bicubic downsampling, which have\nmuch richer-information than real demosaic-upscaled mobile images. The mismatch\nbetween training and inference mobile data heavily blocks the improvement of\npractical super resolution algorithms. (2) These methods cannot effectively\nhandle the blind distortions during super resolution in practical applications.\nIn this work, an end-to-end novel framework, including high-to-low network and\nlow-to-high network, is proposed to solve the above problems with dual\nGenerative Adversarial Networks (GAN). First, the above mismatch problems are\nwell explored with the high-to-low network, where clear high-resolution image\nand the corresponding realistic low-resolution image pairs can be generated.\nMoreover, a large-scale General Mobile Super Resolution Dataset, GMSR, is\nproposed, which can be utilized for training or as a fair comparison benchmark\nfor super resolution methods. Second, an effective low-to-high network (super\nresolution network) is proposed in the framework. Benefiting from the GMSR\ndataset and novel training strategies, the super resolution model can\neffectively handle detail recovery and denoising at the same time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 07:58:13 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Guan", "Jingwei", ""], ["Pan", "Cheng", ""], ["Li", "Songnan", ""], ["Yu", "Dahai", ""]]}, {"id": "1903.11987", "submitter": "Junxin Chen", "authors": "Junxin Chen, Lei Chen, Yicong Zhou", "title": "Universal chosen-ciphertext attack for a family of image encryption\n  schemes", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decades, there is a great popularity employing nonlinear\ndynamics and permutation-substitution architecture for image encryption. There\nare three primary procedures in such encryption schemes, the key schedule\nmodule for producing encryption factors, permutation for image scrambling and\nsubstitution for pixel modification. Under the assumption of chosen-ciphertext\nattack, we evaluate the security of a class of image ciphers which adopts\npixel-level permutation and modular addition for substitution. It is\nmathematically revealed that the mapping from differentials of ciphertexts to\nthose of plaintexts are linear and has nothing to do with the key schedules,\npermutation techniques and encryption rounds. Moreover, a universal\nchosen-ciphertext attack is proposed and validated. Experimental results\ndemonstrate that the plaintexts can be directly reconstructed without any\nsecurity key or encryption elements. Related cryptographic discussions are also\ngiven.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:09:40 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Chen", "Junxin", ""], ["Chen", "Lei", ""], ["Zhou", "Yicong", ""]]}, {"id": "1903.12088", "submitter": "Suiyi Ling", "authors": "Suiyi Ling, Jing Li, Junle Wang, Patrick Le Callet", "title": "GANs-NQM: A Generative Adversarial Networks based No Reference Quality\n  Assessment Metric for RGB-D Synthesized Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a no-reference (NR) quality metric for RGB plus\nimage-depth (RGB-D) synthesis images based on Generative Adversarial Networks\n(GANs), namely GANs-NQM. Due to the failure of the inpainting on dis-occluded\nregions in RGB-D synthesis process, to capture the non-uniformly distributed\nlocal distortions and to learn their impact on perceptual quality are\nchallenging tasks for objective quality metrics. In our study, based on the\ncharacteristics of GANs, we proposed i) a novel training strategy of GANs for\nRGB-D synthesis images using existing large-scale computer vision datasets\nrather than RGB-D dataset; ii) a referenceless quality metric based on the\ntrained discriminator by learning a `Bag of Distortion Word' (BDW) codebook and\na local distortion regions selector; iii) a hole filling inpainter, i.e., the\ngenerator of the trained GANs, for RGB-D dis-occluded regions as a side\noutcome. According to the experimental results on IRCCyN/IVC DIBR database, the\nproposed model outperforms the state-of-the-art quality metrics, in addition,\nis more applicable in real scenarios. The corresponding context inpainter also\nshows appealing results over other inpainting algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:11:42 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Ling", "Suiyi", ""], ["Li", "Jing", ""], ["Wang", "Junle", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1903.12107", "submitter": "Suiyi Ling", "authors": "Suiyi Ling, Jing Li, Zhaohui Che, Xiongkuo Min, Guangtao Zhai, Patrick\n  Le Callet", "title": "Quality Assessment of Free-viewpoint Videos by Quantifying the Elastic\n  Changes of Multi-Scale Motion Trajectories", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual viewpoints synthesis is an essential process for many immersive\napplications including Free-viewpoint TV (FTV). A widely used technique for\nviewpoints synthesis is Depth-Image-Based-Rendering (DIBR) technique. However,\nsuch techniques may introduce challenging non-uniform spatial-temporal\nstructure-related distortions. Most of the existing state-of-the-art quality\nmetrics fail to handle these distortions, especially the temporal structure\ninconsistencies observed during the switch of different viewpoints. To tackle\nthis problem, an elastic metric and multi-scale trajectory based video quality\nmetric (EM-VQM) is proposed in this paper. Dense motion trajectory is first\nused as a proxy for selecting temporal sensitive regions, where local geometric\ndistortions might significantly diminish the perceived quality. Afterwards, the\namount of temporal structure inconsistencies and unsmooth viewpoints\ntransitions are quantified by calculating 1) the amount of motion trajectory\ndeformations with elastic metric and, 2) the spatial-temporal structural\ndissimilarity. According to the comprehensive experimental results on two FTV\nvideo datasets, the proposed metric outperforms the state-of-the-art metrics\ndesigned for free-viewpoint videos significantly and achieves a gain of 12.86%\nand 16.75% in terms of median Pearson linear correlation coefficient values on\nthe two datasets compared to the best one, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:45:45 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Ling", "Suiyi", ""], ["Li", "Jing", ""], ["Che", "Zhaohui", ""], ["Min", "Xiongkuo", ""], ["Zhai", "Guangtao", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1903.12399", "submitter": "Zhuang Chen", "authors": "Zhuang Chen, Qian He, Zhifei Mao, Hwei-Ming Chung, Sabita Maharjan", "title": "A Study on the Characteristics of Douyin Short Videos and Implications\n  for Edge Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Douyin, internationally known as TikTok, has become one of the most\nsuccessful short-video platforms. To maintain its popularity, Douyin has to\nprovide better Quality of Experience (QoE) to its growing user base.\nUnderstanding the characteristics of Douyin videos is thus critical to its\nservice improvement and system design. In this paper, we present an initial\nstudy on the fundamental characteristics of Douyin videos based on a dataset of\nover 260 thousand short videos collected across three months. The\ncharacteristics of Douyin videos are found to be significantly different from\ntraditional online videos, ranging from video bitrate, size, to popularity. In\nparticular, the distributions of the bitrate and size of videos follow Weibull\ndistribution. We further observe that the most popular Douyin videos follow\nZifp's law on video popularity, but the rest of the videos do not. We also\ninvestigate the correlation between popularity metrics used for Douyin videos.\nIt is found that the correlation between the number of views and the number of\nlikes are strong, while other correlations are relatively low. Finally, by\nusing a case study, we demonstrate that the above findings can provide\nimportant guidance on designing an efficient edge caching system.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 08:56:50 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Chen", "Zhuang", ""], ["He", "Qian", ""], ["Mao", "Zhifei", ""], ["Chung", "Hwei-Ming", ""], ["Maharjan", "Sabita", ""]]}]