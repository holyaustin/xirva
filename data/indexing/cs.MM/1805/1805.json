[{"id": "1805.00041", "submitter": "Vaneet Aggarwal", "authors": "Anis Elgabli and Vaneet Aggarwal and Shuai Hao and Feng Qian and\n  Subhabrata Sen", "title": "LBP: Robust Rate Adaptation Algorithm for SVC Video Streaming", "comments": "22 pages, IEEE/ACM Transactions on Networking, 2018. Fixed repetition\n  of references in the last version (minor change)", "journal-ref": null, "doi": "10.1109/TNET.2018.2844123", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video streaming today accounts for up to 55\\% of mobile traffic. In this\npaper, we explore streaming videos encoded using Scalable Video Coding scheme\n(SVC) over highly variable bandwidth conditions such as cellular networks.\nSVC's unique encoding scheme allows the quality of a video chunk to change\nincrementally, making it more flexible and adaptive to challenging network\nconditions compared to other encoding schemes. Our contribution is threefold.\nFirst, we formulate the quality decisions of video chunks constrained by the\navailable bandwidth, the playback buffer, and the chunk deadlines as an\noptimization problem. The objective is to optimize a novel QoE metric that\nmodels a combination of the three objectives of minimizing the stall/skip\nduration of the video, maximizing the playback quality of every chunk, and\nminimizing the number of quality switches. Second, we develop Layered Bin\nPacking (LBP) Adaptation Algorithm, a novel algorithm that solves the proposed\noptimization problem. Moreover, we show that LBP achieves the optimal solution\nof the proposed optimization problem with linear complexity in the number of\nvideo chunks. Third, we propose an online algorithm (online LBP) where several\nchallenges are addressed including handling bandwidth prediction errors, and\nshort prediction duration. Extensive simulations with real bandwidth traces of\npublic datasets reveal the robustness of our scheme and demonstrate its\nsignificant performance improvement as compared to the state-of-the-art SVC\nstreaming algorithms. The proposed algorithm is also implemented on a TCP/IP\nemulation test bed with real LTE bandwidth traces, and the emulation confirms\nthe simulation results and validates that the algorithm can be implemented and\ndeployed on today's mobile devices.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 18:05:16 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 18:16:40 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 07:37:59 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Elgabli", "Anis", ""], ["Aggarwal", "Vaneet", ""], ["Hao", "Shuai", ""], ["Qian", "Feng", ""], ["Sen", "Subhabrata", ""]]}, {"id": "1805.00207", "submitter": "Ioannis Pachoulakis", "authors": "I. Pachoulakis", "title": "Realistic Multimedia Tools based on Physical Models: I. The Spectrum\n  Analyzer and Animator (SA2)", "comments": "8 pages, 4 figures", "journal-ref": "International Conference on Telecommunications and Multimedia\n  (TEMU'08), 16-18 July 2008, Ierapetra, Greece", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present sequence of two articles reports on a custom-built toolkit\nimplementing a technique similar to multi-directional medical tomography to\nsimulate and visualize the composite 3D structure of winds from hot close\ndouble stars. In such hot binaries, the light sources scanning and probing the\ncomposite wind volume are the bright \"surfaces\" (photospheres) of the\nindividual stars. Then, as the Keplerian orbit is traced out and the geometry\npresented to the observer varies, each star constitutes an analyzer upon its\ncompanion's wind. In contrast to medical tomography, however, these targets are\ntoo far to be resolved spatially so we resort to modeling the ultraviolet (UV)\nspectral lines of certain wind ions (e.g., N+4, Si+3, C+3) whose shapes vary\nwith Keplerian phase as the stars revolve around their common centre of mass.\nThe flagships of the toolkit are the Spectrum Analyzer and Animator (SA 2 ) and\nthe Binary 3D Renderer (B3dR). The SA 2 is the subject of the present article\n(paper I). It automates (a) the derivation of light curves from the observed\nspectra and (b) the generation of synthetic binary wind-line profiles which\nreproduce the morphologies and variabilities of the observed wind profiles. The\nsecond tool, the B3dR is discussed in paper II.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 06:45:33 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Pachoulakis", "I.", ""]]}, {"id": "1805.00211", "submitter": "Ioannis Pachoulakis", "authors": "I. Pachoulakis", "title": "Realistic Multimedia Tools based on Physical Models: II. The Binary 3D\n  Renderer (B3dR)", "comments": "7 pages, 3 figures", "journal-ref": "International Conference on Telecommunications and Multimedia\n  (TEMU'08), 16-18 July 2008, Ierapetra, Greece", "doi": null, "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present article reports on the second tool of a custom-built toolkit\nintended to train astronomers into simulating and visualizing the composite 3D\nstructure of winds from hot close double stars by implementing a technique\nwhich is similar to multi-directional medical tomography. The flagships of the\ntoolkit are the Spectrum Analyzer and Animator (SA 2 ) and the Binary 3D\nRenderer (B3dR). Following application of the first tool, SA 2 as detailed in\npaper I, the composite wind structure of the binary has been recovered and the\nB3dR is subsequently employed to visualize the results and simulate the\nrevolution of the entire system (stars, winds and wind-interaction effects)\naround the common centre of mass. The B3dR thus repackages the end product of a\nlengthy physical modeling process to generate realistic multimedia content and\nenable the presentation of the 3D system from the point of view of an observer\non Earth as well as from any other observer location in the Galaxy.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 06:53:44 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Pachoulakis", "I.", ""]]}, {"id": "1805.00313", "submitter": "Xuemeng Song", "authors": "Xuemeng Song, Fuli Feng, Xianjing Han, Xin Yang, Wei Liu, Liqiang Nie", "title": "Neural Compatibility Modeling with Attentive Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the booming fashion sector and its huge potential benefits have\nattracted tremendous attention from many research communities. In particular,\nincreasing research efforts have been dedicated to the complementary clothing\nmatching as matching clothes to make a suitable outfit has become a daily\nheadache for many people, especially those who do not have the sense of\naesthetics. Thanks to the remarkable success of neural networks in various\napplications such as image classification and speech recognition, the\nresearchers are enabled to adopt the data-driven learning methods to analyze\nfashion items. Nevertheless, existing studies overlook the rich valuable\nknowledge (rules) accumulated in fashion domain, especially the rules regarding\nclothing matching. Towards this end, in this work, we shed light on\ncomplementary clothing matching by integrating the advanced deep neural\nnetworks and the rich fashion domain knowledge. Considering that the rules can\nbe fuzzy and different rules may have different confidence levels to different\nsamples, we present a neural compatibility modeling scheme with attentive\nknowledge distillation based on the teacher-student network scheme. Extensive\nexperiments on the real-world dataset show the superiority of our model over\nseveral state-of-the-art baselines. Based upon the comparisons, we observe\ncertain fashion insights that add value to the fashion matching study. As a\nbyproduct, we released the codes, and involved parameters to benefit other\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 01:26:48 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Song", "Xuemeng", ""], ["Feng", "Fuli", ""], ["Han", "Xianjing", ""], ["Yang", "Xin", ""], ["Liu", "Wei", ""], ["Nie", "Liqiang", ""]]}, {"id": "1805.00373", "submitter": "Jayant Gupchup A", "authors": "Jayant Gupchup, Yasaman Hosseinkashi, Martin Ellis, Sam Johnson and\n  Ross Cutler", "title": "Analysis of Problem Tokens to Rank Factors Impacting Quality in VoIP\n  Applications", "comments": null, "journal-ref": "Quality of Multimedia Experience (QoMEX), 2017 Ninth International\n  Conference", "doi": "10.1109/QoMEX.2017.7965648", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-perceived quality-of-experience (QoE) in internet telephony systems is\ncommonly evaluated using subjective ratings computed as a Mean Opinion Score\n(MOS). In such systems, while user MOS can be tracked on an ongoing basis, it\ndoes not give insight into which factors of a call induced any perceived\ndegradation in QoE -- it does not tell us what caused a user to have a\nsub-optimal experience. For effective planning of product improvements, we are\ninterested in understanding the impact of each of these degrading factors,\nallowing the estimation of the return (i.e., the improvement in user QoE) for a\ngiven investment. To obtain such insights, we advocate the use of an\nend-of-call \"problem token questionnaire\" (PTQ) which probes the user about\ncommon call quality issues (e.g., distorted audio or frozen video) which they\nmay have experienced. In this paper, we show the efficacy of this questionnaire\nusing data gathered from over 700,000 end-of-call surveys gathered from Skype\n(a large commercial VoIP application). We present a method to rank call quality\nand reliability issues and address the challenge of isolating independent\nfactors impacting the QoE. Finally, we present representative examples of how\nthese problem tokens have proven to be useful in practice.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 00:31:02 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Gupchup", "Jayant", ""], ["Hosseinkashi", "Yasaman", ""], ["Ellis", "Martin", ""], ["Johnson", "Sam", ""], ["Cutler", "Ross", ""]]}, {"id": "1805.00619", "submitter": "Tianchi Huang", "authors": "Tianchi Huang, Rui-Xiao Zhang, Chao Zhou, and Lifeng Sun", "title": "Delay-Constrained Rate Control for Real-Time Video Streaming with\n  Bounded Neural Network", "comments": null, "journal-ref": null, "doi": "10.1145/3210445.3210446", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate control is widely adopted during video streaming to provide both high\nvideo qualities and low latency under various network conditions. However,\ndespite that many work have been proposed, they fail to tackle one major\nproblem: previous methods determine a future transmission rate as a single for\nvalue which will be used in an entire time-slot, while real-world network\nconditions, unlike lab setup, often suffer from rapid and stochastic changes,\nresulting in the failures of predictions.\n  In this paper, we propose a delay-constrained rate control approach based on\nend-to-end deep learning. The proposed model predicts future bit rate not as a\nsingle value, but as possible bit rate ranges using target delay gradient, with\nwhich the transmission delay is guaranteed. We collect a large scale of\nreal-world live streaming data to train our model, and as a result, it\nautomatically learns the correlation between throughput and target delay\ngradient. We build a testbed to evaluate our approach. Compared with the\nstate-of-the-art methods, our approach demonstrates a better performance in\nbandwidth utilization. In all considered scenarios, a range based rate control\napproach outperforms the one without range by 19% to 35% in average QoE\nimprovement.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 04:24:27 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Huang", "Tianchi", ""], ["Zhang", "Rui-Xiao", ""], ["Zhou", "Chao", ""], ["Sun", "Lifeng", ""]]}, {"id": "1805.01024", "submitter": "Shu Kong", "authors": "Feng Zhou, Shu Kong, Charless Fowlkes, Tao Chen, Baiying Lei", "title": "Fine-Grained Facial Expression Analysis Using Dimensional Emotion Model", "comments": "code: http://www.ics.uci.edu/~skong2/DimensionalEmotionModel.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated facial expression analysis has a variety of applications in\nhuman-computer interaction. Traditional methods mainly analyze prototypical\nfacial expressions of no more than eight discrete emotions as a classification\ntask. However, in practice, spontaneous facial expressions in naturalistic\nenvironment can represent not only a wide range of emotions, but also different\nintensities within an emotion family. In such situation, these methods are not\nreliable or adequate. In this paper, we propose to train deep convolutional\nneural networks (CNNs) to analyze facial expressions explainable in a\ndimensional emotion model. The proposed method accommodates not only a set of\nbasic emotion expressions, but also a full range of other emotions and subtle\nemotion intensities that we both feel in ourselves and perceive in others in\nour daily life. Specifically, we first mapped facial expressions into\ndimensional measures so that we transformed facial expression analysis from a\nclassification problem to a regression one. We then tested our CNN-based\nmethods for facial expression regression and these methods demonstrated\npromising performance. Moreover, we improved our method by a bilinear pooling\nwhich encodes second-order statistics of features. We showed such bilinear-CNN\nmodels significantly outperformed their respective baselines.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 21:08:47 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zhou", "Feng", ""], ["Kong", "Shu", ""], ["Fowlkes", "Charless", ""], ["Chen", "Tao", ""], ["Lei", "Baiying", ""]]}, {"id": "1805.02031", "submitter": "Jen-Yu Liu", "authors": "Jen-Yu Liu, Yi-Hsuan Yang, Shyh-Kang Jeng", "title": "Weakly-supervised Visual Instrument-playing Action Detection in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrument playing is among the most common scenes in music-related videos,\nwhich represent nowadays one of the largest sources of online videos. In order\nto understand the instrument-playing scenes in the videos, it is important to\nknow what instruments are played, when they are played, and where the playing\nactions occur in the scene. While audio-based recognition of instruments has\nbeen widely studied, the visual aspect of the music instrument playing remains\nlargely unaddressed in the literature. One of the main obstacles is the\ndifficulty in collecting annotated data of the action locations for\ntraining-based methods. To address this issue, we propose a weakly-supervised\nframework to find when and where the instruments are played in the videos. We\npropose to use two auxiliary models, a sound model and an object model, to\nprovide supervisions for training the instrument-playing action model. The\nsound model provides temporal supervisions, while the object model provides\nspatial supervisions. They together can simultaneously provide temporal and\nspatial supervisions. The resulted model only needs to analyze the visual part\nof a music video to deduce which, when and where instruments are played. We\nfound that the proposed method significantly improves the localization\naccuracy. We evaluate the result of the proposed method temporally and\nspatially on a small dataset (totally 5,400 frames) that we manually annotated.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 09:52:36 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Liu", "Jen-Yu", ""], ["Yang", "Yi-Hsuan", ""], ["Jeng", "Shyh-Kang", ""]]}, {"id": "1805.02356", "submitter": "Jieli Zhou", "authors": "Xin Qian, Ziyi Zhong, Jieli Zhou", "title": "Multimodal Machine Translation with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal machine translation is one of the applications that integrates\ncomputer vision and language processing. It is a unique task given that in the\nfield of machine translation, many state-of-the-arts algorithms still only\nemploy textual information. In this work, we explore the effectiveness of\nreinforcement learning in multimodal machine translation. We present a novel\nalgorithm based on the Advantage Actor-Critic (A2C) algorithm that specifically\ncater to the multimodal machine translation task of the EMNLP 2018 Third\nConference on Machine Translation (WMT18). We experiment our proposed algorithm\non the Multi30K multilingual English-German image description dataset and the\nFlickr30K image entity dataset. Our model takes two channels of inputs, image\nand text, uses translation evaluation metrics as training rewards, and achieves\nbetter results than supervised learning MLE baseline models. Furthermore, we\ndiscuss the prospects and limitations of using reinforcement learning for\nmachine translation. Our experiment results suggest a promising reinforcement\nlearning solution to the general task of multimodal sequence to sequence\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 06:12:32 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Qian", "Xin", ""], ["Zhong", "Ziyi", ""], ["Zhou", "Jieli", ""]]}, {"id": "1805.02371", "submitter": "Luca Rossetto M.Sc.", "authors": "Luca Rossetto, Ivan Giangreco, Ralph Gasser and Heiko Schuldt", "title": "Competitive Video Retrieval with vitrivr at the Video Browser Showdown\n  2018 - Final Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an after-the-fact summary of the participation of the\nvitrivr system to the 2018 Video Browser Showdown. A particular focus is on\nadditions made since the original publication and the systems performance\nduring the competition.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 07:06:19 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Rossetto", "Luca", ""], ["Giangreco", "Ivan", ""], ["Gasser", "Ralph", ""], ["Schuldt", "Heiko", ""]]}, {"id": "1805.02482", "submitter": "Tianchi Huang", "authors": "Tianchi Huang, Rui-Xiao Zhang, Chao Zhou and Lifeng Sun", "title": "QARC: Video Quality Aware Rate Control for Real-Time Video Streaming via\n  Deep Reinforcement Learning", "comments": "Accepted by ACM Multimedia 2018", "journal-ref": null, "doi": "10.1145/3240508.3240545", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fluctuation of throughput under various network conditions, how to\nchoose a proper bitrate adaptively for real-time video streaming has become an\nupcoming and interesting issue. Recent work focuses on providing high video\nbitrates instead of video qualities. Nevertheless, we notice that there exists\na trade-off between sending bitrate and video quality, which motivates us to\nfocus on how to get a balance between them. In this paper, we propose QARC\n(video Quality Awareness Rate Control), a rate control algorithm that aims to\nhave a higher perceptual video quality with possibly lower sending rate and\ntransmission latency. Starting from scratch, QARC uses deep reinforcement\nlearning(DRL) algorithm to train a neural network to select future bitrates\nbased on previously observed network status and past video frames, and we\ndesign a neural network to predict future perceptual video quality as a vector\nfor taking the place of the raw picture in the DRL's inputs. We evaluate QARC\nover a trace-driven emulation. As excepted, QARC betters existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 12:50:10 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 09:35:45 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 09:40:11 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Huang", "Tianchi", ""], ["Zhang", "Rui-Xiao", ""], ["Zhou", "Chao", ""], ["Sun", "Lifeng", ""]]}, {"id": "1805.02733", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Learning Optical Flow via Dilated Networks and Occlusion Reasoning", "comments": "Accepted at ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant progress that has been made on estimating optical\nflow recently, most estimation methods, including classical and deep learning\napproaches, still have difficulty with multi-scale estimation, real-time\ncomputation, and/or occlusion reasoning. In this paper, we introduce dilated\nconvolution and occlusion reasoning into unsupervised optical flow estimation\nto address these issues. The dilated convolution allows our network to avoid\nupsampling via deconvolution and the resulting gridding artifacts. Dilated\nconvolution also results in a smaller memory footprint which speeds up\ninterference. The occlusion reasoning prevents our network from learning\nincorrect deformations due to occluded image regions during training. Our\nproposed method outperforms state-of-the-art unsupervised approaches on the\nKITTI benchmark. We also demonstrate its generalization capability by applying\nit to action recognition in video.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 20:31:56 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1805.03060", "submitter": "Wenxiao Zhang", "authors": "Wenxiao Zhang, Sikun Lin, Farshid Hassani Bijarbooneh, Hao Fei Cheng,\n  And Pan Hui", "title": "CloudAR: A Cloud-based Framework for Mobile Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation capabilities of recent mobile devices enable natural feature\nprocessing for Augmented Reality (AR). However, mobile AR applications are\nstill faced with scalability and performance challenges. In this paper, we\npropose CloudAR, a mobile AR framework utilizing the advantages of cloud and\nedge computing through recognition task offloading. We explore the design space\nof cloud-based AR exhaustively and optimize the offloading pipeline to minimize\nthe time and energy consumption. We design an innovative tracking system for\nmobile devices which provides lightweight tracking in 6 degree of freedom\n(6DoF) and hides the offloading latency from users' perception. We also design\na multi-object image retrieval pipeline that executes fast and accurate image\nrecognition tasks on servers. In our evaluations, the mobile AR application\nbuilt with the CloudAR framework runs at 30 frames per second (FPS) on average\nwith precise tracking of only 1~2 pixel errors and image recognition of at\nleast 97% accuracy. Our results also show that CloudAR outperforms one of the\nleading commercial AR framework in several performance metrics.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 14:38:58 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhang", "Wenxiao", ""], ["Lin", "Sikun", ""], ["Bijarbooneh", "Farshid Hassani", ""], ["Cheng", "Hao Fei", ""], ["Hui", "And Pan", ""]]}, {"id": "1805.03105", "submitter": "Pan Gao", "authors": "Pan Gao, Cagri Ozcinar, and Aljosa Smolic", "title": "Optimization of Occlusion-Inducing Depth Pixels in 3-D Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of occlusion-inducing depth pixels in depth map coding has\nreceived little attention in the literature, since their associated texture\npixels are occluded in the synthesized view and their effect on the synthesized\nview is considered negligible. However, the occlusion-inducing depth pixels\nstill need to consume the bits to be transmitted, and will induce geometry\ndistortion that inherently exists in the synthesized view. In this paper, we\npropose an efficient depth map coding scheme specifically for the\nocclusion-inducing depth pixels by using allowable depth distortions. Firstly,\nwe formulate a problem of minimizing the overall geometry distortion in the\nocclusion subject to the bit rate constraint, for which the depth distortion is\nproperly adjusted within the set of allowable depth distortions that introduce\nthe same disparity error as the initial depth distortion. Then, we propose a\ndynamic programming solution to find the optimal depth distortion vector for\nthe occlusion. The proposed algorithm can improve the coding efficiency without\nalteration of the occlusion order. Simulation results confirm the performance\nimprovement compared to other existing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 15:28:23 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Gao", "Pan", ""], ["Ozcinar", "Cagri", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1805.03894", "submitter": "Xiaoyi He", "authors": "Xiaoyi He, Qiang Hu, Xintong Han, Xiaoyun Zhang, Chongyang Zhang,\n  Weiyao Lin", "title": "Enhancing HEVC Compressed Videos with a Partition-masked Convolutional\n  Neural Network", "comments": "Accepted by ICIP 2018", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451086", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a partition-masked Convolution Neural Network (CNN)\nto achieve compressed-video enhancement for the state-of-the-art coding\nstandard, High Efficiency Video Coding (HECV). More precisely, our method\nutilizes the partition information produced by the encoder to guide the quality\nenhancement process. In contrast to existing CNN-based approaches, which only\ntake the decoded frame as the input to the CNN, the proposed approach considers\nthe coding unit (CU) size information and combines it with the distorted\ndecoded frame such that the degradation introduced by HEVC is reduced more\nefficiently. Experimental results show that our approach leads to over 9.76%\nBD-rate saving on benchmark sequences, which achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 08:56:30 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 12:58:32 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["He", "Xiaoyi", ""], ["Hu", "Qiang", ""], ["Han", "Xintong", ""], ["Zhang", "Xiaoyun", ""], ["Zhang", "Chongyang", ""], ["Lin", "Weiyao", ""]]}, {"id": "1805.03967", "submitter": "Zhen Long", "authors": "Zhen Long, Yipeng Liu, Longxi Chen, Ce Zhu", "title": "Low Rank Tensor Completion for Multiway Visual Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor completion recovers missing entries of multiway data. Teh missing of\nentries could often be caused during teh data acquisition and transformation.\nIn dis paper, we provide an overview of recent development in low rank tensor\ncompletion for estimating teh missing components of visual data, e. g. , color\nimages and videos. First, we categorize these methods into two groups based on\nteh different optimization models. One optimizes factors of tensor\ndecompositions wif predefined tensor rank. Teh other iteratively updates teh\nestimated tensor via minimizing teh tensor rank. Besides, we summarize teh\ncorresponding algorithms to solve those optimization problems in details.\nNumerical experiments are given to demonstrate teh performance comparison when\ndifferent methods are applied to color image and video processing.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 08:51:26 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Long", "Zhen", ""], ["Liu", "Yipeng", ""], ["Chen", "Longxi", ""], ["Zhu", "Ce", ""]]}, {"id": "1805.04837", "submitter": "Yang Cao", "authors": "Yang Cao, Zeyu Xu, Peng Qin, Tao Jiang", "title": "Video Processing on the Edge for Multimedia IoT Systems", "comments": "7 pages, 5 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we first survey the current situation of video processing on\nthe edge for multimedia Internet-of-Things (M-IoT) systems in three typical\nscenarios, i.e., smart cities, satellite networks, and Internet-of-Vehicles. By\nsummarizing a general model of the edge video processing, the importance of\ndeveloping an edge computing platform is highlighted. Then, we give a method of\nimplementing cooperative video processing on an edge computing platform based\non light-weighted virtualization technologies. Performance evaluation is\nconducted and some insightful observations can be obtained. Moreover, we\nsummarize challenges and opportunities of realizing effective edge video\nprocessing for M-IoT systems.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 07:54:34 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Cao", "Yang", ""], ["Xu", "Zeyu", ""], ["Qin", "Peng", ""], ["Jiang", "Tao", ""]]}, {"id": "1805.05575", "submitter": "Wei Zhou", "authors": "Ya Zhou, Wei Zhou, Ping An, and Zhibo Chen", "title": "Visual Comfort Assessment for Stereoscopic Image Retargeting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, visual comfort assessment (VCA) for 3D/stereoscopic content\nhas aroused extensive attention. However, much less work has been done on the\nperceptual evaluation of stereoscopic image retargeting. In this paper, we\nfirst build a Stereoscopic Image Retargeting Database (SIRD), which contains\nsource images and retargeted images produced by four typical stereoscopic\nretargeting methods. Then, the subjective experiment is conducted to assess\nfour aspects of visual distortion, i.e. visual comfort, image quality, depth\nquality and the overall quality. Furthermore, we propose a Visual Comfort\nAssessment metric for Stereoscopic Image Retargeting (VCA-SIR). Based on the\ncharacteristics of stereoscopic retargeted images, the proposed model\nintroduces novel features like disparity range, boundary disparity as well as\ndisparity intensity distribution into the assessment model. Experimental\nresults demonstrate that VCA-SIR can achieve high consistency with subjective\nperception.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 05:53:31 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Zhou", "Ya", ""], ["Zhou", "Wei", ""], ["An", "Ping", ""], ["Chen", "Zhibo", ""]]}, {"id": "1805.06021", "submitter": "Christopher Tralie", "authors": "Christopher Tralie, Matthew Berger", "title": "Topological Eulerian Synthesis of Slow Motion Periodic Videos", "comments": "9 pages, 5 Figures. IEEE International Conference on Image\n  Processing, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the problem of taking a video that is comprised of multiple\nperiods of repetitive motion, and reordering the frames of the video into a\nsingle period, producing a detailed, single cycle video of motion. This problem\nis challenging, as such videos often contain noise, drift due to camera motion\nand from cycle to cycle, and irrelevant background motion/occlusions, and these\nfactors can confound the relevant periodic motion we seek in the video. To\naddress these issues in a simple and efficient manner, we introduce a tracking\nfree Eulerian approach for synthesizing a single cycle of motion. Our approach\nis geometric: we treat each frame as a point in high-dimensional Euclidean\nspace, and analyze the sliding window embedding formed by this sequence of\npoints, which yields samples along a topological loop regardless of the type of\nperiodic motion. We combine tools from topological data analysis and spectral\ngeometric analysis to estimate the phase of each window, and we exploit the\nsliding window structure to robustly reorder frames. We show quantitative\nresults that highlight the robustness of our technique to camera shake, noise,\nand occlusions, and qualitative results of single-cycle motion synthesis across\na variety of scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 20:19:46 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Tralie", "Christopher", ""], ["Berger", "Matthew", ""]]}, {"id": "1805.06121", "submitter": "Xiaodan Song", "authors": "Xiaodan Song, Jiabao Yao, Lulu Zhou, Li Wang, Xiaoyang Wu, Di Xie and\n  Shiliang Pu", "title": "A practical convolutional neural network as loop filter for intra frame", "comments": "Accepted by ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop filters are used in video coding to remove artifacts or improve\nperformance. Recent advances in deploying convolutional neural network (CNN) to\nreplace traditional loop filters show large gains but with problems for\npractical application. First, different model is used for frames encoded with\ndifferent quantization parameter (QP), respectively. It is expensive for\nhardware. Second, float points operation in CNN leads to inconsistency between\nencoding and decoding across different platforms. Third, redundancy within CNN\nmodel consumes precious computational resources.\n  This paper proposes a CNN as the loop filter for intra frames and proposes a\nscheme to solve the above problems. It aims to design a single CNN model with\nlow redundancy to adapt to decoded frames with different qualities and ensure\nconsistency. To adapt to reconstructions with different qualities, both\nreconstruction and QP are taken as inputs. After training, the obtained model\nis compressed to reduce redundancy. To ensure consistency, dynamic fixed points\n(DFP) are adopted in testing CNN. Parameters in the compressed model are first\nquantized to DFP and then used for inference of CNN. Outputs of each layer in\nCNN are computed by DFP operations. Experimental results on JEM 7.0 report\n3.14%, 5.21%, 6.28% BD-rate savings for luma and two chroma components with all\nintra configuration when replacing all traditional filters.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 04:06:01 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Song", "Xiaodan", ""], ["Yao", "Jiabao", ""], ["Zhou", "Lulu", ""], ["Wang", "Li", ""], ["Wu", "Xiaoyang", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1805.06181", "submitter": "Wook-Hyung Kim", "authors": "Wook-Hyung Kim, Seung-Hun Nam, Ji-Hyeon Kang, and Heung-Kyu Lee", "title": "Robust curvelet domain watermarking technique that preserves cleanness\n  of high quality images", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking inserts invisible data into content to protect copyright. The\nembedded information provides proof of authorship and facilitates tracking\nillegal distribution, etc. Current robust watermarking techniques have been\nproposed to preserve inserted copyright information from various attacks, such\nas content modification and watermark removal attack. However, since the\nwatermark is inserted in the form of noise, there is an inevitable effect of\nreducing content visual quality. In general, more robust watermarking\ntechniques tend to have larger effect on the quality, and content creators and\nusers are often reluctant to insert watermarks. Thus, there is a demand for a\nwatermark that maintains maximum image quality, even if the watermark\nperformance is slightly inferior. Therefore, we propose a watermarking\ntechnique that maximizes invisibility while maintaining sufficient robustness\nand data capacity enough to be applied for real situations. The proposed method\nminimizes watermarking energy by adopting curvelet domain multi-directional\ndecomposition to maximize invisibility, and maximizes robustness against signal\nprocessing attack by watermarking pattern suitable for curvelet transformation.\nThe method is also robust against geometric attack by employing watermark\ndetection method utilizing curvelet characteristics. The proposed method showed\nvery good results of 57.65 dB peak signal-to-noise ratio in fidelity tests, and\nmean opinion score showed that images treated with the proposed method were\nhardly distinguishable from the originals. The proposed technique also showed\ngood robustness against signal processing and geometric attacks compared with\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 08:07:47 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Kim", "Wook-Hyung", ""], ["Nam", "Seung-Hun", ""], ["Kang", "Ji-Hyeon", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "1805.06199", "submitter": "Wook-Hyung Kim", "authors": "Wook-Hyung Kim, Jong-Uk Hou, Seung-Min Mun, and Heung-Kyu Lee", "title": "Convolutional Neural Network Architecture for Recovering Watermark\n  Synchronization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since real-time contents can be captured and downloaded very easily,\ncopyright infringement has become a serious problem. In order to reduce the\nloss caused by copyright infringement, copyright owners insert a watermark in\nthe content to protect the copyright using illegal distribution route tracking\nand copyright authentication. However, whereas many existing watermarking\ntechniques are robust to signal distortion such as compression, they are\nvulnerable to geometric distortion that causes synchronization errors. In\nparticular, capturing real-time content in Internet browsers and smartphone\napplications is problematic because geometric distortion such as scaling and\ntranslation frequently occurs. In this paper, we propose a convolutional neural\nnetwork-based template architecture that compensates for the disadvantages of\nexisting watermarking techniques that are vulnerable to geometric distortion.\nThe proposed template consists of a template generation network, a template\nextraction network, and a template matching network. The template generation\nnetwork generates a template in the form of noise and the template is inserted\ninto certain pre-defined spatial locations of the image. The extraction network\ndetects spatial locations where the template is inserted in the image. Finally,\nthe template matching network estimates the parameters of the geometric\ndistortion by comparing the shape of spatial locations where the template was\ninserted with the locations where the template was detected. It is possible to\nrecover an image in its original geometrical form using the estimated\nparameters, and as a result, watermarks applied using existing watermarking\ntechniques that are vulnerable to geometric distortion can be decoded normally.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 09:07:08 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Kim", "Wook-Hyung", ""], ["Hou", "Jong-Uk", ""], ["Mun", "Seung-Min", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "1805.06335", "submitter": "Nagabhushan Eswara", "authors": "Nagabhushan Eswara, Hemanth P. Sethuram, Soumen Chakraborty, Kiran\n  Kuchi, Abhinav Kumar, Sumohana S. Channappayya", "title": "Modeling Continuous Video QoE Evolution: A State Space Approach", "comments": "7 pages, 3 figures, conference", "journal-ref": "IEEE International Conference on Multimedia and Expo (ICME), July\n  2018", "doi": "10.1109/ICME.2018.8486557", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rapid increase in the video traffic together with an increasing demand for\nhigher quality videos has put a significant load on content delivery networks\nin the recent years. Due to the relatively limited delivery infrastructure, the\nvideo users in HTTP streaming often encounter dynamically varying quality over\ntime due to rate adaptation, while the delays in video packet arrivals result\nin rebuffering events. The user quality-of-experience (QoE) degrades and varies\nwith time because of these factors. Thus, it is imperative to monitor the QoE\ncontinuously in order to minimize these degradations and deliver an optimized\nQoE to the users. Towards this end, we propose a nonlinear state space model\nfor efficiently and effectively predicting the user QoE on a continuous time\nbasis. The QoE prediction using the proposed approach relies on a state space\nthat is defined by a set of carefully chosen time varying QoE determining\nfeatures. An evaluation of the proposed approach conducted on two publicly\navailable continuous QoE databases shows a superior QoE prediction performance\nover the state-of-the-art QoE modeling approaches. The evaluation results also\ndemonstrate the efficacy of the selected features and the model order employed\nfor predicting the QoE. Finally, we show that the proposed model is completely\nstate controllable and observable, so that the potential of state space\nmodeling approaches can be exploited for further improving QoE prediction.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 14:01:07 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Eswara", "Nagabhushan", ""], ["Sethuram", "Hemanth P.", ""], ["Chakraborty", "Soumen", ""], ["Kuchi", "Kiran", ""], ["Kumar", "Abhinav", ""], ["Channappayya", "Sumohana S.", ""]]}, {"id": "1805.06782", "submitter": "Vincent Labatut", "authors": "Xavier Bost (LIA), Vincent Labatut (LIA), Serigne Gueye (LIA), Georges\n  Linar\\`es (LIA)", "title": "Extraction and Analysis of Dynamic Conversational Networks from TV\n  Series", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.07811", "journal-ref": "In: Social Network Based Big Data Analysis and Applications,\n  p.55-84, Springer, 2018", "doi": "10.1007/978-3-319-78196-9_3", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and characterizing the dynamics of modern tv series subplots is\nan open problem. One way is to study the underlying social network of\ninteractions between the characters. Standard dynamic network extraction\nmethods rely on temporal integration, either over the whole considered period,\nor as a sequence of several time-slices. However, they turn out to be\ninappropriate in the case of tv series, because the scenes shown onscreen\nalternatively focus on parallel storylines, and do not necessarily respect a\ntraditional chronology. In this article, we introduce Narrative Smoothing, a\nnovel network extraction method taking advantage of the plot properties to\nsolve some of their limitations. We apply our method to a corpus of 3 popular\nseries, and compare it to both standard approaches. Narrative smoothing leads\nto more relevant observations when it comes to the characterization of the\nprotagonists and their relationships, confirming its appropriateness to model\nthe intertwined storylines constituting the plots.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 08:18:53 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Gueye", "Serigne", "", "LIA"], ["Linar\u00e8s", "Georges", "", "LIA"]]}, {"id": "1805.06960", "submitter": "Ravi Shekhar", "authors": "Ravi Shekhar, Tim Baumgartner, Aashish Venkatesh, Elia Bruni,\n  Raffaella Bernardi, Raquel Fernandez", "title": "Ask No More: Deciding when to guess in referential visual dialogue", "comments": "COLING 2018 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our goal is to explore how the abilities brought in by a dialogue manager can\nbe included in end-to-end visually grounded conversational agents. We make\ninitial steps towards this general goal by augmenting a task-oriented visual\ndialogue model with a decision-making component that decides whether to ask a\nfollow-up question to identify a target referent in an image, or to stop the\nconversation to make a guess. Our analyses show that adding a decision making\ncomponent produces dialogues that are less repetitive and that include fewer\nunnecessary questions, thus potentially leading to more efficient and less\nunnatural interactions.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 20:32:08 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 10:43:56 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Shekhar", "Ravi", ""], ["Baumgartner", "Tim", ""], ["Venkatesh", "Aashish", ""], ["Bruni", "Elia", ""], ["Bernardi", "Raffaella", ""], ["Fernandez", "Raquel", ""]]}, {"id": "1805.07479", "submitter": "Cheng Ju", "authors": "Cheng Ju, James Li, Bram Wasti, Shengbo Guo", "title": "Semisupervised Learning on Heterogeneous Graphs and its Applications to\n  Facebook News Feed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised learning is a fundamental machine learning\nproblem, and has been well studied. Most studies focus on homogeneous networks\n(e.g. citation network, friend network). In the present paper, we propose the\nHeterogeneous Embedding Label Propagation (HELP) algorithm, a graph-based\nsemi-supervised deep learning algorithm, for graphs that are characterized by\nheterogeneous node types. Empirically, we demonstrate the effectiveness of this\nmethod in domain classification tasks with Facebook user-domain interaction\ngraph, and compare the performance of the proposed HELP algorithm with the\nstate of the art algorithms. We show that the HELP algorithm improves the\npredictive performance across multiple tasks, together with semantically\nmeaningful embedding that are discriminative for downstream classification or\nregression tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 23:58:07 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 02:20:50 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Ju", "Cheng", ""], ["Li", "James", ""], ["Wasti", "Bram", ""], ["Guo", "Shengbo", ""]]}, {"id": "1805.08008", "submitter": "Lin Gao", "authors": "Lin Gao, Ming Tang, Haitian Pang, Jianwei Huang, Lifeng Sun", "title": "Performance Bound Analysis for Crowdsourced Mobile Video Streaming", "comments": "This manuscript serves as the online technical report for the paper\n  published in the IEEE Conference on Information Sciences and Systems (CISS\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive bitrate (ABR) streaming enables video users to adapt the playing\nbitrate to the real-time network conditions to achieve the desirable quality of\nexperience (QoE). In this work, we propose a novel crowdsourced streaming\nframework for multi-user ABR video streaming over wireless networks. This\nframework enables the nearby mobile video users to crowdsource their radio\nlinks and resources for cooperative video streaming. We focus on analyzing the\nsocial welfare performance bound of the proposed crowdsourced streaming system.\nDirectly solving this bound is challenging due to the asynchronous operations\nof users. To this end, we introduce a virtual time-slotted system with the\nsynchronized operations, and formulate the associated social welfare\noptimization problem as a linear programming. We show that the optimal social\nwelfare performance of the virtual system provides effective upper-bound and\nlower-bound for the optimal performance (bound) of the original asynchronous\nsystem, hence characterizes the feasible performance region of the proposed\ncrowdsourced streaming system. The performance bounds derived in this work can\nserve as a benchmark for the future online algorithm design and incentive\nmechanism design.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 12:08:03 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Gao", "Lin", ""], ["Tang", "Ming", ""], ["Pang", "Haitian", ""], ["Huang", "Jianwei", ""], ["Sun", "Lifeng", ""]]}, {"id": "1805.08632", "submitter": "Xiang Chen", "authors": "Xiang Chen", "title": "Towards Global Optimization in Display Advertising by Integrating\n  Multimedia Metrics with Real-Time Bidding", "comments": "In proceedings of ACM Multimedia'17 (Doctoral Symposium), Mountain\n  View, CA, USA", "journal-ref": null, "doi": "10.1145/3123266.3123966", "report-no": null, "categories": "cs.GT cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time bidding (RTB) has become a new norm in display advertising where a\npublisher uses auction models to sell online user's page view to advertisers.\nIn RTB, the ad with the highest bid price will be displayed to the user. This\nad displaying process is biased towards the publisher. In fact, the benefits of\nthe advertiser and the user have been rarely discussed. Towards the global\noptimization, we argue that all stakeholders' benefits should be considered. To\nthis end, we propose a novel computation framework where multimedia techniques\nand auction theory are integrated. This doctoral research mainly focus on 1)\nfiguring out the multimedia metrics that affect the effectiveness of online\nadvertising; 2) integrating the discovered metrics into the RTB framework. We\nhave presented some preliminary results and discussed the future directions.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 04:02:19 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Chen", "Xiang", ""]]}, {"id": "1805.09366", "submitter": "Zining Zhu", "authors": "Zining Zhu, Jekaterina Novikova, Frank Rudzicz", "title": "Semi-supervised classification by reaching consensus among modalities", "comments": "NIPS IRASL Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated abilities to learn complex structures, but\nthey can be restricted by available data. Recently, Consensus Networks (CNs)\nwere proposed to alleviate data sparsity by utilizing features from multiple\nmodalities, but they too have been limited by the size of labeled data. In this\npaper, we extend CN to Transductive Consensus Networks (TCNs), suitable for\nsemi-supervised learning. In TCNs, different modalities of input are compressed\ninto latent representations, which we encourage to become indistinguishable\nduring iterative adversarial training. To understand TCNs two mechanisms,\nconsensus and classification, we put forward its three variants in ablation\nstudies on these mechanisms. To further investigate TCN models, we treat the\nlatent representations as probability distributions and measure their\nsimilarities as the negative relative Jensen-Shannon divergences. We show that\na consensus state beneficial for classification desires a stable but imperfect\nsimilarity between the representations. Overall, TCNs outperform or align with\nthe best benchmark algorithms given 20 to 200 labeled samples on the Bank\nMarketing and the DementiaBank datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 18:19:11 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 17:56:13 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhu", "Zining", ""], ["Novikova", "Jekaterina", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1805.09701", "submitter": "Pan Lu", "authors": "Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, Jianyong Wang", "title": "R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual\n  Question Answering", "comments": "10 pages, 5 figures, accepted as an oral paper in SIGKDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3220036", "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Visual Question Answering (VQA) has emerged as one of the most\nsignificant tasks in multimodal learning as it requires understanding both\nvisual and textual modalities. Existing methods mainly rely on extracting image\nand question features to learn their joint feature embedding via multimodal\nfusion or attention mechanism. Some recent studies utilize external\nVQA-independent models to detect candidate entities or attributes in images,\nwhich serve as semantic knowledge complementary to the VQA task. However, these\ncandidate entities or attributes might be unrelated to the VQA task and have\nlimited semantic capacities. To better utilize semantic knowledge in images, we\npropose a novel framework to learn visual relation facts for VQA. Specifically,\nwe build up a Relation-VQA (R-VQA) dataset based on the Visual Genome dataset\nvia a semantic similarity module, in which each data consists of an image, a\ncorresponding question, a correct answer and a supporting relation fact. A\nwell-defined relation detector is then adopted to predict visual\nquestion-related relation facts. We further propose a multi-step attention\nmodel composed of visual attention and semantic attention sequentially to\nextract related visual knowledge and semantic knowledge. We conduct\ncomprehensive experiments on the two benchmark datasets, demonstrating that our\nmodel achieves state-of-the-art performance and verifying the benefit of\nconsidering visual relation facts.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 14:43:30 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 03:45:04 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Lu", "Pan", ""], ["Ji", "Lei", ""], ["Zhang", "Wei", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""], ["Wang", "Jianyong", ""]]}, {"id": "1805.10942", "submitter": "Laurent Amsaleg", "authors": "Herwig Lejsek, Bj\\\"orn {\\TH}\\'or J\\'onsson, Laurent Amsaleg,\n  Fri{\\dh}rik Hei{\\dh}ar \\'Asmundsson", "title": "Dynamicity and Durability in Scalable Visual Instance Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual instance search involves retrieving from a collection of images the\nones that contain an instance of a visual query. Systems designed for visual\ninstance search face the major challenge of scalability: a collection of a few\nmillion images used for instance search typically creates a few billion\nfeatures that must be indexed. Furthermore, as real image collections grow\nrapidly, systems must also provide dynamicity, i.e., be able to handle on-line\ninsertions while concurrently serving retrieval operations. Durability, which\nis the ability to recover correctly from software and hardware crashes, is the\nnatural complement of dynamicity. Durability, however, has rarely been\nintegrated within scalable and dynamic high-dimensional indexing solutions.\nThis article addresses the issue of dynamicity and durability for scalable\nindexing of very large and rapidly growing collections of local features for\ninstance retrieval. By extending the NV-tree, a scalable disk-based\nhigh-dimensional index, we show how to implement the ACID properties of\ntransactions which ensure both dynamicity and durability. We present a detailed\nperformance evaluation of the transactional NV-tree: (i) We show that the\ninsertion throughput is excellent despite the overhead for enforcing the ACID\nproperties; (ii) We also show that this transactional index is truly scalable\nusing a standard image benchmark embedded in collections of up to 28.5 billion\nhigh-dimensional vectors; the largest single-server evaluations reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 07:39:55 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 11:21:40 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Lejsek", "Herwig", ""], ["J\u00f3nsson", "Bj\u00f6rn \u00de\u00f3r", ""], ["Amsaleg", "Laurent", ""], ["\u00c1smundsson", "Fri\u00f0rik Hei\u00f0ar", ""]]}, {"id": "1805.11203", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Philip A. Chou, Ming-Ting Sun, Maolong Tang, Shanshe\n  Wang, Siwei Ma, Wen Gao", "title": "Surface Light Field Compression using a Point Cloud Codec", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field (LF) representations aim to provide photo-realistic,\nfree-viewpoint viewing experiences. However, the most popular LF\nrepresentations are images from multiple views. Multi-view image-based\nrepresentations generally need to restrict the range or degrees of freedom of\nthe viewing experience to what can be interpolated in the image domain,\nessentially because they lack explicit geometry information. We present a new\nsurface light field (SLF) representation based on explicit geometry, and a\nmethod for SLF compression. First, we map the multi-view images of a scene onto\na 3D geometric point cloud. The color of each point in the point cloud is a\nfunction of viewing direction known as a view map. We represent each view map\nefficiently in a B-Spline wavelet basis. This representation is capable of\nmodeling diverse surface materials and complex lighting conditions in a highly\nscalable and adaptive manner. The coefficients of the B-Spline wavelet\nrepresentation are then compressed spatially. To increase the spatial\ncorrelation and thus improve compression efficiency, we introduce a smoothing\nterm to make the coefficients more similar across the 3D space. We compress the\ncoefficients spatially using existing point cloud compression (PCC) methods. On\nthe decoder side, the scene is rendered efficiently from any viewing direction\nby reconstructing the view map at each point. In contrast to multi-view\nimage-based LF approaches, our method supports photo-realistic rendering of\nreal-world scenes from arbitrary viewpoints, i.e., with an unlimited six\ndegrees of freedom (6DOF). In terms of rate and distortion, experimental\nresults show that our method achieves superior performance with lighter decoder\ncomplexity compared with a reference image-plus-geometry compression (IGC)\nscheme, indicating its potential in practical virtual and augmented reality\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 00:08:30 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Zhang", "Xiang", ""], ["Chou", "Philip A.", ""], ["Sun", "Ming-Ting", ""], ["Tang", "Maolong", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "1805.11254", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Yunwu Lin, Lei Zhu, XinPan Yuan, Jun Long and Fang\n  Huang", "title": "Hierarchical One Permutation Hashing: Efficient Multimedia Near\n  Duplicate Detection", "comments": "Accepted to appear at Multimedia Tools and Applications", "journal-ref": null, "doi": "10.1007/s11042-018-6178-z", "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in multimedia technologies and the proliferation of smart\nphone, digital cameras, storage devices, there are a rapidly growing massive\namount of multimedia data collected in many applications such as multimedia\nretrieval and management system, in which the data element is composed of text,\nimage, video and audio. Consequently, the study of multimedia near duplicate\ndetection has attracted significant concern from research organizations and\ncommercial communities. Traditional solution minwish hashing (\\minwise) faces\ntwo challenges: expensive preprocessing time and lower comparison speed. Thus,\nthis work first introduce a hashing method called one permutation hashing\n(\\oph) to shun the costly preprocessing time. Based on \\oph, a more efficient\nstrategy group based one permutation hashing (\\goph) is developed to deal with\nthe high comparison time. Based on the fact that the similarity of most\nmultimedia data is not very high, this work design an new hashing method namely\nhierarchical one permutation hashing (\\hoph) to further improve the\nperformance. Comprehensive experiments on real multimedia datasets clearly show\nthat with similar accuracy \\hoph is five to seven times faster than\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 05:48:14 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 07:42:15 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Lin", "Yunwu", ""], ["Zhu", "Lei", ""], ["Yuan", "XinPan", ""], ["Long", "Jun", ""], ["Huang", "Fang", ""]]}, {"id": "1805.12176", "submitter": "Kevin Joslyn", "authors": "Kevin Joslyn, Naifan Zhuang, Kien A. Hua", "title": "Deep Segment Hash Learning for Music Generation", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music generation research has grown in popularity over the past decade,\nthanks to the deep learning revolution that has redefined the landscape of\nartificial intelligence. In this paper, we propose a novel approach to music\ngeneration inspired by musical segment concatenation methods and hash learning\nalgorithms. Given a segment of music, we use a deep recurrent neural network\nand ranking-based hash learning to assign a forward hash code to the segment to\nretrieve candidate segments for continuation with matching backward hash codes.\nThe proposed method is thus called Deep Segment Hash Learning (DSHL). To the\nbest of our knowledge, DSHL is the first end-to-end segment hash learning\nmethod for music generation, and the first to use pair-wise training with\nsegments of music. We demonstrate that this method is capable of generating\nmusic which is both original and enjoyable, and that DSHL offers a promising\nnew direction for music generation research.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 18:49:35 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Joslyn", "Kevin", ""], ["Zhuang", "Naifan", ""], ["Hua", "Kien A.", ""]]}]