[{"id": "1705.00242", "submitter": "Huy Kang Kim", "authors": "Hana Kim, Seongil Yang, Huy Kang Kim", "title": "Crime Scene Re-investigation: A Postmortem Analysis of Game Account\n  Stealers' Behaviors", "comments": "7 pages, 8 figures, In Proceedings of the 15th Annual Workshop on\n  Network and Systems Support for Games (NetGames 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As item trading becomes more popular, users can change their game items or\nmoney into real money more easily. At the same time, hackers turn their eyes on\nstealing other users game items or money because it is much easier to earn\nmoney than traditional gold-farming by running game bots. Game companies\nprovide various security measures to block account- theft attempts, but many\nsecurity measures on the user-side are disregarded by users because of lack of\nusability. In this study, we propose a server-side account theft detection\nsystem base on action sequence analysis to protect game users from malicious\nhackers. We tested this system in the real Massively Multiplayer Online Role\nPlaying Game (MMORPG). By analyzing users full game play log, our system can\nfind the particular action sequences of hackers with high accuracy. Also, we\ncan trace where the victim accounts stolen money goes.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 21:54:50 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Kim", "Hana", ""], ["Yang", "Seongil", ""], ["Kim", "Huy Kang", ""]]}, {"id": "1705.00341", "submitter": "Chris Martens", "authors": "Ryan Alexander, Chris Martens", "title": "Deriving Quests from Open World Mechanics", "comments": "To appear at Foundations of Digital Games (FDG) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open world games present players with more freedom than games with linear\nprogression structures. However, without clearly-defined objectives, they often\nleave players without a sense of purpose. Most of the time, quests and\nobjectives are hand-authored and overlaid atop an open world's mechanics. But\nwhat if they could be generated organically from the gameplay itself? The goal\nof our project was to develop a model of the mechanics in Minecraft that could\nbe used to determine the ideal placement of objectives in an open world\nsetting. We formalized the game logic of Minecraft in terms of logical rules\nthat can be manipulated in two ways: they may be executed to generate graphs\nrepresentative of the player experience when playing an open world game with\nlittle developer direction; and they may be statically analyzed to determine\ndependency orderings, feedback loops, and bottlenecks. These analyses may then\nbe used to place achievements on gameplay actions algorithmically.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 16:47:57 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Alexander", "Ryan", ""], ["Martens", "Chris", ""]]}, {"id": "1705.00548", "submitter": "Reza Farahbakhsh", "authors": "Reza Farahbakhsh, Angel Cuevas, Ruben Cuevas, Roberto Gonzalez, Noel\n  Crespi", "title": "Understanding the evolution of multimedia content in the Internet\n  through BitTorrent glasses", "comments": "Farahbakhsh, Reza, et al. \"Understanding the evolution of multimedia\n  content in the internet through bittorrent glasses.\" IEEE Network 27.6\n  (2013): 80-88", "journal-ref": "IEEE Network 27.6 (2013): 80-88", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Internet traffic is mostly dominated by multimedia content and the\nprediction is that this trend will intensify in the future. Therefore, main\nInternet players, such as ISPs, content delivery platforms (e.g. Youtube,\nBitorrent, Netflix, etc) or CDN operators, need to understand the evolution of\nmultimedia content availability and popularity in order to adapt their\ninfrastructures and resources to satisfy clients requirements while they\nminimize their costs. This paper presents a thorough analysis on the evolution\nof multimedia content available in BitTorrent. Specifically, we analyze the\nevolution of four relevant metrics across different content categories: content\navailability, content popularity, content size and user's feedback. To this end\nwe leverage a large-scale dataset formed by 4 snapshots collected from the most\npopular BitTorrent portal, namely The Pirate Bay, between Nov. 2009 and Feb.\n2012. Overall our dataset is formed by more than 160k content that attracted\nmore than 185M of download sessions.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 14:45:00 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Farahbakhsh", "Reza", ""], ["Cuevas", "Angel", ""], ["Cuevas", "Ruben", ""], ["Gonzalez", "Roberto", ""], ["Crespi", "Noel", ""]]}, {"id": "1705.00581", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin and Luc Van Gool", "title": "Query-adaptive Video Summarization via Quality-aware Relevance\n  Estimation", "comments": "ACM Multimedia 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the problem of automatic video summarization has recently received a\nlot of attention, the problem of creating a video summary that also highlights\nelements relevant to a search query has been less studied. We address this\nproblem by posing query-relevant summarization as a video frame subset\nselection problem, which lets us optimise for summaries which are\nsimultaneously diverse, representative of the entire video, and relevant to a\ntext query. We quantify relevance by measuring the distance between frames and\nqueries in a common textual-visual semantic embedding space induced by a neural\nnetwork. In addition, we extend the model to capture query-independent\nproperties, such as frame quality. We compare our method against previous state\nof the art on textual-visual embeddings for thumbnail selection and show that\nour model outperforms them on relevance prediction. Furthermore, we introduce a\nnew dataset, annotated with diversity and query-specific relevance labels. On\nthis dataset, we train and test our complete model for video summarization and\nshow that it outperforms standard baselines such as Maximal Marginal Relevance.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 16:28:18 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 13:18:56 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Gygli", "Michael", ""], ["Volokitin", "Anna", ""], ["Van Gool", "Luc", ""]]}, {"id": "1705.00726", "submitter": "Nematollah Zarmehi", "authors": "Nematollah Zarmehi and Mohammad Reza Aref", "title": "Optimum Decoder for Multiplicative Spread Spectrum Image Watermarking\n  with Laplacian Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the multiplicative spread spectrum watermarking\nmethod for the image. The information bit is spreaded into middle-frequency\nDiscrete Cosine Transform (DCT) coefficients of each block of an image using a\ngenerated pseudo-random sequence. Unlike the conventional signal modeling, we\nsuppose that both signal and noise are distributed with Laplacian distribution\nbecause the sample loss of digital media can be better modeled with this\ndistribution than the Gaussian one. We derive the optimum decoder for the\nproposed embedding method thanks to the maximum likelihood decoding scheme. We\nalso analyze our watermarking system in the presence of noise and provide\nanalytical evaluations and several simulations. The results show that it has\nthe suitable performance and transparency required for watermarking\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 22:08:10 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Zarmehi", "Nematollah", ""], ["Aref", "Mohammad Reza", ""]]}, {"id": "1705.01123", "submitter": "Patrick Seeling", "authors": "Brian Bauman and Patrick Seeling", "title": "Towards Predictions of the Image Quality of Experience for Augmented\n  Reality Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) devices are commonly head-worn to overlay\ncontext-dependent information into the field of view of the device operators.\nOne particular scenario is the overlay of still images, either in a traditional\nfashion, or as spherical, i.e., immersive, content. For both media types, we\nevaluate the interplay of user ratings as Quality of Experience (QoE) with (i)\nthe non-referential BRISQUE objective image quality metric and (ii) human\nsubject dry electrode EEG signals gathered with a commercial device.\nAdditionally, we employ basic machine learning approaches to assess the\npossibility of QoE predictions based on rudimentary subject data. Corroborating\nprior research for the overall scenario, we find strong correlations for both\napproaches with user ratings as Mean Opinion Scores, which we consider as QoE\nmetric. In prediction scenarios based on data subsets, we find good performance\nfor the objective metric as well as the EEG-based approach. While the objective\nmetric can yield high QoE prediction accuracies overall, it is limited i its\napplication for individual subjects. The subject-based EEG approach, on the\nother hand, enables good predictability of the QoE for both media types, but\nwith better performance for regular content. Our results can be employed in\npractical scenarios by content and network service providers to optimize the\nuser experience in augmented reality scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 18:17:50 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Bauman", "Brian", ""], ["Seeling", "Patrick", ""]]}, {"id": "1705.01359", "submitter": "Moin Nabi", "authors": "Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot,\n  Moin Nabi, Enver Sangineto, Raffaella Bernardi", "title": "FOIL it! Find One mismatch between Image and Language caption", "comments": "To appear at ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1024", "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to understand whether current language and vision\n(LaVi) models truly grasp the interaction between the two modalities. To this\nend, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates\nimages with both correct and \"foil\" captions, that is, descriptions of the\nimage that are highly similar to the original ones, but contain one single\nmistake (\"foil word\"). We show that current LaVi models fall into the traps of\nthis data and perform badly on three tasks: a) caption classification (correct\nvs. foil); b) foil word detection; c) foil word correction. Humans, in\ncontrast, have near-perfect performance on those tasks. We demonstrate that\nmerely utilising language cues is not enough to model FOIL-COCO and that it\nchallenges the state-of-the-art by requiring a fine-grained understanding of\nthe relation between text and image.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:07:13 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Shekhar", "Ravi", ""], ["Pezzelle", "Sandro", ""], ["Klimovich", "Yauhen", ""], ["Herbelot", "Aurelie", ""], ["Nabi", "Moin", ""], ["Sangineto", "Enver", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1705.01457", "submitter": "Nematollah Zarmehi", "authors": "Nematollah Zarmehi, Sina Shahsavari and Farokh Marvasti", "title": "Comparison of Uniform and Random Sampling for Speech and Music Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will provide a comparison between uniform and random\nsampling for speech and music signals. There are various sampling and recovery\nmethods for audio signals. Here, we only investigate uniform and random schemes\nfor sampling and basic low-pass filtering and iterative method with adaptive\nthresholding for recovery. The simulation results indicate that uniform\nsampling with cubic spline interpolation outperforms other sampling and\nrecovery methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 21:23:22 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 15:26:54 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Zarmehi", "Nematollah", ""], ["Shahsavari", "Sina", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1705.01759", "submitter": "Hou-Ning Hu", "authors": "Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu Cheng, Yung-Ju\n  Chang, Min Sun", "title": "Deep 360 Pilot: Learning a Deep Agent for Piloting through 360{\\deg}\n  Sports Video", "comments": "13 pages, 8 figures, To appear in CVPR 2017 as an Oral paper. The\n  first two authors contributed equally to this work.\n  https://aliensunmin.github.io/project/360video/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watching a 360{\\deg} sports video requires a viewer to continuously select a\nviewing angle, either through a sequence of mouse clicks or head movements. To\nrelieve the viewer from this \"360 piloting\" task, we propose \"deep 360 pilot\"\n-- a deep learning-based agent for piloting through 360{\\deg} sports videos\nautomatically. At each frame, the agent observes a panoramic image and has the\nknowledge of previously selected viewing angles. The task of the agent is to\nshift the current viewing angle (i.e. action) to the next preferred one (i.e.,\ngoal). We propose to directly learn an online policy of the agent from data. We\nuse the policy gradient technique to jointly train our pipeline: by minimizing\n(1) a regression loss measuring the distance between the selected and ground\ntruth viewing angles, (2) a smoothness loss encouraging smooth transition in\nviewing angle, and (3) maximizing an expected reward of focusing on a\nforeground object. To evaluate our method, we build a new 360-Sports video\ndataset consisting of five sports domains. We train domain-specific agents and\nachieve the best performance on viewing angle selection accuracy and transition\nsmoothness compared to [51] and other baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 09:26:58 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Hu", "Hou-Ning", ""], ["Lin", "Yen-Chen", ""], ["Liu", "Ming-Yu", ""], ["Cheng", "Hsien-Tzu", ""], ["Chang", "Yung-Ju", ""], ["Sun", "Min", ""]]}, {"id": "1705.01762", "submitter": "Theodoros Karagkioules Mr", "authors": "Theodoros Karagkioules, Dimitrios Tsilimantos, Cyril Concolato and\n  Stefan Valentin", "title": "A Comparative Case Study of HTTP Adaptive Streaming Algorithms in Mobile\n  Networks", "comments": "6 pages", "journal-ref": null, "doi": "10.1145/3083165.3083170", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTTP Adaptive Streaming (HAS) techniques are now the dominant solution for\nvideo delivery in mobile networks. Over the past few years, several HAS\nalgorithms have been introduced in order to improve user quality-of-experience\n(QoE) by bit-rate adaptation. Their difference is mainly the required input\ninformation, ranging from network characteristics to application-layer\nparameters such as the playback buffer. Interestingly, despite the recent\noutburst in scientific papers on the topic, a comprehensive comparative study\nof the main algorithm classes is still missing. In this paper we provide such\ncomparison by evaluating the performance of the state-of-the-art HAS algorithms\nper class, based on data from field measurements. We provide a systematic study\nof the main QoE factors and the impact of the target buffer level. We conclude\nthat this target buffer level is a critical classifier for the studied HAS\nalgorithms. While buffer-based algorithms show superior QoE in most of the\ncases, their performance may differ at the low target buffer levels of live\nstreaming services. Overall, we believe that our findings provide valuable\ninsight for the design and choice of HAS algorithms according to networks\nconditions and service requirements.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 09:39:12 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Karagkioules", "Theodoros", ""], ["Tsilimantos", "Dimitrios", ""], ["Concolato", "Cyril", ""], ["Valentin", "Stefan", ""]]}, {"id": "1705.01854", "submitter": "Massimo Iuliani Massimo Iuliani", "authors": "Massimo Iuliani, Marco Fontani, Dasara Shullani, and Alessandro Piva", "title": "A Hybrid Approach to Video Source Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia Forensics allows to determine whether videos or images have been\ncaptured with the same device, and thus, eventually, by the same person.\nCurrently, the most promising technology to achieve this task, exploits the\nunique traces left by the camera sensor into the visual content. Anyway, image\nand video source identification are still treated separately from one another.\nThis approach is limited and anachronistic if we consider that most of the\nvisual media are today acquired using smartphones, that capture both images and\nvideos. In this paper we overcome this limitation by exploring a new approach\nthat allows to synergistically exploit images and videos to study the device\nfrom which they both come. Indeed, we prove it is possible to identify the\nsource of a digital video by exploiting a reference sensor pattern noise\ngenerated from still images taken by the same device of the query video. The\nproposed method provides comparable or even better performance, when compared\nto the current video identification strategies, where a reference pattern is\nestimated from video frames. We also show how this strategy can be effective\neven in case of in-camera digitally stabilized videos, where a non-stabilized\nreference is not available, by solving some state-of-the-art limitations. We\nexplore a possible direct application of this result, that is social media\nprofile linking, i.e. discovering relationships between two or more social\nmedia profiles by comparing the visual contents - images or videos - shared\ntherein.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 14:24:41 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Iuliani", "Massimo", ""], ["Fontani", "Marco", ""], ["Shullani", "Dasara", ""], ["Piva", "Alessandro", ""]]}, {"id": "1705.02861", "submitter": "Mauricio Toro", "authors": "Mauricio Toro, Myriam Desainte-Catherine", "title": "Concurrent Constraint Conditional-Branching Timed Interactive Scores", "comments": "Extended version of paper accepted in the Sound and Music Conference\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia scenarios have multimedia content and interactive events\nassociated with computer programs. Interactive Scores (IS) is a formalism to\nrepresent such scenarios by temporal objects, temporal relations (TRs) and\ninteractive events. IS describe TRs, but IS cannot represent TRs together with\nconditional branching. We propose a model for conditional branching timed IS in\nthe Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We ran a\nprototype of our model in Ntccrt (a real-time capable interpreter for ntcc) and\nthe response time was acceptable for real-time interaction. An advantage of\nntcc over Max/MSP or Petri Nets is that conditions and global constraints are\nrepresented declaratively.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 22:36:44 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Toro", "Mauricio", ""], ["Desainte-Catherine", "Myriam", ""]]}, {"id": "1705.02940", "submitter": "Rui Ma", "authors": "Rui Ma and Thomas Maugey and Pascal Frossard", "title": "Optimized Data Representation for Interactive Multiview Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrary to traditional media streaming services where a unique media\ncontent is delivered to different users, interactive multiview navigation\napplications enable users to choose their own viewpoints and freely navigate in\na 3-D scene. The interactivity brings new challenges in addition to the\nclassical rate-distortion trade-off, which considers only the compression\nperformance and viewing quality. On the one hand, interactivity necessitates\nsufficient viewpoints for richer navigation; on the other hand, it requires to\nprovide low bandwidth and delay costs for smooth navigation during view\ntransitions. In this paper, we formally describe the novel trade-offs posed by\nthe navigation interactivity and classical rate-distortion criterion. Based on\nan original formulation, we look for the optimal design of the data\nrepresentation by introducing novel rate and distortion models and practical\nsolving algorithms. Experiments show that the proposed data representation\nmethod outperforms the baseline solution by providing lower resource\nconsumptions and higher visual quality in all navigation configurations, which\ncertainly confirms the potential of the proposed data representation in\npractical interactive navigation systems.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 16:04:51 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 03:26:30 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 08:10:34 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Ma", "Rui", ""], ["Maugey", "Thomas", ""], ["Frossard", "Pascal", ""]]}, {"id": "1705.03531", "submitter": "David Barina", "authors": "Stanislav Svoboda, David Barina", "title": "New Transforms for JPEG Format", "comments": "preprint submitted to SCCG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional discrete cosine transform (DCT) can be found in the heart\nof many image compression algorithms. Specifically, the JPEG format uses a\nlossy form of compression based on that transform. Since the standardization of\nthe JPEG, many other transforms become practical in lossy data compression.\nThis article aims to analyze the use of these transforms as the DCT replacement\nin the JPEG compression chain. Each transform is examined for different image\ndatasets and subsequently compared to other transforms using the peak\nsignal-to-noise ratio (PSNR). Our experiments show that an overlapping\nvariation of the DCT, the local cosine transform (LCT), overcame the original\nblock-wise transform at low bitrates. At high bitrates, the discrete wavelet\ntransform employing the Cohen-Daubechies-Feauveau 9/7 wavelet offers about the\nsame compression performance as the DCT.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 20:34:44 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Svoboda", "Stanislav", ""], ["Barina", "David", ""]]}, {"id": "1705.04911", "submitter": "Tarek El-Ganainy", "authors": "Tarek El-Ganainy", "title": "Spatiotemporal Rate Adaptive Tiled Scheme for 360 Sports Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent rise of interest in Virtual Reality (VR) came with the\navailability of commodity commercial VR products, such as the Head Mounted\nDisplays (HMD) created by Oculus and other vendors. One of the main\napplications of virtual reality that has been recently adopted is streaming\nsports events. For instance, the last olympics held in Rio De Janeiro was\nstreamed over the Internet for users to view on VR headsets or using 360 video\nplayers. A big challenge for streaming VR sports events is the users limited\nbandwidth and the amount of data required to transmit 360 videos. While 360\nvideo demands high bandwidth, at any time instant users are only viewing a\nsmall portion of the video according to the HMD field of view (FOV). Many\napproaches have been proposed in the literature such as proposing new\nrepresentations (e.g. pyramid and offset-cubemap) and tiling the video and\nstreaming the tiles currently being viewed. In this paper, we propose a tiled\nstreaming framework, where we provide a degrading quality model similar to the\nstate-of-the-art offset-cubemap while minimizing its storage requirements at\nthe server side. We conduct objective studies showing the effectiveness of our\napproach providing smooth degradation of quality from the user FOV to the back\nof the 360 space. In addition, we conduct subjective studies showing that users\ntend to prefer our proposed scheme over offset-cubemap in low bandwidth\nconnections, and they don't feel difference for higher bandwidth connections.\nThat is, we achieve better perceived quality with huge storage savings up to\n670%.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 02:46:06 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["El-Ganainy", "Tarek", ""]]}, {"id": "1705.05103", "submitter": "Vedran Vukoti\\'c", "authors": "Vedran Vukotic, Christian Raymond, Guillaume Gravier", "title": "Generative Adversarial Networks for Multimodal Representation Learning\n  in Video Hyperlinking", "comments": "4 pages, 1 figure, 2 tables, published at ACM International\n  Conference in Multimedia Retrieval (ICMR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous multimodal representations suitable for multimodal information\nretrieval are usually obtained with methods that heavily rely on multimodal\nautoencoders. In video hyperlinking, a task that aims at retrieving video\nsegments, the state of the art is a variation of two interlocked networks\nworking in opposing directions. These systems provide good multimodal\nembeddings and are also capable of translating from one representation space to\nthe other. Operating on representation spaces, these networks lack the ability\nto operate in the original spaces (text or image), which makes it difficult to\nvisualize the crossmodal function, and do not generalize well to unseen data.\nRecently, generative adversarial networks have gained popularity and have been\nused for generating realistic synthetic data and for obtaining high-level,\nsingle-modal latent representation spaces. In this work, we evaluate the\nfeasibility of using GANs to obtain multimodal representations. We show that\nGANs can be used for multimodal representation learning and that they provide\nmultimodal representations that are superior to representations obtained with\nmultimodal autoencoders. Additionally, we illustrate the ability of visualizing\ncrossmodal translations that can provide human-interpretable insights on\nlearned GAN-based video hyperlinking models.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:52:07 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Vukotic", "Vedran", ""], ["Raymond", "Christian", ""], ["Gravier", "Guillaume", ""]]}, {"id": "1705.06694", "submitter": "Tommy Nilsson", "authors": "Kevin K. Bowden, Tommy Nilsson, Christine P. Spencer, Kubra Cengiz,\n  Alexandru Ghitulescu, Jelte B. van Waterschoot", "title": "I Probe, Therefore I Am: Designing a Virtual Journalist with Human\n  Emotions", "comments": "eNTERFACE16 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By utilizing different communication channels, such as verbal language,\ngestures or facial expressions, virtually embodied interactive humans hold a\nunique potential to bridge the gap between human-computer interaction and\nactual interhuman communication. The use of virtual humans is consequently\nbecoming increasingly popular in a wide range of areas where such a natural\ncommunication might be beneficial, including entertainment, education, mental\nhealth research and beyond. Behind this development lies a series of\ntechnological advances in a multitude of disciplines, most notably natural\nlanguage processing, computer vision, and speech synthesis. In this paper we\ndiscuss a Virtual Human Journalist, a project employing a number of novel\nsolutions from these disciplines with the goal to demonstrate their viability\nby producing a humanoid conversational agent capable of naturally eliciting and\nreacting to information from a human user. A set of qualitative and\nquantitative evaluation sessions demonstrated the technical feasibility of the\nsystem whilst uncovering a number of deficits in its capacity to engage users\nin a way that would be perceived as natural and emotionally engaging. We argue\nthat naturalness should not always be seen as a desirable goal and suggest that\ndeliberately suppressing the naturalness of virtual human interactions, such as\nby altering its personality cues, might in some cases yield more desirable\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:01:10 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Nilsson", "Tommy", ""], ["Spencer", "Christine P.", ""], ["Cengiz", "Kubra", ""], ["Ghitulescu", "Alexandru", ""], ["van Waterschoot", "Jelte B.", ""]]}, {"id": "1705.06709", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Viktor Rozgic, Sancar Adali", "title": "Learning Spatiotemporal Features for Infrared Action Recognition with 3D\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared (IR) imaging has the potential to enable more robust action\nrecognition systems compared to visible spectrum cameras due to lower\nsensitivity to lighting conditions and appearance variability. While the action\nrecognition task on videos collected from visible spectrum imaging has received\nmuch attention, action recognition in IR videos is significantly less explored.\nOur objective is to exploit imaging data in this modality for the action\nrecognition task. In this work, we propose a novel two-stream 3D convolutional\nneural network (CNN) architecture by introducing the discriminative code layer\nand the corresponding discriminative code loss function. The proposed network\nprocesses IR image and the IR-based optical flow field sequences. We pretrain\nthe 3D CNN model on the visible spectrum Sports-1M action dataset and finetune\nit on the Infrared Action Recognition (InfAR) dataset. To our best knowledge,\nthis is the first application of the 3D CNN to action recognition in the IR\ndomain. We conduct an elaborate analysis of different fusion schemes (weighted\naverage, single and double-layer neural nets) applied to different 3D CNN\noutputs. Experimental results demonstrate that our approach can achieve\nstate-of-the-art average precision (AP) performances on the InfAR dataset: (1)\nthe proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our\n3D CNN model applied to the optical flow fields achieves the best reported\nsingle stream 75.42% AP.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:26:34 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Rozgic", "Viktor", ""], ["Adali", "Sancar", ""]]}, {"id": "1705.07273", "submitter": "Corneliu Ilisescu", "authors": "Corneliu Ilisescu and Halil Aytac Kanaci and Matteo Romagnoli and\n  Neill D. F. Campbell and Gabriel J. Brostow", "title": "Responsive Action-based Video Synthesis", "comments": "10 pages, 12 figures, 1 table, accepted and published in Proceedings\n  of the 2017 CHI Conference on Human Factors in Computing Systems", "journal-ref": null, "doi": "10.1145/3025453.3025880", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose technology to enable a new medium of expression, where video\nelements can be looped, merged, and triggered, interactively. Like audio, video\nis easy to sample from the real world but hard to segment into clean reusable\nelements. Reusing a video clip means non-linear editing and compositing with\nnovel footage. The new context dictates how carefully a clip must be prepared,\nso our end-to-end approach enables previewing and easy iteration.\n  We convert static-camera videos into loopable sequences, synthesizing them in\nresponse to simple end-user requests. This is hard because a) users want\nessentially semantic-level control over the synthesized video content, and b)\nautomatic loop-finding is brittle and leaves users limited opportunity to work\nthrough problems. We propose a human-in-the-loop system where adding effort\ngives the user progressively more creative control. Artists help us evaluate\nhow our trigger interfaces can be used for authoring of videos and\nvideo-performances.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 07:46:31 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ilisescu", "Corneliu", ""], ["Kanaci", "Halil Aytac", ""], ["Romagnoli", "Matteo", ""], ["Campbell", "Neill D. F.", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1705.07543", "submitter": "Hyerin Kim", "authors": "Hye-Rin Kim, Yeong-Seok Kim, Seon Joo Kim, In-Kwon Lee", "title": "Building Emotional Machines: Recognizing Image Emotions through Deep\n  Neural Networks", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image is a very effective tool for conveying emotions. Many researchers\nhave investigated in computing the image emotions by using various features\nextracted from images. In this paper, we focus on two high level features, the\nobject and the background, and assume that the semantic information of images\nis a good cue for predicting emotion. An object is one of the most important\nelements that define an image, and we find out through experiments that there\nis a high correlation between the object and the emotion in images. Even with\nthe same object, there may be slight difference in emotion due to different\nbackgrounds, and we use the semantic information of the background to improve\nthe prediction performance. By combining the different levels of features, we\nbuild an emotion based feed forward deep neural network which produces the\nemotion values of a given image. The output emotion values in our framework are\ncontinuous values in the 2-dimensional space (Valence and Arousal), which are\nmore effective than using a few number of emotion categories in describing\nemotions. Experiments confirm the effectiveness of our network in predicting\nthe emotion of images.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 02:56:23 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 07:56:49 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Kim", "Hye-Rin", ""], ["Kim", "Yeong-Seok", ""], ["Kim", "Seon Joo", ""], ["Lee", "In-Kwon", ""]]}, {"id": "1705.07788", "submitter": "Krzysztof Szczypiorski", "authors": "Krzysztof Szczypiorski and Wojciech Zydecki", "title": "StegIbiza: Steganography in Club Music Implemented in Python", "comments": "4 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the implementation of steganography method called\nStegIbiza, which uses tempo modulation as hidden message carrier. With the use\nof Python scripting language, a bit string was encoded and decoded using WAV\nand MP3 files. Once the message was hidden into a music files, an internet\nradio was created to evaluate broadcast possibilities. No dedicated music or\nsignal processing equipment was used in this StegIbiza implementation\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:56:49 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Szczypiorski", "Krzysztof", ""], ["Zydecki", "Wojciech", ""]]}, {"id": "1705.07839", "submitter": "Ivo Sousa", "authors": "Ivo Sousa, Maria Paula Queluz and Ant\\'onio Rodrigues", "title": "A Survey on QoE-oriented Wireless Resources Scheduling", "comments": "Revised version: updated according to the most recent related\n  literature; added references; corrected typos", "journal-ref": "J.Netw.Comput.Appl 158 (2020) 102594", "doi": "10.1016/j.jnca.2020.102594", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future wireless systems are expected to provide a wide range of services to\nmore and more users. Advanced scheduling strategies thus arise not only to\nperform efficient radio resource management, but also to provide fairness among\nthe users. On the other hand, the users' perceived quality, i.e., Quality of\nExperience (QoE), is becoming one of the main drivers within the schedulers\ndesign. In this context, this paper starts by providing a comprehension of what\nis QoE and an overview of the evolution of wireless scheduling techniques.\nAfterwards, a survey on the most recent QoE-based scheduling strategies for\nwireless systems is presented, highlighting the application/service of the\ndifferent approaches reported in the literature, as well as the parameters that\nwere taken into account for QoE optimization. Therefore, this paper aims at\nhelping readers interested in learning the basic concepts of QoE-oriented\nwireless resources scheduling, as well as getting in touch with its current\nresearch frontier.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:33:30 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 21:53:41 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 14:08:39 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Sousa", "Ivo", ""], ["Queluz", "Maria Paula", ""], ["Rodrigues", "Ant\u00f3nio", ""]]}, {"id": "1705.08214", "submitter": "Michael Gygli", "authors": "Michael Gygli", "title": "Ridiculously Fast Shot Boundary Detection with Fully Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shot boundary detection (SBD) is an important component of many video\nanalysis tasks, such as action recognition, video indexing, summarization and\nediting. Previous work typically used a combination of low-level features like\ncolor histograms, in conjunction with simple models such as SVMs. Instead, we\npropose to learn shot detection end-to-end, from pixels to final shot\nboundaries. For training such a model, we rely on our insight that all shot\nboundaries are generated. Thus, we create a dataset with one million frames and\nautomatically generated transitions such as cuts, dissolves and fades. In order\nto efficiently analyze hours of videos, we propose a Convolutional Neural\nNetwork (CNN) which is fully convolutional in time, thus allowing to use a\nlarge temporal context without the need to repeatedly processing frames. With\nthis architecture our method obtains state-of-the-art results while running at\nan unprecedented speed of more than 120x real-time.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:39:51 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gygli", "Michael", ""]]}, {"id": "1705.08266", "submitter": "David Barina", "authors": "David Barina and Michal Kula and Michal Matysek and Pavel Zemcik", "title": "Accelerating Discrete Wavelet Transforms on GPUs", "comments": "preprint submitted to ICIP 2017. arXiv admin note: substantial text\n  overlap with arXiv:1704.08657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional discrete wavelet transform has a huge number of\napplications in image-processing techniques. Until now, several papers compared\nthe performance of such transform on graphics processing units (GPUs). However,\nall of them only dealt with lifting and convolution computation schemes. In\nthis paper, we show that corresponding horizontal and vertical lifting parts of\nthe lifting scheme can be merged into non-separable lifting units, which halves\nthe number of steps. We also discuss an optimization strategy leading to a\nreduction in the number of arithmetic operations. The schemes were assessed\nusing the OpenCL and pixel shaders. The proposed non-separable lifting scheme\noutperforms the existing schemes in many cases, irrespective of its higher\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 14:42:19 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Barina", "David", ""], ["Kula", "Michal", ""], ["Matysek", "Michal", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1705.08616", "submitter": "Mehdi Sharifzadeh", "authors": "Mehdi Sharifzadeh, Chirag Agarwal, Mahdi Salarian, Dan Schonfeld", "title": "A New Parallel Message-distribution Technique for Cost-based\n  Steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two novel approaches to increase performance bounds of\nimage steganography under the criteria of minimizing distortion. First, in\norder to efficiently use the images' capacities, we propose using parallel\nimages in the embedding stage. The result is then used to prove sub-optimality\nof the message distribution technique used by all cost based algorithms\nincluding HUGO, S-UNIWARD, and HILL. Second, a new distribution approach is\npresented to further improve the security of these algorithms. Experiments show\nthat this distribution method avoids embedding in smooth regions and thus\nachieves a better performance, measured by state-of-the-art steganalysis, when\ncompared with the current used distribution.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 05:47:23 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 18:23:55 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Sharifzadeh", "Mehdi", ""], ["Agarwal", "Chirag", ""], ["Salarian", "Mahdi", ""], ["Schonfeld", "Dan", ""]]}, {"id": "1705.08733", "submitter": "Theodoros Karagkioules Mr", "authors": "Dimitrios Tsilimantos, Theodoros Karagkioules, Amaya Nogales-G\\'omez\n  and Stefan Valentin", "title": "Traffic Profiling for Mobile Video Streaming", "comments": "7 pages, 11 figures. Accepted for publication in the proceedings of\n  IEEE ICC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel system that provides key parameters of HTTP\nAdaptive Streaming (HAS) sessions to the lower layers of the protocol stack. A\nnon-intrusive traffic profiling solution is proposed that observes packet flows\nat the transmit queue of base stations, edge-routers, or gateways. By analyzing\nIP flows in real time, the presented scheme identifies different phases of an\nHAS session and estimates important application-layer parameters, such as\nplay-back buffer state and video encoding rate. The introduced estimators only\nuse IP-layer information, do not require standardization and work even with\ntraffic that is encrypted via Transport Layer Security (TLS). Experimental\nresults for a popular video streaming service clearly verify the high accuracy\nof the proposed solution. Traffic profiling, thus, provides a valuable\nalternative to cross-layer signaling and Deep Packet Inspection (DPI) in order\nto perform efficient network optimization for video streaming.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 12:51:39 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Tsilimantos", "Dimitrios", ""], ["Karagkioules", "Theodoros", ""], ["Nogales-G\u00f3mez", "Amaya", ""], ["Valentin", "Stefan", ""]]}, {"id": "1705.08809", "submitter": "Theodoros Karagkioules Mr", "authors": "Sami Mekki, Theodoros Karagkioules and Stefan Valentin", "title": "HTTP adaptive streaming with indoors-outdoors detection in mobile\n  networks", "comments": "6 pages, 6 figures. Accepted at CNTCV: Communication and Networking\n  Techniqes for Contemporary Video Workshop of INFOCOM'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mobile networks, users may lose coverage when entering a building due to\nthe high signal attenuation at windows and walls. Under such conditions,\nservices with minimum bit-rate requirements, such as video streaming, often\nshow poor Quality-of-Experience (QoE). We will present a Bayesian detector that\ncombines measurements from two Smartphone sensors to decide if a user is inside\na building or not. Based on this coverage classification, we will propose an\nHTTP adaptive streaming (HAS) algorithm to increase playback stability at a\nhigh average bitrate. Measurements in a typical office building show high\naccuracy for the presented detector and superior QoE for the proposed HAS\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:03:09 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Mekki", "Sami", ""], ["Karagkioules", "Theodoros", ""], ["Valentin", "Stefan", ""]]}, {"id": "1705.09776", "submitter": "Xinfeng Zhang", "authors": "Lingyu Duan, Wei Sun, Xinfeng Zhang, Shiqi Wang, Jie Chen, Jianxiong\n  Yin, Simon See, Tiejun Huang, Alex C. Kot, Wen Gao", "title": "Fast MPEG-CDVS Encoder with GPU-CPU Hybrid Computing", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2794203", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compact descriptors for visual search (CDVS) standard from ISO/IEC moving\npictures experts group (MPEG) has succeeded in enabling the interoperability\nfor efficient and effective image retrieval by standardizing the bitstream\nsyntax of compact feature descriptors. However, the intensive computation of\nCDVS encoder unfortunately hinders its widely deployment in industry for\nlarge-scale visual search. In this paper, we revisit the merits of low\ncomplexity design of CDVS core techniques and present a very fast CDVS encoder\nby leveraging the massive parallel execution resources of GPU. We elegantly\nshift the computation-intensive and parallel-friendly modules to the\nstate-of-the-arts GPU platforms, in which the thread block allocation and the\nmemory access are jointly optimized to eliminate performance loss. In addition,\nthose operations with heavy data dependence are allocated to CPU to resolve the\nextra but non-necessary computation burden for GPU. Furthermore, we have\ndemonstrated the proposed fast CDVS encoder can work well with those\nconvolution neural network approaches which has harmoniously leveraged the\nadvantages of GPU platforms, and yielded significant performance improvements.\nComprehensive experimental results over benchmarks are evaluated, which has\nshown that the fast CDVS encoder using GPU-CPU hybrid computing is promising\nfor scalable visual search.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 06:59:37 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 11:26:11 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Duan", "Lingyu", ""], ["Sun", "Wei", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shiqi", ""], ["Chen", "Jie", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""], ["Huang", "Tiejun", ""], ["Kot", "Alex C.", ""], ["Gao", "Wen", ""]]}, {"id": "1705.10642", "submitter": "Xiang Chen", "authors": "Xiang Chen, Bowei Chen, Mohan Kankanhalli", "title": "Optimizing Trade-offs Among Stakeholders in Real-Time Bidding by\n  Incorporating Multimedia Metrics", "comments": "In Proceedings of the 40th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR) 2017, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying banner advertisements (in short, ads) on webpages has usually been\ndiscussed as an Internet economics topic where a publisher uses auction models\nto sell an online user's page view to advertisers and the one with the highest\nbid can have her ad displayed to the user. This is also called \\emph{real-time\nbidding} (RTB) and the ad displaying process ensures that the publisher's\nbenefit is maximized or there is an equilibrium in ad auctions. However, the\nbenefits of the other two stakeholders -- the advertiser and the user -- have\nbeen rarely discussed. In this paper, we propose a two-stage computational\nframework that selects a banner ad based on the optimized trade-offs among all\nstakeholders. The first stage is still auction based and the second stage\nre-ranks ads by considering the benefits of all stakeholders. Our metric\nvariables are: the publisher's revenue, the advertiser's utility, the ad\nmemorability, the ad click-through rate (CTR), the contextual relevance, and\nthe visual saliency. To the best of our knowledge, this is the first work that\noptimizes trade-offs among all stakeholders in RTB by incorporating multimedia\nmetrics. An algorithm is also proposed to determine the optimal weights of the\nmetric variables. We use both ad auction datasets and multimedia datasets to\nvalidate the proposed framework. Our experimental results show that the\npublisher can significantly improve the other stakeholders' benefits by\nslightly reducing her revenue in the short-term. In the long run, advertisers\nand users will be more engaged, the increased demand of advertising and the\nincreased supply of page views can then boost the publisher's revenue.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 13:48:35 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 10:06:18 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 11:11:31 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Chen", "Xiang", ""], ["Chen", "Bowei", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1705.10742", "submitter": "Martin Jaggi", "authors": "Tina Fang, Martin Jaggi, Katerina Argyraki", "title": "Generating Steganographic Text with LSTMs", "comments": "ACL 2017 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by concerns for user privacy, we design a steganographic system\n(\"stegosystem\") that enables two users to exchange encrypted messages without\nan adversary detecting that such an exchange is taking place. We propose a new\nlinguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network.\nWe demonstrate our approach on the Twitter and Enron email datasets and show\nthat it yields high-quality steganographic text while significantly improving\ncapacity (encrypted bits per word) relative to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:52:48 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Fang", "Tina", ""], ["Jaggi", "Martin", ""], ["Argyraki", "Katerina", ""]]}]