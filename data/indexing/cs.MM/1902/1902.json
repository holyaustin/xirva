[{"id": "1902.00380", "submitter": "Mingliang Xu", "authors": "Chaochao Li, Pei Lv, Dinesh Manocha, Hua Wang, Yafei Li, Bing Zhou,\n  and Mingliang Xu", "title": "ACSEE: Antagonistic Crowd Simulation Model with Emotional Contagion and\n  Evolutionary Game Theory", "comments": null, "journal-ref": "IEEE Transactions on Affective Computing (2019)", "doi": "10.1109/TAFFC.2019.2954394", "report-no": null, "categories": "cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antagonistic crowd behaviors are often observed in cases of serious conflict.\nAntagonistic emotions, which is the typical psychological state of agents in\ndifferent roles (i.e. cops, activists, and civilians) in crowd violent scenes,\nand the way they spread through contagion in a crowd are important causes of\ncrowd antagonistic behaviors. Moreover, games, which refers to the interaction\nbetween opposing groups adopting different strategies to obtain higher benefits\nand less casualties, determine the level of crowd violence. We present an\nantagonistic crowd simulation model, ACSEE, which is integrated with\nantagonistic emotional contagion and evolutionary game theories. Our approach\nmodels the antagonistic emotions between agents in different roles using two\ncomponents: mental emotion and external emotion. We combine enhanced\nsusceptible-infectious-susceptible (SIS) and game approaches to evaluate the\nrole of antagonistic emotional contagion in crowd violence. Our evolutionary\ngame theoretic approach incorporates antagonistic emotional contagion through\ndeterrent force, which is modelled by a mixture of emotional forces and\nphysical forces defeating the opponents. Antagonistic emotional contagion and\nevolutionary game theories influence each other to determine antagonistic crowd\nbehaviors. We evaluate our approach on real-world scenarios consisting of\ndifferent kinds of agents. We also compare the simulated crowd behaviors with\nreal-world crowd videos and use our approach to predict the trends of crowd\nmovements in violence incidents. We investigate the impact of various factors\n(number of agents, emotion, strategy, etc.) on the outcome of crowd violence.\nWe present results from user studies suggesting that our model can simulate\nantagonistic crowd behaviors similar to those seen in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 16:21:43 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 01:25:06 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 12:17:38 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Li", "Chaochao", ""], ["Lv", "Pei", ""], ["Manocha", "Dinesh", ""], ["Wang", "Hua", ""], ["Li", "Yafei", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""]]}, {"id": "1902.00637", "submitter": "Kexin Tang", "authors": "Kexin Tang, Nuowen Kan, Junni Zou, Xiao Fu, Mingyi Hong, Hongkai Xiong", "title": "Multiuser Video Streaming Rate Adaptation: A Physical Layer\n  Resource-Aware Deep Reinforcement Learning Approach", "comments": "29 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-user video streaming service optimization problem over a\ntime-varying and mutually interfering multi-cell wireless network. The key\nresearch challenge is to appropriately adapt each user's video streaming rate\naccording to the radio frequency environment (e.g., channel fading and\ninterference level) and service demands (e.g., play request), so that the\nusers' long-term experience for watching videos can be optimized. To address\nthe above challenge, we propose a novel two-level cross-layer optimization\nframework for multiuser adaptive video streaming over wireless networks. The\nkey idea is to jointly design the physical layer optimization-based beamforming\nscheme (performed at the base stations) and the application layer Deep\nReinforcement Learning (DRL)-based scheme (performed at the user terminals), so\nthat a highly complex multi-user, cross-layer, time-varying video streaming\nproblem can be decomposed into relatively simple problems and solved\neffectively. Our strategy represents a significant departure for the existing\nschemes where either short-term user experience optimization is considered, or\nonly single-user point-to-point long-term optimization is considered. Extensive\nsimulations based on real-data sets show that the proposed cross-layer design\nis effective and promising.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 03:45:43 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tang", "Kexin", ""], ["Kan", "Nuowen", ""], ["Zou", "Junni", ""], ["Fu", "Xiao", ""], ["Hong", "Mingyi", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1902.00680", "submitter": "Charles Martin", "authors": "Charles P Martin and Jim Torresen", "title": "Data Driven Analysis of Tiny Touchscreen Performance with MicroJam", "comments": null, "journal-ref": "Computer Music Journal, 43(4), 41-57 (2020)", "doi": "10.1162/comj_a_00536", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread adoption of mobile devices, such as smartphones and tablets,\nhas made touchscreens a common interface for musical performance. New mobile\nmusical instruments have been designed that embrace collaborative creation and\nthat explore the affordances of mobile devices, as well as their constraints.\nWhile these have been investigated from design and user experience\nperspectives, there is little examination of the performers' musical outputs.\nIn this work, we introduce a constrained touchscreen performance app, MicroJam,\ndesigned to enable collaboration between performers, and engage in a novel\ndata-driven analysis of more than 1600 performances using the app. MicroJam\nconstrains performances to five seconds, and emphasises frequent and casual\nmusic making through a social media-inspired interface. Performers collaborate\nby replying to performances, adding new musical layers that are played back at\nthe same time. Our analysis shows that users tend to focus on the centre and\ndiagonals of the touchscreen area, and tend to swirl or swipe rather than tap.\nWe also observe that while long swipes dominate the visual appearance of\nperformances, the majority of interactions are short with limited expressive\npossibilities. Our findings are summarised into a set of design recommendations\nfor MicroJam and other touchscreen apps for social musical interaction.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 09:35:37 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Martin", "Charles P", ""], ["Torresen", "Jim", ""]]}, {"id": "1902.01286", "submitter": "Yang Hao", "authors": "Zhongliang Yang, Hao Yang, Yuting Hu, Yongfeng Huang, Yu-Jin Zhang", "title": "Real-Time Steganalysis for Stream Media Based on Multi-channel\n  Convolutional Sliding Windows", "comments": "13 pages, summit to ieee transactions on information forensics and\n  security (tifs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous VoIP steganalysis methods face great challenges in detecting speech\nsignals at low embedding rates, and they are also generally difficult to\nperform real-time detection, making them hard to truly maintain cyberspace\nsecurity. To solve these two challenges, in this paper, combined with the\nsliding window detection algorithm and Convolution Neural Network we propose a\nreal-time VoIP steganalysis method which based on multi-channel convolution\nsliding windows. In order to analyze the correlations between frames and\ndifferent neighborhood frames in a VoIP signal, we define multi channel sliding\ndetection windows. Within each sliding window, we design two feature extraction\nchannels which contain multiple convolution layers with multiple convolution\nkernels each layer to extract correlation features of the input signal. Then\nbased on these extracted features, we use a forward fully connected network for\nfeature fusion. Finally, by analyzing the statistical distribution of these\nfeatures, the discriminator will determine whether the input speech signal\ncontains covert information or not.We designed several experiments to test the\nproposed model's detection ability under various conditions, including\ndifferent embedding rates, different speech length, etc. Experimental results\nshowed that the proposed model outperforms all the previous methods, especially\nin the case of low embedding rate, which showed state-of-the-art performance.\nIn addition, we also tested the detection efficiency of the proposed model, and\nthe results showed that it can achieve almost real-time detection of VoIP\nspeech signals.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 16:23:56 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Yang", "Zhongliang", ""], ["Yang", "Hao", ""], ["Hu", "Yuting", ""], ["Huang", "Yongfeng", ""], ["Zhang", "Yu-Jin", ""]]}, {"id": "1902.01372", "submitter": "Amrita Mazumdar", "authors": "Amrita Mazumdar, Brandon Haynes, Magdalena Balazinska, Luis Ceze,\n  Alvin Cheung, Mark Oskin", "title": "Vignette: Perceptual Compression for Video Storage and Processing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed videos constitute 70% of Internet traffic, and video upload growth\nrates far outpace compute and storage improvement trends. Past work in\nleveraging perceptual cues like saliency, i.e., regions where viewers focus\ntheir perceptual attention, reduces compressed video size while maintaining\nperceptual quality, but requires significant changes to video codecs and\nignores the data management of this perceptual information.\n  In this paper, we propose Vignette, a compression technique and storage\nmanager for perception-based video compression. Vignette complements\noff-the-shelf compression software and hardware codec implementations.\nVignette's compression technique uses a neural network to predict saliency\ninformation used during transcoding, and its storage manager integrates\nperceptual information into the video storage system to support a perceptual\ncompression feedback loop. Vignette's saliency-based optimizations reduce\nstorage by up to 95% with minimal quality loss, and Vignette videos lead to\npower savings of 50% on mobile phones during video playback. Our results\ndemonstrate the benefit of embedding information about the human visual system\ninto the architecture of video storage systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:39:44 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Mazumdar", "Amrita", ""], ["Haynes", "Brandon", ""], ["Balazinska", "Magdalena", ""], ["Ceze", "Luis", ""], ["Cheung", "Alvin", ""], ["Oskin", "Mark", ""]]}, {"id": "1902.03123", "submitter": "Tamara Bulatovic", "authors": "Radoje Darmanovic, Tamara Bulatovic, Seid Salkovic", "title": "Iris Image Processing in Compressive Sensing Scenario", "comments": "Student paper submitted to The 8th Mediterranean Conference on\n  Embedded Computing - MECO'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper observes the application of the Compressive Sensing in\nreconstruction of the under-sampled iris images. Iris recognition represents\nform of biometric identification whose usage in real applications is growing.\nCompressive Sensing represents a novel form of sparse signal acquisition and\nrecovering when small amount of data is a available. Different sparsity domains\nare considered and compared using various number of available image pixels. The\ntheory is verified on iris images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 10:15:43 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Darmanovic", "Radoje", ""], ["Bulatovic", "Tamara", ""], ["Salkovic", "Seid", ""]]}, {"id": "1902.03389", "submitter": "Yuki Saito", "authors": "Hiroki Tamaru, Yuki Saito, Shinnosuke Takamichi, Tomoki Koriyama,\n  Hiroshi Saruwatari", "title": "Generative Moment Matching Network-based Random Modulation Post-filter\n  for DNN-based Singing Voice Synthesis and Neural Double-tracking", "comments": "5 pages, to appear in IEEE ICASSP 2019 (Paper Code: SLP-P22.11,\n  Session: Speech Synthesis III)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generative moment matching network (GMMN)-based\npost-filter that provides inter-utterance pitch variation for deep neural\nnetwork (DNN)-based singing voice synthesis. The natural pitch variation of a\nhuman singing voice leads to a richer musical experience and is used in\ndouble-tracking, a recording method in which two performances of the same\nphrase are recorded and mixed to create a richer, layered sound. However,\nsinging voices synthesized using conventional DNN-based methods never vary\nbecause the synthesis process is deterministic and only one waveform is\nsynthesized from one musical score. To address this problem, we use a GMMN to\nmodel the variation of the modulation spectrum of the pitch contour of natural\nsinging voices and add a randomized inter-utterance variation to the pitch\ncontour generated by conventional DNN-based singing voice synthesis.\nExperimental evaluations suggest that 1) our approach can provide perceptible\ninter-utterance pitch variation while preserving speech quality. We extend our\napproach to double-tracking, and the evaluation demonstrates that 2) GMMN-based\nneural double-tracking is perceptually closer to natural double-tracking than\nconventional signal processing-based artificial double-tracking is.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 07:49:42 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Tamaru", "Hiroki", ""], ["Saito", "Yuki", ""], ["Takamichi", "Shinnosuke", ""], ["Koriyama", "Tomoki", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "1902.03878", "submitter": "Ralph Gasser", "authors": "Ralph Gasser, Luca Rossetto, Heiko Schuldt", "title": "Towards an All-Purpose Content-Based Multimedia Information Retrieval\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of multimedia collections - in terms of size, heterogeneity, and\nvariety of media types - necessitates systems that are able to conjointly deal\nwith several forms of media, especially when it comes to searching for\nparticular objects. However, existing retrieval systems are organized in silos\nand treat different media types separately. As a consequence, retrieval across\nmedia types is either not supported at all or subject to major limitations. In\nthis paper, we present vitrivr, a content-based multimedia information\nretrieval stack. As opposed to the keyword search approach implemented by most\nmedia management systems, vitrivr makes direct use of the object's content to\nfacilitate different types of similarity search, such as Query-by-Example or\nQuery-by-Sketch, for and, most importantly, across different media types -\nnamely, images, audio, videos, and 3D models. Furthermore, we introduce a new\nweb-based user interface that enables easy-to-use, multimodal retrieval from\nand browsing in mixed media collections. The effectiveness of vitrivr is shown\non the basis of a user study that involves different query and media types. To\nthe best of our knowledge, the full vitrivr stack is unique in that it is the\nfirst multimedia retrieval system that seamlessly integrates support for four\ndifferent types of media. As such, it paves the way towards an all-purpose,\ncontent-based multimedia information retrieval system.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 13:58:04 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Gasser", "Ralph", ""], ["Rossetto", "Luca", ""], ["Schuldt", "Heiko", ""]]}, {"id": "1902.03988", "submitter": "Sahar Sadrizadeh", "authors": "Sahar Sadrizadeh, Nematollah Zarmehi, Ehsan Asadi, Hamidreza Abin, and\n  Farokh Marvasti", "title": "A Fast Iterative Method for Removing Impulsive Noise from Sparse Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method to reconstruct a signal corrupted by\nnoise where both signal and noise are sparse but in different domains. The\nproblem investigated in this paper arises in different applications such as\nimpulsive noise removal from images, audios and videos, decomposition of\nlow-rank and sparse components of matrices, and separation of texts from\nimages. First, we provide a cost function for our problem and then present an\niterative method to find its local minimum. The analysis of the algorithm is\nalso provided. As an application of this problem, we apply our algorithm for\nimpulsive noise Salt-and-Pepper noise (SPN) and Random-Valued Impulsive Noise\n(RVIN)) removal from images and compare our results with other notable\nalgorithms in the literature. Furthermore, we apply our algorithm for removing\nclicks from audio signals. Simulation results show that our algorithms is\nsimple and fast, and it outperforms other state-of-the-art methods in terms of\nreconstruction quality and/or complexity.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 18:22:18 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 05:39:11 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sadrizadeh", "Sahar", ""], ["Zarmehi", "Nematollah", ""], ["Asadi", "Ehsan", ""], ["Abin", "Hamidreza", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1902.04067", "submitter": "Vaneet Aggarwal", "authors": "Abubakr O. Al-Abbasi and Vaneet Aggarwal and Moo-Ryong Ra", "title": "Multi-tier Caching Analysis in CDN-based Over-the-top Video Streaming\n  Systems", "comments": "Accepted to IEEE/ACM TON, 2019. arXiv admin note: substantial text\n  overlap with arXiv:1807.01147", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet video traffic has been been rapidly increasing and is further\nexpected to increase with the emerging 5G applications such as higher\ndefinition videos, IoT and augmented/virtual reality applications. As end-users\nconsume video in massive amounts and in an increasing number of ways, the\ncontent distribution network (CDN) should be efficiently managed to improve the\nsystem efficiency. The streaming service can include multiple caching tiers, at\nthe distributed servers and the edge routers, and efficient content management\nat these locations affect the quality of experience (QoE) of the end users. In\nthis paper, we propose a model for video streaming systems, typically composed\nof a centralized origin server, several CDN sites, and edge-caches located\ncloser to the end user. We comprehensively consider different systems design\nfactors including the limited caching space at the CDN sites, allocation of CDN\nfor a video request, choice of different ports (or paths) from the CDN and the\ncentral storage, bandwidth allocation, the edge-cache capacity, and the caching\npolicy. We focus on minimizing a performance metric, stall duration tail\nprobability (SDTP), and present a novel and efficient algorithm accounting for\nthe multiple design flexibilities. The theoretical bounds with respect to the\nSDTP metric are also analyzed and presented. The implementation on a\nvirtualized cloud system managed by Openstack demonstrate that the proposed\nalgorithms can significantly improve the SDTP metric, compared to the baseline\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 01:23:00 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Al-Abbasi", "Abubakr O.", ""], ["Aggarwal", "Vaneet", ""], ["Ra", "Moo-Ryong", ""]]}, {"id": "1902.04169", "submitter": "Li Li", "authors": "Li Li, Zhu Li, Shan Liu, Houqiang Li", "title": "Occupancy-map-based rate distortion optimization for video-based point\n  cloud compression", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art video-based point cloud compression scheme projects the\n3D point cloud to 2D patch by patch and organizes the patches into frames to\ncompress them using the efficient video compression scheme. Such a scheme shows\na good trade-off between the number of points projected and the video\ncontinuity to utilize the video compression scheme. However, some unoccupied\npixels between different patches are compressed using almost the same quality\nwith the occupied pixels, which will lead to the waste of lots of bits since\nthe unoccupied pixels are useless for the reconstructed point cloud. In this\npaper, we propose to consider only the rate instead of the rate distortion cost\nfor the unoccupied pixels during the rate distortion optimization process. The\nproposed scheme can be applied to both the geometry and attribute frames. The\nexperimental results show that the proposed algorithm can achieve an average of\n11.9% and 15.4% bitrate savings for the geometry and attribute, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 22:20:45 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Li", "Li", ""], ["Li", "Zhu", ""], ["Liu", "Shan", ""], ["Li", "Houqiang", ""]]}, {"id": "1902.04191", "submitter": "Manoranjan Paul PhD", "authors": "Rui Dusselaar and Manoranjan Paul", "title": "A block-based inter-band predictor using multilayer propagation neural\n  network for hyperspectral image compression", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a block-based inter-band predictor (BIP) with multilayer\npropagation neural network model (MLPNN) is presented by a completely new\nframework. This predictor can combine with diversity entropy coding methods.\nHyperspectral (HS) images are composed by a series high similarity spectral\nbands. Our assumption is to use trained MLPNN predict the succeeding bands\nbased on current band information. The purpose is to explore whether BIP-MLPNN\ncan provide better image predictive results with high efficiency. The algorithm\nalso changed from the traditional compression methods encoding images pixel by\npixel, the compression process only encodes the weights and the biases vectors\nof BIP-MLPNN which require few bits to transfer. The decoder will reconstruct a\nband by using the same structure of the network at the encoder side. The\nBIP-MLPNN decoder does not need to be trained as the weights and biases have\nalready been transmitted. We can easily reconstruct the succeeding bands by\nusing the BIP-MLPNN decoder. The experimental results indicate that BIP-MLPNN\npredictor outperforms the CCSDS-123 HS image coding standard. Due to a good\napproximation of the target band, the proposed method outperforms the CCSDS-123\nby more than 2.0dB PSNR image quality in the predicted bands. Moreover, the\nproposed method provides high quality image e.g., 30 to 40dB PSNR at very low\nbit rate (less than 0.1 bpppb) and outperforms the existing methods e.g., JPEG,\n3DSPECK, 3DSPIHT and in terms of rate-distortion performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 00:26:38 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Dusselaar", "Rui", ""], ["Paul", "Manoranjan", ""]]}, {"id": "1902.04397", "submitter": "Stefan Balke", "authors": "Meinard M\\\"uller and Andreas Arzt and Stefan Balke and Matthias Dorfer\n  and Gerhard Widmer", "title": "Cross-Modal Music Retrieval and Applications: An Overview of Key\n  Methodologies", "comments": null, "journal-ref": "IEEE Signal Processing Magazine (Volume: 36, Issue: 1, Jan. 2019)", "doi": "10.1109/MSP.2018.2868887", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a rapid growth of digitally available music data, including\naudio recordings, digitized images of sheet music, album covers and liner\nnotes, and video clips. This huge amount of data calls for retrieval strategies\nthat allow users to explore large music collections in a convenient way. More\nprecisely, there is a need for cross-modal retrieval algorithms that, given a\nquery in one modality (e.g., a short audio excerpt), find corresponding\ninformation and entities in other modalities (e.g., the name of the piece and\nthe sheet music). This goes beyond exact audio identification and subsequent\nretrieval of metainformation as performed by commercial applications like\nShazam [1].\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 14:13:35 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["M\u00fcller", "Meinard", ""], ["Arzt", "Andreas", ""], ["Balke", "Stefan", ""], ["Dorfer", "Matthias", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1902.04902", "submitter": "Bin Song", "authors": "Yinghua Li, Bin Song, Jie Guo, Xiaojiang Du, Mohsen Guizani", "title": "Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and\n  Nonlocal Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 13:39:45 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Li", "Yinghua", ""], ["Song", "Bin", ""], ["Guo", "Jie", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1902.04985", "submitter": "Bashir Sadiq Mr", "authors": "H. A. Abdulkareem, A. M. S. Tekanyi, I. Yau and B. O. Sadiq", "title": "Development of Video Frame Enhancement Technique Using Pixel Intensity\n  Analysis", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper developed a brightness enhancement technique for video frame pixel\nintensity improvement. Frames extracted from the six sample video data used in\nthis work were stored in the form of images in a buffer. Noise was added to the\nextracted image frames to vary the intensity of their pixels so that the pixel\nvalues of the noisy images differ from their true values in order to determine\nthe efficiency of the developed technique. Simulation results showed that, the\ndeveloped technique was efficient with an improved pixel intensity and\nhistogram distribution. The Peak to Signal Noise Ratio evaluation showed that\nthe efficiency of the developed technique for both grayscale and coloured video\nframes were improved by PSNR of 12.45%, 16.32%, 27.57% and 19.83% over the grey\nlevel colour (black and white) for the NAELS1.avi, NAELS2.avi, NTA1.avi and\nNTA2.avi respectively. Also, a percentage improvement of 28.93% and 31.68% were\nobtained for the coloured image over the grey level image for Akiyo.avi and\nForman.avi benchmark video frame, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 16:24:46 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Abdulkareem", "H. A.", ""], ["Tekanyi", "A. M. S.", ""], ["Yau", "I.", ""], ["Sadiq", "B. O.", ""]]}, {"id": "1902.05179", "submitter": "Saeed  Ranjbar Alvar", "authors": "Saeed Ranjbar Alvar and Ivan V. Baji\\'c", "title": "Multi-task learning with compressible features for Collaborative\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising way to deploy Artificial Intelligence (AI)-based services on\nmobile devices is to run a part of the AI model (a deep neural network) on the\nmobile itself, and the rest in the cloud. This is sometimes referred to as\ncollaborative intelligence. In this framework, intermediate features from the\ndeep network need to be transmitted to the cloud for further processing. We\nstudy the case where such features are used for multiple purposes in the cloud\n(multi-tasking) and where they need to be compressible in order to allow\nefficient transmission to the cloud. To this end, we introduce a new loss\nfunction that encourages feature compressibility while improving system\nperformance on multiple tasks. Experimental results show that with the\ncompression-friendly loss, one can achieve around 20% bitrate reduction without\nsacrificing the performance on several vision-related tasks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 01:28:17 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 21:06:23 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Alvar", "Saeed Ranjbar", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "1902.05347", "submitter": "Federico Simonetta", "authors": "Federico Simonetta, Stavros Ntalampiras, Federico Avanzini", "title": "Multimodal music information processing and retrieval: survey and future\n  challenges", "comments": null, "journal-ref": "in 2019 International Workshop on Multilayer Music Representation\n  and Processing, Milano, IEEE 2019", "doi": "10.1109/MMRP.2019.00012", "report-no": null, "categories": "cs.MM cs.IR cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Towards improving the performance in various music information processing\ntasks, recent studies exploit different modalities able to capture diverse\naspects of music. Such modalities include audio recordings, symbolic music\nscores, mid-level representations, motion, and gestural data, video recordings,\neditorial or cultural tags, lyrics and album cover arts. This paper critically\nreviews the various approaches adopted in Music Information Processing and\nRetrieval and highlights how multimodal algorithms can help Music Computing\napplications. First, we categorize the related literature based on the\napplication they address. Subsequently, we analyze existing information fusion\napproaches, and we conclude with the set of challenges that Music Information\nRetrieval and Sound and Music Computing research communities should focus in\nthe next years.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:36:01 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Simonetta", "Federico", ""], ["Ntalampiras", "Stavros", ""], ["Avanzini", "Federico", ""]]}, {"id": "1902.05386", "submitter": "Andrej Joki\\'c", "authors": "Andrej Jokic, Nikola Vukovic", "title": "License Plate Recognition with Compressive Sensing Based Feature\n  Extraction", "comments": "Student paper submitted to The 8th Mediterranean Conference on\n  Embedded Computing - MECO'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License plate recognition is the key component to many automatic traffic\ncontrol systems. It enables the automatic identification of vehicles in many\napplications. Such systems must be able to identify vehicles from images taken\nin various conditions including low light, rain, snow, etc. In order to reduce\nthe complexity and cost of the hardware required for such devices, the\nalgorithm should be as efficient as possible. This paper proposes a license\nplate recognition system which uses a new approach based on compressive sensing\ntechniques for dimensionality reduction and feature extraction. Dimensionality\nreduction will enable precise classification with less training data while\ndemanding less computational power. Based on the extracted features, character\nrecognition and classification is done by a Support Vector Machine classifier.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 11:58:56 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Jokic", "Andrej", ""], ["Vukovic", "Nikola", ""]]}, {"id": "1902.06927", "submitter": "Kele Xu", "authors": "Chaojie Zhao and Peng Zhang and Jian Zhu and Chengrui Wu and Huaimin\n  Wang and Kele Xu", "title": "Predicting tongue motion in unlabeled ultrasound videos using\n  convolutional LSTM neural network", "comments": "Accepted by ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge in speech production research is to predict future tongue\nmovements based on a short period of past tongue movements. This study tackles\nspeaker-dependent tongue motion prediction problem in unlabeled ultrasound\nvideos with convolutional long short-term memory (ConvLSTM) networks. The model\nhas been tested on two different ultrasound corpora. ConvLSTM outperforms\n3-dimensional convolutional neural network (3DCNN) in predicting the\n9\\textsuperscript{th} frames based on 8 preceding frames, and also demonstrates\ngood capacity to predict only the tongue contours in future frames. Further\ntests reveal that ConvLSTM can also learn to predict tongue movements in more\ndistant frames beyond the immediately following frames. Our codes are available\nat: https://github.com/shuiliwanwu/ConvLstm-ultrasound-videos.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 07:11:28 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Zhao", "Chaojie", ""], ["Zhang", "Peng", ""], ["Zhu", "Jian", ""], ["Wu", "Chengrui", ""], ["Wang", "Huaimin", ""], ["Xu", "Kele", ""]]}, {"id": "1902.06990", "submitter": "Mamoona Asghar Dr.", "authors": "Rizwan A. Shah, Mamoona N. Asghar, Saima Abdullah, Martin Fleury, and\n  Neelam Gohar", "title": "Effectiveness of Crypto-Transcoding for H.264/AVC and HEVC Video\n  Bit-streams", "comments": "Revision-3 Version of Multimedia Tools and Application (Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To avoid delays arising from a need to decrypt a video prior to transcoding\nand then re-encrypt it afterwards, this paper assesses a selective encryption\n(SE) content protection scheme. The scheme is suited to both recent\nstandardized codecs, namely H.264/Advanced Video Coding (AVC) and High\nEfficiency Video Coding (HEVC). Specifically, the paper outlines a joint\ncrypto-transcoding scheme for secure transrating of a video bitstream. That is\nto say it generates new video bitrates, possibly as part of an HTTP Adaptive\nStreaming (HAS) content delivery network. The scheme will reduce the bitrate to\none or more lower desired bit-rate without consuming time in the\nencryption/decryption process, which would be the case when full encryption is\nused. In addition, the decryption key no longer needs to be exposed at\nintermediate middleboxes, including when transrating is performed in a cloud\ndatacenter. The effectiveness of the scheme is variously evaluated: by\nexamination of the SE generated visual distortion; by the extent of\ncomputational and bitrate overheads; and by choice of cipher when encrypting\nthe selected elements within the bitstream. Results indicate that there\nremains: a content; quantization level (after transrating of an encrypted\nvideo); and codec-type dependency to any distortion introduced. A further\nrecommendation is that the Advanced Encryption Standard (AES) is preferred for\nSE to lightweight XOR encryption, despite it being taken up elsewhere as a\nreal-time encryption method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 10:59:26 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Shah", "Rizwan A.", ""], ["Asghar", "Mamoona N.", ""], ["Abdullah", "Saima", ""], ["Fleury", "Martin", ""], ["Gohar", "Neelam", ""]]}, {"id": "1902.08314", "submitter": "Ivo Trowitzsch", "authors": "Ivo Trowitzsch, Jalil Taghia, Youssef Kashef, Klaus Obermayer", "title": "The NIGENS General Sound Events Database", "comments": "update to v4: added classification rate table, corrections, updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational auditory scene analysis is gaining interest in the last years.\nTrailing behind the more mature field of speech recognition, it is particularly\ngeneral sound event detection that is attracting increasing attention. Crucial\nfor training and testing reasonable models is having available enough suitable\ndata -- until recently, general sound event databases were hardly found. We\nrelease and present a database with 714 wav files containing isolated high\nquality sound events of 14 different types, plus 303 `general' wav files of\nanything else but these 14 types. All sound events are strongly labeled with\nperceptual on- and offset times, paying attention to omitting in-between\nsilences. The amount of isolated sound events, the quality of annotations, and\nthe particular general sound class distinguish NIGENS from other databases.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 23:51:59 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 10:51:31 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 19:53:34 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2020 23:22:00 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Trowitzsch", "Ivo", ""], ["Taghia", "Jalil", ""], ["Kashef", "Youssef", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1902.09581", "submitter": "Nikolaos Thomos", "authors": "Pantelis Maniotis, Eirina Bourtsoulatze, and Nikolaos Thomos", "title": "Tile-Based Joint Caching and Delivery of $360^o$ Videos in Heterogeneous\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent surge of applications involving the use of $360^o$ video\nchallenges mobile networks infrastructure, as $360^o$ video files are of\nsignificant size, and current delivery and edge caching architectures are\nunable to guarantee their timely delivery. In this paper, we investigate the\nproblem of joint collaborative content-aware caching and delivery of $360^o$\nvideos in a video on demand setting. The proposed scheme takes advantage of\n$360^o$ video encoding in multiple tiles and layers to make fine-grained\ndecisions regarding which tiles to cache in each Small Base Station (SBS), and\nwhere to deliver them from to the end users, as users may reside in the\ncoverage area of multiple SBSs. This permits to cache the most popular tiles in\nthe SBSs, while the remaining tiles may be obtained through the backhaul. In\naddition, we explicitly consider the time delivery constraints to ensure\ncontinuous video playback. To reduce the computational complexity of the\noptimization problem, we simplify it by introducing a fairness constraint. This\nallows us to split the original problem into subproblems corresponding to\nGroups of Pictures (GoP). Each of the subproblems is then solved with the\nmethod of Lagrange partial relaxation. Finally, we evaluate the performance of\nthe proposed method for various system parameters and compare it with schemes\nthat do not consider $360^o$ video encoding into multiple tiles and quality\nlayers, as well as with two variants of the proposed method one that considers\nlayered encoding and SBSs collaboration and another that uses tiles encoding\nbut with no SBSs collaboration. The results showcase the benefits coming from\ncaching and delivery decisions on per tile basis and the importance of\nexploiting SBSs collaboration.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 19:36:25 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 11:16:51 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Maniotis", "Pantelis", ""], ["Bourtsoulatze", "Eirina", ""], ["Thomos", "Nikolaos", ""]]}, {"id": "1902.09707", "submitter": "Qunliang Xing", "authors": "Qunliang Xing, Zhenyu Guan, Mai Xu, Ren Yang, Tie Liu, Zulin Wang", "title": "MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on\n  Compressed Video", "comments": "Accepted to TPAMI in September, 2019. v6 updates: correct units in\n  Fig. 11; correct author info; delete bio photos. arXiv admin note: text\n  overlap with arXiv:1803.04680", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2944806", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed great success in applying deep learning to\nenhance the quality of compressed image/video. The existing approaches mainly\nfocus on enhancing the quality of a single frame, not considering the\nsimilarity between consecutive frames. Since heavy fluctuation exists across\ncompressed video frames as investigated in this paper, frame similarity can be\nutilized for quality enhancement of low-quality frames given their neighboring\nhigh-quality frames. This task is Multi-Frame Quality Enhancement (MFQE).\nAccordingly, this paper proposes an MFQE approach for compressed video, as the\nfirst attempt in this direction. In our approach, we firstly develop a\nBidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak\nQuality Frames (PQFs) in compressed video. Then, a novel Multi-Frame\nConvolutional Neural Network (MF-CNN) is designed to enhance the quality of\ncompressed video, in which the non-PQF and its nearest two PQFs are the input.\nIn MF-CNN, motion between the non-PQF and PQFs is compensated by a motion\ncompensation subnet. Subsequently, a quality enhancement subnet fuses the\nnon-PQF and compensated PQFs, and then reduces the compression artifacts of the\nnon-PQF. Also, PQF quality is enhanced in the same way. Finally, experiments\nvalidate the effectiveness and generalization ability of our MFQE approach in\nadvancing the state-of-the-art quality enhancement of compressed video. The\ncode is available at https://github.com/RyanXingQL/MFQEv2.0.git.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 02:35:55 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 15:52:25 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 05:20:30 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 08:53:17 GMT"}, {"version": "v5", "created": "Mon, 6 Jul 2020 10:03:35 GMT"}, {"version": "v6", "created": "Sat, 3 Oct 2020 13:17:46 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Xing", "Qunliang", ""], ["Guan", "Zhenyu", ""], ["Xu", "Mai", ""], ["Yang", "Ren", ""], ["Liu", "Tie", ""], ["Wang", "Zulin", ""]]}, {"id": "1902.10102", "submitter": "Alexandre Garcia", "authors": "Alexandre Garcia, Slim Essid, Florence d'Alch\\'e-Buc, Chlo\\'e Clavel", "title": "A multimodal movie review corpus for fine-grained opinion mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a set of opinion annotations for the POM movie\nreview dataset, composed of 1000 videos. The annotation campaign is motivated\nby the development of a hierarchical opinion prediction framework allowing one\nto predict the different components of the opinions (e.g. polarity and aspect)\nand to identify the corresponding textual spans. The resulting annotations have\nbeen gathered at two granularity levels: a coarse one (opinionated span) and a\nfiner one (span of opinion components). We introduce specific categories in\norder to make the annotation of opinions easier for movie reviews. For example,\nsome categories allow the discovery of user recommendation and preference in\nmovie reviews. We provide a quantitative analysis of the annotations and report\nthe inter-annotator agreement under the different levels of granularity. We\nprovide thus the first set of ground-truth annotations which can be used for\nthe task of fine-grained multimodal opinion prediction. We provide an analysis\nof the data gathered through an inter-annotator study and show that a linear\nstructured predictor learns meaningful features even for the prediction of\nscarce labels. Both the annotations and the baseline system are made publicly\navailable. https://github.com/eusip/POM/\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 18:30:50 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 09:07:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Garcia", "Alexandre", ""], ["Essid", "Slim", ""], ["d'Alch\u00e9-Buc", "Florence", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "1902.10107", "submitter": "Weidi Xie", "authors": "Weidi Xie, Arsha Nagrani, Joon Son Chung, Andrew Zisserman", "title": "Utterance-level Aggregation For Speaker Recognition In The Wild", "comments": "To appear in: International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), 2019. (Oral Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this paper is speaker recognition \"in the wild\"-where\nutterances may be of variable length and also contain irrelevant signals.\nCrucial elements in the design of deep networks for this task are the type of\ntrunk (frame level) network, and the method of temporal aggregation. We propose\na powerful speaker recognition deep network, using a \"thin-ResNet\" trunk\narchitecture, and a dictionary-based NetVLAD or GhostVLAD layer to aggregate\nfeatures across time, that can be trained end-to-end. We show that our network\nachieves state of the art performance by a significant margin on the VoxCeleb1\ntest set for speaker recognition, whilst requiring fewer parameters than\nprevious methods. We also investigate the effect of utterance length on\nperformance, and conclude that for \"in the wild\" data, a longer length is\nbeneficial.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 18:34:05 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 19:13:14 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Xie", "Weidi", ""], ["Nagrani", "Arsha", ""], ["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1902.10647", "submitter": "Ralph Gasser", "authors": "Luca Rossetto, Mahnaz Amiri Parian, Ralph Gasser, Ivan Giangreco,\n  Silvan Heller, Heiko Schuldt", "title": "Deep Learning-based Concept Detection in vitrivr at the Video Browser\n  Showdown 2019 - Final Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an after-the-fact summary of the participation of the\nvitrivr system to the 2019 Video Browser Showdown. Analogously to last year's\nreport, the focus of this paper lies on additions made since the original\npublication and the system's performance during the competition.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:29:16 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Rossetto", "Luca", ""], ["Parian", "Mahnaz Amiri", ""], ["Gasser", "Ralph", ""], ["Giangreco", "Ivan", ""], ["Heller", "Silvan", ""], ["Schuldt", "Heiko", ""]]}, {"id": "1902.10707", "submitter": "Pawe{\\l} Korus", "authors": "Pawel Korus, Nasir Memon", "title": "Neural Imaging Pipelines - the Scourge or Hope of Forensics?", "comments": "Manuscript + supplement; currently under review; compressed figures\n  to minimize file size. arXiv admin note: text overlap with arXiv:1812.01516", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic analysis of digital photographs relies on intrinsic statistical\ntraces introduced at the time of their acquisition or subsequent editing. Such\ntraces are often removed by post-processing (e.g., down-sampling and\nre-compression applied upon distribution in the Web) which inhibits reliable\nprovenance analysis. Increasing adoption of computational methods within\ndigital cameras further complicates the process and renders explicit\nmathematical modeling infeasible. While this trend challenges forensic analysis\neven in near-acquisition conditions, it also creates new opportunities. This\npaper explores end-to-end optimization of the entire image acquisition and\ndistribution workflow to facilitate reliable forensic analysis at the end of\nthe distribution channel, where state-of-the-art forensic techniques fail. We\ndemonstrate that a neural network can be trained to replace the entire photo\ndevelopment pipeline, and jointly optimized for high-fidelity photo rendering\nand reliable provenance analysis. Such optimized neural imaging pipeline\nallowed us to increase image manipulation detection accuracy from approx. 45%\nto over 90%. The network learns to introduce carefully crafted artifacts, akin\nto digital watermarks, which facilitate subsequent manipulation detection.\nAnalysis of performance trade-offs indicates that most of the gains can be\nobtained with only minor distortion. The findings encourage further research\ntowards building more reliable imaging pipelines with explicit\nprovenance-guaranteeing properties.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:08:09 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Korus", "Pawel", ""], ["Memon", "Nasir", ""]]}, {"id": "1902.10905", "submitter": "Dahuin Jung", "authors": "Dahuin Jung, Ho Bae, Hyun-Soo Choi, Sungroh Yoon", "title": "PixelSteganalysis: Pixel-wise Hidden Information Removal with Low Visual\n  Degradation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to detect and remove secret images that are hidden in natural\nimages using deep-learning algorithms. Our technique is the first work to\neffectively disable covert communications and transactions that use\ndeep-learning steganography. We address the problem by exploiting sophisticated\npixel distributions and edge areas of images using a deep neural network. Based\non the given information, we adaptively remove secret information at the pixel\nlevel. We also introduce a new quantitative metric called destruction rate\nsince the decoding method of deep-learning steganography is approximate\n(lossy), which is different from conventional steganography. We evaluate our\ntechnique using three public benchmarks in comparison with conventional\nsteganalysis methods and show that the decoding rate improves by 10 ~ 20%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:36:29 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Jung", "Dahuin", ""], ["Bae", "Ho", ""], ["Choi", "Hyun-Soo", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1902.11100", "submitter": "Sergey Belim", "authors": "S.V. Belim, D.E. Vilkhovskiy", "title": "Usage of analytic hierarchy process for steganographic inserts detection\n  in images", "comments": null, "journal-ref": "2016 Dynamics of Systems, Mechanisms and Machines (Dynamics),\n  Omsk, Russia, pp. 1-5", "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the method of steganography detection, which is formed\nby replacing the least significant bit (LSB). Detection is performed by\ndividing the image into layers and making an analysis of zero-layer of adjacent\nbits for every bit. First-layer and second-layer are analyzed too. Hierarchies\nanalysis method is used for making decision if current bit is changed.\nWeighting coefficients as part of the analytic hierarchy process are formed on\nthe values of bits. Then a matrix of corrupted pixels is generated.\nVisualization of matrix with corrupted pixels allows to determine size,\nlocation and presence of the embedded message. Computer experiment was\nperformed. Message was embedded in a bounded rectangular area of the image.\nThis method demonstrated efficiency even at low filling container, less than\n10\\%. Widespread statistical methods are unable to detect this steganographic\ninsert. The location and size of the embedded message can be determined with an\nerror which is not exceeding to five pixels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 12:34:21 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Belim", "S. V.", ""], ["Vilkhovskiy", "D. E.", ""]]}, {"id": "1902.11113", "submitter": "Dahuin Jung", "authors": "Dahuin Jung, Ho Bae, Hyun-Soo Choi, and Sungroh Yoon", "title": "PixelSteganalysis: Destroying Hidden Information with a Low Degree of\n  Visual Degradation", "comments": "The updated version of this paper is uploaded in arXiv:1902.10905 as\n  a revised title. Sorry for inconvenience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is the science of unnoticeably concealing a secret message\nwithin a certain image, called a cover image. The cover image with the secret\nmessage is called a stego image. Steganography is commonly used for illegal\npurposes such as terrorist activities and pornography. To thwart covert\ncommunications and transactions, attacking algorithms against steganography,\ncalled steganalysis, exist. Currently, there are many studies implementing deep\nlearning to the steganography algorithm. However, conventional steganalysis is\nno longer effective for deep learning based steganography algorithms. Our\nframework is the first one to disturb covert communications and transactions\nvia the recent deep learning-based steganography algorithms. We first extract a\nsophisticated pixel distribution of the potential stego image from the\nauto-regressive model induced by deep learning. Using the extracted pixel\ndistributions, we detect whether an image is the stego or not at the pixel\nlevel. Each pixel value is adjusted as required and the adjustment induces an\neffective removal of the secret image. Because the decoding method of deep\nlearning-based steganography algorithms is approximate (lossy), which is\ndifferent from the conventional steganography, we propose a new quantitative\nmetric that is more suitable for measuring the accurate effect. We evaluate our\nmethod using three public benchmarks in comparison with a conventional\nsteganalysis method and show up to a 20% improvement in terms of decoding rate.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 10:16:09 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 00:52:14 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Jung", "Dahuin", ""], ["Bae", "Ho", ""], ["Choi", "Hyun-Soo", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1902.11187", "submitter": "Andjela Draganic", "authors": "Andjela Draganic", "title": "Analysis of non-stationary multicomponent signals with a focus on the\n  Compressive Sensing approach", "comments": "PhD Thesis, in Bosnian", "journal-ref": "University of Montenegro, Faculty of Electrical Engineering, 2018", "doi": null, "report-no": null, "categories": "eess.SP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterization of multicomponent signals with a particular emphasis on\nmusical and communication signals is one of the problems studied in the\ndissertation. In order to provide an efficient analysis of the multicomponent\nsignals, the possibility to separate signal components is observed. The\nprocedure for decomposition and classification of the signal components whose\nenergy and physical characteristics differ in the time-frequency domain is\nproposed in this work. A special focus in the dissertation is on the\napplication of the compressive sensing approach in multicomponent signals. The\ncompressive sensing method becomes popular in the field of signal processing\nuntil recently, and its application in various fields can increase the\nacquisition and transmission speed, reduce the complexity of devices, and\nreduce energy consumption. The procedure that applies the compressive sensing\nin the classification of the wireless communication signals is proposed. The\nalgorithms for reconstruction of the compressive sensed signals are intensively\ndeveloping, and therefore special emphasis in the dissertation is devoted to\nthe hardware implementation of one of the algorithms for sparse signal\nreconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 12:55:14 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Draganic", "Andjela", ""]]}]