[{"id": "2007.00558", "submitter": "Daniel Berj\\'on", "authors": "Pablo Carballeira, Carlos Carmona, C\\'esar D\\'iaz, Daniel Berj\\'on,\n  Daniel Corregidor, Juli\\'an Cabrera, Francisco Mor\\'an, Carmen Doblado,\n  Sergio Arnaldo, Mar\\'ia del Mar Mart\\'in, Narciso Garc\\'ia", "title": "FVV Live: A real-time free-viewpoint video system with consumer\n  electronics hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FVV Live is a novel end-to-end free-viewpoint video system, designed for low\ncost and real-time operation, based on off-the-shelf components. The system has\nbeen designed to yield high-quality free-viewpoint video using consumer-grade\ncameras and hardware, which enables low deployment costs and easy installation\nfor immersive event-broadcasting or videoconferencing.\n  The paper describes the architecture of the system, including acquisition and\nencoding of multiview plus depth data in several capture servers and virtual\nview synthesis on an edge server. All the blocks of the system have been\ndesigned to overcome the limitations imposed by hardware and network, which\nimpact directly on the accuracy of depth data and thus on the quality of\nvirtual view synthesis. The design of FVV Live allows for an arbitrary number\nof cameras and capture servers, and the results presented in this paper\ncorrespond to an implementation with nine stereo-based depth cameras.\n  FVV Live presents low motion-to-photon and end-to-end delays, which enables\nseamless free-viewpoint navigation and bilateral immersive communications.\nMoreover, the visual quality of FVV Live has been assessed through subjective\nassessment with satisfactory results, and additional comparative tests show\nthat it is preferred over state-of-the-art DIBR alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:40:28 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Carballeira", "Pablo", ""], ["Carmona", "Carlos", ""], ["D\u00edaz", "C\u00e9sar", ""], ["Berj\u00f3n", "Daniel", ""], ["Corregidor", "Daniel", ""], ["Cabrera", "Juli\u00e1n", ""], ["Mor\u00e1n", "Francisco", ""], ["Doblado", "Carmen", ""], ["Arnaldo", "Sergio", ""], ["Mart\u00edn", "Mar\u00eda del Mar", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "2007.00938", "submitter": "Xinyu Huang", "authors": "Xinyu Huang and Lijun He", "title": "Playback experience driven cross layer optimisation of APP, transport\n  and MAC layer for video clients over long-term evolution system", "comments": "12 pages, 11 figures", "journal-ref": "IET Communications(2020)", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional communication system, information of APP (Application) layer,\ntransport layer and MAC (Media Access Control)layer has not been fully\ninteracted,which inevitably leads to inconsistencies among TCP congestion\nstate, clients'requirements and resource allocation. To solve the problem, we\npropose a joint optimization framework, which consists of APP layer, transport\nlayer and MAC layer, to improve the video clients'playback experience and\nsystem throughput. First, a client requirement aware autonomous packet drop\nstrategy, based on packet importance, channel condition and playback status, is\ndeveloped to decrease the network load and the probability of rebuffering\nevents. Further, TCP (Transmission Control Protocol) state aware downlink and\nuplink resource allocation schemes are proposed to achieve smooth video\ntransmission and steady ACK (Acknowledgement) feedback respectively. For\ndownlink scheme, maximum transmission capacity requirement for each client is\ncalculated based on feedback ACK information from transport layer to avoid\nallocating excessive resource to the client, whose ACK feedback is blocked due\nto bad uplink channel condition. For uplink scheme, information of RTO\n(Retransmission Timeout) and TCP congestion window are utilized to indicate ACK\nscheduling priority. The simulation results show that our algorithm can\nsignficantly improve the system throughput and the clients'playback continuity\nwith acceptable video quality.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 07:28:31 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Huang", "Xinyu", ""], ["He", "Lijun", ""]]}, {"id": "2007.01055", "submitter": "Zhen Long", "authors": "Zhen Long, Ce Zhu, Jiani Liu, Yipeng Liu", "title": "Bayesian Low Rank Tensor Ring Model for Image Completion", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3062195", "report-no": null, "categories": "stat.ML cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank tensor ring model is powerful for image completion which recovers\nmissing entries in data acquisition and transformation. The recently proposed\ntensor ring (TR) based completion algorithms generally solve the low rank\noptimization problem by alternating least squares method with predefined ranks,\nwhich may easily lead to overfitting when the unknown ranks are set too large\nand only a few measurements are available. In this paper, we present a Bayesian\nlow rank tensor ring model for image completion by automatically learning the\nlow rank structure of data. A multiplicative interaction model is developed for\nthe low-rank tensor ring decomposition, where core factors are enforced to be\nsparse by assuming their entries obey Student-T distribution. Compared with\nmost of the existing methods, the proposed one is free of parameter-tuning, and\nthe TR ranks can be obtained by Bayesian inference. Numerical Experiments,\nincluding synthetic data, color images with different sizes and YaleFace\ndataset B with respect to one pose, show that the proposed approach outperforms\nstate-of-the-art ones, especially in terms of recovery accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 02:58:25 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Long", "Zhen", ""], ["Zhu", "Ce", ""], ["Liu", "Jiani", ""], ["Liu", "Yipeng", ""]]}, {"id": "2007.01089", "submitter": "Tamami Nakano", "authors": "Tamami Nakano, Atsuya Sakata, Akihiro Kishimoto", "title": "Estimating Blink Probability for Highlight Detection in Figure Skating\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highlight detection in sports videos has a broad viewership and huge\ncommercial potential. It is thus imperative to detect highlight scenes more\nsuitably for human interest with high temporal accuracy. Since people\ninstinctively suppress blinks during attention-grabbing events and\nsynchronously generate blinks at attention break points in videos, the\ninstantaneous blink rate can be utilized as a highly accurate temporal\nindicator of human interest. Therefore, in this study, we propose a novel,\nautomatic highlight detection method based on the blink rate. The method trains\na one-dimensional convolution network (1D-CNN) to assess blink rates at each\nvideo frame from the spatio-temporal pose features of figure skating videos.\nExperiments show that the method successfully estimates the blink rate in 94%\nof the video clips and predicts the temporal change in the blink rate around a\njump event with high accuracy. Moreover, the method detects not only the\nrepresentative athletic action, but also the distinctive artistic expression of\nfigure skating performance as key frames. This suggests that the\nblink-rate-based supervised learning approach enables high-accuracy highlight\ndetection that more closely matches human sensibility.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:23:03 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Nakano", "Tamami", ""], ["Sakata", "Atsuya", ""], ["Kishimoto", "Akihiro", ""]]}, {"id": "2007.02072", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Pranay Gupta, Anirudh Thatipelli, Aditya Aggarwal, Shubh Maheshwari,\n  Neel Trivedi, Sourav Das, Ravi Kiran Sarvadevabhatla", "title": "Quo Vadis, Skeleton Action Recognition ?", "comments": "To appear in International Journal of Computer Vision (IJCV). Project\n  page: https://skeleton.iiit.ac.in/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study current and upcoming frontiers across the landscape\nof skeleton-based human action recognition. To study skeleton-action\nrecognition in the wild, we introduce Skeletics-152, a curated and 3-D\npose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale\naction dataset. We extend our study to include out-of-context actions by\nintroducing Skeleton-Mimetics, a dataset derived from the recently introduced\nMimetics dataset. We also introduce Metaphorics, a dataset with caption-style\nannotated YouTube videos of the popular social game Dumb Charades and\ninterpretative dance performances. We benchmark state-of-the-art models on the\nNTU-120 dataset and provide multi-layered assessment of the results. The\nresults from benchmarking the top performers of NTU-120 on the newly introduced\ndatasets reveal the challenges and domain gap induced by actions in the wild.\nOverall, our work characterizes the strengths and limitations of existing\napproaches and datasets. Via the introduced datasets, our work enables new\nfrontiers for human action recognition.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 11:02:21 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 16:30:54 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gupta", "Pranay", ""], ["Thatipelli", "Anirudh", ""], ["Aggarwal", "Aditya", ""], ["Maheshwari", "Shubh", ""], ["Trivedi", "Neel", ""], ["Das", "Sourav", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2007.02232", "submitter": "Renato J Cintra", "authors": "R. J. Cintra", "title": "An Integer Approximation Method for Discrete Sinusoidal Transforms", "comments": "13 pages, 5 figures, 8 tables", "journal-ref": "Circuits, Systems, and Signal Processing, vol. 30, n. 6, 2011", "doi": "10.1007/s00034-011-9318-5", "report-no": null, "categories": "eess.SP cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate methods have been considered as a means to the evaluation of\ndiscrete transforms. In this work, we propose and analyze a class of integer\ntransforms for the discrete Fourier, Hartley, and cosine transforms (DFT, DHT,\nand DCT), based on simple dyadic rational approximation methods. The introduced\nmethod is general, applicable to several block-lengths, whereas existing\napproaches are usually dedicated to specific transform sizes. The suggested\napproximate transforms enjoy low multiplicative complexity and the\northogonality property is achievable via matrix polar decomposition. We show\nthat the obtained transforms are competitive with archived methods in\nliterature. New 8-point square wave approximate transforms for the DFT, DHT,\nand DCT are also introduced as particular cases of the introduced methodology.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 03:37:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cintra", "R. J.", ""]]}, {"id": "2007.02268", "submitter": "Lijie Wang", "authors": "Lijie Wang, Xueting Wang and Toshihiko Yamasaki", "title": "Image Aesthetics Prediction Using Multiple Patches Preserving the\n  Original Aspect Ratio of Contents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of social networking services has created an increasing demand for\nselecting, editing, and generating impressive images. This trend increases the\nimportance of evaluating image aesthetics as a complementary function of\nautomatic image processing. We propose a multi-patch method, named MPA-Net\n(Multi-Patch Aggregation Network), to predict image aesthetics scores by\nmaintaining the original aspect ratios of contents in the images. Through an\nexperiment involving the large-scale AVA dataset, which contains 250,000\nimages, we show that the effectiveness of the equal-interval multi-patch\nselection approach for aesthetics score prediction is significant compared to\nthe single-patch prediction and random patch selection approaches. For this\ndataset, MPA-Net outperforms the neural image assessment algorithm, which was\nregarded as a baseline method. In particular, MPA-Net yields a 0.073 (11.5%)\nhigher linear correlation coefficient (LCC) of aesthetics scores and a 0.088\n(14.4%) higher Spearman's rank correlation coefficient (SRCC). MPA-Net also\nreduces the mean square error (MSE) by 0.0115 (4.18%) and achieves results for\nthe LCC and SRCC that are comparable to those of the state-of-the-art\ncontinuous aesthetics score prediction methods. Most notably, MPA-Net yields a\nsignificant lower MSE especially for images with aspect ratios far from 1.0,\nindicating that MPA-Net is useful for a wide range of image aspect ratios.\nMPA-Net uses only images and does not require external information during the\ntraining nor prediction stages. Therefore, MPA-Net has great potential for\napplications aside from aesthetics score prediction such as other human\nsubjectivity prediction.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 08:49:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wang", "Lijie", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2007.02393", "submitter": "Seung-Hun Nam", "authors": "Seung-Hun Nam, Wonhyuk Ahn, In-Jae Yu, Myung-Joon Kwon, Minseok Son,\n  Heung-Kyu Lee", "title": "Deep Convolutional Neural Network for Identifying Seam-Carving Forgery", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2020.3037662", "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seam carving is a representative content-aware image retargeting approach to\nadjust the size of an image while preserving its visually prominent content. To\nmaintain visually important content, seam-carving algorithms first calculate\nthe connected path of pixels, referred to as the seam, according to a defined\ncost function and then adjust the size of an image by removing and duplicating\nrepeatedly calculated seams. Seam carving is actively exploited to overcome\ndiversity in the resolution of images between applications and devices; hence,\ndetecting the distortion caused by seam carving has become important in image\nforensics. In this paper, we propose a convolutional neural network (CNN)-based\napproach to classifying seam-carving-based image retargeting for reduction and\nexpansion. To attain the ability to learn low-level features, we designed a CNN\narchitecture comprising five types of network blocks specialized for capturing\nsubtle signals. An ensemble module is further adopted to both enhance\nperformance and comprehensively analyze the features in the local areas of the\ngiven image. To validate the effectiveness of our work, extensive experiments\nbased on various CNN-based baselines were conducted. Compared to the baselines,\nour work exhibits state-of-the-art performance in terms of three-class\nclassification (original, seam inserted, and seam removed). In addition, our\nmodel with the ensemble module is robust for various unseen cases. The\nexperimental results also demonstrate that our method can be applied to\nlocalize both seam-removed and seam-inserted areas.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:20:51 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 09:33:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nam", "Seung-Hun", ""], ["Ahn", "Wonhyuk", ""], ["Yu", "In-Jae", ""], ["Kwon", "Myung-Joon", ""], ["Son", "Minseok", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "2007.02460", "submitter": "Spyridon Mastorakis", "authors": "Xin Zhong, Pei-Chi Huang, Spyridon Mastorakis, Frank Y. Shih", "title": "An Automated and Robust Image Watermarking Scheme Based on Deep Neural\n  Networks", "comments": "This paper has been accepted for publication by the IEEE Transactions\n  on Multimedia. The copyright is with the IEEE. DOI: 10.1109/TMM.2020.3006415", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image watermarking is the process of embedding and extracting a\nwatermark covertly on a cover-image. To dynamically adapt image watermarking\nalgorithms, deep learning-based image watermarking schemes have attracted\nincreased attention during recent years. However, existing deep learning-based\nwatermarking methods neither fully apply the fitting ability to learn and\nautomate the embedding and extracting algorithms, nor achieve the properties of\nrobustness and blindness simultaneously. In this paper, a robust and blind\nimage watermarking scheme based on deep learning neural networks is proposed.\nTo minimize the requirement of domain knowledge, the fitting ability of deep\nneural networks is exploited to learn and generalize an automated image\nwatermarking algorithm. A deep learning architecture is specially designed for\nimage watermarking tasks, which will be trained in an unsupervised manner to\navoid human intervention and annotation. To facilitate flexible applications,\nthe robustness of the proposed scheme is achieved without requiring any prior\nknowledge or adversarial examples of possible attacks. A challenging case of\nwatermark extraction from phone camera-captured images demonstrates the\nrobustness and practicality of the proposal. The experiments, evaluation, and\napplication cases confirm the superiority of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 22:23:31 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhong", "Xin", ""], ["Huang", "Pei-Chi", ""], ["Mastorakis", "Spyridon", ""], ["Shih", "Frank Y.", ""]]}, {"id": "2007.03410", "submitter": "Mahmoud Darwich", "authors": "Mahmoud Darwich, Yasser Ismail, Talal Darwich, Magdy Bayoumi", "title": "Cost-Efficient Storage for On-Demand Video Streaming on Cloud", "comments": "International IEEE World Forum for Internet of Things", "journal-ref": null, "doi": "10.1109/WF-IoT48130.2020.9221374", "report-no": null, "categories": "cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video stream is converted to several formats to support the user's device,\nthis conversion process is called video transcoding, which imposes high storage\nand powerful resources. With emerging of cloud technology, video stream\ncompanies adopted to process video on the cloud. Generally, many formats of the\nsame video are made (pre-transcoded) and streamed to the adequate user's\ndevice. However, pre-transcoding demands huge storage space and incurs a\nhigh-cost to the video stream companies. More importantly, the pre-transcoding\nof video streams could be hierarchy carried out through different storage types\nin the cloud. To minimize the storage cost, in this paper, we propose a method\nto store video streams in the hierarchical storage of the cloud. Particularly,\nwe develop a method to decide which video stream should be pre-transcoded in\nits suitable cloud storage to minimize the overall cost. Experimental\nsimulation and results show the effectiveness of our approach, specifically,\nwhen the percentage of frequently accessed videos is high in repositories, the\nproposed approach minimizes the overall cost by up to 40 percent.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 13:26:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Darwich", "Mahmoud", ""], ["Ismail", "Yasser", ""], ["Darwich", "Talal", ""], ["Bayoumi", "Magdy", ""]]}, {"id": "2007.03617", "submitter": "Petros Spachos", "authors": "Katherine McLeod, Petros Spachos, Konstantinos Plataniotis", "title": "Smartphone-based Wellness Assessment Using Mobile Environmental Sensor", "comments": null, "journal-ref": null, "doi": "10.1109/JSYST.2020.3004558", "report-no": null, "categories": "cs.CY cs.HC cs.MM cs.SI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health and general wellness are becoming a growing concern in our\nsociety. Environmental factors contribute to mental illness and have the power\nto affect a person's wellness. This work presents a smartphone-based wellness\nassessment system and examines if there is any correlation with one's\nenvironment and their wellness. The introduced system was initiated in response\nto a growing need for individualized and independent mental health care and\nevaluated through experimentation. The participants were given an Android\nsmartphone and a mobile sensor board and they were asked to complete a brief\npsychological survey three times per day. During the survey completion, the\nboard in their possession is reading environmental data. The five environmental\nvariables collected are temperature, humidity, air pressure, luminosity, and\nnoise level. Upon submission of the survey, the results of the survey and the\nenvironmental data are sent to a server for further processing. Three\nexperiments with 62 participants in total have been completed. The correlation\nmost regularly deemed statistically significant was that of light and audio and\nstress.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:23:32 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["McLeod", "Katherine", ""], ["Spachos", "Petros", ""], ["Plataniotis", "Konstantinos", ""]]}, {"id": "2007.03815", "submitter": "Ping Hu", "authors": "Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin,\n  Kate Saenko, Stan Sclaroff", "title": "Real-time Semantic Segmentation with Fast Attention", "comments": "project page: https://cs-people.bu.edu/pinghu/FANet.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep CNN based models for semantic segmentation, high accuracy relies on\nrich spatial context (large receptive fields) and fine spatial details (high\nresolution), both of which incur high computational costs. In this paper, we\npropose a novel architecture that addresses both challenges and achieves\nstate-of-the-art performance for semantic segmentation of high-resolution\nimages and videos in real-time. The proposed architecture relies on our fast\nspatial attention, which is a simple yet efficient modification of the popular\nself-attention mechanism and captures the same rich spatial context at a small\nfraction of the computational cost, by changing the order of operations.\nMoreover, to efficiently process high-resolution input, we apply an additional\nspatial reduction to intermediate feature stages of the network with minimal\nloss in accuracy thanks to the use of the fast attention module to fuse\nfeatures. We validate our method with a series of experiments, and show that\nresults on multiple datasets demonstrate superior performance with better\naccuracy and speed compared to existing approaches for real-time semantic\nsegmentation. On Cityscapes, our network achieves 74.4$\\%$ mIoU at 72 FPS and\n75.5$\\%$ mIoU at 58 FPS on a single Titan X GPU, which is~$\\sim$50$\\%$ faster\nthan the state-of-the-art while retaining the same accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:37:16 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 22:44:34 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Hu", "Ping", ""], ["Perazzi", "Federico", ""], ["Heilbron", "Fabian Caba", ""], ["Wang", "Oliver", ""], ["Lin", "Zhe", ""], ["Saenko", "Kate", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2007.03922", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Xiaomeng She, Jin Tang and Bin Luo", "title": "Reversible data hiding in encrypted images based on pixel prediction and\n  multi-MSB planes rearrangement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great concern has arisen in the field of reversible data hiding in encrypted\nimages (RDHEI) due to the development of cloud storage and privacy protection.\nRDHEI is an effective technology that can embed additional data after image\nencryption, extract additional data error-free and reconstruct original images\nlosslessly. In this paper, a high-capacity and fully reversible RDHEI method is\nproposed, which is based on pixel prediction and multi-MSB (most significant\nbit) planes rearrangement. First, the median edge detector (MED) predictor is\nused to calculate the predicted value. Next, unlike previous methods, in our\nproposed method, signs of prediction errors (PEs) are represented by one bit\nplane and absolute values of PEs are represented by other bit planes. Then, we\ndivide bit planes into uniform blocks and non-uniform blocks, and rearrange\nthese blocks. Finally, according to different pixel prediction schemes,\ndifferent numbers of additional data are embedded adaptively. The experimental\nresults prove that our method has higher embedding capacity compared with\nstate-of-the-art RDHEI methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 06:59:00 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 09:45:24 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 07:26:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yin", "Zhaoxia", ""], ["She", "Xiaomeng", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "2007.04057", "submitter": "Zhaoxia Yin", "authors": "Youqing Wu, Wenjing Ma, Yinyin Peng, Ruiling Zhang and Zhaoxia Yin", "title": "Reversible Data Hiding in Encrypted Images Based on Bit plane\n  Compression of Prediction Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a technology that can prevent the information of original image and\nadditional information from being disclosed, the reversible data hiding in\nencrypted images (RDHEI) has been widely concerned by researchers. How to\nfurther improve the performance of RDHEI methods has become a focus of\nresearch. To this end, this work proposes a high-capacity RDHEI method based on\nbit plane compression of prediction error. Firstly, to reserve the room for\nembedding information, the image owner rearranges and compresses the bit plane\nof prediction error. Next, the image after reserving room is encrypted with a\nserect key. Finally, the information hiding device embeds the additional\ninformation into the reserved room. This paper makes full use of the\ncorrelation between adjacent pixels. Experimental results show that this method\ncan realize the real reversibility and provide higher embedding capacity than\nstate-of-the-art works.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:14:38 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 07:55:26 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wu", "Youqing", ""], ["Ma", "Wenjing", ""], ["Peng", "Yinyin", ""], ["Zhang", "Ruiling", ""], ["Yin", "Zhaoxia", ""]]}, {"id": "2007.04134", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Stavros Petridis, Maja Pantic", "title": "Learning Speech Representations from Raw Audio by Joint Audiovisual\n  Self-Supervision", "comments": "Accepted at the Workshop on Self-supervision in Audio and Speech at\n  ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intuitive interaction between the audio and visual modalities is valuable\nfor cross-modal self-supervised learning. This concept has been demonstrated\nfor generic audiovisual tasks like video action recognition and acoustic scene\nclassification. However, self-supervision remains under-explored for\naudiovisual speech. We propose a method to learn self-supervised speech\nrepresentations from the raw audio waveform. We train a raw audio encoder by\ncombining audio-only self-supervision (by predicting informative audio\nattributes) with visual self-supervision (by generating talking faces from\naudio). The visual pretext task drives the audio representations to capture\ninformation related to lip movements. This enriches the audio encoder with\nvisual information and the encoder can be used for evaluation without the\nvisual modality. Our method attains competitive performance with respect to\nexisting self-supervised audio features on established isolated word\nclassification benchmarks, and significantly outperforms other methods at\nlearning from fewer labels. Notably, our method also outperforms fully\nsupervised training, thus providing a strong initialization for speech related\ntasks. Our results demonstrate the potential of multimodal self-supervision in\naudiovisual speech for learning good audio representations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:07:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Shukla", "Abhinav", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2007.04660", "submitter": "Konstantinos Drossos", "authors": "Emre \\c{C}ak{\\i}r and Konstantinos Drossos and Tuomas Virtanen", "title": "Multi-task Regularization Based on Infrequent Classes for Audio\n  Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio captioning is a multi-modal task, focusing on using natural language\nfor describing the contents of general audio. Most audio captioning methods are\nbased on deep neural networks, employing an encoder-decoder scheme and a\ndataset with audio clips and corresponding natural language descriptions (i.e.\ncaptions). A significant challenge for audio captioning is the distribution of\nwords in the captions: some words are very frequent but acoustically\nnon-informative, i.e. the function words (e.g. \"a\", \"the\"), and other words are\ninfrequent but informative, i.e. the content words (e.g. adjectives, nouns). In\nthis paper we propose two methods to mitigate this class imbalance problem.\nFirst, in an autoencoder setting for audio captioning, we weigh each word's\ncontribution to the training loss inversely proportional to its number of\noccurrences in the whole dataset. Secondly, in addition to multi-class,\nword-level audio captioning task, we define a multi-label side task based on\nclip-level content word detection by training a separate decoder. We use the\nloss from the second task to regularize the jointly trained encoder for the\naudio captioning task. We evaluate our method using Clotho, a recently\npublished, wide-scale audio captioning dataset, and our results show an\nincrease of 37\\% relative improvement with SPIDEr metric over the baseline\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:38:54 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["\u00c7ak\u0131r", "Emre", ""], ["Drossos", "Konstantinos", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "2007.05025", "submitter": "Rohit Agrawal", "authors": "Rohit Agrawal", "title": "$\\ell_1$SABMIS: $\\ell_1$-minimization and sparse approximation based\n  blind multi-image steganography scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography plays a vital role in achieving secret data security by\nembedding it into cover media. The cover media and the secret data can be text\nor multimedia, such as images, videos, etc. In this paper, we propose a novel\n$\\ell_1$-minimization and sparse approximation based blind multi-image\nsteganography scheme, termed $\\ell_1$SABMIS. By using $\\ell_1$SABMIS, multiple\nsecret images can be hidden in a single cover image. In $\\ell_1$SABMIS, we\nsampled cover image into four sub-images, sparsify each sub-image block-wise,\nand then obtain linear measurements. Next, we obtain DCT (Discrete Cosine\nTransform) coefficients of the secret images and then embed them into the cover\nimage\\textquotesingle s linear measurements.\n  We perform experiments on several standard gray-scale images, and evaluate\nembedding capacity, PSNR (peak signal-to-noise ratio) value, mean SSIM\n(structural similarity) index, NCC (normalized cross-correlation) coefficient,\nNAE (normalized absolute error), and entropy. The value of these assessment\nmetrics indicates that $\\ell_1$SABMIS outperforms similar existing\nsteganography schemes. That is, we successfully hide more than two secret\nimages in a single cover image without degrading the cover image significantly.\nAlso, the extracted secret images preserve good visual quality, and\n$\\ell_1$SABMIS is resistant to steganographic attack.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 18:47:38 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Agrawal", "Rohit", ""]]}, {"id": "2007.05764", "submitter": "Puneet Kumar", "authors": "Ankit Sharma, Puneet Kumar, Vikas Maddukuri, Nagasai Madamshettib,\n  Kishore KG, Sahit Sai Sriram Kavurub, Balasubramanian Raman and Partha Pratim\n  Roy", "title": "Fast Griffin Lim based Waveform Generation Strategy for Text-to-Speech\n  Synthesis", "comments": "Accepted for publication in Springer Multimedia Tools and\n  Applications Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of text-to-speech (TTS) systems heavily depends on\nspectrogram to waveform generation, also known as the speech reconstruction\nphase. The time required for the same is known as synthesis delay. In this\npaper, an approach to reduce speech synthesis delay has been proposed. It aims\nto enhance the TTS systems for real-time applications such as digital\nassistants, mobile phones, embedded devices, etc. The proposed approach applies\nFast Griffin Lim Algorithm (FGLA) instead Griffin Lim algorithm (GLA) as\nvocoder in the speech synthesis phase. GLA and FGLA are both iterative, but the\nconvergence rate of FGLA is faster than GLA. The proposed approach is tested on\nLJSpeech, Blizzard and Tatoeba datasets and the results for FGLA are compared\nagainst GLA and neural Generative Adversarial Network (GAN) based vocoder. The\nperformance is evaluated based on synthesis delay and speech quality. A 36.58%\nreduction in speech synthesis delay has been observed. The quality of the\noutput speech has improved, which is advocated by higher Mean opinion scores\n(MOS) and faster convergence with FGLA as opposed to GLA.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 13:10:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sharma", "Ankit", ""], ["Kumar", "Puneet", ""], ["Maddukuri", "Vikas", ""], ["Madamshettib", "Nagasai", ""], ["KG", "Kishore", ""], ["Kavurub", "Sahit Sai Sriram", ""], ["Raman", "Balasubramanian", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "2007.06292", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dhaval Salwala, Edward Curry", "title": "Knowledge Graph Driven Approach to Represent Video Streams for\n  Spatiotemporal Event Pattern Matching in Complex Event Processing", "comments": "31 pages, 14 Figures, Publication accepted in International Journal\n  of Graph Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Processing (CEP) is an event processing paradigm to perform\nreal-time analytics over streaming data and match high-level event patterns.\nPresently, CEP is limited to process structured data stream. Video streams are\ncomplicated due to their unstructured data model and limit CEP systems to\nperform matching over them. This work introduces a graph-based structure for\ncontinuous evolving video streams, which enables the CEP system to query\ncomplex video event patterns. We propose the Video Event Knowledge Graph\n(VEKG), a graph driven representation of video data. VEKG models video objects\nas nodes and their relationship interaction as edges over time and space. It\ncreates a semantic knowledge representation of video data derived from the\ndetection of high-level semantic concepts from the video using an ensemble of\ndeep learning models. A CEP-based state optimization - VEKG-Time Aggregated\nGraph (VEKG-TAG) is proposed over VEKG representation for faster event\ndetection. VEKG-TAG is a spatiotemporal graph aggregation method that provides\na summarized view of the VEKG graph over a given time length. We defined a set\nof nine event pattern rules for two domains (Activity Recognition and Traffic\nManagement), which act as a query and applied over VEKG graphs to discover\ncomplex event patterns. To show the efficacy of our approach, we performed\nextensive experiments over 801 video clips across 10 datasets. The proposed\nVEKG approach was compared with other state-of-the-art methods and was able to\ndetect complex event patterns over videos with F-Score ranging from 0.44 to\n0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%\nand 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,\nachieving sub-second median latency of 4-20 milliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:20:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yadav", "Piyush", ""], ["Salwala", "Dhaval", ""], ["Curry", "Edward", ""]]}, {"id": "2007.07032", "submitter": "Christian Timmerer", "authors": "Andrew Perkis, Christian Timmerer, Sabina Barakovi\\'c, Jasmina\n  Barakovi\\'c Husi\\'c, S{\\o}ren Bech, Sebastian Bosse, Jean Botev, Kjell\n  Brunnstr\\\"om, Luis Cruz, Katrien De Moor, Andrea de Polo Saibanti, Wouter\n  Durnez, Sebastian Egger-Lampl, Ulrich Engelke, Tiago H. Falk, Jes\\'us\n  Guti\\'errez, Asim Hameed, Andrew Hines, Tanja Kojic, Dragan Kukolj, Eirini\n  Liotou, Dragorad Milovanovic, Sebastian M\\\"oller, Niall Murray, Babak Naderi,\n  Manuela Pereira, Stuart Perry, Antonio Pinheiro, Andres Pinilla, Alexander\n  Raake, Sarvesh Rajesh Agrawal, Ulrich Reiter, Rafael Rodrigues, Raimund\n  Schatz, Peter Schelkens, Steven Schmidt, Saeed Shafiee Sabet, Ashutosh\n  Singla, Lea Skorin-Kapov, Mirko Suznjevic, Stefan Uhrig, Sara Vlahovi\\'c,\n  Jan-Niklas Voigt-Antons, Saman Zadtootaghaj", "title": "QUALINET White Paper on Definitions of Immersive Media Experience (IMEx)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the coming of age of virtual/augmented reality and interactive media,\nnumerous definitions, frameworks, and models of immersion have emerged across\ndifferent fields ranging from computer graphics to literary works. Immersion is\noftentimes used interchangeably with presence as both concepts are closely\nrelated. However, there are noticeable interdisciplinary differences regarding\ndefinitions, scope, and constituents that are required to be addressed so that\na coherent understanding of the concepts can be achieved. Such consensus is\nvital for paving the directionality of the future of immersive media\nexperiences (IMEx) and all related matters. The aim of this white paper is to\nprovide a survey of definitions of immersion and presence which leads to a\ndefinition of immersive media experience (IMEx). The Quality of Experience\n(QoE) for immersive media is described by establishing a relationship between\nthe concepts of QoE and IMEx followed by application areas of immersive media\nexperience. Influencing factors on immersive media experience are elaborated as\nwell as the assessment of immersive media experience. Finally, standardization\nactivities related to IMEx are highlighted and the white paper is concluded\nwith an outlook related to future developments.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:59:42 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 23:04:36 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Perkis", "Andrew", ""], ["Timmerer", "Christian", ""], ["Barakovi\u0107", "Sabina", ""], ["Husi\u0107", "Jasmina Barakovi\u0107", ""], ["Bech", "S\u00f8ren", ""], ["Bosse", "Sebastian", ""], ["Botev", "Jean", ""], ["Brunnstr\u00f6m", "Kjell", ""], ["Cruz", "Luis", ""], ["De Moor", "Katrien", ""], ["Saibanti", "Andrea de Polo", ""], ["Durnez", "Wouter", ""], ["Egger-Lampl", "Sebastian", ""], ["Engelke", "Ulrich", ""], ["Falk", "Tiago H.", ""], ["Guti\u00e9rrez", "Jes\u00fas", ""], ["Hameed", "Asim", ""], ["Hines", "Andrew", ""], ["Kojic", "Tanja", ""], ["Kukolj", "Dragan", ""], ["Liotou", "Eirini", ""], ["Milovanovic", "Dragorad", ""], ["M\u00f6ller", "Sebastian", ""], ["Murray", "Niall", ""], ["Naderi", "Babak", ""], ["Pereira", "Manuela", ""], ["Perry", "Stuart", ""], ["Pinheiro", "Antonio", ""], ["Pinilla", "Andres", ""], ["Raake", "Alexander", ""], ["Agrawal", "Sarvesh Rajesh", ""], ["Reiter", "Ulrich", ""], ["Rodrigues", "Rafael", ""], ["Schatz", "Raimund", ""], ["Schelkens", "Peter", ""], ["Schmidt", "Steven", ""], ["Sabet", "Saeed Shafiee", ""], ["Singla", "Ashutosh", ""], ["Skorin-Kapov", "Lea", ""], ["Suznjevic", "Mirko", ""], ["Uhrig", "Stefan", ""], ["Vlahovi\u0107", "Sara", ""], ["Voigt-Antons", "Jan-Niklas", ""], ["Zadtootaghaj", "Saman", ""]]}, {"id": "2007.07099", "submitter": "Di Ma", "authors": "Di Ma, Fan Zhang, and David R. Bull", "title": "MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2020.3043064", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel convolutional neural network (CNN)\narchitecture, MFRNet, for post-processing (PP) and in-loop filtering (ILF) in\nthe context of video compression. This network consists of four Multi-level\nFeature review Residual dense Blocks (MFRBs), which are connected using a\ncascading structure. Each MFRB extracts features from multiple convolutional\nlayers using dense connections and a multi-level residual learning structure.\nIn order to further improve information flow between these blocks, each of them\nalso reuses high dimensional features from the previous MFRB. This network has\nbeen integrated into PP and ILF coding modules for both HEVC (HM 16.20) and VVC\n(VTM 7.0), and fully evaluated under the JVET Common Test Conditions using the\nRandom Access configuration. The experimental results show significant and\nconsistent coding gains over both anchor codecs (HEVC HM and VVC VTM) and also\nover other existing CNN-based PP/ILF approaches based on Bjontegaard Delta\nmeasurements using both PSNR and VMAF for quality assessment. When MFRNet is\nintegrated into HM 16.20, gains up to 16.0% (BD-rate VMAF) are demonstrated for\nILF, and up to 21.0% (BD-rate VMAF) for PP. The respective gains for VTM 7.0\nare up to 5.1% for ILF and up to 7.1% for PP.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:19:32 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 21:59:54 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ma", "Di", ""], ["Zhang", "Fan", ""], ["Bull", "David R.", ""]]}, {"id": "2007.07244", "submitter": "Xianchao Wu", "authors": "Xianchao Wu and Chengyuan Wang and Qinying Lei", "title": "Transformer-XL Based Music Generation with Multiple Sequences of\n  Time-valued Notes", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art AI based classical music creation algorithms such as\nMusic Transformer are trained by employing single sequence of notes with\ntime-shifts. The major drawback of absolute time interval expression is the\ndifficulty of similarity computing of notes that share the same note value yet\ndifferent tempos, in one or among MIDI files. In addition, the usage of single\nsequence restricts the model to separately and effectively learn music\ninformation such as harmony and rhythm. In this paper, we propose a framework\nwith two novel methods to respectively track these two shortages, one is the\nconstruction of time-valued note sequences that liberate note values from\ntempos and the other is the separated usage of four sequences, namely, former\nnote on to current note on, note on to note off, pitch, and velocity, for\njointly training of four Transformer-XL networks. Through training on a 23-hour\npiano MIDI dataset, our framework generates significantly better and hour-level\nlonger music than three state-of-the-art baselines, namely Music Transformer,\nDeepJ, and single sequence-based Transformer-XL, evaluated automatically and\nmanually.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 20:16:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wu", "Xianchao", ""], ["Wang", "Chengyuan", ""], ["Lei", "Qinying", ""]]}, {"id": "2007.07817", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Edward Curry", "title": "VidCEP: Complex Event Processing Framework to Detect Spatiotemporal\n  Patterns in Video Streams", "comments": "10 pages, 19 figures, Paper published in IEEE BigData 2019", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9006018", "report-no": null, "categories": "cs.CV cs.DB cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video data is highly expressive and has traditionally been very difficult for\na machine to interpret. Querying event patterns from video streams is\nchallenging due to its unstructured representation. Middleware systems such as\nComplex Event Processing (CEP) mine patterns from data streams and send\nnotifications to users in a timely fashion. Current CEP systems have inherent\nlimitations to query video streams due to their unstructured data model and\nlack of expressive query language. In this work, we focus on a CEP framework\nwhere users can define high-level expressive queries over videos to detect a\nrange of spatiotemporal event patterns. In this context, we propose: i) VidCEP,\nan in-memory, on the fly, near real-time complex event matching framework for\nvideo streams. The system uses a graph-based event representation for video\nstreams which enables the detection of high-level semantic concepts from video\nusing cascades of Deep Neural Network models, ii) a Video Event Query language\n(VEQL) to express high-level user queries for video streams in CEP, iii) a\ncomplex event matcher to detect spatiotemporal video event patterns by matching\nexpressive user queries over video data. The proposed approach detects\nspatiotemporal video event patterns with an F-score ranging from 0.66 to 0.89.\nVidCEP maintains near real-time performance with an average throughput of 70\nframes per second for 5 parallel videos with sub-second matching latency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:43:37 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yadav", "Piyush", ""], ["Curry", "Edward", ""]]}, {"id": "2007.08301", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin and Longfei Ke", "title": "Robust adaptive steganography based on dither modulation and\n  modification with re-compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional adaptive steganography is a technique used for covert\ncommunication with high security, but it is invalid in the case of stego images\nare sent to legal receivers over networks which is lossy, such as JPEG\ncompression of channels. To deal with such problem, robust adaptive\nsteganography is proposed to enable the receiver to extract secret messages\nfrom the damaged stego images. Previous works utilize reverse engineering and\ncompression-resistant domain constructing to implement robust adaptive\nsteganography. In this paper, we adopt modification with re-compression scheme\nto improve the robustness of stego sequences in stego images. To balance\nsecurity and robustness, we move the embedding domain to the low frequency\nregion of DCT (Discrete Cosine Transform) coefficients to improve the security\nof robust adaptive steganography. In addition, we add additional check codes to\nfurther reduce the average extraction error rate based on the framework of\nE-DMAS (Enhancing Dither Modulation based robust Adaptive Steganography).\nCompared with GMAS (Generalized dither Modulation based robust Adaptive\nSteganography) and E-DMAS, experiment results show that our scheme can achieve\nstrong robustness and improve the security of robust adaptive steganography\ngreatly when the channel quality factor is known.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:41:15 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 13:00:21 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Ke", "Longfei", ""]]}, {"id": "2007.08948", "submitter": "Emin Zerman", "authors": "Mair\\'ead Grogan, Emin Zerman, Gareth W. Young, Aljosa Smolic", "title": "A Case Study on Video Color Transfer: Exploring User Motivations,\n  Expectations, and Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia and creativity software products are being used to edit and\ncontrol various elements of creative media practices. These days, the technical\naffordances of mobile multimedia devices and the advent of high-speed 5G\ninternet access mean that these abilities are simpler and more readily\navailable to be harnessed by mobile applications. In this paper, using a\nprototype application, we discuss how potential users of such technology are\nmotivated to use a video recoloring application and explore the role that user\nexpectation and satisfaction play in this process. By exploring this topic and\nfocusing on the human-computer interaction, we found that color transfer\ninteractions are driven by several intrinsic motivations and that user\nexpectations and satisfaction ratings can be maintained via clear\nvisualizations of the processes to be undertaken. Furthermore, we reveal the\nspecific language that users use to communicate video recoloring when regarding\nuser motivations, expectations, and satisfaction. This research provides\nimportant information for developers of state-of-art recoloring processes and\ncontributes to dialogues surrounding the users of mobile multimedia technology\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:50:24 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Grogan", "Mair\u00e9ad", ""], ["Zerman", "Emin", ""], ["Young", "Gareth W.", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2007.09758", "submitter": "Alireza Parchami", "authors": "Alireza Parchami, Mojtaba Mahdavi", "title": "Full Quaternion Representation of Color images: A Case Study on\n  QSVD-based Color Image Compression", "comments": "15 pages, 16 figures, 1 table, submitted to Signal Processing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, channels of a color image have been processed individually,\nor the image has been converted to grayscale one with respect to color image\nprocessing. Pure quaternion representation of color images solves this issue as\nit allows images to be processed in a holistic space. Nevertheless, it brings\nadditional costs due to the extra fourth dimension. In this paper, we propose\nan approach for representing color images with full quaternion numbers that\nenables us to process color images holistically without additional cost in\ntime, space and computation. With taking auto- and cross-correlation of color\nchannels into account, an autoencoder neural network is used to generate a\nglobal model for transforming a color image into a full quaternion matrix. To\nevaluate the model, we use UCID dataset, and the results indicate that the\nmodel has an acceptable performance on color images. Moreover, we propose a\ncompression method based on the generated model and QSVD as a case study. The\nmethod is compared with the same compression method using pure quaternion\nrepresentation and is assessed with UCID dataset. The results demonstrate that\nthe compression method using the proposed full quaternion representation fares\nbetter than the other in terms of time, quality, and size of compressed files.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 19:13:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Parchami", "Alireza", ""], ["Mahdavi", "Mojtaba", ""]]}, {"id": "2007.09902", "submitter": "Hang Zhou", "authors": "Hang Zhou, Xudong Xu, Dahua Lin, Xiaogang Wang, Ziwei Liu", "title": "Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating\n  Source Separation", "comments": "To appear in Proceedings of the European Conference on Computer\n  Vision (ECCV), 2020. Code, models, and video results are available on our\n  webpage: https://hangz-nju-cuhk.github.io/projects/Sep-Stereo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stereophonic audio is an indispensable ingredient to enhance human auditory\nexperience. Recent research has explored the usage of visual information as\nguidance to generate binaural or ambisonic audio from mono ones with stereo\nsupervision. However, this fully supervised paradigm suffers from an inherent\ndrawback: the recording of stereophonic audio usually requires delicate devices\nthat are expensive for wide accessibility. To overcome this challenge, we\npropose to leverage the vastly available mono data to facilitate the generation\nof stereophonic audio. Our key observation is that the task of visually\nindicated audio separation also maps independent audios to their corresponding\nvisual positions, which shares a similar objective with stereophonic audio\ngeneration. We integrate both stereo generation and source separation into a\nunified framework, Sep-Stereo, by considering source separation as a particular\ntype of audio spatialization. Specifically, a novel associative pyramid network\narchitecture is carefully designed for audio-visual feature fusion. Extensive\nexperiments demonstrate that our framework can improve the stereophonic audio\ngeneration results while performing accurate sound separation with a shared\nbackbone.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 06:20:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhou", "Hang", ""], ["Xu", "Xudong", ""], ["Lin", "Dahua", ""], ["Wang", "Xiaogang", ""], ["Liu", "Ziwei", ""]]}, {"id": "2007.10058", "submitter": "Surabhi Nath", "authors": "Surabhi S. Nath, Vishaal Udandarao, Jainendra Shukla", "title": "It's LeVAsa not LevioSA! Latent Encodings for Valence-Arousal Structure\n  Alignment", "comments": "5 pages, 4 figures and 3 tables", "journal-ref": null, "doi": "10.1145/3430984.3431037", "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, great strides have been made in the field of affective\ncomputing. Several models have been developed to represent and quantify\nemotions. Two popular ones include (i) categorical models which represent\nemotions as discrete labels, and (ii) dimensional models which represent\nemotions in a Valence-Arousal (VA) circumplex domain. However, there is no\nstandard for annotation mapping between the two labelling methods. We build a\nnovel algorithm for mapping categorical and dimensional model labels using\nannotation transfer across affective facial image datasets. Further, we utilize\nthe transferred annotations to learn rich and interpretable data\nrepresentations using a variational autoencoder (VAE). We present \"LeVAsa\", a\nVAE model that learns implicit structure by aligning the latent space with the\nVA space. We evaluate the efficacy of LeVAsa by comparing performance with the\nVanilla VAE using quantitative and qualitative analysis on two benchmark\naffective image datasets. Our results reveal that LeVAsa achieves high\nlatent-circumplex alignment which leads to improved downstream categorical\nemotion prediction. The work also demonstrates the trade-off between degree of\nalignment and quality of reconstructions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:52:26 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 22:40:30 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 18:24:01 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Nath", "Surabhi S.", ""], ["Udandarao", "Vishaal", ""], ["Shukla", "Jainendra", ""]]}, {"id": "2007.10060", "submitter": "Dong Wang", "authors": "Dong Wang, Yunpeng Bai, Ying Li", "title": "Multispectral Pan-sharpening via Dual-Channel Convolutional Network with\n  Convolutional LSTM Based Hierarchical Spatial-Spectral Feature Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral pan-sharpening aims at producing a high resolution (HR)\nmultispectral (MS) image in both spatial and spectral domains by fusing a\npanchromatic (PAN) image and a corresponding MS image. In this paper, we\npropose a novel dual-channel network (DCNet) framework for MS pan-sharpening.\nIn our DCNet, the dual-channel backbone involves a spatial channel to capture\nspatial information with a 2D CNN, and a spectral channel to extract spectral\ninformation with a 3D CNN. This heterogeneous 2D/3D CNN architecture can\nminimize causing spectral information distortion, which typically happens in\nconventional 2D CNN models. In order to fully integrate the spatial and\nspectral features captured from different levels, we introduce a multi-level\nfusion strategy. Specifically, a spatial-spectral CLSTM (S$^2$-CLSTM) module is\nproposed for fusing the hierarchical spatial and spectral features, which can\neffectively capture correlations among multi-level features. The S$^2$-CLSTM\nmodule attaches two fusion ways: the intra-level fusion via bi-directional\nlateral connections and inter-level fusion via the cell state in the\nS$^2$-CLSTM. Finally, the ideal HR-MS image is recovered by a reconstruction\nmodule. Extensive experiments have been conducted at both simulated lower scale\nand the original scale of real-world datasets. Compared with the\nstate-of-the-art methods, the proposed DCNet achieves superior or competitive\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:52:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Dong", ""], ["Bai", "Yunpeng", ""], ["Li", "Ying", ""]]}, {"id": "2007.10240", "submitter": "Zehua Ma", "authors": "Zehua Ma, Weiming Zhang, Han Fang, Xiaoyi Dong, Linfeng Geng, and\n  Nenghai Yu", "title": "Local Geometric Distortions Resilient Watermarking Scheme Based on\n  Symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an efficient watermark attack method, geometric distortions destroy the\nsynchronization between watermark encoder and decoder. And the local geometric\ndistortion is a famous challenge in the watermark field. Although a lot of\ngeometric distortions resilient watermarking schemes have been proposed, few of\nthem perform well against local geometric distortion like random bending attack\n(RBA). To address this problem, this paper proposes a novel watermark\nsynchronization process and the corresponding watermarking scheme. In our\nscheme, the watermark bits are represented by random patterns. The message is\nencoded to get a watermark unit, and the watermark unit is flipped to generate\na symmetrical watermark. Then the symmetrical watermark is embedded into the\nspatial domain of the host image in an additive way. In watermark extraction,\nwe first get the theoretically mean-square error minimized estimation of the\nwatermark. Then the auto-convolution function is applied to this estimation to\ndetect the symmetry and get a watermark units map. According to this map, the\nwatermark can be accurately synchronized, and then the extraction can be done.\nExperimental results demonstrate the excellent robustness of the proposed\nwatermarking scheme to local geometric distortions, global geometric\ndistortions, common image processing operations, and some kinds of combined\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:27:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ma", "Zehua", ""], ["Zhang", "Weiming", ""], ["Fang", "Han", ""], ["Dong", "Xiaoyi", ""], ["Geng", "Linfeng", ""], ["Yu", "Nenghai", ""]]}, {"id": "2007.10558", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Dingzeyu Li, and Chenliang Xu", "title": "Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video\n  Parsing", "comments": "ECCV 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new problem, named audio-visual video parsing,\nwhich aims to parse a video into temporal event segments and label them as\neither audible, visible, or both. Such a problem is essential for a complete\nunderstanding of the scene depicted inside a video. To facilitate exploration,\nwe collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual\nvideo parsing in a weakly-supervised manner. This task can be naturally\nformulated as a Multimodal Multiple Instance Learning (MMIL) problem.\nConcretely, we propose a novel hybrid attention network to explore unimodal and\ncross-modal temporal contexts simultaneously. We develop an attentive MMIL\npooling method to adaptively explore useful audio and visual content from\ndifferent temporal extent and modalities. Furthermore, we discover and mitigate\nmodality bias and noisy label issues with an individual-guided learning\nmechanism and label smoothing technique, respectively. Experimental results\nshow that the challenging audio-visual video parsing can be achieved even with\nonly video-level weak labels. Our proposed framework can effectively leverage\nunimodal and cross-modal temporal contexts and alleviate modality bias and\nnoisy labels problems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 01:53:31 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Tian", "Yapeng", ""], ["Li", "Dingzeyu", ""], ["Xu", "Chenliang", ""]]}, {"id": "2007.10662", "submitter": "Tianshui Chen", "authors": "Jie Wu, Tianshui Chen, Hefeng Wu, Zhi Yang, Guangchun Luo, Liang Lin", "title": "Fine-Grained Image Captioning with Global-Local Discriminative Objective", "comments": "Accepted by TMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in recent years in image captioning, an\nactive topic in the fields of vision and language. However, existing methods\ntend to yield overly general captions and consist of some of the most frequent\nwords/phrases, resulting in inaccurate and indistinguishable descriptions (see\nFigure 1). This is primarily due to (i) the conservative characteristic of\ntraditional training objectives that drives the model to generate correct but\nhardly discriminative captions for similar images and (ii) the uneven word\ndistribution of the ground-truth captions, which encourages generating highly\nfrequent words/phrases while suppressing the less frequent but more concrete\nones. In this work, we propose a novel global-local discriminative objective\nthat is formulated on top of a reference model to facilitate generating\nfine-grained descriptive captions. Specifically, from a global perspective, we\ndesign a novel global discriminative constraint that pulls the generated\nsentence to better discern the corresponding image from all others in the\nentire dataset. From the local perspective, a local discriminative constraint\nis proposed to increase attention such that it emphasizes the less frequent but\nmore concrete words/phrases, thus facilitating the generation of captions that\nbetter describe the visual details of the given images. We evaluate the\nproposed method on the widely used MS-COCO dataset, where it outperforms the\nbaseline methods by a sizable margin and achieves competitive performance over\nexisting leading approaches. We also conduct self-retrieval experiments to\ndemonstrate the discriminability of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 08:46:02 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wu", "Jie", ""], ["Chen", "Tianshui", ""], ["Wu", "Hefeng", ""], ["Yang", "Zhi", ""], ["Luo", "Guangchun", ""], ["Lin", "Liang", ""]]}, {"id": "2007.10695", "submitter": "Yudhik Agrawal", "authors": "Yudhik Agrawal, Samyak Jain, Emily Carlson, Petri Toiviainen, Vinoo\n  Alluri", "title": "Towards Multimodal MIR: Predicting individual differences from\n  music-induced movement", "comments": "Appearing in the proceedings of the 21st International Society for\n  Music Information Retrieval Conference (ISMIR 2020) (camera-ready version)", "journal-ref": "International Society for Music Information Retrieval Conference,\n  (2020) 54-61", "doi": "10.5281/zenodo.4245368", "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the field of Music Information Retrieval grows, it is important to take\ninto consideration the multi-modality of music and how aspects of musical\nengagement such as movement and gesture might be taken into account. Bodily\nmovement is universally associated with music and reflective of important\nindividual features related to music preference such as personality, mood, and\nempathy. Future multimodal MIR systems may benefit from taking these aspects\ninto account. The current study addresses this by identifying individual\ndifferences, specifically Big Five personality traits, and scores on the\nEmpathy and Systemizing Quotients (EQ/SQ) from participants' free dance\nmovements. Our model successfully explored the unseen space for personality as\nwell as EQ, SQ, which has not previously been accomplished for the latter. R2\nscores for personality, EQ, and SQ were 76.3%, 77.1%, and 86.7% respectively.\nAs a follow-up, we investigated which bodily joints were most important in\ndefining these traits. We discuss how further research may explore how the\nmapping of these traits to movement patterns can be used to build a more\npersonalized, multi-modal recommendation system, as well as potential\ntherapeutic applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:10:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Agrawal", "Yudhik", ""], ["Jain", "Samyak", ""], ["Carlson", "Emily", ""], ["Toiviainen", "Petri", ""], ["Alluri", "Vinoo", ""]]}, {"id": "2007.11119", "submitter": "Ziv Epstein", "authors": "Ziv Epstein, Oc\\'eane Boulais, Skylar Gordon, and Matt Groh", "title": "Interpolating GANs to Scaffold Autotelic Creativity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent space modeled by generative adversarial networks (GANs) represents\na large possibility space. By interpolating categories generated by GANs, it is\npossible to create novel hybrid images. We present \"Meet the Ganimals,\" a\ncasual creator built on interpolations of BigGAN that can generate novel,\nhybrid animals called ganimals by efficiently searching this possibility space.\nLike traditional casual creators, the system supports a simple creative flow\nthat encourages rapid exploration of the possibility space. Users can discover\nnew ganimals, create their own, and share their reactions to aesthetic,\nemotional, and morphological characteristics of the ganimals. As users provide\ninput to the system, the system adapts and changes the distribution of\ncategories upon which ganimals are generated. As one of the first GAN-based\ncasual creators, Meet the Ganimals is an example how casual creators can\nleverage human curation and citizen science to discover novel artifacts within\na large possibility space.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:29:07 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Epstein", "Ziv", ""], ["Boulais", "Oc\u00e9ane", ""], ["Gordon", "Skylar", ""], ["Groh", "Matt", ""]]}, {"id": "2007.11346", "submitter": "Mononito Goswami Mr.", "authors": "Mononito Goswami, Minkush Manuja and Maitree Leekha", "title": "Towards Social & Engaging Peer Learning: Predicting Backchanneling and\n  Disengagement in Children", "comments": "14 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social robots and interactive computer applications have the potential to\nfoster early language development in young children by acting as peer learning\ncompanions. However, studies have found that children only trust robots which\nbehave in a natural and interpersonal manner. To help robots come across as\nengaging and attentive peer learning companions, we develop models to predict\nwhether the listener will lose attention (Listener Disengagement Prediction,\nLDP) and the extent to which a robot should generate backchanneling responses\n(Backchanneling Extent Prediction, BEP) in the next few seconds. We pose LDP\nand BEP as time series classification problems and conduct several experiments\nto assess the impact of different time series characteristics and feature sets\non the predictive performance of our model. Using statistics & machine\nlearning, we also examine which socio-demographic factors influence the amount\nof time children spend backchanneling and listening to their peers. To lend\ninterpretability to our models, we also analyzed critical features responsible\nfor their predictive performance. Our experiments revealed the utility of\nmultimodal features such as pupil dilation, blink rate, head movements, facial\naction units which have never been used before. We also found that the dynamics\nof time series features are rich predictors of listener disengagement and\nbackchanneling.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:16:42 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Goswami", "Mononito", ""], ["Manuja", "Minkush", ""], ["Leekha", "Maitree", ""]]}, {"id": "2007.11634", "submitter": "Pavan Madhusudana", "authors": "Pavan C. Madhusudana, Xiangxu Yu, Neil Birkbeck, Yilin Wang, Balu\n  Adsumilli, Alan C. Bovik", "title": "Subjective and Objective Quality Assessment of High Frame Rate Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High frame rate (HFR) videos are becoming increasingly common with the\ntremendous popularity of live, high-action streaming content such as sports.\nAlthough HFR contents are generally of very high quality, high bandwidth\nrequirements make them challenging to deliver efficiently, while simultaneously\nmaintaining their quality. To optimize trade-offs between bandwidth\nrequirements and video quality, in terms of frame rate adaptation, it is\nimperative to understand the intricate relationship between frame rate and\nperceptual video quality. Towards advancing progression in this direction we\ndesigned a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR)\ndataset, which is comprised of 480 videos having 6 different frame rates,\nobtained from 16 diverse contents. In order to understand the combined effects\nof compression and frame rate adjustment, we also processed videos at 5\ncompression levels at each frame rate. To obtain subjective labels on the\nvideos, we conducted a human study yielding 19,000 human quality ratings\nobtained from a pool of 85 human subjects. We also conducted a holistic\nevaluation of existing state-of-the-art Full and No-Reference video quality\nalgorithms, and statistically benchmarked their performance on the new\ndatabase. The LIVE-YT-HFR database has been made available online for public\nuse and evaluation purposes, with hopes that it will help advance research in\nthis exciting video technology direction. It may be obtained at\n\\url{https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 19:11:42 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Madhusudana", "Pavan C.", ""], ["Yu", "Xiangxu", ""], ["Birkbeck", "Neil", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2007.11888", "submitter": "Siyu Huang", "authors": "Tao Jin, Siyu Huang, Ming Chen, Yingming Li, Zhongfei Zhang", "title": "SBAT: Video Captioning with Sparse Boundary-Aware Transformer", "comments": "Appearing at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of applying the transformer structure\nto video captioning effectively. The vanilla transformer is proposed for\nuni-modal language generation task such as machine translation. However, video\ncaptioning is a multimodal learning problem, and the video features have much\nredundancy between different time steps. Based on these concerns, we propose a\nnovel method called sparse boundary-aware transformer (SBAT) to reduce the\nredundancy in video representation. SBAT employs boundary-aware pooling\noperation for scores from multihead attention and selects diverse features from\ndifferent scenarios. Also, SBAT includes a local correlation scheme to\ncompensate for the local information loss brought by sparse operation. Based on\nSBAT, we further propose an aligned cross-modal encoding scheme to boost the\nmultimodal interaction. Experimental results on two benchmark datasets show\nthat SBAT outperforms the state-of-the-art methods under most of the metrics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 09:57:25 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jin", "Tao", ""], ["Huang", "Siyu", ""], ["Chen", "Ming", ""], ["Li", "Yingming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "2007.13159", "submitter": "Yash Goyal", "authors": "Aayush Surana, Yash Goyal, Manish Shrivastava, Suvi Saarikallio, Vinoo\n  Alluri", "title": "Tag2Risk: Harnessing Social Music Tags for Characterizing Depression\n  Risk", "comments": "Appearing in the proceedings of ISMIR 2020. Aayush Surana and Yash\n  Goyal contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Musical preferences have been considered a mirror of the self. In this age of\nBig Data, online music streaming services allow us to capture ecologically\nvalid music listening behavior and provide a rich source of information to\nidentify several user-specific aspects. Studies have shown musical engagement\nto be an indirect representation of internal states including internalized\nsymptomatology and depression. The current study aims at unearthing patterns\nand trends in the individuals at risk for depression as it manifests in\nnaturally occurring music listening behavior. Mental well-being scores, musical\nengagement measures, and listening histories of Last.fm users (N=541) were\nacquired. Social tags associated with each listener's most popular tracks were\nanalyzed to unearth the mood/emotions and genres associated with the users.\nResults revealed that social tags prevalent in the users at risk for depression\nwere predominantly related to emotions depicting Sadness associated with genre\ntags representing neo-psychedelic-, avant garde-, dream-pop. This study will\nopen up avenues for an MIR-based approach to characterizing and predicting risk\nfor depression which can be helpful in early detection and additionally provide\nbases for designing music recommendations accordingly.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 16:02:10 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Surana", "Aayush", ""], ["Goyal", "Yash", ""], ["Shrivastava", "Manish", ""], ["Saarikallio", "Suvi", ""], ["Alluri", "Vinoo", ""]]}, {"id": "2007.13314", "submitter": "Zhi Chen", "authors": "Zhi Chen, Sen Wang, Jingjing Li, Zi Huang", "title": "Rethinking Generative Zero-Shot Learning: An Ensemble Learning\n  Perspective for Recognising Visual Patches", "comments": "ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is commonly used to address the very pervasive\nproblem of predicting unseen classes in fine-grained image classification and\nother tasks. One family of solutions is to learn synthesised unseen visual\nsamples produced by generative models from auxiliary semantic information, such\nas natural language descriptions. However, for most of these models,\nperformance suffers from noise in the form of irrelevant image backgrounds.\nFurther, most methods do not allocate a calculated weight to each semantic\npatch. Yet, in the real world, the discriminative power of features can be\nquantified and directly leveraged to improve accuracy and reduce computational\ncomplexity. To address these issues, we propose a novel framework called\nmulti-patch generative adversarial nets (MPGAN) that synthesises local patch\nfeatures and labels unseen classes with a novel weighted voting strategy. The\nprocess begins by generating discriminative visual features from noisy text\ndescriptions for a set of predefined local patches using multiple specialist\ngenerative models. The features synthesised from each patch for unseen classes\nare then used to construct an ensemble of diverse supervised classifiers, each\ncorresponding to one local patch. A voting strategy averages the probability\ndistributions output from the classifiers and, given that some patches are more\ndiscriminative than others, a discrimination-based attention mechanism helps to\nweight each patch accordingly. Extensive experiments show that MPGAN has\nsignificantly greater accuracy than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 05:49:44 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 03:50:28 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 01:14:32 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Chen", "Zhi", ""], ["Wang", "Sen", ""], ["Li", "Jingjing", ""], ["Huang", "Zi", ""]]}, {"id": "2007.13477", "submitter": "Sean Kross", "authors": "Sean Kross, Jeffrey T. Leek, John Muschelli", "title": "Ari: The Automated R Instructor", "comments": "- reformatted section headings - added several citations - linted and\n  reformatted code chunks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the ari package for automatically generating technology-focused\neducational videos. The goal of the package is to create reproducible videos,\nwith the ability to change and update video content seamlessly. We present\nseveral examples of generating videos including using R Markdown slide decks,\nPowerPoint slides, or simple images as source material. We also discuss how ari\ncan help instructors reach new audiences through programmatically translating\nmaterials into other languages.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 17:28:31 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 04:16:20 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kross", "Sean", ""], ["Leek", "Jeffrey T.", ""], ["Muschelli", "John", ""]]}, {"id": "2007.14084", "submitter": "Serhan G\\\"ul", "authors": "Serhan G\\\"ul, Sebastian Bosse, Dimitri Podborski, Thomas Schierl,\n  Cornelius Hellge", "title": "Kalman Filter-based Head Motion Prediction for Cloud-based Mixed Reality", "comments": "Accepted at the ACM Multimedia Conference (ACMMM) 2020. 9 pages, 9\n  figures", "journal-ref": "Proceedings of the 28th ACM International Conference on Multimedia\n  (2020) 3632-3641", "doi": "10.1145/3394171.3413699", "report-no": null, "categories": "cs.MM eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric video allows viewers to experience highly-realistic 3D content\nwith six degrees of freedom in mixed reality (MR) environments. Rendering\ncomplex volumetric videos can require a prohibitively high amount of\ncomputational power for mobile devices. A promising technique to reduce the\ncomputational burden on mobile devices is to perform the rendering at a cloud\nserver. However, cloud-based rendering systems suffer from an increased\ninteraction (motion-to-photon) latency that may cause registration errors in MR\nenvironments. One way of reducing the effective latency is to predict the\nviewer's head pose and render the corresponding view from the volumetric video\nin advance. In this paper, we design a Kalman filter for head motion prediction\nin our cloud-based volumetric video streaming system. We analyze the\nperformance of our approach using recorded head motion traces and compare its\nperformance to an autoregression model for different prediction intervals\n(look-ahead times). Our results show that the Kalman filter can predict head\norientations 0.5 degrees more accurately than the autoregression model for a\nlook-ahead time of 60 ms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:41:22 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["G\u00fcl", "Serhan", ""], ["Bosse", "Sebastian", ""], ["Podborski", "Dimitri", ""], ["Schierl", "Thomas", ""], ["Hellge", "Cornelius", ""]]}, {"id": "2007.14267", "submitter": "Yat Hong Lam", "authors": "Yat-Hong Lam, Alireza Zare, Francesco Cricri, Jani Lainema, Miska\n  Hannuksela", "title": "Efficient Adaptation of Neural Network Filter for Video Compression", "comments": "Accepted in ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient finetuning methodology for neural-network filters\nwhich are applied as a postprocessing artifact-removal step in video coding\npipelines. The fine-tuning is performed at encoder side to adapt the neural\nnetwork to the specific content that is being encoded. In order to maximize the\nPSNR gain and minimize the bitrate overhead, we propose to finetune only the\nconvolutional layers' biases. The proposed method achieves convergence much\nfaster than conventional finetuning approaches, making it suitable for\npractical applications. The weight-update can be included into the video\nbitstream generated by the existing video codecs. We show that our method\nachieves up to 9.7% average BD-rate gain when compared to the state-of-art\nVersatile Video Coding (VVC) standard codec on 7 test sequences.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:24:28 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 09:07:25 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lam", "Yat-Hong", ""], ["Zare", "Alireza", ""], ["Cricri", "Francesco", ""], ["Lainema", "Jani", ""], ["Hannuksela", "Miska", ""]]}, {"id": "2007.14560", "submitter": "Vishal Kaushal", "authors": "Vishal Kaushal, Suraj Kothawade, Rishabh Iyer, Ganesh Ramakrishnan", "title": "Realistic Video Summarization through VISIOCITY: A New Benchmark and\n  Evaluation Framework", "comments": "19 pages, 1 figure, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video summarization is still an unsolved problem due to several\nchallenges. We take steps towards making automatic video summarization more\nrealistic by addressing them. Firstly, the currently available datasets either\nhave very short videos or have few long videos of only a particular type. We\nintroduce a new benchmarking dataset VISIOCITY which comprises of longer videos\nacross six different categories with dense concept annotations capable of\nsupporting different flavors of video summarization and can be used for other\nvision problems. Secondly, for long videos, human reference summaries are\ndifficult to obtain. We present a novel recipe based on pareto optimality to\nautomatically generate multiple reference summaries from indirect ground truth\npresent in VISIOCITY. We show that these summaries are at par with human\nsummaries. Thirdly, we demonstrate that in the presence of multiple ground\ntruth summaries (due to the highly subjective nature of the task), learning\nfrom a single combined ground truth summary using a single loss function is not\na good idea. We propose a simple recipe VISIOCITY-SUM to enhance an existing\nmodel using a combination of losses and demonstrate that it beats the current\nstate of the art techniques when tested on VISIOCITY. We also show that a\nsingle measure to evaluate a summary, as is the current typical practice, falls\nshort. We propose a framework for better quantitative assessment of summary\nquality which is closer to human judgment than a single measure, say F1. We\nreport the performance of a few representative techniques of video\nsummarization on VISIOCITY assessed using various measures and bring out the\nlimitation of the techniques and/or the assessment mechanism in modeling human\njudgment and demonstrate the effectiveness of our evaluation framework in doing\nso.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 02:44:35 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 09:42:26 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kaushal", "Vishal", ""], ["Kothawade", "Suraj", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "2007.14580", "submitter": "T.J. Tsai", "authors": "Mengyi Shan and TJ Tsai", "title": "Improved Handling of Repeats and Jumps in Audio-Sheet Image\n  Synchronization", "comments": "8 pages, 5 figures. Accepted paper at the International Society for\n  Music Information Retrieval Conference (ISMIR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the problem of automatically generating piano score\nfollowing videos given an audio recording and raw sheet music images. Whereas\nprevious works focus on synthetic sheet music where the data has been cleaned\nand preprocessed, we instead focus on developing a system that can cope with\nthe messiness of raw, unprocessed sheet music PDFs from IMSLP. We investigate\nhow well existing systems cope with real scanned sheet music, filler pages and\nunrelated pieces or movements, and discontinuities due to jumps and repeats. We\nfind that a significant bottleneck in system performance is handling jumps and\nrepeats correctly. In particular, we find that a previously proposed Jump DTW\nalgorithm does not perform robustly when jump locations are unknown a priori.\nWe propose a novel alignment algorithm called Hierarchical DTW that can handle\njumps and repeats even when jump locations are not known. It first performs\nalignment at the feature level on each sheet music line, and then performs a\nsecond alignment at the segment level. By operating at the segment level, it is\nable to encode domain knowledge about how likely a particular jump is. Through\ncarefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we\nshow that Hierarachical DTW significantly outperforms Jump DTW in handling\nvarious types of jumps.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:04:07 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Shan", "Mengyi", ""], ["Tsai", "TJ", ""]]}, {"id": "2007.14726", "submitter": "Di Ma", "authors": "Di Ma, Fan Zhang and David R. Bull", "title": "Video compression with low complexity CNN-based spatial resolution\n  adaptation", "comments": null, "journal-ref": null, "doi": "10.1117/12.2567633", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It has recently been demonstrated that spatial resolution adaptation can be\nintegrated within video compression to improve overall coding performance by\nspatially down-sampling before encoding and super-resolving at the decoder.\nSignificant improvements have been reported when convolutional neural networks\n(CNNs) were used to perform the resolution up-sampling. However, this approach\nsuffers from high complexity at the decoder due to the employment of CNN-based\nsuper-resolution. In this paper, a novel framework is proposed which supports\nthe flexible allocation of complexity between the encoder and decoder. This\napproach employs a CNN model for video down-sampling at the encoder and uses a\nLanczos3 filter to reconstruct full resolution at the decoder. The proposed\nmethod was integrated into the HEVC HM 16.20 software and evaluated on JVET UHD\ntest sequences using the All Intra configuration. The experimental results\ndemonstrate the potential of the proposed approach, with significant bitrate\nsavings (more than 10%) over the original HEVC HM, coupled with reduced\ncomputational complexity at both encoder (29%) and decoder (10%).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 10:20:36 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ma", "Di", ""], ["Zhang", "Fan", ""], ["Bull", "David R.", ""]]}, {"id": "2007.14856", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Unsupervised Generative Adversarial Alignment Representation for Sheet\n  music, Audio and Lyrics", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sheet music, audio, and lyrics are three main modalities during writing a\nsong. In this paper, we propose an unsupervised generative adversarial\nalignment representation (UGAAR) model to learn deep discriminative\nrepresentations shared across three major musical modalities: sheet music,\nlyrics, and audio, where a deep neural network based architecture on three\nbranches is jointly trained. In particular, the proposed model can transfer the\nstrong relationship between audio and sheet music to audio-lyrics and\nsheet-lyrics pairs by learning the correlation in the latent shared subspace.\nWe apply CCA components of audio and sheet music to establish new ground truth.\nThe generative (G) model learns the correlation of two couples of transferred\npairs to generate new audio-sheet pair for a fixed lyrics to challenge the\ndiscriminative (D) model. The discriminative model aims at distinguishing the\ninput which is from the generative model or the ground truth. The two models\nsimultaneously train in an adversarial way to enhance the ability of deep\nalignment representation learning. Our experimental results demonstrate the\nfeasibility of our proposed UGAAR for alignment representation learning among\nsheet music, audio, and lyrics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:18:15 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "2007.14913", "submitter": "Prakhar Kulshreshtha", "authors": "Prakhar Kulshreshtha and Tanaya Guha", "title": "Dynamic Character Graph via Online Face Clustering for Movie Analysis", "comments": "accepted for publication in Multimedia Tools and Applications (MMTA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective approach to automated movie content analysis involves building a\nnetwork (graph) of its characters. Existing work usually builds a static\ncharacter graph to summarize the content using metadata, scripts or manual\nannotations. We propose an unsupervised approach to building a dynamic\ncharacter graph that captures the temporal evolution of character interaction.\nWe refer to this as the character interaction graph(CIG). Our approach has two\ncomponents:(i) an online face clustering algorithm that discovers the\ncharacters in the video stream as they appear, and (ii) simultaneous creation\nof a CIG using the temporal dynamics of the resulting clusters. We demonstrate\nthe usefulness of the CIG for two movie analysis tasks: narrative structure\n(acts) segmentation, and major character retrieval. Our evaluation on\nfull-length movies containing more than 5000 face tracks shows that the\nproposed approach achieves superior performance for both the tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:37:30 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Kulshreshtha", "Prakhar", ""], ["Guha", "Tanaya", ""]]}, {"id": "2007.15096", "submitter": "Luis Sequeira Dr", "authors": "Jose Saldana, Julian Fernandez Navajas, Jose Ruiz Mas, Luis Sequeira,\n  Luis Casadesus", "title": "Comparison of Multiplexing Policies for FPS Games in terms of Subjective\n  Quality", "comments": null, "journal-ref": "Proc. II Workshop on Multimedia Data Coding and Transmission 2012,\n  Jornadas Sarteco. Elche (Spain). Sept. 2012. ISBN: 978-84-695-4472-3", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares two policies which can be used for multiplexing the\ntraffic of a number of players of a First Person Shooter game. A network\nscenario in which a number of players share an access network has been\nsimulated, in order to compare the policies in terms of a subjective quality\nestimator. The first policy, namely timeout, achieves higher bandwidth savings,\nwhile the second one, period, introduces less delay and jitter. The results\nshow that the difference in terms of QoE is only significant when the number of\nplayers is small. Thus, in order to make the correct decision, the concrete\nnetwork scenario and the characteristics of the router would have to be\nconsidered in each case, taking into account the estimation of the subjective\nquality that can be expected.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:32:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Saldana", "Jose", ""], ["Navajas", "Julian Fernandez", ""], ["Mas", "Jose Ruiz", ""], ["Sequeira", "Luis", ""], ["Casadesus", "Luis", ""]]}, {"id": "2007.15269", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Yang Cheng, Shaoyuan Xu, Qian Lin, Jan Allebach", "title": "The Blessing and the Curse of the Noise behind Facial Landmark\n  Annotations", "comments": "10 pages, 10 figures, accepted to Electronic Imaging Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolving algorithms for 2D facial landmark detection empower people to\nrecognize faces, analyze facial expressions, etc. However, existing methods\nstill encounter problems of unstable facial landmarks when applied to videos.\nBecause previous research shows that the instability of facial landmarks is\ncaused by the inconsistency of labeling quality among the public datasets, we\nwant to have a better understanding of the influence of annotation noise in\nthem. In this paper, we make the following contributions: 1) we propose two\nmetrics that quantitatively measure the stability of detected facial landmarks,\n2) we model the annotation noise in an existing public dataset, 3) we\ninvestigate the influence of different types of noise in training face\nalignment neural networks, and propose corresponding solutions. Our results\ndemonstrate improvements in both accuracy and stability of detected facial\nlandmarks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:13:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Cheng", "Yang", ""], ["Xu", "Shaoyuan", ""], ["Lin", "Qian", ""], ["Allebach", "Jan", ""]]}, {"id": "2007.15271", "submitter": "Cecilia Pasquini", "authors": "Mattia Bonomi and Cecilia Pasquini and Giulia Boato", "title": "Dynamic texture analysis for detecting fake faces in video sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creation of manipulated multimedia content involving human characters has\nreached in the last years unprecedented realism, calling for automated\ntechniques to expose synthetically generated faces in images and videos. This\nwork explores the analysis of spatio-temporal texture dynamics of the video\nsignal, with the goal of characterizing and distinguishing real and fake\nsequences. We propose to build a binary decision on the joint analysis of\nmultiple temporal segments and, in contrast to previous approaches, to exploit\nthe textural dynamics of both the spatial and temporal dimensions. This is\nachieved through the use of Local Derivative Patterns on Three Orthogonal\nPlanes (LDP-TOP), a compact feature representation known to be an important\nasset for the detection of face spoofing attacks. Experimental analyses on\nstate-of-the-art datasets of manipulated videos show the discriminative power\nof such descriptors in separating real and fake sequences, and also identifying\nthe creation method used. Linear Support Vector Machines (SVMs) are used which,\ndespite the lower complexity, yield comparable performance to previously\nproposed deep models for fake content detection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:21:24 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bonomi", "Mattia", ""], ["Pasquini", "Cecilia", ""], ["Boato", "Giulia", ""]]}, {"id": "2007.15366", "submitter": "Luis Sequeira Dr", "authors": "Idelkys Quintana, Jose Saldana, Jose Ruiz Mas, Julian Fern\\'andez\n  Navajas, Luis A. Casadesus Pazos, Luis Sequeira", "title": "Influencia del Buffer del Router en la Distribuc\\'ion de Video P2P-TV", "comments": "in Spanish", "journal-ref": "Actas del XXVII Simposium Nacional de la Union Cient\\'ifica\n  Internacional de Radio (URSI 2012). Elche (Spain). Sept. 2012. ISBN:\n  978-84-695-4327-6", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a study of the behaviour of the router buffer when\nmanaging the traffic of P2P-TV applications, where a number of peers exchange\nvideo content. First, a summary of the characteristics of SOPCast is presented.\nThen, the results obtained in simulation tests using different buffer policies\nare presented. Real traces of the application, obtained from a research\nproject, have been used for the tests, sharing the Internet access with\ndifferent amounts of background traffic. The results show that a similar buffer\nbehaviour for all the access technologies. In addition, the big amount of small\npackets generated may impair the video traffic, thus avoiding the\nretransmission of the contents by the application.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 10:38:57 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Quintana", "Idelkys", ""], ["Saldana", "Jose", ""], ["Mas", "Jose Ruiz", ""], ["Navajas", "Julian Fern\u00e1ndez", ""], ["Pazos", "Luis A. Casadesus", ""], ["Sequeira", "Luis", ""]]}, {"id": "2007.15373", "submitter": "Luis Sequeira Dr", "authors": "Jose Saldana, Luis Sequeira, Julian Fernandez-Navajas, Jose Ruiz-Mas", "title": "Traffic Optimization for TCP-based Massive Multiplayer Online Games", "comments": null, "journal-ref": "Proc. International Symposium on Performance Evaluation of\n  Computer and Telecommunication Systems SPECTS 2012, July 8-11, 2012, Genoa,\n  Italy. ISBN: 978-1-4673-2235-5", "doi": null, "report-no": null, "categories": "cs.NI cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the use of a traffic optimization technique named TCM\n(Tunneling, Compressing and Multiplexing) to reduce the bandwidth of MMORPGs\n(Massively Multiplayer Online Role-Playing Games), which employ TCP to provide\na soft real-time service. In order to optimize the traffic and to improve\nbandwidth efficiency, TCM can be applied when the packets of a number of\nplayers share the same link, which occurs in some scenarios, as e.g. the\ntraffic between proxies and servers of game-supporting infrastructures. First,\nTCP/IP headers are compressed using standard algorithms that avoid sending\nrepeated fields; next, a number of packets are blended into a bigger one and\nfinally, they are sent using a tunnel. The expected compressed header size has\nbeen obtained using traffic traces of a real game. Next, simulations using a\ntraffic model of a popular MMORPG have been performed in order to estimate the\nexpected bandwidth savings and the reduction in packets per second. The\nobtained bandwidth saving is about 60 percent. Packets per second are also\nsignificantly reduced. In addition, the added delays are shown to be small\nenough so as not to impair layers' experienced quality.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 10:50:23 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Saldana", "Jose", ""], ["Sequeira", "Luis", ""], ["Fernandez-Navajas", "Julian", ""], ["Ruiz-Mas", "Jose", ""]]}, {"id": "2007.15797", "submitter": "Xuan Dong", "authors": "Xuan Dong and Donald S. Williamson", "title": "A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality\n  Ratings of Real-World Signals", "comments": "Proceeding of INTERSPEECH", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-world capabilities of objective speech quality measures are limited\nsince current measures (1) are developed from simulated data that does not\nadequately model real environments; or they (2) predict objective scores that\nare not always strongly correlated with subjective ratings. Additionally, a\nlarge dataset of real-world signals with listener quality ratings does not\ncurrently exist, which would help facilitate real-world assessment. In this\npaper, we collect and predict the perceptual quality of real-world speech\nsignals that are evaluated by human listeners. We first collect a large quality\nrating dataset by conducting crowdsourced listening studies on two real-world\ncorpora. We further develop a novel approach that predicts human quality\nratings using a pyramid bidirectional long short term memory (pBLSTM) network\nwith an attention mechanism. The results show that the proposed model achieves\nstatistically lower estimation errors than prior assessment approaches, where\nthe predicted scores strongly correlate with human judgments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 01:46:06 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Dong", "Xuan", ""], ["Williamson", "Donald S.", ""]]}, {"id": "2007.15829", "submitter": "Yadan Luo", "authors": "Yadan Luo, Zi Huang, Zijian Wang, Zheng Zhang, Mahsa Baktashmotlagh", "title": "Adversarial Bipartite Graph Learning for Video Domain Adaptation", "comments": "Proceedings of the 28th ACM International Conference on Multimedia\n  (MM '20)", "journal-ref": null, "doi": "10.1145/3394171.3413897", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation techniques, which focus on adapting models between\ndistributionally different domains, are rarely explored in the video\nrecognition area due to the significant spatial and temporal shifts across the\nsource (i.e. training) and target (i.e. test) domains. As such, recent works on\nvisual domain adaptation which leverage adversarial learning to unify the\nsource and target video representations and strengthen the feature\ntransferability are not highly effective on the videos. To overcome this\nlimitation, in this paper, we learn a domain-agnostic video classifier instead\nof learning domain-invariant representations, and propose an Adversarial\nBipartite Graph (ABG) learning framework which directly models the\nsource-target interactions with a network topology of the bipartite graph.\nSpecifically, the source and target frames are sampled as heterogeneous\nvertexes while the edges connecting two types of nodes measure the affinity\namong them. Through message-passing, each vertex aggregates the features from\nits heterogeneous neighbors, forcing the features coming from the same class to\nbe mixed evenly. Explicitly exposing the video classifier to such cross-domain\nrepresentations at the training and test stages makes our model less biased to\nthe labeled source data, which in-turn results in achieving a better\ngeneralization on the target domain. To further enhance the model capacity and\ntestify the robustness of the proposed architecture on difficult transfer\ntasks, we extend our model to work in a semi-supervised setting using an\nadditional video-level bipartite graph. Extensive experiments conducted on four\nbenchmarks evidence the effectiveness of the proposed approach over the SOTA\nmethods on the task of video recognition.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:48:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Luo", "Yadan", ""], ["Huang", "Zi", ""], ["Wang", "Zijian", ""], ["Zhang", "Zheng", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2007.16054", "submitter": "Francesco Cricri", "authors": "Nannan Zou and Honglei Zhang and Francesco Cricri and Hamed R.\n  Tavakoli and Jani Lainema and Miska Hannuksela and Emre Aksu and Esa Rahtu", "title": "Learning to Learn to Compress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an end-to-end meta-learned system for image\ncompression. Traditional machine learning based approaches to image compression\ntrain one or more neural network for generalization performance. However, at\ninference time, the encoder or the latent tensor output by the encoder can be\noptimized for each test image. This optimization can be regarded as a form of\nadaptation or benevolent overfitting to the input content. In order to reduce\nthe gap between training and inference conditions, we propose a new training\nparadigm for learned image compression, which is based on meta-learning. In a\nfirst phase, the neural networks are trained normally. In a second phase, the\nModel-Agnostic Meta-learning approach is adapted to the specific case of image\ncompression, where the inner-loop performs latent tensor overfitting, and the\nouter loop updates both encoder and decoder neural networks based on the\noverfitting performance. Furthermore, after meta-learning, we propose to\noverfit and cluster the bias terms of the decoder on training image patches, so\nthat at inference time the optimal content-specific bias terms can be selected\nat encoder-side. Finally, we propose a new probability model for lossless\ncompression, which combines concepts from both multi-scale and super-resolution\nprobability model approaches. We show the benefits of all our proposed ideas\nvia carefully designed experiments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:13:53 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 16:18:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zou", "Nannan", ""], ["Zhang", "Honglei", ""], ["Cricri", "Francesco", ""], ["Tavakoli", "Hamed R.", ""], ["Lainema", "Jani", ""], ["Hannuksela", "Miska", ""], ["Aksu", "Emre", ""], ["Rahtu", "Esa", ""]]}, {"id": "2007.16070", "submitter": "Luis Sequeira Dr", "authors": "Jose Saldana, Mirko Suznjevic, Luis Sequeira, Julian\n  Fernandez-Navajas, Maja Matijasevic, Jose Ruiz-Mas", "title": "The Effect of TCP Variants on the Coexistence of MMORPG and Best-Effort\n  Traffic", "comments": null, "journal-ref": "8th International Workshop on Networking Issues in Multimedia\n  Entertainment (NIME'12), Munich, Germany, July 30, 2012. ISBN:\n  978-1-4673-1543-2", "doi": "10.1109/ICCCN.2012.6289245", "report-no": null, "categories": "cs.NI cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study TCP flows coexistence between Massive Multiplayer Online Role\nPlaying Games (MMORPGs) and other TCP applications, by taking World of Warcraft\n(WoW) and a file transfer application based on File Transfer Protocol (FTP) as\nan example. Our focus is on the effects of the sender buffer size and FTP\ncross-traffic on the queuing delay experienced by the (MMORPG) game traffic. A\nnetwork scenario corresponding to a real life situation in an ADSL access\nnetwork has been simulated by using NS2. Three TCP variants, namely TCP SACK,\nTCP New Reno, and TCP Vegas, have been considered for cross-traffic. The\nresults show that TCP Vegas is able to maintain a constant rate while competing\nwith the game traffic, since it prevents packet loss and high queuing delays by\nnot increasing the sender window size. TCP SACK and TCP New Reno, on the other\nhand, tend to continuously increase the sender window size, thus potentially\nallowing higher packet loss and causing undesired delays for the game traffic.\nIn terms of buffer size, we have established that smaller buffers are better\nfor MMORPG applications, while larger buffers contribute to a higher overall\ndelay.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 11:03:23 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Saldana", "Jose", ""], ["Suznjevic", "Mirko", ""], ["Sequeira", "Luis", ""], ["Fernandez-Navajas", "Julian", ""], ["Matijasevic", "Maja", ""], ["Ruiz-Mas", "Jose", ""]]}, {"id": "2007.16170", "submitter": "Philippe Esling", "authors": "Philippe Esling, Ninon Devis, Adrien Bitton, Antoine Caillon, Axel\n  Chemla--Romeu-Santos, Constance Douwes", "title": "Diet deep generative audio models with structured lottery", "comments": "8 pages, 5 figures. Proceedings of the 23rd International Conference\n  on Digital Audio Effects (DAFx-20), Vienna, Austria, September 8-12, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models have provided extremely successful solutions in most\naudio application fields. However, the high accuracy of these models comes at\nthe expense of a tremendous computation cost. This aspect is almost always\noverlooked in evaluating the quality of proposed models. However, models should\nnot be evaluated without taking into account their complexity. This aspect is\nespecially critical in audio applications, which heavily relies on specialized\nembedded hardware with real-time constraints. In this paper, we build on recent\nobservations that deep models are highly overparameterized, by studying the\nlottery ticket hypothesis on deep generative audio models. This hypothesis\nstates that extremely efficient small sub-networks exist in deep models and\nwould provide higher accuracy than larger models if trained in isolation.\nHowever, lottery tickets are found by relying on unstructured masking, which\nmeans that resulting models do not provide any gain in either disk size or\ninference time. Instead, we develop here a method aimed at performing\nstructured trimming. We show that this requires to rely on global selection and\nintroduce a specific criterion based on mutual information. First, we confirm\nthe surprising result that smaller models provide higher accuracy than their\nlarge counterparts. We further show that we can remove up to 95% of the model\nweights without significant degradation in accuracy. Hence, we can obtain very\nlight models for generative audio across popular methods such as Wavenet, SING\nor DDSP, that are up to 100 times smaller with commensurate accuracy. We study\nthe theoretical bounds for embedding these models on Raspberry Pi and Arduino,\nand show that we can obtain generative models on CPU with equivalent quality as\nlarge GPU models. Finally, we discuss the possibility of implementing deep\ngenerative audio models on embedded platforms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:43:10 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Esling", "Philippe", ""], ["Devis", "Ninon", ""], ["Bitton", "Adrien", ""], ["Caillon", "Antoine", ""], ["Chemla--Romeu-Santos", "Axel", ""], ["Douwes", "Constance", ""]]}, {"id": "2007.16187", "submitter": "Philippe Esling", "authors": "Philippe Esling, Theis Bazin, Adrien Bitton, Tristan Carsault, Ninon\n  Devis", "title": "Ultra-light deep MIR by trimming lottery tickets", "comments": "8 pages, 2 figures. 21st International Society for Music Information\n  Retrieval Conference 11-15 October 2020, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current state-of-the-art results in Music Information Retrieval are largely\ndominated by deep learning approaches. These provide unprecedented accuracy\nacross all tasks. However, the consistently overlooked downside of these models\nis their stunningly massive complexity, which seems concomitantly crucial to\ntheir success. In this paper, we address this issue by proposing a model\npruning method based on the lottery ticket hypothesis. We modify the original\napproach to allow for explicitly removing parameters, through structured\ntrimming of entire units, instead of simply masking individual weights. This\nleads to models which are effectively lighter in terms of size, memory and\nnumber of operations. We show that our proposal can remove up to 90% of the\nmodel parameters without loss of accuracy, leading to ultra-light deep MIR\nmodels. We confirm the surprising result that, at smaller compression ratios\n(removing up to 85% of a network), lighter models consistently outperform their\nheavier counterparts. We exhibit these results on a large array of MIR tasks\nincluding audio classification, pitch recognition, chord extraction, drum\ntranscription and onset estimation. The resulting ultra-light deep learning\nmodels for MIR can run on CPU, and can even fit on embedded devices with\nminimal degradation of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:30:28 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Esling", "Philippe", ""], ["Bazin", "Theis", ""], ["Bitton", "Adrien", ""], ["Carsault", "Tristan", ""], ["Devis", "Ninon", ""]]}]