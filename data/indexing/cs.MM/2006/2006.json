[{"id": "2006.00143", "submitter": "Mengyan Li", "authors": "Jun Yu, Mengyan Li, Xinlong Hao and Guochen Xie", "title": "Deep Fusion Siamese Network for Automatic Kinship Verification", "comments": "8 pages, 8 figures", "journal-ref": "2020 15th IEEE Conference on Automatic Face and Gesture\n  Recognition; 4th Recognizing Families In the Wild (RFIW)", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic kinship verification aims to determine whether some individuals\nbelong to the same family. It is of great research significance to help missing\npersons reunite with their families. In this work, the challenging problem is\nprogressively addressed in two respects. First, we propose a deep siamese\nnetwork to quantify the relative similarity between two individuals. When given\ntwo input face images, the deep siamese network extracts the features from them\nand fuses these features by combining and concatenating. Then, the fused\nfeatures are fed into a fully-connected network to obtain the similarity score\nbetween two faces, which is used to verify the kinship. To improve the\nperformance, a jury system is also employed for multi-model fusion. Second, two\ndeep siamese networks are integrated into a deep triplet network for\ntri-subject (i.e., father, mother and child) kinship verification, which is\nintended to decide whether a child is related to a pair of parents or not.\nSpecifically, the obtained similarity scores of father-child and mother-child\nare weighted to generate the parent-child similarity score for kinship\nverification. Recognizing Families In the Wild (RFIW) is a challenging kinship\nrecognition task with multiple tracks, which is based on Families in the Wild\n(FIW), a large-scale and comprehensive image database for automatic kinship\nrecognition. The Kinship Verification (track I) and Tri-Subject Verification\n(track II) are supported during the ongoing RFIW2020 Challenge. Our team\n(ustc-nelslip) ranked 1st in track II, and 3rd in track I. The code is\navailable at https://github.com/gniknoil/FG2020-kinship.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 01:43:59 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 12:19:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Yu", "Jun", ""], ["Li", "Mengyan", ""], ["Hao", "Xinlong", ""], ["Xie", "Guochen", ""]]}, {"id": "2006.00190", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Rishabh Baghel, Ravi Kiran Sarvadevabhatla", "title": "OPAL-Net: A Generative Model for Part-based Object Layout Generation", "comments": "Code repository at https://github.com/atmacvit/opalnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose OPAL-Net, a novel hierarchical architecture for part-based layout\ngeneration of objects from multiple categories using a single unified model. We\nadopt a coarse-to-fine strategy involving semantically conditioned\nautoregressive generation of bounding box layouts and pixel-level part layouts\nfor objects. We use Graph Convolutional Networks, Deep Recurrent Networks along\nwith custom-designed Conditional Variational Autoencoders to enable flexible,\ndiverse and category-aware generation of object layouts. We train OPAL-Net on\nPASCAL-Parts dataset. The generated samples and corresponding evaluation scores\ndemonstrate the versatility of OPAL-Net compared to ablative variants and\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 06:25:19 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Baghel", "Rishabh", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2006.00497", "submitter": "Qi Yang", "authors": "Qi Yang, Zhan Ma, Yiling Xu, Zhu Li, Jun Sun", "title": "Inferring Point Cloud Quality via Graph Similarity", "comments": "This paper has been accepted by TPAMI, 21, Decemebr, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the GraphSIM -- an objective metric to accurately predict the\nsubjective quality of point cloud with superimposed geometry and color\nimpairments. Motivated by the facts that human vision system is more sensitive\nto the high spatial-frequency components (e.g., contours, edges), and weighs\nmore to the local structural variations rather individual point intensity, we\nfirst extract geometric keypoints by resampling the reference point cloud\ngeometry information to form the object skeleton; we then construct local\ngraphs centered at these keypoints for both reference and distorted point\nclouds, followed by collectively aggregating color gradient moments (e.g.,\nzeroth, first, and second) that are derived between all other points and\ncentered keypoint in the same local graph for significant feature similarity\n(a.k.a., local significance) measurement; Final similarity index is obtained by\npooling the local graph significance across all color channels and by averaging\nacross all graphs. Our GraphSIM is validated using two large and independent\npoint cloud assessment datasets that involve a wide range of impairments (e.g.,\nre-sampling, compression, additive noise), reliably demonstrating the\nstate-of-the-art performance for all distortions with noticeable gains in\npredicting the subjective mean opinion score (MOS), compared with those\npoint-wise distance-based metrics adopted in standardization reference\nsoftware. Ablation studies have further shown that GraphSIM is generalized to\nvarious scenarios with consistent performance by examining its key modules and\nparameters.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 11:57:03 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 03:44:01 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Yang", "Qi", ""], ["Ma", "Zhan", ""], ["Xu", "Yiling", ""], ["Li", "Zhu", ""], ["Sun", "Jun", ""]]}, {"id": "2006.00785", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Benet Oriol, Jordi Luque, Ferran Diego and Xavier Giro-i-Nieto", "title": "Transcription-Enriched Joint Embeddings for Spoken Descriptions of\n  Images and Videos", "comments": "Accepted for presentation at EPIC@CVPR2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an effective approach for training unique embedding\nrepresentations by combining three simultaneous modalities: image and spoken\nand textual narratives. The proposed methodology departs from a baseline system\nthat spawns a embedding space trained with only spoken narratives and image\ncues. Our experiments on the EPIC-Kitchen and Places Audio Caption datasets\nshow that introducing the human-generated textual transcriptions of the spoken\nnarratives helps to the training procedure yielding to get better embedding\nrepresentations. The triad speech, image and words allows for a better estimate\nof the point embedding and show an improving of the performance within tasks\nlike image and speech retrieval, even when text third modality, text, is not\npresent in the task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 08:18:15 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Oriol", "Benet", ""], ["Luque", "Jordi", ""], ["Diego", "Ferran", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2006.01318", "submitter": "Tarek Elgamal", "authors": "Tarek Elgamal, Shu Shi, Varun Gupta, Rittwik Jana, Klara Nahrstedt", "title": "SiEVE: Semantically Encoded Video Analytics on Edge and Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision and neural networks have made it possible\nfor more surveillance videos to be automatically searched and analyzed by\nalgorithms rather than humans. This happened in parallel with advances in edge\ncomputing where videos are analyzed over hierarchical clusters that contain\nedge devices, close to the video source. However, the current video analysis\npipeline has several disadvantages when dealing with such advances. For\nexample, video encoders have been designed for a long time to please human\nviewers and be agnostic of the downstream analysis task (e.g., object\ndetection). Moreover, most of the video analytics systems leverage 2-tier\narchitecture where the encoded video is sent to either a remote cloud or a\nprivate edge server but does not efficiently leverage both of them. In response\nto these advances, we present SIEVE, a 3-tier video analytics system to reduce\nthe latency and increase the throughput of analytics over video streams. In\nSIEVE, we present a novel technique to detect objects in compressed video\nstreams. We refer to this technique as semantic video encoding because it\nallows video encoders to be aware of the semantics of the downstream task\n(e.g., object detection). Our results show that by leveraging semantic video\nencoding, we achieve close to 100% object detection accuracy with decompressing\nonly 3.5% of the video frames which results in more than 100x speedup compared\nto classical approaches that decompress every video frame.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 23:44:34 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Elgamal", "Tarek", ""], ["Shi", "Shu", ""], ["Gupta", "Varun", ""], ["Jana", "Rittwik", ""], ["Nahrstedt", "Klara", ""]]}, {"id": "2006.01339", "submitter": "Jun-Ho Choi", "authors": "Jun-Ho Choi, Jun-Hyuk Kim, Jong-Seok Lee", "title": "SRZoo: An integrated repository for super-resolution using deep learning", "comments": "Accepted in ICASSP 2020, code available at\n  https://github.com/idearibosome/srzoo", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9054533", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based image processing algorithms, including image\nsuper-resolution methods, have been proposed with significant improvement in\nperformance in recent years. However, their implementations and evaluations are\ndispersed in terms of various deep learning frameworks and various evaluation\ncriteria. In this paper, we propose an integrated repository for the\nsuper-resolution tasks, named SRZoo, to provide state-of-the-art\nsuper-resolution models in a single place. Our repository offers not only\nconverted versions of existing pre-trained models, but also documentation and\ntoolkits for converting other models. In addition, SRZoo provides\nplatform-agnostic image reconstruction tools to obtain super-resolved images\nand evaluate the performance in place. It also brings the opportunity of\nextension to advanced image-based researches and other image processing models.\nThe software, documentation, and pre-trained models are publicly available on\nGitHub.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 01:46:09 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Choi", "Jun-Ho", ""], ["Kim", "Jun-Hyuk", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2006.01780", "submitter": "Rahat Yeasin Emon", "authors": "Rahat Yeasin Emon", "title": "A Novel Nudity Detection Algorithm for Web and Mobile Application\n  Development", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our current web and mobile application development runtime nude image\ncontent detection is very important. This paper presents a runtime nudity\ndetection method for web and mobile application development. We use two\nparameters to detect the nude content of an image. One is the number of skin\npixels another is face region. A skin color model based on RGB, HSV color\nspaces are used to detect skin pixels in an image. Google vision api is used to\ndetect the face region. By the percentage of skin regions and face regions an\nimage is identified nude or not. The success of this algorithm exists in\ndetecting skin regions and face regions. The skin detection algorithm can\ndetect skin 95% accurately with a low false-positive rate and the google vision\napi for web and mobile applications can detect face 99% accurately with less\nthan 1 second time. From the experimental analysis, we have seen that the\nproposed algorithm can detect 95% percent accurately the nudity of an image.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:00:47 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 15:29:09 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Emon", "Rahat Yeasin", ""]]}, {"id": "2006.01939", "submitter": "Chinmaya Patnayak", "authors": "Chinmaya Patnayak, Pradipta Roy, Bibekanand Patnaik", "title": "A New Chaos and Permutation Based Algorithm for Image and Video\n  Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images and video sequences carry large volumes of highly correlated and\nredundant data. Applications like military and telecommunication require\nencryption methods to protect the data from unwanted access. This requirement\nin most cases needs to be realized in real-time. In this paper, we propose a\nfast new Fiestal-structured approach for image and video encryption based on a\nchaotic random sequence generator and a Permutation-Inverse Permutation (PIP)\npixel transform. This approach utilizes mathematical functions and transforms\nwith low complexity. The algorithm at the same time, ensures no drastic pay off\nin terms of encryption quality. This renders the algorithm with promising scope\nfor real time applications and easy hardware implementation. MATLAB simulation\nof the algorithm establishes its high quality of encryption in terms of\nelevated entropy values and negligible correlation of the encrypted data with\nthe original. Simulation results also show high sensitivity to slight variation\nin keys ensuring high security.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:54:39 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Patnayak", "Chinmaya", ""], ["Roy", "Pradipta", ""], ["Patnaik", "Bibekanand", ""]]}, {"id": "2006.02434", "submitter": "Mohammad Rajiur Rahman", "authors": "Mohammad Rajiur Rahman, Jaspal Subhlok and Shishir Shah", "title": "Visual Summarization of Lecture Video Segments for Enhanced Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lecture videos are an increasingly important learning resource for higher\neducation. However, the challenge of quickly finding the content of interest in\na lecture video is an important limitation of this format. This paper\nintroduces visual summarization of lecture video segments to enhance\nnavigation. A lecture video is divided into segments based on the\nframe-to-frame similarity of content. The user navigates the lecture video\ncontent by viewing a single frame visual and textual summary of each segment.\nThe paper presents a novel methodology to generate the visual summary of a\nlecture video segment by computing similarities between images extracted from\nthe segment and employing a graph-based algorithm to identify the subset of\nmost representative images. The results from this research are integrated into\na real-world lecture video management portal called Videopoints. To collect\nground truth for evaluation, a survey was conducted where multiple users\nmanually provided visual summaries for 40 lecture video segments. The users\nalso stated whether any images were not selected for the summary because they\nwere similar to other selected images. The graph based algorithm for\nidentifying summary images achieves 78% precision and 72% F1-measure with\nfrequently selected images as the ground truth, and 94% precision and 72%\nF1-measure with the union of all user selected images as the ground truth. For\n98% of algorithm selected visual summary images, at least one user also\nselected that image for their summary or considered it similar to another image\nthey selected. Over 65% of automatically generated summaries were rated as good\nor very good by the users on a 4-point scale from poor to very good. Overall,\nthe results establish that the methodology introduced in this paper produces\ngood quality visual summaries that are practically useful for lecture video\nnavigation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:53:54 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Rahman", "Mohammad Rajiur", ""], ["Subhlok", "Jaspal", ""], ["Shah", "Shishir", ""]]}, {"id": "2006.03714", "submitter": "Alireza Javaheri", "authors": "Alireza Javaheri, Catarina Brites, Fernando Pereira and Jo\\~ao Ascenso", "title": "Improving PSNR-based Quality Metrics Performance For Point Cloud\n  Geometry", "comments": "This article is accepted in 27th International Conference on Image\n  Processing (ICIP 2020)", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9191233", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increased interest in immersive applications has drawn attention to\nemerging 3D imaging representation formats, notably light fields and point\nclouds (PCs). Nowadays, PCs are one of the most popular 3D media formats, due\nto recent developments in PC acquisition, namely with new depth sensors and\nsignal processing algorithms. To obtain high fidelity 3D representations of\nvisual scenes a huge amount of PC data is typically acquired, which demands\nefficient compression solutions. As in 2D media formats, the final perceived PC\nquality plays an important role in the overall user experience and, thus,\nobjective metrics capable to measure the PC quality in a reliable way are\nessential. In this context, this paper proposes and evaluates a set of\nobjective quality metrics for the geometry component of PC data, which plays a\nvery important role in the final perceived quality. Based on the popular PSNR\nPC geometry quality metric, the novel improved PSNR-based metrics are proposed\nby exploiting the intrinsic PC characteristics and the rendering process that\nmust occur before visualization. The experimental results show the superiority\nof the best-proposed metrics over the state-of-the-art, obtaining an\nimprovement of up to 32% in the Pearson correlation coefficient.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 22:10:41 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Javaheri", "Alireza", ""], ["Brites", "Catarina", ""], ["Pereira", "Fernando", ""], ["Ascenso", "Jo\u00e3o", ""]]}, {"id": "2006.03898", "submitter": "Sachin Singh", "authors": "Sachin Singh, Victor Sanchez and Tanaya Guha", "title": "Ensemble Network for Ranking Images Based on Visual Appeal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational framework for ranking images (group photos in\nparticular) taken at the same event within a short time span. The ranking is\nexpected to correspond with human perception of overall appeal of the images.\nWe hypothesize and provide evidence through subjective analysis that the\nfactors that appeal to humans are its emotional content, aesthetics and image\nquality. We propose a network which is an ensemble of three information\nchannels, each predicting a score corresponding to one of the three visual\nappeal factors. For group emotion estimation, we propose a convolutional neural\nnetwork (CNN) based architecture for predicting group emotion from images. This\nnew architecture enforces the network to put emphasis on the important regions\nin the images, and achieves comparable results to the state-of-the-art. Next,\nwe develop a network for the image ranking task that combines group emotion,\naesthetics and image quality scores. Owing to the unavailability of suitable\ndatabases, we created a new database of manually annotated group photos taken\nduring various social events. We present experimental results on this database\nand other benchmark databases whenever available. Overall, our experiments show\nthat the proposed framework can reliably predict the overall appeal of images\nwith results closely corresponding to human ranking.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 15:51:38 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Singh", "Sachin", ""], ["Sanchez", "Victor", ""], ["Guha", "Tanaya", ""]]}, {"id": "2006.03903", "submitter": "Flavio Bertini", "authors": "Flavio Bertini, Rajesh Sharma, Danilo Montesi", "title": "Are Social Networks Watermarking Us or Are We (Unawarely) Watermarking\n  Ourself?", "comments": "43 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, Social Networks (SNs) have deeply changed many aspects of\nsociety, and one of the most widespread behaviours is the sharing of pictures.\nHowever, malicious users often exploit shared pictures to create fake profiles\nleading to the growth of cybercrime. Thus, keeping in mind this scenario,\nauthorship attribution and verification through image watermarking techniques\nare becoming more and more important. In this paper, firstly, we investigate\nhow 13 most popular SNs treat the uploaded pictures, in order to identify a\npossible implementation of image watermarking techniques by respective SNs.\nSecondly, on these 13 SNs, we test the robustness of several image watermarking\nalgorithms. Finally, we verify whether a method based on the Photo-Response\nNon-Uniformity (PRNU) technique can be successfully used as a watermarking\napproach for authorship attribution and verification of pictures on SNs. The\nproposed method is robust enough in spite of the fact that the pictures get\ndowngraded during the uploading process by SNs. The results of our analysis on\na real dataset of 8,400 pictures show that the proposed method is more\neffective than other watermarking techniques and can help to address serious\nquestions about privacy and security on SNs.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 16:08:24 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bertini", "Flavio", ""], ["Sharma", "Rajesh", ""], ["Montesi", "Danilo", ""]]}, {"id": "2006.03921", "submitter": "Marcin Plata", "authors": "Marcin Plata, Piotr Syga", "title": "Robust watermarking with double detector-discriminator approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel deep framework for a watermarking - a\ntechnique of embedding a transparent message into an image in a way that allows\nretrieving the message from a (perturbed) copy, so that copyright infringement\ncan be tracked. For this technique, it is essential to extract the information\nfrom the image even after imposing some digital processing operations on it.\nOur framework outperforms recent methods in the context of robustness against\nnot only spectrum of attacks (e.g. rotation, resizing, Gaussian smoothing) but\nalso against compression, especially JPEG. The bit accuracy of our method is at\nleast 0.86 for all types of distortions. We also achieved 0.90 bit accuracy for\nJPEG while recent methods provided at most 0.83. Our method retains high\ntransparency and capacity as well. Moreover, we present our double\ndetector-discriminator approach - a scheme to detect and discriminate if the\nimage contains the embedded message or not, which is crucial for real-life\nwatermarking systems and up to now was not investigated using neural networks.\nWith this, we design a testing formula to validate our extended approach and\ncompared it with a common procedure. We also present an alternative method of\nbalancing between image quality and robustness on attacks which is easily\napplicable to the framework.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 17:15:45 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Plata", "Marcin", ""], ["Syga", "Piotr", ""]]}, {"id": "2006.05117", "submitter": "Huaizheng Zhang", "authors": "Huaizheng Zhang, Yuanming Li, Qiming Ai, Yong Luo, Yonggang Wen,\n  Yichao Jin and Nguyen Binh Duong Ta", "title": "Hysia: Serving DNN-Based Video-to-Retail Applications in Cloud", "comments": "4 pages, 4 figures", "journal-ref": "In Proceedings of the 28th ACM International Conference on\n  Multimedia (2020) 4457-4460", "doi": "10.1145/3394171.3414536", "report-no": null, "categories": "cs.MM cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining \\underline{v}ideo streaming and online \\underline{r}etailing (V2R)\nhas been a growing trend recently. In this paper, we provide practitioners and\nresearchers in multimedia with a cloud-based platform named Hysia for easy\ndevelopment and deployment of V2R applications. The system consists of: 1) a\nback-end infrastructure providing optimized V2R related services including data\nengine, model repository, model serving and content matching; and 2) an\napplication layer which enables rapid V2R application prototyping. Hysia\naddresses industry and academic needs in large-scale multimedia by: 1)\nseamlessly integrating state-of-the-art libraries including NVIDIA video SDK,\nFacebook faiss, and gRPC; 2) efficiently utilizing GPU computation; and 3)\nallowing developers to bind new models easily to meet the rapidly changing deep\nlearning (DL) techniques. On top of that, we implement an orchestrator for\nfurther optimizing DL model serving performance. Hysia has been released as an\nopen source project on GitHub, and attracted considerable attention. We have\npublished Hysia to DockerHub as an official image for seamless integration and\ndeployment in current cloud environments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 08:45:53 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhang", "Huaizheng", ""], ["Li", "Yuanming", ""], ["Ai", "Qiming", ""], ["Luo", "Yong", ""], ["Wen", "Yonggang", ""], ["Jin", "Yichao", ""], ["Ta", "Nguyen Binh Duong", ""]]}, {"id": "2006.06165", "submitter": "David Shamma", "authors": "David A. Shamma and Tony Dunnigan and Lyndon Kennedy", "title": "Automatic Photo to Ideophone Manga Matching", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo applications offer tools for annotation via text and stickers.\nIdeophones, mimetic and onomatopoeic words, which are common in graphic novels,\nhave yet to be explored for photo annotation use. We present a method for\nautomatic ideophone recommendation and positioning of the text on photos. These\nannotations are accomplished by obtaining a list of ideophones with English\ndefinitions and applying a suite of visual object detectors to the image. Next,\na semantic embedding maps the visual objects to the possible relevant\nideophones. Our system stands in contrast to traditional computer vision-based\nannotation systems, which stop at recommending object and scene-level\nannotation, by providing annotations that are communicative, fun, and engaging.\nWe test these annotations in Japanese and find they carry a strong preference\nand increase enjoyment and sharing likelihood when compared to unannotated and\nobject-based annotated photos.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 03:01:43 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Shamma", "David A.", ""], ["Dunnigan", "Tony", ""], ["Kennedy", "Lyndon", ""]]}, {"id": "2006.06392", "submitter": "Luka Murn", "authors": "Luka Murn, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta\n  Mrak", "title": "Interpreting CNN for Low Complexity Learned Sub-pixel Motion\n  Compensation in Video Coding", "comments": "27th IEEE International Conference on Image Processing, 25-28 Oct\n  2020, Abu Dhabi, United Arab Emirates", "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP),\n  2020, pp. 798-802", "doi": "10.1109/ICIP40778.2020.9191193", "report-no": null, "categories": "eess.IV cs.CC cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning has shown great potential in image and video compression tasks.\nHowever, it brings bit savings at the cost of significant increases in coding\ncomplexity, which limits its potential for implementation within practical\napplications. In this paper, a novel neural network-based tool is presented\nwhich improves the interpolation of reference samples needed for fractional\nprecision motion compensation. Contrary to previous efforts, the proposed\napproach focuses on complexity reduction achieved by interpreting the\ninterpolation filters learned by the networks. When the approach is implemented\nin the Versatile Video Coding (VVC) test model, up to 4.5% BD-rate saving for\nindividual sequences is achieved compared with the baseline VVC, while the\ncomplexity of learned interpolation is significantly reduced compared to the\napplication of full neural network.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 13:10:20 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Murn", "Luka", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""], ["Mrak", "Marta", ""]]}, {"id": "2006.07186", "submitter": "Susanne M Hoffmann", "authors": "Susanne M Hoffmann", "title": "The genesis of Hippachus' celestial globe", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": "10.5281/zenodo.1477980", "report-no": null, "categories": "physics.hist-ph astro-ph.IM cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarises briefly and in English some of the results of the book\nHoffmann: Hipparchs Himmelsglobus, Springer, 2017 that had to be written in\nGerman. The globe of Hipparchus is not preserved. For that reason, it has been\na source of much speculation and scientific inquiry during the last few\ncenturies. This study presents a new analysis of the data given in the\ncommentary on Aratus' poem by Hipparchus, in comparison with other contemporary\nBabylonian and Greek astronomical data, as well as their predecessors in the\nfirst millennium and their successors up to Ptolemy. The result of all these\nstudies are the following: i) although the data of Ptolemy and Hipparchus are\nundoubtedly correlated, it is certainly also wrong to accuse Ptolemy having\nsimply copied and transformed it without correct citation; ii) although\nHipparchus presumably observed most of his star catalogue with his own\ninstruments, we cannot neglect Babylonian influences. Hipparchus was educated\nin Greek astronomy but, in his time, there are traces of Babylonian influences\nsince at least two centuries. Since we are unable to definitely prove that\nHipparchus used Babylonian data, we are not sure if there are direct Babylonian\ninfluences in his time or as a consequence of his education only. Finally, we\npresent a virtual 3D-image showing what the globe of Hipparchus might have\nlooked like.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:47:20 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Hoffmann", "Susanne M", ""]]}, {"id": "2006.08315", "submitter": "Ruixiang Tang", "authors": "Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu, Na Zou, Xia Hu", "title": "Mitigating Gender Bias in Captioning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has made substantial progress with huge supporting image\ncollections sourced from the web. However, recent studies have pointed out that\ncaptioning datasets, such as COCO, contain gender bias found in web corpora. As\na result, learning models could heavily rely on the learned priors and image\ncontext for gender identification, leading to incorrect or even offensive\nerrors. To encourage models to learn correct gender features, we reorganize the\nCOCO dataset and present two new splits COCO-GB V1 and V2 datasets where the\ntrain and test sets have different gender-context joint distribution. Models\nrelying on contextual cues will suffer from huge gender prediction errors on\nthe anti-stereotypical test data. Benchmarking experiments reveal that most\ncaptioning models learn gender bias, leading to high gender prediction errors,\nespecially for women. To alleviate the unwanted bias, we propose a new Guided\nAttention Image Captioning model (GAIC) which provides self-guidance on visual\nattention to encourage the model to capture correct gender visual evidence.\nExperimental results validate that GAIC can significantly reduce gender\nprediction errors with a competitive caption quality. Our codes and the\ndesigned benchmark datasets are available at\nhttps://github.com/datamllab/Mitigating_Gender_Bias_In_Captioning_System.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:16:19 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:27:12 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 22:01:37 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2021 15:49:55 GMT"}, {"version": "v5", "created": "Fri, 22 Jan 2021 03:09:07 GMT"}, {"version": "v6", "created": "Mon, 15 Feb 2021 08:21:06 GMT"}, {"version": "v7", "created": "Tue, 20 Apr 2021 21:48:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Tang", "Ruixiang", ""], ["Du", "Mengnan", ""], ["Li", "Yuening", ""], ["Liu", "Zirui", ""], ["Zou", "Na", ""], ["Hu", "Xia", ""]]}, {"id": "2006.08322", "submitter": "Ziwei Wang", "authors": "Ziwei Wang, Zi Huang, Yadan Luo, Huimin Lu", "title": "ORD: Object Relationship Discovery for Visual Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advancement of image captioning and visual question answering\nat single-round level, the question of how to generate multi-round dialogue\nabout visual content has not yet been well explored.Existing visual dialogue\nmethods encode the image into a fixed feature vector directly, concatenated\nwith the question and history embeddings to predict the response.Some recent\nmethods tackle the co-reference resolution problem using co-attention mechanism\nto cross-refer relevant elements from the image, history, and the target\nquestion.However, it remains challenging to reason visual relationships, since\nthe fine-grained object-level information is omitted before co-attentive\nreasoning. In this paper, we propose an object relationship discovery (ORD)\nframework to preserve the object interactions for visual dialogue generation.\nSpecifically, a hierarchical graph convolutional network (HierGCN) is proposed\nto retain the object nodes and neighbour relationships locally, and then\nrefines the object-object connections globally to obtain the final graph\nembeddings. A graph attention is further incorporated to dynamically attend to\nthis graph-structured representation at the response reasoning stage. Extensive\nexperiments have proved that the proposed method can significantly improve the\nquality of dialogue by utilising the contextual information of visual\nrelationships. The model achieves superior performance over the\nstate-of-the-art methods on the Visual Dialog dataset, increasing MRR from\n0.6222 to 0.6447, and recall@1 from 48.48% to 51.22%.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:25:40 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Ziwei", ""], ["Huang", "Zi", ""], ["Luo", "Yadan", ""], ["Lu", "Huimin", ""]]}, {"id": "2006.08335", "submitter": "Bofan Xue", "authors": "Bofan Xue, David Chan, John Canny", "title": "A Dataset and Benchmarks for Multimedia Social Analysis", "comments": "Published as a workshop paper at \"Multimodality Learning\" (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new publicly available dataset with the goal of advancing\nmulti-modality learning by offering vision and language data within the same\ncontext. This is achieved by obtaining data from a social media website with\nposts containing multiple paired images/videos and text, along with comment\ntrees containing images/videos and/or text. With a total of 677k posts, 2.9\nmillion post images, 488k post videos, 1.4 million comment images, 4.6 million\ncomment videos, and 96.9 million comments, data from different modalities can\nbe jointly used to improve performances for a variety of tasks such as image\ncaptioning, image classification, next frame prediction, sentiment analysis,\nand language modeling. We present a wide range of statistics for our dataset.\nFinally, we provide baseline performance analysis for one of the regression\ntasks using pre-trained models and several fully connected networks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 11:33:01 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Xue", "Bofan", ""], ["Chan", "David", ""], ["Canny", "John", ""]]}, {"id": "2006.08521", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Xinchen Du, Vincent Karas, Stefan M\\\"uller, Bj\\\"orn W.\n  Schuller", "title": "Domain Adaptation with Joint Learning for Generic, Optical Car Part\n  Recognition and Detection Systems (Go-CaRD)", "comments": "Demonstration and instructions to obtain data and models:\n  https://github.com/lstappen/GoCarD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for the automatic recognition and detection of automotive parts are\ncrucial in several emerging research areas in the development of intelligent\nvehicles. They enable, for example, the detection and modelling of interactions\nbetween human and the vehicle. In this paper, we quantitatively and\nqualitatively explore the efficacy of deep learning architectures for the\nclassification and localisation of 29 interior and exterior vehicle regions on\nthree novel datasets. Furthermore, we experiment with joint and transfer\nlearning approaches across datasets and point out potential applications of our\nsystems. Our best network architecture achieves an F1 score of 93.67 % for\nrecognition, while our best localisation approach utilising state-of-the-art\nbackbone networks achieve a mAP of 63.01 % for detection. The MuSe-CAR-Part\ndataset, which is based on a large variety of human-car interactions in videos,\nthe weights of the best models, and the code is publicly available to academic\nparties for benchmarking and future research.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:28:53 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 21:23:49 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Stappen", "Lukas", ""], ["Du", "Xinchen", ""], ["Karas", "Vincent", ""], ["M\u00fcller", "Stefan", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2006.09199", "submitter": "Andrew Rouditchenko", "authors": "Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj\n  Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio\n  Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, James Glass", "title": "AVLnet: Learning Audio-Visual Language Representations from\n  Instructional Videos", "comments": "A version of this work has been accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:38:03 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 18:44:50 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Rouditchenko", "Andrew", ""], ["Boggust", "Angie", ""], ["Harwath", "David", ""], ["Chen", "Brian", ""], ["Joshi", "Dhiraj", ""], ["Thomas", "Samuel", ""], ["Audhkhasi", "Kartik", ""], ["Kuehne", "Hilde", ""], ["Panda", "Rameswar", ""], ["Feris", "Rogerio", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Torralba", "Antonio", ""], ["Glass", "James", ""]]}, {"id": "2006.09208", "submitter": "Hana Alghamdi", "authors": "Hana Alghamdi and Rozenn Dahyot", "title": "Iterative Nadaraya-Watson Distribution Transfer for Colour Grading", "comments": "6 pages, 6 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:2005.09015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method with Nadaraya-Watson that maps one N-dimensional\ndistribution to another taking into account available information about\ncorrespondences. We extend the 2D/3D problem to higher dimensions by encoding\noverlapping neighborhoods of data points and solve the high dimensional problem\nin 1D space using an iterative projection approach. To show potentials of this\nmapping, we apply it to colour transfer between two images that exhibit\noverlapped scene. Experiments show quantitative and qualitative improvements\nover previous state of the art colour transfer methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 00:14:03 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Alghamdi", "Hana", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "2006.09243", "submitter": "Kunal Swami", "authors": "Kunal Swami, Prasanna Vishnu Bondada, Pankaj Kumar Bajpai", "title": "AcED: Accurate and Edge-consistent Monocular Depth Estimation", "comments": "Accepted in IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image depth estimation is a challenging problem. The current\nstate-of-the-art method formulates the problem as that of ordinal regression.\nHowever, the formulation is not fully differentiable and depth maps are not\ngenerated in an end-to-end fashion. The method uses a na\\\"ive threshold\nstrategy to determine per-pixel depth labels, which results in significant\ndiscretization errors. For the first time, we formulate a fully differentiable\nordinal regression and train the network in end-to-end fashion. This enables us\nto include boundary and smoothness constraints in the optimization function,\nleading to smooth and edge-consistent depth maps. A novel per-pixel confidence\nmap computation for depth refinement is also proposed. Extensive evaluation of\nthe proposed model on challenging benchmarks reveals its superiority over\nrecent state-of-the-art methods, both quantitatively and qualitatively.\nAdditionally, we demonstrate practical utility of the proposed method for\nsingle camera bokeh solution using in-house dataset of challenging real-life\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 15:21:00 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Swami", "Kunal", ""], ["Bondada", "Prasanna Vishnu", ""], ["Bajpai", "Pankaj Kumar", ""]]}, {"id": "2006.09833", "submitter": "Hao Hao Tan", "authors": "Hao Hao Tan, Yin-Jyun Luo, Dorien Herremans", "title": "Generative Modelling for Controllable Audio Synthesis of Expressive\n  Piano Performance", "comments": null, "journal-ref": "Published at ICML Workshop on Machine Learning for Media Discovery\n  Workshop (ML4MD) 2020", "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a controllable neural audio synthesizer based on Gaussian Mixture\nVariational Autoencoders (GM-VAE), which can generate realistic piano\nperformances in the audio domain that closely follows temporal conditions of\ntwo essential style features for piano performances: articulation and dynamics.\nWe demonstrate how the model is able to apply fine-grained style morphing over\nthe course of synthesizing the audio. This is based on conditions which are\nlatent variables that can be sampled from the prior or inferred from other\npieces. One of the envisioned use cases is to inspire creative and brand new\ninterpretations for existing pieces of piano music.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 12:54:41 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 03:44:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tan", "Hao Hao", ""], ["Luo", "Yin-Jyun", ""], ["Herremans", "Dorien", ""]]}, {"id": "2006.10260", "submitter": "Madhawa Vidanapathirana", "authors": "Madhawa Vidanapathirana, Supriya Pandhre, Sonia Raychaudhuri, Anjali\n  Khurana", "title": "Video Moment Localization using Object Evidence and Reverse Captioning", "comments": "7 pages. 6 figures. For source code, refer\n  https://github.com/madhawav/MML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of language-based temporal localization of moments in\nuntrimmed videos. Compared to temporal localization with fixed categories, this\nproblem is more challenging as the language-based queries have no predefined\nactivity classes and may also contain complex descriptions. Current\nstate-of-the-art model MAC addresses it by mining activity concepts from both\nvideo and language modalities. This method encodes the semantic activity\nconcepts from the verb/object pair in a language query and leverages visual\nactivity concepts from video activity classification prediction scores. We\npropose \"Multi-faceted VideoMoment Localizer\" (MML), an extension of MAC model\nby the introduction of visual object evidence via object segmentation masks and\nvideo understanding features via video captioning. Furthermore, we improve\nlanguage modelling in sentence embedding. We experimented on Charades-STA\ndataset and identified that MML outperforms MAC baseline by 4.93% and 1.70% on\nR@1 and R@5metrics respectively. Our code and pre-trained model are publicly\navailable at https://github.com/madhawav/MML.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 03:45:49 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Vidanapathirana", "Madhawa", ""], ["Pandhre", "Supriya", ""], ["Raychaudhuri", "Sonia", ""], ["Khurana", "Anjali", ""]]}, {"id": "2006.10553", "submitter": "Elad Liebman", "authors": "Elad Liebman and Peter Stone", "title": "Artificial Musical Intelligence: A Survey", "comments": "99 pages, 5 figures, preprint: currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers have been used to analyze and create music since they were first\nintroduced in the 1950s and 1960s. Beginning in the late 1990s, the rise of the\nInternet and large scale platforms for music recommendation and retrieval have\nmade music an increasingly prevalent domain of machine learning and artificial\nintelligence research. While still nascent, several different approaches have\nbeen employed to tackle what may broadly be referred to as \"musical\nintelligence.\" This article provides a definition of musical intelligence,\nintroduces a taxonomy of its constituent components, and surveys the wide range\nof AI methods that can be, and have been, brought to bear in its pursuit, with\na particular emphasis on machine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 04:46:32 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Liebman", "Elad", ""], ["Stone", "Peter", ""]]}, {"id": "2006.10884", "submitter": "Nitish Nag", "authors": "Dhruv Upadhyay, Vaibhav Pandey, Nitish Nag, Ramesh Jain", "title": "N=1 Modelling of Lifestyle Impact on SleepPerformance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep is critical to leading a healthy lifestyle. Each day, most people go to\nsleep without any idea about how their night's rest is going to be. For an\nactivity that humans spend around a third of their life doing, there is a\nsurprising amount of mystery around it. Despite current research, creating\npersonalized sleep models in real-world settings has been challenging. Existing\nliterature provides several connections between daily activities and sleep\nquality. Unfortunately, these insights do not generalize well in many\nindividuals. For these reasons, it is important to create a personalized sleep\nmodel. This research proposes a sleep model that can identify causal\nrelationships between daily activities and sleep quality and present the user\nwith specific feedback about how their lifestyle affects their sleep. Our\nmethod uses N-of-1 experiments on longitudinal user data and event mining to\ngenerate understanding between lifestyle choices (exercise, eating, circadian\nrhythm) and their impact on sleep quality. Our experimental results identified\nand quantified relationships while extracting confounding variables through a\ncausal framework. These insights can be used by the user or a personal health\nnavigator to provide guidance in improving sleep.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 22:43:35 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Upadhyay", "Dhruv", ""], ["Pandey", "Vaibhav", ""], ["Nag", "Nitish", ""], ["Jain", "Ramesh", ""]]}, {"id": "2006.11161", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, John Britto, and M. Mani Roja", "title": "iSeeBetter: Spatio-temporal video super-resolution using recurrent\n  generative back-projection networks", "comments": "11 pages, 6 figures, 4 tables, Project Page:\n  https://iseebetter.amanchadha.com/", "journal-ref": "Springer Journal of Computational Visual Media, Tsinghua\n  University Press, 6(3):307-317, 2020", "doi": "10.1007/s41095-020-0175-7", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning-based models have enhanced the performance of single-image\nsuper-resolution (SISR). However, applying SISR successively to each video\nframe leads to a lack of temporal coherency. Convolutional neural networks\n(CNNs) outperform traditional approaches in terms of image quality metrics such\nas peak signal to noise ratio (PSNR) and structural similarity (SSIM). However,\ngenerative adversarial networks (GANs) offer a competitive advantage by being\nable to mitigate the issue of a lack of finer texture details, usually seen\nwith CNNs when super-resolving at large upscaling factors. We present\niSeeBetter, a novel GAN-based spatio-temporal approach to video\nsuper-resolution (VSR) that renders temporally consistent super-resolution\nvideos. iSeeBetter extracts spatial and temporal information from the current\nand neighboring frames using the concept of recurrent back-projection networks\nas its generator. Furthermore, to improve the \"naturality\" of the\nsuper-resolved image while eliminating artifacts seen with traditional\nalgorithms, we utilize the discriminator from super-resolution generative\nadversarial network (SRGAN). Although mean squared error (MSE) as a primary\nloss-minimization objective improves PSNR/SSIM, these metrics may not capture\nfine details in the image resulting in misrepresentation of perceptual quality.\nTo address this, we use a four-fold (MSE, perceptual, adversarial, and\ntotal-variation (TV)) loss function. Our results demonstrate that iSeeBetter\noffers superior VSR fidelity and surpasses state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 01:36:30 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:34:00 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 21:38:05 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 00:45:38 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Chadha", "Aman", ""], ["Britto", "John", ""], ["Roja", "M. Mani", ""]]}, {"id": "2006.11284", "submitter": "Omid Jafari", "authors": "Omid Jafari, Parth Nagarkar, Jonathan Monta\\~no", "title": "Improving Locality Sensitive Hashing by Efficiently Finding Projected\n  Nearest Neighbors", "comments": "arXiv admin note: text overlap with arXiv:2003.06415", "journal-ref": "SISAP 2020. Lecture Notes in Computer Science, vol 12440.\n  Springer, Cham", "doi": "10.1007/978-3-030-60936-8_25", "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search in high-dimensional spaces is an important task for many\nmultimedia applications. Due to the notorious curse of dimensionality,\napproximate nearest neighbor techniques are preferred over exact searching\ntechniques since they can return good enough results at a much better speed.\nLocality Sensitive Hashing (LSH) is a very popular random hashing technique for\nfinding approximate nearest neighbors. Existing state-of-the-art Locality\nSensitive Hashing techniques that focus on improving performance of the overall\nprocess, mainly focus on minimizing the total number of IOs while sacrificing\nthe overall processing time. The main time-consuming process in LSH techniques\nis the process of finding neighboring points in projected spaces. We present a\nnovel index structure called radius-optimized Locality Sensitive Hashing\n(roLSH). With the help of sampling techniques and Neural Networks, we present\ntwo techniques to find neighboring points in projected spaces efficiently,\nwithout sacrificing the accuracy of the results. Our extensive experimental\nanalysis on real datasets shows the performance benefit of roLSH over existing\nstate-of-the-art LSH techniques.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:46:30 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""], ["Monta\u00f1o", "Jonathan", ""]]}, {"id": "2006.11285", "submitter": "Omid Jafari", "authors": "Omid Jafari, Parth Nagarkar", "title": "Experimental Analysis of Locality Sensitive Hashing Techniques for\n  High-Dimensional Approximate Nearest Neighbor Searches", "comments": "arXiv admin note: text overlap with arXiv:2003.06415", "journal-ref": "ADC 2021. Lecture Notes in Computer Science, vol. 12610. Springer,\n  Cham, pp. 62-73", "doi": "10.1007/978-3-030-69377-0_6", "report-no": null, "categories": "cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding nearest neighbors in high-dimensional spaces is a fundamental\noperation in many multimedia retrieval applications. Exact tree-based indexing\napproaches are known to suffer from the notorious curse of dimensionality for\nhigh-dimensional data. Approximate searching techniques sacrifice some accuracy\nwhile returning good enough results for faster performance. Locality Sensitive\nHashing (LSH) is a very popular technique for finding approximate nearest\nneighbors in high-dimensional spaces. Apart from providing theoretical\nguarantees on the query results, one of the main benefits of LSH techniques is\ntheir good scalability to large datasets because they are external memory\nbased. The most dominant costs for existing LSH techniques are the algorithm\ntime and the index I/Os required to find candidate points. Existing works do\nnot compare both of these dominant costs in their evaluation. In this\nexperimental survey paper, we show the impact of both these costs on the\noverall performance of the LSH technique. We compare three state-of-the-art\ntechniques on four real-world datasets, and show that, in contrast to recent\nworks, C2LSH is still the state-of-the-art algorithm in terms of performance\nwhile achieving similar accuracy as its recent competitors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:57:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""]]}, {"id": "2006.11405", "submitter": "Chongyang Bai", "authors": "Chongyang Bai, Haipeng Chen, Srijan Kumar, Jure Leskovec, V.S.\n  Subrahmanian", "title": "M2P2: Multimodal Persuasion Prediction using Adaptive Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying persuasive speakers in an adversarial environment is a critical\ntask. In a national election, politicians would like to have persuasive\nspeakers campaign on their behalf. When a company faces adverse publicity, they\nwould like to engage persuasive advocates for their position in the presence of\nadversaries who are critical of them. Debates represent a common platform for\nthese forms of adversarial persuasion. This paper solves two problems: the\nDebate Outcome Prediction (DOP) problem predicts who wins a debate while the\nIntensity of Persuasion Prediction (IPP) problem predicts the change in the\nnumber of votes before and after a speaker speaks. Though DOP has been\npreviously studied, we are the first to study IPP. Past studies on DOP fail to\nleverage two important aspects of multimodal data: 1) multiple modalities are\noften semantically aligned, and 2) different modalities may provide diverse\ninformation for prediction. Our M2P2 (Multimodal Persuasion Prediction)\nframework is the first to use multimodal (acoustic, visual, language) data to\nsolve the IPP problem. To leverage the alignment of different modalities while\nmaintaining the diversity of the cues they provide, M2P2 devises a novel\nadaptive fusion learning framework which fuses embeddings obtained from two\nmodules -- an alignment module that extracts shared information between\nmodalities and a heterogeneity module that learns the weights of different\nmodalities with guidance from three separately trained unimodal reference\nmodels. We test M2P2 on the popular IQ2US dataset designed for DOP. We also\nintroduce a new dataset called QPS (from Qipashuo, a popular Chinese debate TV\nshow ) for IPP. M2P2 significantly outperforms 3 recent baselines on both\ndatasets. Our code and QPS dataset can be found at\nhttp://snap.stanford.edu/m2p2/.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:47:24 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bai", "Chongyang", ""], ["Chen", "Haipeng", ""], ["Kumar", "Srijan", ""], ["Leskovec", "Jure", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "2006.11418", "submitter": "Renato J Cintra", "authors": "D. R. Canterle, T. L. T. da Silveira, F. M. Bayer, R. J. Cintra", "title": "A Multiparametric Class of Low-complexity Transforms for Image and Video\n  Coding", "comments": "Fixed Figure 1 and typos in the reference list", "journal-ref": "Signal Processing, Volume 176, November 2020", "doi": "10.1016/j.sigpro.2020.107685", "report-no": null, "categories": "eess.SP cs.CV cs.MM eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete transforms play an important role in many signal processing\napplications, and low-complexity alternatives for classical transforms became\npopular in recent years. Particularly, the discrete cosine transform (DCT) has\nproven to be convenient for data compression, being employed in well-known\nimage and video coding standards such as JPEG, H.264, and the recent high\nefficiency video coding (HEVC). In this paper, we introduce a new class of\nlow-complexity 8-point DCT approximations based on a series of works published\nby Bouguezel, Ahmed and Swamy. Also, a multiparametric fast algorithm that\nencompasses both known and novel transforms is derived. We select the\nbest-performing DCT approximations after solving a multicriteria optimization\nproblem, and submit them to a scaling method for obtaining larger size\ntransforms. We assess these DCT approximations in both JPEG-like image\ncompression and video coding experiments. We show that the optimal DCT\napproximations present compelling results in terms of coding efficiency and\nimage quality metrics, and require only few addition or bit-shifting\noperations, being suitable for low-complexity and low-power systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:56:58 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Canterle", "D. R.", ""], ["da Silveira", "T. L. T.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "2006.11424", "submitter": "Pavan Madhusudana", "authors": "Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan\n  C. Bovik", "title": "Capturing Video Frame Rate Variations via Entropic Differencing", "comments": null, "journal-ref": "IEEE Signal Processing Letters. 27 (2020) 1809-1813", "doi": "10.1109/LSP.2020.3028687", "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High frame rate videos are increasingly getting popular in recent years,\ndriven by the strong requirements of the entertainment and streaming industries\nto provide high quality of experiences to consumers. To achieve the best\ntrade-offs between the bandwidth requirements and video quality in terms of\nframe rate adaptation, it is imperative to understand the effects of frame rate\non video quality. In this direction, we devise a novel statistical entropic\ndifferencing method based on a Generalized Gaussian Distribution model\nexpressed in the spatial and temporal band-pass domains, which measures the\ndifference in quality between reference and distorted videos. The proposed\ndesign is highly generalizable and can be employed when the reference and\ndistorted sequences have different frame rates. Our proposed model correlates\nvery well with subjective scores in the recently proposed LIVE-YT-HFR database\nand achieves state of the art performance when compared with existing\nmethodologies.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 22:16:52 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 01:02:00 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Madhusudana", "Pavan C.", ""], ["Birkbeck", "Neil", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2006.11539", "submitter": "Yijun Quan", "authors": "Yijun Quan and Chang-Tsun Li", "title": "On Addressing the Impact of ISO Speed upon PRNU and Forgery Detection", "comments": "The paper is accepted to IEEE Transactions on Information Forensics\n  and Security with the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo Response Non-Uniformity (PRNU) has been used as a powerful device\nfingerprint for image forgery detection because image forgeries can be revealed\nby finding the absence of the PRNU in the manipulated areas. The correlation\nbetween an image's noise residual with the device's reference PRNU is often\ncompared with a decision threshold to check the existence of the PRNU. A PRNU\ncorrelation predictor is usually used to determine this decision threshold\nassuming the correlation is content-dependent. However, we found that not only\nthe correlation is content-dependent, but it also depends on the camera\nsensitivity setting. \\textit{Camera sensitivity}, commonly known by the name of\n\\textit{ISO speed}, is an important attribute in digital photography. In this\nwork, we will show the PRNU correlation's dependency on ISO speed. Due to such\ndependency, we postulate that a correlation predictor is ISO speed-specific,\ni.e. \\textit{reliable correlation predictions can only be made when a\ncorrelation predictor is trained with images of similar ISO speeds to the image\nin question}. We report the experiments we conducted to validate the postulate.\nIt is realized that in the real-world, information about the ISO speed may not\nbe available in the metadata to facilitate the implementation of our postulate\nin the correlation prediction process. We hence propose a method called\nContent-based Inference of ISO Speeds (CINFISOS) to infer the ISO speed from\nthe image content.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 10:23:54 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Quan", "Yijun", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "2006.11610", "submitter": "Huirong Huang", "authors": "Huirong Huang, Zhiyong Wu, Shiyin Kang, Dongyang Dai, Jia Jia,\n  Tianxiao Fu, Deyi Tuo, Guangzhi Lei, Peng Liu, Dan Su, Dong Yu, Helen Meng", "title": "Speaker Independent and Multilingual/Mixlingual Speech-Driven Talking\n  Head Generation Using Phonetic Posteriorgrams", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating 3D speech-driven talking head has received more and more attention\nin recent years. Recent approaches mainly have following limitations: 1) most\nspeaker-independent methods need handcrafted features that are time-consuming\nto design or unreliable; 2) there is no convincing method to support\nmultilingual or mixlingual speech as input. In this work, we propose a novel\napproach using phonetic posteriorgrams (PPG). In this way, our method doesn't\nneed hand-crafted features and is more robust to noise compared to recent\napproaches. Furthermore, our method can support multilingual speech as input by\nbuilding a universal phoneme space. As far as we know, our model is the first\nto support multilingual/mixlingual speech as input with convincing results.\nObjective and subjective experiments have shown that our model can generate\nhigh quality animations given speech from unseen languages or speakers and be\nrobust to noise.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 16:32:43 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Huang", "Huirong", ""], ["Wu", "Zhiyong", ""], ["Kang", "Shiyin", ""], ["Dai", "Dongyang", ""], ["Jia", "Jia", ""], ["Fu", "Tianxiao", ""], ["Tuo", "Deyi", ""], ["Lei", "Guangzhi", ""], ["Liu", "Peng", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""], ["Meng", "Helen", ""]]}, {"id": "2006.11905", "submitter": "Purva Tendulkar", "authors": "Purva Tendulkar, Abhishek Das, Aniruddha Kembhavi, Devi Parikh", "title": "Feel The Music: Automatically Generating A Dance For An Input Song", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general computational approach that enables a machine to\ngenerate a dance for any input music. We encode intuitive, flexible heuristics\nfor what a 'good' dance is: the structure of the dance should align with the\nstructure of the music. This flexibility allows the agent to discover creative\ndances. Human studies show that participants find our dances to be more\ncreative and inspiring compared to meaningful baselines. We also evaluate how\nperception of creativity changes based on different presentations of the dance.\nOur code is available at https://github.com/purvaten/feel-the-music.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 20:29:50 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 20:11:23 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Tendulkar", "Purva", ""], ["Das", "Abhishek", ""], ["Kembhavi", "Aniruddha", ""], ["Parikh", "Devi", ""]]}, {"id": "2006.12697", "submitter": "Tran Huyen Thi Thanh", "authors": "Huyen T. T. Tran, Nam Pham Ngoc, Truong Cong Thang", "title": "A Study on Impacts of Multiple Factors on Video Qualify of Experience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTTP Adaptive Streaming (HAS) has become a cost-effective means for\nmultimedia delivery nowadays. However, how the quality of experience (QoE) is\njointly affected by 1) varying perceptual quality and 2) interruptions is not\nwell-understood. In this paper, we present the first attempt to quantitatively\nquantify the relative impacts of these factors on the QoE of streaming\nsessions. To achieve this purpose, we first model the impacts of the factors\nusing histograms, which represent the frequency distributions of the individual\nfactors in a session. By using a large dataset, various insights into the\nrelative impacts of these factors are then provided, serving as suggestions to\nimprove the QoE of streaming sessions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 01:53:21 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tran", "Huyen T. T.", ""], ["Ngoc", "Nam Pham", ""], ["Thang", "Truong Cong", ""]]}, {"id": "2006.13125", "submitter": "Tianyi Li", "authors": "Tianyi Li, Mai Xu, Runzhi Tang, Ying Chen and Qunliang Xing", "title": "DeepQTMT: A Deep Learning Approach for Fast QTMT-based CU Partition of\n  Intra-mode VVC", "comments": "14 pages, 10 figures, 7 tables. Published in IEEE Transactions on\n  Image Processing (TIP), 2021", "journal-ref": "in IEEE Transactions on Image Processing, vol. 30, pp. 5377-5390,\n  2021", "doi": "10.1109/TIP.2021.3083447", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Versatile Video Coding (VVC), as the latest standard, significantly improves\nthe coding efficiency over its ancestor standard High Efficiency Video Coding\n(HEVC), but at the expense of sharply increased complexity. In VVC, the\nquad-tree plus multi-type tree (QTMT) structure of coding unit (CU) partition\naccounts for over 97% of the encoding time, due to the brute-force search for\nrecursive rate-distortion (RD) optimization. Instead of the brute-force QTMT\nsearch, this paper proposes a deep learning approach to predict the QTMT-based\nCU partition, for drastically accelerating the encoding process of intra-mode\nVVC. First, we establish a large-scale database containing sufficient CU\npartition patterns with diverse video content, which can facilitate the\ndata-driven VVC complexity reduction. Next, we propose a multi-stage exit CNN\n(MSE-CNN) model with an early-exit mechanism to determine the CU partition, in\naccord with the flexible QTMT structure at multiple stages. Then, we design an\nadaptive loss function for training the MSE-CNN model, synthesizing both the\nuncertain number of split modes and the target on minimized RD cost. Finally, a\nmulti-threshold decision scheme is developed, achieving desirable trade-off\nbetween complexity and RD performance. Experimental results demonstrate that\nour approach can reduce the encoding time of VVC by 44.65%-66.88% with the\nnegligible Bj{\\o}ntegaard delta bit-rate (BD-BR) of 1.322%-3.188%, which\nsignificantly outperforms other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:21:21 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 05:37:34 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 09:06:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Tianyi", ""], ["Xu", "Mai", ""], ["Tang", "Runzhi", ""], ["Chen", "Ying", ""], ["Xing", "Qunliang", ""]]}, {"id": "2006.13608", "submitter": "Shengyu Zhang", "authors": "Shengyu Zhang, Ziqi Tan, Jin Yu, Zhou Zhao, Kun Kuang, Tan Jiang,\n  Jingren Zhou, Hongxia Yang, Fei Wu", "title": "Comprehensive Information Integration Modeling Framework for Video\n  Titling", "comments": "11 pages, 6 figures, to appear in KDD 2020 proceedings", "journal-ref": null, "doi": "10.1145/3394486.3403325", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, consumer-generated videos, which in general deliver consumers'\nindividual preferences for the different aspects of certain products, are\nmassive in volume. To recommend these videos to potential consumers more\neffectively, diverse and catchy video titles are critical. However,\nconsumer-generated videos seldom accompany appropriate titles. To bridge this\ngap, we integrate comprehensive sources of information, including the content\nof consumer-generated videos, the narrative comment sentences supplied by\nconsumers, and the product attributes, in an end-to-end modeling framework.\nAlthough automatic video titling is very useful and demanding, it is much less\naddressed than video captioning. The latter focuses on generating sentences\nthat describe videos as a whole while our task requires the product-aware\nmulti-grained video analysis. To tackle this issue, the proposed method\nconsists of two processes, i.e., granular-level interaction modeling and\nabstraction-level story-line summarization. Specifically, the granular-level\ninteraction modeling first utilizes temporal-spatial landmark cues, descriptive\nwords, and abstractive attributes to builds three individual graphs and\nrecognizes the intra-actions in each graph through Graph Neural Networks (GNN).\nThen the global-local aggregation module is proposed to model inter-actions\nacross graphs and aggregate heterogeneous graphs into a holistic graph\nrepresentation. The abstraction-level story-line summarization further\nconsiders both frame-level video features and the holistic graph to utilize the\ninteractions between products and backgrounds, and generate the story-line\ntopic of the video. We collect a large-scale dataset accordingly from\nreal-world data in Taobao, a world-leading e-commerce platform, and will make\nthe desensitized version publicly available to nourish further development of\nthe research community...\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 10:38:15 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Zhang", "Shengyu", ""], ["Tan", "Ziqi", ""], ["Yu", "Jin", ""], ["Zhao", "Zhou", ""], ["Kuang", "Kun", ""], ["Jiang", "Tan", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""], ["Wu", "Fei", ""]]}, {"id": "2006.14239", "submitter": "Navid Mahmoudian Bidgoli", "authors": "Navid Mahmoudian Bidgoli, Thomas Maugey, Aline Roumy", "title": "Fine granularity access in interactive compression of 360-degree images\n  based on rate-adaptive channel codes", "comments": "accepted to be published in IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2020.3017890", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new interactive compression scheme for\nomnidirectional images. This requires two characteristics: efficient\ncompression of data, to lower the storage cost, and random access ability to\nextract part of the compressed stream requested by the user (for reducing the\ntransmission rate). For efficient compression, data needs to be predicted by a\nseries of references that have been pre-defined and compressed. This contrasts\nwith the spirit of random accessibility. We propose a solution for this problem\nbased on incremental codes implemented by rate-adaptive channel codes. This\nscheme encodes the image while adapting to any user request and leads to an\nefficient coding that is flexible in extracting data depending on the available\ninformation at the decoder. Therefore, only the information that is needed to\nbe displayed at the user's side is transmitted during the user's request, as if\nthe request was already known at the encoder. The experimental results\ndemonstrate that our coder obtains a better transmission rate than the\nstate-of-the-art tile-based methods at a small cost in storage. Moreover, the\ntransmission rate grows gradually with the size of the request and avoids a\nstaircase effect, which shows the perfect suitability of our coder for\ninteractive transmission.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 08:13:48 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 13:45:21 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Bidgoli", "Navid Mahmoudian", ""], ["Maugey", "Thomas", ""], ["Roumy", "Aline", ""]]}, {"id": "2006.14348", "submitter": "Eli Shlizerman", "authors": "Kun Su, Xiulong Liu, Eli Shlizerman", "title": "Audeo: Audio Generation for a Silent Performance Video", "comments": "Please see associated video at\n  https://www.youtube.com/watch?v=8rS3VgjG7_c", "journal-ref": "Advances in neural information processing 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel system that gets as an input video frames of a musician\nplaying the piano and generates the music for that video. Generation of music\nfrom visual cues is a challenging problem and it is not clear whether it is an\nattainable goal at all. Our main aim in this work is to explore the\nplausibility of such a transformation and to identify cues and components able\nto carry the association of sounds with visual events. To achieve the\ntransformation we built a full pipeline named `\\textit{Audeo}' containing three\ncomponents. We first translate the video frames of the keyboard and the\nmusician hand movements into raw mechanical musical symbolic representation\nPiano-Roll (Roll) for each video frame which represents the keys pressed at\neach time step. We then adapt the Roll to be amenable for audio synthesis by\nincluding temporal correlations. This step turns out to be critical for\nmeaningful audio generation. As a last step, we implement Midi synthesizers to\ngenerate realistic music. \\textit{Audeo} converts video to audio smoothly and\nclearly with only a few setup constraints. We evaluate \\textit{Audeo} on `in\nthe wild' piano performance videos and obtain that their generated music is of\nreasonable audio quality and can be successfully recognized with high precision\nby popular music identification software.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:58:59 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Su", "Kun", ""], ["Liu", "Xiulong", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2006.14438", "submitter": "Xiaowei Tang", "authors": "Xiao-Wei Tang, Xin-Lin Huang, Fei Hu", "title": "QoE-Driven UAV-Enabled Pseudo-Analog Wireless Video Broadcast: A Joint\n  Optimization of Power and Trajectory", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2020", "doi": null, "report-no": null, "categories": "cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive demands for high quality mobile video services have caused\nheavy overload to the existing cellular networks. Although the small cell has\nbeen proposed to alleviate such a problem, the network operators may not be\ninterested in deploying numerous base stations (BSs) due to expensive\ninfrastructure construction and maintenance. The unmanned aerial vehicles\n(UAVs) can provide the low-cost and quick deployment, which can support\nhigh-quality line-of-sight communications and have become promising mobile BSs.\nIn this paper, we propose a quality-of-experience (QoE)-driven UAV-enabled\npseudo-analog wireless video broadcast scheme, which provides mobile video\nbroadcast services for ground users (GUs). Due to limited energy available in\nUAV, the aim of the proposed scheme is to maximize the minimum peak\nsignal-to-noise ratio (PSNR) of GUs' video reconstruction quality by jointly\noptimizing the transmission power allocation strategy and the UAV trajectory.\nFirstly, the reconstructed video quality at GUs is defined under the\nconstraints of the UAV's total energy and motion mechanism, and the proposed\nscheme is formulated as a complex non-convex optimization problem. Then, the\noptimization problem is simplified to obtain a tractable suboptimal solution\nwith the help of the block coordinate descent model and the successive convex\napproximation model. Finally, the experimental results are presented to show\nthe effectiveness of the proposed scheme. Specifically, the proposed scheme can\nachieve over 1.6dB PSNR gains in terms of GUs' minimum PSNR, compared with the\nstate-of-the-art schemes, e.g., DVB, SoftCast, and SharpCast.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 14:20:35 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Tang", "Xiao-Wei", ""], ["Huang", "Xin-Lin", ""], ["Hu", "Fei", ""]]}, {"id": "2006.15131", "submitter": "Soumyabrata Dev", "authors": "Ivan Bacher, Hossein Javidnia, Soumyabrata Dev, Rahul Agrahari, Murhaf\n  Hossari, Matthew Nicholson, Clare Conran, Jian Tang, Peng Song, David\n  Corrigan, Fran\\c{c}ois Piti\\'e", "title": "An Advert Creation System for 3D Product Placements", "comments": "Published in Proc. European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the evolution of video-sharing platforms has attracted\na significant amount of investments on contextual advertising. The common\ncontextual advertising platforms utilize the information provided by users to\nintegrate 2D visual ads into videos. The existing platforms face many technical\nchallenges such as ad integration with respect to occluding objects and 3D ad\nplacement. This paper presents a Video Advertisement Placement & Integration\n(Adverts) framework, which is capable of perceiving the 3D geometry of the\nscene and camera motion to blend 3D virtual objects in videos and create the\nillusion of reality. The proposed framework contains several modules such as\nmonocular depth estimation, object segmentation, background-foreground\nseparation, alpha matting and camera tracking. Our experiments conducted using\nAdverts framework indicates the significant potential of this system in\ncontextual ad integration, and pushing the limits of advertising industry using\nmixed reality technologies.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:41:50 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Bacher", "Ivan", ""], ["Javidnia", "Hossein", ""], ["Dev", "Soumyabrata", ""], ["Agrahari", "Rahul", ""], ["Hossari", "Murhaf", ""], ["Nicholson", "Matthew", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Song", "Peng", ""], ["Corrigan", "David", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "2006.15349", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc G\\'orriz, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta\n  Mrak", "title": "Chroma Intra Prediction with attention-based CNN architectures", "comments": "27th IEEE International Conference on Image Processing, 25-28 Oct\n  2020, Abu Dhabi, United Arab Emirates", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CC cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks can be used in video coding to improve chroma\nintra-prediction. In particular, usage of fully-connected networks has enabled\nbetter cross-component prediction with respect to traditional linear models.\nNonetheless, state-of-the-art architectures tend to disregard the location of\nindividual reference samples in the prediction process. This paper proposes a\nnew neural network architecture for cross-component intra-prediction. The\nnetwork uses a novel attention module to model spatial relations between\nreference and predicted samples. The proposed approach is integrated into the\nVersatile Video Coding (VVC) prediction pipeline. Experimental results\ndemonstrate compression gains over the latest VVC anchor compared with\nstate-of-the-art chroma intra-prediction methods based on neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 12:11:17 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["G\u00f3rriz", "Marc", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""], ["Mrak", "Marta", ""]]}, {"id": "2006.15984", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Yang Du and Yuan Ji", "title": "Towards Versatility: Lossless Data Hiding in JPEG Bitstream via\n  Table-irrelevant Code Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing lossless data hiding (LDH) methods for JPEG bitstream embed\ndata by constructing the code mapping between used codes and unused codes.\nPrevious studies only apply to the JPEG bitstream encoded with the standard\nHuffman table, in which unused codes usually exist hence the construction of\ncode mapping can be guaranteed. However, for the JPEG bitstream encoded with\nthe optimized Huffman table, the codes are all used so code mapping is out of\nwork. With this concern in mind, we aim to propose a generic LDH method\napplicable to any JPEG bitstream. To this end, in this paper, we first design a\ncode mapping scheme named the table-irrelevant code mapping (TCM), in which the\nmapped codes are generated rather than selected from the unused codes in the\noriginal bitstream so versatility is achieved. Then, We explicitly formulate\nthe construction of TCM as a combinatorial optimization problem and\nsubsequently solve the problem by leveraging the genetic algorithm.\nConsequently, the high embedding capacity can be achieved and the file size\nexpansion in the marked bitstream is also well alleviated. Finally, we embed\nadditional data and modify the file header to guarantee the marked bitstream is\ncompatible with popular JPEG decoders. Experimental results demonstrate that\nthe proposed method significantly outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:32:16 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 07:31:55 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Du", "Yang", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.16625", "submitter": "In-Jae Yu", "authors": "In-Jae Yu, Wonhyuk Ahn, Seung-Hun Nam, Heung-Kyu Lee", "title": "BitMix: Data Augmentation for Image Steganalysis", "comments": null, "journal-ref": null, "doi": "10.1049/el.2020.1951", "report-no": null, "categories": "eess.IV cs.CR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) for image steganalysis demonstrate better\nperformances with employing concepts from high-level vision tasks. The major\nemployed concept is to use data augmentation to avoid overfitting due to\nlimited data. To augment data without damaging the message embedding, only\nrotating multiples of 90 degrees or horizontally flipping are used in\nsteganalysis, which generates eight fixed results from one sample. To overcome\nthis limitation, we propose BitMix, a data augmentation method for spatial\nimage steganalysis. BitMix mixes a cover and stego image pair by swapping the\nrandom patch and generates an embedding adaptive label with the ratio of the\nnumber of pixels modified in the swapped patch to those in the cover-stego\npair. We explore optimal hyperparameters, the ratio of applying BitMix in the\nmini-batch, and the size of the bounding box for swapping patch. The results\nreveal that using BitMix improves the performance of spatial image steganalysis\nand better than other data augmentation methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:36:21 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yu", "In-Jae", ""], ["Ahn", "Wonhyuk", ""], ["Nam", "Seung-Hun", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "2006.16893", "submitter": "Daniel Berj\\'on", "authors": "Daniel Berj\\'on, Pablo Carballeira, Juli\\'an Cabrera, Carlos Carmona,\n  Daniel Corregidor, C\\'esar D\\'iaz, Francisco Mor\\'an, Narciso Garc\\'ia", "title": "FVV Live: Real-Time, Low-Cost, Free Viewpoint Video", "comments": null, "journal-ref": null, "doi": "10.1109/ICMEW46912.2020.9105977", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FVV Live is a novel real-time, low-latency, end-to-end free viewpoint system\nincluding capture, transmission, synthesis on an edge server and visualization\nand control on a mobile terminal. The system has been specially designed for\nlow-cost and real-time operation, only using off-the-shelf components.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:21:53 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Berj\u00f3n", "Daniel", ""], ["Carballeira", "Pablo", ""], ["Cabrera", "Juli\u00e1n", ""], ["Carmona", "Carlos", ""], ["Corregidor", "Daniel", ""], ["D\u00edaz", "C\u00e9sar", ""], ["Mor\u00e1n", "Francisco", ""], ["Garc\u00eda", "Narciso", ""]]}]