[{"id": "1802.00337", "submitter": "Ivan Martinovi\\'c", "authors": "Ivan Martinovic, Vesna Mandic", "title": "Biomedical Signals Reconstruction Under the Compressive Sensing Approach", "comments": "paper submitted to the 7th Mediterranean Conference on Embedded\n  Computing - MECO'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper analyses the possibility to recover different biomedical signals if\nlimited number of samples is available. Having in mind that monitoring of\nhealth condition is done by measuring and observing key parameters such as\nheart activity through electrocardiogram or anatomy and body processes through\nmagnetic resonance imaging, it is important to keep the quality of the\nreconstructed signal as better as possible. To recover the signal from limited\nset of available coefficients, the Compressive Sensing approach and\noptimization algorithms are used. The theory is verified by the experimental\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 14:23:26 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Martinovic", "Ivan", ""], ["Mandic", "Vesna", ""]]}, {"id": "1802.02308", "submitter": "Ye Yao", "authors": "Ye Yao", "title": "Computer-Aided Annotation for Video Tampering Dataset of Forensic\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The annotation of video tampering dataset is a boring task that takes a lot\nof manpower and financial resources. At present, there is no published\nliterature which is capable to improve the annotation efficiency of forged\nvideos. We presented a computer-aided annotation method for video tampering\ndataset in this paper. This annotation method can be utilized to label the\nframes of forged video sequences. By means of comparing the original video\nframes with the forged video frames, we can locate the position and the\ntrajectory of the forged areas of the forged video frames. Then, we select\nseveral key points on the temporal domain according to the trajectory of the\nforged areas, and mark the forged area of the forged frames in the key point\nwith a mouse. Finally, we use the linear prediction algorithm based on the\ncoordinates of the key positions in the temporal domain to generate the\nannotation information of forged areas in other video frames which without\nmanually labeled. If the bounding box generated by the computer-aided algorithm\ndeviates from the actual location of the forged area, we can use the mouse to\nchange the position of the bounding box during the preview period. This method\ncombines the manual annotation with computer-aided annotation. It solves the\nproblems of the inaccuracy of annotation by computer-aided as well as the low\nefficiency of annotation manually, and meet the needs of annotation for an\nenormous amount of forged videos in the research of video passive forensics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 05:09:13 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Yao", "Ye", ""]]}, {"id": "1802.02668", "submitter": "Yi Zhu", "authors": "Yi Zhu and Xueqing Deng and Shawn Newsam", "title": "Fine-Grained Land Use Classification at the City Scale Using\n  Ground-Level Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform fine-grained land use mapping at the city scale using ground-level\nimages. Mapping land use is considerably more difficult than mapping land cover\nand is generally not possible using overhead imagery as it requires close-up\nviews and seeing inside buildings. We postulate that the growing collections of\ngeoreferenced, ground-level images suggest an alternate approach to this\ngeographic knowledge discovery problem. We develop a general framework that\nuses Flickr images to map 45 different land-use classes for the City of San\nFrancisco. Individual images are classified using a novel convolutional neural\nnetwork containing two streams, one for recognizing objects and another for\nrecognizing scenes. This network is trained in an end-to-end manner directly on\nthe labeled training images. We propose several strategies to overcome the\nnoisiness of our user-generated data including search-based training set\naugmentation and online adaptive training. We derive a ground truth map of San\nFrancisco in order to evaluate our method. We demonstrate the effectiveness of\nour approach through geo-visualization and quantitative analysis. Our framework\nachieves over 29% recall at the individual land parcel level which represents a\nstrong baseline for the challenging 45-way land use classification problem\nespecially given the noisiness of the image data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 23:01:13 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Zhu", "Yi", ""], ["Deng", "Xueqing", ""], ["Newsam", "Shawn", ""]]}, {"id": "1802.02774", "submitter": "Chengming Xu", "authors": "Chengming Xu, Yanwei Fu, Bing Zhang, Zitian Chen, Yu-Gang Jiang,\n  Xiangyang Xue", "title": "Learning to score the figure skating sports videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets at learning to score the figure skating sports videos. To\naddress this task, we propose a deep architecture that includes two\ncomplementary components, i.e., Self-Attentive LSTM and Multi-scale\nConvolutional Skip LSTM. These two components can efficiently learn the local\nand global sequential information in each video. Furthermore, we present a\nlarge-scale figure skating sports video dataset -- FisV dataset. This dataset\nincludes 500 figure skating videos with the average length of 2 minutes and 50\nseconds. Each video is annotated by two scores of nine different referees,\ni.e., Total Element Score(TES) and Total Program Component Score (PCS). Our\nproposed model is validated on FisV and MIT-skate datasets. The experimental\nresults show the effectiveness of our models in learning to score the figure\nskating videos.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 09:53:56 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 03:08:51 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 08:31:18 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Xu", "Chengming", ""], ["Fu", "Yanwei", ""], ["Zhang", "Bing", ""], ["Chen", "Zitian", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1802.03937", "submitter": "Yehuda Dar", "authors": "Yehuda Dar, Michael Elad, and Alfred M. Bruckstein", "title": "Compression for Multiple Reconstructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a method for optimizing the lossy compression for a\nnetwork of diverse reconstruction systems. We focus on adapting a standard\nimage compression method to a set of candidate displays, presenting the\ndecompressed signals to viewers. Each display is modeled as a linear operator\napplied after decompression, and its probability to serve a network user. We\nformulate a complicated operational rate-distortion optimization trading-off\nthe network's expected mean-squared reconstruction error and the compression\nbit-cost. Using the alternating direction method of multipliers (ADMM) we\ndevelop an iterative procedure where the network structure is separated from\nthe compression method, enabling the reliance on standard compression\ntechniques. We present experimental results showing our method to be the best\napproach for adjusting high bit-rate image compression (using the\nstate-of-the-art HEVC standard) to a set of displays modeled as blur\ndegradations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 08:51:04 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Dar", "Yehuda", ""], ["Elad", "Michael", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1802.04668", "submitter": "Akisato Kimura", "authors": "Yusuke Mukuta, Akisato Kimura, David B Adrian, Zoubin Ghahramani", "title": "Weakly supervised collective feature learning from curated media", "comments": "Published in the Proceedings of AAAI Conferenrence on Artificial\n  Intelligence (AAAI2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art in feature learning relies on the supervised\nlearning of large-scale datasets consisting of target content items and their\nrespective category labels. However, constructing such large-scale\nfully-labeled datasets generally requires painstaking manual effort. One\npossible solution to this problem is to employ community contributed text tags\nas weak labels, however, the concepts underlying a single text tag strongly\ndepends on the users. We instead present a new paradigm for learning\ndiscriminative features by making full use of the human curation process on\nsocial networking services (SNSs). During the process of content curation, SNS\nusers collect content items manually from various sources and group them by\ncontext, all for their own benefit. Due to the nature of this process, we can\nassume that (1) content items in the same group share the same semantic concept\nand (2) groups sharing the same images might have related semantic concepts.\nThrough these insights, we can define human curated groups as weak labels from\nwhich our proposed framework can learn discriminative features as a\nrepresentation in the space of semantic concepts the users intended when\ncreating the groups. We show that this feature learning can be formulated as a\nproblem of link prediction for a bipartite graph whose nodes corresponds to\ncontent items and human curated groups, and propose a novel method for feature\nlearning based on sparse coding or network fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 15:05:10 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Mukuta", "Yusuke", ""], ["Kimura", "Akisato", ""], ["Adrian", "David B", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1802.04936", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Esteban Moro, Manuel Cebrian and Iyad Rahwan", "title": "MemeSequencer: Sparse Matching for Embedding Image Macros", "comments": "9 pages (+2 pages references), camera ready version for International\n  World Wide Web Conference (WWW) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the creation, mutation, and propagation of social media\ncontent on the Internet is an essential problem in computational social\nscience, affecting areas ranging from marketing to political mobilization. A\nfirst step towards understanding the evolution of images online is the analysis\nof rapidly modifying and propagating memetic imagery or `memes'. However, a\npitfall in proceeding with such an investigation is the current incapability to\nproduce a robust semantic space for such imagery, capable of understanding\ndifferences in Image Macros. In this study, we provide a first step in the\nsystematic study of image evolution on the Internet, by proposing an algorithm\nbased on sparse representations and deep learning to decouple various types of\ncontent in such images and produce a rich semantic embedding. We demonstrate\nthe benefits of our approach on a variety of tasks pertaining to memes and\nImage Macros, such as image clustering, image retrieval, topic prediction and\nvirality prediction, surpassing the existing methods on each. In addition to\nits utility on quantitative tasks, our method opens up the possibility of\nobtaining the first large-scale understanding of the evolution and propagation\nof memetic imagery.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 02:53:20 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Moro", "Esteban", ""], ["Cebrian", "Manuel", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1802.05114", "submitter": "Danko Petric BSc", "authors": "Danko Petric, Marija Milinkovic", "title": "Comparison between CS and JPEG in terms of image compression", "comments": "Paper sent for 7th Mediterranean Conference on Embedded Computing\n  MECO 2018, Budva, Montenegro", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comparison between two approaches, JPEG and Compressive Sensing, is done\nin the paper. The approaches are compared in terms of image compression.\nComparison is done by measuring the image quality versus number of samples used\nfor image recovering. Images are visually compared. Also, numerical quality\nvalue, PSNR, is calculated and compared for the two approaches. It is shown\nthat images, recovered by using the Compressive Sensing approach, have higher\nPSNR values compared to the images under JPEG compression. Difference is larger\nin grayscale images with small number of details, like e.g. medical images\n(x-ray). The theory is supported by the experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 09:55:16 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Petric", "Danko", ""], ["Milinkovic", "Marija", ""]]}, {"id": "1802.05115", "submitter": "Valentina Konatar", "authors": "Valentina Konatar and Maja Vesovic", "title": "The Hermite and Fourier transforms in sparse reconstruction of\n  sinusoidal signals", "comments": "Student paper submitted to Mediterranean Conference on Embedded\n  Computing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper observes the Hermite and the Fourier Transform domains in terms of\nFrequency Hopping Spread Spectrum signals sparsification. Sparse signals can be\nrecovered from a reduced set of samples by using the Compressive Sensing\napproach. The under-sampling and the reconstruction of those signals are also\nanalyzed in this paper. The number of measurements (available signal samples)\nis varied and reconstruction performance is tested in all considered cases and\nfor both observed domains. The signal recovery is done using an adaptive\ngradient based algorithm. The theory is verified with the experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 19:59:38 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Konatar", "Valentina", ""], ["Vesovic", "Maja", ""]]}, {"id": "1802.05178", "submitter": "Keunwoo Choi Mr", "authors": "Adib Mehrabi, Keunwoo Choi, Simon Dixon, Mark Sandler", "title": "Similarity measures for vocal-based drum sample retrieval using deep\n  convolutional auto-encoders", "comments": "ICASSP 2018 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expressive nature of the voice provides a powerful medium for\ncommunicating sonic ideas, motivating recent research on methods for query by\nvocalisation. Meanwhile, deep learning methods have demonstrated\nstate-of-the-art results for matching vocal imitations to imitated sounds, yet\nlittle is known about how well learned features represent the perceptual\nsimilarity between vocalisations and queried sounds. In this paper, we address\nthis question using similarity ratings between vocal imitations and imitated\ndrum sounds. We use a linear mixed effect regression model to show how features\nlearned by convolutional auto-encoders (CAEs) perform as predictors for\nperceptual similarity between sounds. Our experiments show that CAEs outperform\nthree baseline feature sets (spectrogram-based representations, MFCCs, and\ntemporal features) at predicting the subjective similarity ratings. We also\ninvestigate how the size and shape of the encoded layer effects the predictive\npower of the learned features. The results show that preservation of temporal\ninformation is more important than spectral resolution for this application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:08:09 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Mehrabi", "Adib", ""], ["Choi", "Keunwoo", ""], ["Dixon", "Simon", ""], ["Sandler", "Mark", ""]]}, {"id": "1802.05348", "submitter": "Chuang Ye", "authors": "Chuang Ye, M. Cenk Gursoy, and Senem Velipasalar", "title": "Power Control and Mode Selection for VBR Video Streaming in D2D Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of power control for streaming\nvariable-bit-rate (VBR) videos in a device-to-device (D2D) wireless network. A\nVBR video traffic model that considers video frame sizes and playout buffers at\nthe mobile users is adopted. A setup with one pair of D2D users (DUs) and one\ncellular user (CU) is considered and three modes, namely cellular mode,\ndedicated mode and reuse mode, are employed. Mode selection for the data\ndelivery is determined and the transmit powers of the base station (BS) and\ndevice transmitter are optimized with the goal of maximizing the overall\ntransmission rate while VBR video data can be delivered to the CU and DU\nwithout causing playout buffer underflows or overflows. A low-complexity\nalgorithm is proposed. Through simulations with VBR video traces over fading\nchannels, we demonstrate that video delivery with mode selection and power\ncontrol achieves a better performance than just using a single mode throughout\nthe transmission.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 22:56:20 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Ye", "Chuang", ""], ["Gursoy", "M. Cenk", ""], ["Velipasalar", "Senem", ""]]}, {"id": "1802.05884", "submitter": "Lee Prangnell", "authors": "Lee Prangnell, Miguel Hern\\'andez-Cabronero, Victor Sanchez", "title": "Coding Block-Level Perceptual Video Coding for 4:4:4 Data in HEVC", "comments": "Preprint: 2017 IEEE International Conference on Image Processing\n  (ICIP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing consumer demand for high bit-depth 4:4:4 HD video data\nplayback due to its superior perceptual visual quality compared with standard\n8-bit subsampled 4:2:0 video data. Due to vast file sizes and associated\nbitrates, it is desirable to compress raw high bit-depth 4:4:4 HD video\nsequences as much as possible without incurring a discernible decrease in\nvisual quality. In this paper, we propose a Coding Block (CB)-level perceptual\nvideo coding technique for HEVC named Full Color Perceptual Quantization\n(FCPQ). FCPQ is designed to adjust the Quantization Parameter (QP) at the CB\nlevel (i.e., the luma CB and the chroma Cb and Cr CBs) according to the\nvariances of pixel data in each CB. FCPQ is based on the default perceptual\nquantization method in HEVC called AdaptiveQP. AdaptiveQP adjusts the QP of an\nentire CU based only on the spatial activity of the constituent luma CB. As\ndemonstrated in this paper, by not accounting for the spatial activity of the\nconstituent chroma CBs, as is the case with AdaptiveQP, coding performance can\nbe significantly affected; this is because the variance of pixel data in a luma\nCB is notably different from the variances of pixel data in chroma Cb and Cr\nCBs. FCPQ, therefore, addresses this problem. In terms of coding performance,\nFCPQ achieves BD-Rate improvements of up to 39.5% (Y), 16% (Cb) and 29.9% (Cr)\ncompared with AdaptiveQP.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 10:20:34 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Prangnell", "Lee", ""], ["Hern\u00e1ndez-Cabronero", "Miguel", ""], ["Sanchez", "Victor", ""]]}, {"id": "1802.06057", "submitter": "Zhan Ma", "authors": "Shaowei Xie and Qiu Shen and Yiling Xu and Qiaojian Qian and Shaowei\n  Wang and Zhan Ma and Wenjun Zhang", "title": "Viewport Adaptation-Based Immersive Video Streaming: Perceptual Modeling\n  and Applications", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive video offers the freedom to navigate inside virtualized\nenvironment. Instead of streaming the bulky immersive videos entirely, a\nviewport (also referred to as field of view, FoV) adaptive streaming is\npreferred. We often stream the high-quality content within current viewport,\nwhile reducing the quality of representation elsewhere to save the network\nbandwidth consumption. Consider that we could refine the quality when focusing\non a new FoV, in this paper, we model the perceptual impact of the quality\nvariations (through adapting the quantization stepsize and spatial resolution)\nwith respect to the refinement duration, and yield a product of two closed-form\nexponential functions that well explain the joint quantization and resolution\ninduced quality impact. Analytical model is cross-validated using another set\nof data, where both Pearson and Spearman's rank correlation coefficients are\nclose to 0.98. Our work is devised to optimize the adaptive FoV streaming of\nthe immersive video under limited network resource. Numerical results show that\nour proposed model significantly improves the quality of experience of users,\nwith about 9.36\\% BD-Rate (Bjontegaard Delta Rate) improvement on average as\ncompared to other representative methods, particularly under the limited\nbandwidth.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 18:09:41 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Xie", "Shaowei", ""], ["Shen", "Qiu", ""], ["Xu", "Yiling", ""], ["Qian", "Qiaojian", ""], ["Wang", "Shaowei", ""], ["Ma", "Zhan", ""], ["Zhang", "Wenjun", ""]]}, {"id": "1802.06852", "submitter": "Zeeshan Bhatti", "authors": "Zeeshan Bhatti, Ahsan Abro, Abdul Rehman Gillal, Mostafa Karbasi", "title": "Be-Educated: Multimedia Learning through 3D Animation", "comments": "10 pages, 32 figures", "journal-ref": "INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND EMERGING\n  TECHNOLOGIES,(IJCET)- VOL1(1) DECEMBER 2017- 13-22", "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multimedia learning tools and techniques are placing its importance with\nlarge scale in education sector. With the help of multimedia learning, various\ncomplex phenomenon and theories can be explained and taught easily and\nconveniently. This project aims to teach and spread the importance of education\nand respecting the tools of education: pen, paper, pencil, rubber. To achieve\nthis cognitive learning, a 3D animated movie has been developed using\nprinciples of multimedia learning with 3D cartoon characters resembling the\nactual educational objects, where the buildings have also been modelled to\nresemble real books and diaries. For modelling and animation of these\ncharacters, polygon mesh tools are used in 3D Studio Max. Additionally, the\nfinal composition of video and audio is performed in adobe premiere. This 3D\nanimated video aims to highlight a message of importance for education and\nstationary. The Moral of movie is that do not waste your stationary material,\nuse your Pen and Paper for the purpose they are made for. To be a good citizen\nyou have to Be-Educated yourself and for that you need to give value to Pen.\nThe final rendered and composited 3D animated video reflects this moral and\nportrays the intended message with very vibrant visuals\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:08:50 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Bhatti", "Zeeshan", ""], ["Abro", "Ahsan", ""], ["Gillal", "Abdul Rehman", ""], ["Karbasi", "Mostafa", ""]]}, {"id": "1802.07119", "submitter": "Behrouz Bolourian Haghighi", "authors": "Behrouz Bolourian Haghighi, Amir Hossein Taherinia, Reza Monsefi", "title": "TRLF: An Effective Semi-fragile Watermarking Method for Tamper Detection\n  and Recovery based on LWT and FNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method for tamper detection and recovery using\nsemi-fragile data hiding, based on Lifting Wavelet Transform (LWT) and\nFeed-Forward Neural Network (FNN). In TRLF, first, the host image is decomposed\nup to one level using LWT, and the Discrete Cosine Transform (DCT) is applied\nto each 2*2 blocks of diagonal details. Next, a random binary sequence is\nembedded in each block as the watermark by correlating $DC$ coefficients. In\nauthentication stage, first, the watermarked image geometry is reconstructed by\nusing Speeded Up Robust Features (SURF) algorithm and extract watermark bits by\nusing FNN. Afterward, logical exclusive-or operation between original and\nextracted watermark is applied to detect tampered region. Eventually, in the\nrecovery stage, tampered regions are recovered by image digest which is\ngenerated by inverse halftoning technique. The performance and efficiency of\nTRLF and its robustness against various geometric, non-geometric and hybrid\nattacks are reported. From the experimental results, it can be seen that TRLF\nis superior in terms of robustness and quality of the digest and watermarked\nimage respectively, compared to the-state-of-the-art fragile and semi-fragile\nwatermarking methods. In addition, imperceptibility has been improved by using\ndifferent correlation steps as the gain factor for flat (smooth) and texture\n(rough) blocks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 18:36:35 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Haghighi", "Behrouz Bolourian", ""], ["Taherinia", "Amir Hossein", ""], ["Monsefi", "Reza", ""]]}, {"id": "1802.07180", "submitter": "Tamara Koljen\\v{s}i\\'c", "authors": "Tamara Koljensic, Caslav Labudovic", "title": "Comparison of threshold-based algorithms for sparse signal recovery", "comments": "student paper submitted to the za 7th Mediterranean Conference on\n  Embedded Computing - MECO'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensively growing approach in signal processing and acquisition, the\nCompressive Sensing approach, allows sparse signals to be recovered from small\nnumber of randomly acquired signal coefficients. This paper analyses some of\nthe commonly used threshold-based algorithms for sparse signal reconstruction.\nSignals satisfy the conditions required by the Compressive Sensing theory. The\nOrthogonal Matching Pursuit, Iterative Hard Thresholding and Single Iteration\nReconstruction algorithms are observed. Comparison in terms of reconstruction\nerror and execution time is performed within the experimental part of the\npaper.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 12:54:31 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Koljensic", "Tamara", ""], ["Labudovic", "Caslav", ""]]}, {"id": "1802.08994", "submitter": "Xue Zhang", "authors": "Xue Zhang, Laura Toni, Pascal Frossard, Yao Zhao, Chunyu Lin", "title": "Adaptive Streaming in Interactive Multiview Video Systems", "comments": "17 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview applications endow final users with the possibility to freely\nnavigate within 3D scenes with minimum-delay. A real feeling of scene\nnavigation is enabled by transmitting multiple high-quality camera views, which\ncan be used to synthesize additional virtual views to offer a smooth\nnavigation. However, when network resources are limited, not all camera views\ncan be sent at high quality. It is therefore important, yet challenging, to\nfind the right tradeoff between coding artifacts (reducing the quality of\ncamera views) and virtual synthesis artifacts (reducing the number of camera\nviews sent to users). To this aim, we propose an optimal transmission strategy\nfor interactive multiview HTTP adaptive streaming (HAS). We propose a problem\nformulation to select the optimal set of camera views that the client requests\nfor downloading, such that the navigation quality experienced by the user is\noptimized while the bandwidth constraints are satisfied. We show that our\noptimization problem is NP-hard, and we therefore develop an optimal solution\nbased on the dynamic programming algorithm with polynomial time complexity. To\nfurther simplify the deployment, we present a suboptimal greedy algorithm with\neffective performance and lower complexity. The proposed controller is\nevaluated in theoretical and realistic settings characterized by realistic\nnetwork statistics estimation, buffer management and server-side representation\noptimization. Simulation results show significant improvement in terms of\nnavigation quality compared with alternative baseline multiview adaptation\nlogic solutions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 12:19:09 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 05:54:54 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 09:09:02 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 02:55:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Xue", ""], ["Toni", "Laura", ""], ["Frossard", "Pascal", ""], ["Zhao", "Yao", ""], ["Lin", "Chunyu", ""]]}, {"id": "1802.09065", "submitter": "Zhan Ma", "authors": "Peiyao Guo and Qiu Shen and Zhan Ma and David J. Brady and Yao Wang", "title": "Perceptual Quality Assessment of Immersive Images Considering Peripheral\n  Vision Impact", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional images/videos are often rendered within the central vision area\nof the human visual system (HVS) with uniform quality. Recent virtual reality\n(VR) device with head mounted display (HMD) extends the field of view (FoV)\nsignificantly to include both central and peripheral vision areas. It exhibits\nthe unequal image quality sensation among these areas because of the\nnon-uniform distribution of photoreceptors on our retina. We propose to study\nthe sensation impact on the image subjective quality with respect to the\neccentric angle $\\theta$ across different vision areas. Often times, image\nquality is controlled by the quantization stepsize $q$ and spatial resolution\n$s$, separately and jointly. Therefore, the sensation impact can be understood\nby exploring the $q$ and/or $s$ in terms of the $\\theta$, resulting in\nself-adaptive analytical models that have shown quite impressive accuracy\nthrough independent cross validations. These models can further be applied to\ngive different quality weights at different regions, so as to significantly\nreduce the transmission data size but without subjective quality loss. As\ndemonstrated in a gigapixel imaging system, we have shown that the image\nrendering can be speed up about 10$\\times$ with the model guided unequal\nquality scales, in comparison to the the legacy scheme with uniform quality\nscales everywhere.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 19:15:33 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Guo", "Peiyao", ""], ["Shen", "Qiu", ""], ["Ma", "Zhan", ""], ["Brady", "David J.", ""], ["Wang", "Yao", ""]]}, {"id": "1802.09079", "submitter": "Sandipan Choudhuri", "authors": "Sandipan Choudhuri, Kaustav Basu, Arunabha Sen", "title": "User Satisfaction-Driven Bandwidth Allocation for Image Transmission in\n  a Crowded Environment", "comments": "7 Pages", "journal-ref": "MMTC Communications - Frontiers, SPECIAL ISSUE ON Multiple\n  Wireless Technologies and IoT in Industry: Applications and Challenges, Vol.\n  12, No. 6, November 2017", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major portion of postings on social networking sites constitute high\nquality digital images and videos. These images and videos require a fairly\nlarge amount of bandwidth during transmission. Accordingly, high quality image\nand video postings become a challenge for the network service provider,\nespecially in a crowded environment where bandwidth is in high demand. In this\npaper we present a user satisfaction driven bandwidth allocation scheme for\nimage transmission in such environments. In an image, there are always objects\nthat stand out more than others. The reason behind some set of objects being\nmore important in a scene is based on a number of visual, as well as, cognitive\nfactors. Being motivated by the fact that user satisfaction is more dependent\non the quality of these salient objects in an image than non-salient ones, we\npropose a quantifiable metric for measuring user-satisfiability (based on image\nquality and delay of transmission). The bandwidth allocation technique proposed\nthereafter, ensures that this user-satisfiability is maximized. Unlike the\nexisting approaches that utilize some fixed set of non-linear functions for\nframing the user-satisfiability index, our metric is modelled over customer\nsurvey data, where the unknown parameters are trained with machine learning\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 21:13:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Choudhuri", "Sandipan", ""], ["Basu", "Kaustav", ""], ["Sen", "Arunabha", ""]]}, {"id": "1802.09333", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu, Wei Wang, Jing Dong, Hongxia Wang and Lizhi Xiong", "title": "The Cut and Dominating Set Problem in A Steganographer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A steganographer network corresponds to a graphic structure that the involved\nvertices (or called nodes) denote social entities such as the data encoders and\ndata decoders, and the associated edges represent any real communicable\nchannels or other social links that could be utilized for steganography. Unlike\ntraditional steganographic algorithms, a steganographer network models\nsteganographic communication by an abstract way such that the concerned\nunderlying characteristics of steganography are quantized as analyzable\nparameters in the network. In this paper, we will analyze two problems in a\nsteganographer network. The first problem is a passive attack to a\nsteganographer network where a network monitor has collected a list of\nsuspicious vertices corresponding to the data encoders or decoders. The network\nmonitor expects to break (disconnect) the steganographic communication down\nbetween the suspicious vertices while keeping the cost as low as possible. The\nsecond one relates to determining a set of vertices corresponding to the data\nencoders (senders) such that all vertices can share a message by neighbors. We\npoint that, the two problems are equivalent to the minimum cut problem and the\nminimum-weight dominating set problem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 04:06:33 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Wu", "Hanzhou", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Wang", "Hongxia", ""], ["Xiong", "Lizhi", ""]]}, {"id": "1802.10495", "submitter": "Yu-Siang Huang", "authors": "Yu-Siang Huang, Szu-Yu Chou, Yi-Hsuan Yang", "title": "Pop Music Highlighter: Marking the Emotion Keypoints", "comments": "Transactions of the ISMIR vol. 1, no. 1", "journal-ref": null, "doi": "10.5334/tismir.14", "report-no": null, "categories": "eess.AS cs.AI cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of music highlight extraction is to get a short consecutive segment\nof a piece of music that provides an effective representation of the whole\npiece. In a previous work, we introduced an attention-based convolutional\nrecurrent neural network that uses music emotion classification as a surrogate\ntask for music highlight extraction, for Pop songs. The rationale behind that\napproach is that the highlight of a song is usually the most emotional part.\nThis paper extends our previous work in the following two aspects. First,\nmethodology-wise we experiment with a new architecture that does not need any\nrecurrent layers, making the training process faster. Moreover, we compare a\nlate-fusion variant and an early-fusion variant to study which one better\nexploits the attention mechanism. Second, we conduct and report an extensive\nset of experiments comparing the proposed attention-based methods against a\nheuristic energy-based method, a structural repetition-based method, and a few\nother simple feature-based methods for this task. Due to the lack of\npublic-domain labeled data for highlight extraction, following our previous\nwork we use the RWC POP 100-song data set to evaluate how the detected\nhighlights overlap with any chorus sections of the songs. The experiments\ndemonstrate the effectiveness of our methods over competing methods. For\nreproducibility, we open source the code and pre-trained model at\nhttps://github.com/remyhuang/pop-music-highlighter/.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:02:47 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 18:34:47 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Huang", "Yu-Siang", ""], ["Chou", "Szu-Yu", ""], ["Yang", "Yi-Hsuan", ""]]}]