[{"id": "1906.00697", "submitter": "Bin Li", "authors": "Xiaoyu Shi, Benedetta Tondi, Bin Li, Mauro Barni", "title": "CNN-based Steganalysis and Parametric Adversarial Embedding: a\n  Game-Theoretic Framework", "comments": "Adversarial embedding, deep learning, steganography, steganalysis,\n  game theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN-based steganalysis has recently achieved very good performance in\ndetecting content-adaptive steganography. At the same time, recent works have\nshown that, by adopting an approach similar to that used to build adversarial\nexamples, a steganographer can adopt an adversarial embedding strategy to\neffectively counter a target CNN steganalyzer. In turn, the good performance of\nthe steganalyzer can be restored by retraining the CNN with adversarial stego\nimages. A problem with this model is that, arguably, at training time the\nsteganalizer is not aware of the exact parameters used by the steganograher for\nadversarial embedding and, vice versa, the steganographer does not know how the\nimages that will be used to train the steganalyzer are generated. In order to\nexit this apparent deadlock, we introduce a game theoretic framework wherein\nthe problem of setting the parameters of the steganalyzer and the\nsteganographer is solved in a strategic way. More specifically, a non-zero sum\ngame is first formulated to model the problem, and then instantiated by\nconsidering a specific adversarial embedding scheme setting its operating\nparameters in a game-theoretic fashion. Our analysis shows that the equilibrium\nsolution of the non zero-sum game can be conveniently found by solving an\nassociated zero-sum game, thus reducing greatly the complexity of the problem.\nThen we run several experiments to derive the optimum strategies for the\nsteganographer and the staganalyst in a game-theoretic sense, and to evaluate\nthe performance of the game at the equilibrium, characterizing the loss with\nrespect to the conventional non-adversarial case. Eventually, by leveraging on\nthe analysis of the equilibrium point of the game, we introduce a new strategy\nto improve the reliability of the steganalysis, which shows the benefits of\naddressing the security issue in a game-theoretic perspective.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 10:48:36 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Shi", "Xiaoyu", ""], ["Tondi", "Benedetta", ""], ["Li", "Bin", ""], ["Barni", "Mauro", ""]]}, {"id": "1906.00750", "submitter": "Elena Cipressi", "authors": "Elena Cipressi and Maria Luisa Merani", "title": "Effects of Packet Loss and Jitter on VoLTE Call Quality", "comments": "Paper accepted at CoNEXT 2018 Student Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work performs a preliminary, comparative analysis of the end-to-end\nquality guaranteed by Voice over LTE (VoLTE), examining several millions of\nVoLTE calls that employ two popular speech audio codecs, namely, Adaptive\nMulti-Rate (AMR) and Adaptive Multi-Rate WideBand (AMR-WB). To assess call\nquality, VQmon, an enhanced version of the standardized E-Model, is utilized.\nThe study reveals to what extent AMR-WB based calls are more robust against\nnetwork impairments than their narrowband counterparts; it further shows that\nthe dependence of call quality on the packet loss rate is approximately\nexponential when the AMR codec is used, whereas it is nearly linear for the\nAMR-WB codec.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:13:45 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Cipressi", "Elena", ""], ["Merani", "Maria Luisa", ""]]}, {"id": "1906.02497", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhijie Lin, Zhou Zhao and Zhenxin Xiao", "title": "Cross-Modal Interaction Networks for Query-Based Moment Retrieval in\n  Videos", "comments": "Accepted by SIGIR 2019 as a full paper", "journal-ref": "SIGIR, 2019, pages 655-664", "doi": "10.1145/3331184.3331235", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based moment retrieval aims to localize the most relevant moment in an\nuntrimmed video according to the given natural language query. Existing works\noften only focus on one aspect of this emerging task, such as the query\nrepresentation learning, video context modeling or multi-modal fusion, thus\nfail to develop a comprehensive system for further performance improvement. In\nthis paper, we introduce a novel Cross-Modal Interaction Network (CMIN) to\nconsider multiple crucial factors for this challenging task, including (1) the\nsyntactic structure of natural language queries; (2) long-range semantic\ndependencies in video context and (3) the sufficient cross-modal interaction.\nSpecifically, we devise a syntactic GCN to leverage the syntactic structure of\nqueries for fine-grained representation learning, propose a multi-head\nself-attention to capture long-range semantic dependencies from video context,\nand next employ a multi-stage cross-modal interaction to explore the potential\nrelations of video and query contents. The extensive experiments demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:45:58 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 10:18:43 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhang", "Zhu", ""], ["Lin", "Zhijie", ""], ["Zhao", "Zhou", ""], ["Xiao", "Zhenxin", ""]]}, {"id": "1906.02671", "submitter": "Nicholas Waytowich", "authors": "Nicholas Waytowich, Sean L. Barton, Vernon Lawhern, Ethan Stump and\n  Garrett Warnell", "title": "Grounding Natural Language Commands to StarCraft II Game States for\n  Narration-Guided Reinforcement Learning", "comments": "10 pages, 3 figures. Published at SPIE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep reinforcement learning techniques have led to agents that are\nsuccessfully able to learn to perform a number of tasks that had been\npreviously unlearnable, these techniques are still susceptible to the\nlongstanding problem of {\\em reward sparsity}. This is especially true for\ntasks such as training an agent to play StarCraft II, a real-time strategy game\nwhere reward is only given at the end of a game which is usually very long.\nWhile this problem can be addressed through reward shaping, such approaches\ntypically require a human expert with specialized knowledge. Inspired by the\nvision of enabling reward shaping through the more-accessible paradigm of\nnatural-language narration, we investigate to what extent we can contextualize\nthese narrations by grounding them to the goal-specific states. We present a\nmutual-embedding model using a multi-input deep-neural network that projects a\nsequence of natural language commands into the same high-dimensional\nrepresentation space as corresponding goal states. We show that using this\nmodel we can learn an embedding space with separable and distinct clusters that\naccurately maps natural-language commands to corresponding game states . We\nalso discuss how this model can allow for the use of narrations as a robust\nform of reward shaping to improve RL performance and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:43:40 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Waytowich", "Nicholas", ""], ["Barton", "Sean L.", ""], ["Lawhern", "Vernon", ""], ["Stump", "Ethan", ""], ["Warnell", "Garrett", ""]]}, {"id": "1906.03175", "submitter": "Zhaoxia Yin", "authors": "Xiaoqing Liu, Yinyin Peng, Jie Wang, Zhaoxia Yin", "title": "Image Encryption Algorithm Based on Facebook Social Network", "comments": "in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facebook is the online social networks (OSNs) platform with the largest\nnumber of users in the world today, information protection based on Facebook\nsocial network platform have important practical significance. Since the\ninformation users share on social networks is often based on images, this paper\nproposes a more secure image encryption algorithm based on Facebook social\nnetwork platform to ensure the loss of information as much as possible. When\nthe sender encrypts the image for uploading, it can first resist the third\nparty's attack on the encrypted image and prevent the image data from leaking,\nsimultaneously processed by some unknown processing such as compression and\nfiltering of the image on the Facebook platform, the receiver can still decrypt\nthe corresponding image data.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 07:08:22 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Liu", "Xiaoqing", ""], ["Peng", "Yinyin", ""], ["Wang", "Jie", ""], ["Yin", "Zhaoxia", ""]]}, {"id": "1906.03238", "submitter": "Jarek Duda dr", "authors": "Jarek Duda", "title": "Parametric context adaptive Laplace distribution for multimedia\n  compression", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression often subtracts prediction and encodes the difference\n(residue) e.g. assuming Laplace distribution, for example for images, videos,\naudio, or numerical data. Its performance is strongly dependent on the proper\nchoice of width (scale parameter) of this parametric distribution, can be\nimproved if optimizing it based on local situation like context. For example in\npopular LOCO-I \\cite{loco} (JPEG-LS) lossless image compressor there is used 3\ndimensional context quantized into 365 discrete possibilities treated\nindependently. This article discusses inexpensive approaches for exploiting\ntheir dependencies with autoregressive ARCH-like context dependent models for\nparameters of parametric distribution for residue, also evolving in time for\nadaptive case. For example tested such 4 or 11 parameter models turned out to\nprovide similar performance as 365 parameter LOCO-I model for 48 tested images.\nBeside smaller headers, such reduction of number of parameters can lead to\nbetter generalization. In contrast to context quantization approaches,\nparameterized models also allow to directly use higher dimensional contexts,\nfor example using information from all 3 color channels, further pixels, some\nadditional region classifiers, or from interleaving multi-scale scanning - for\nwhich there is proposed Haar upscale scan combining advantages of Haar wavelets\nwith possibility of scanning exploiting local contexts.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:43:46 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 10:35:58 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 13:39:34 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 11:51:00 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Duda", "Jarek", ""]]}, {"id": "1906.03395", "submitter": "Lee Prangnell", "authors": "Lee Prangnell", "title": "Frequency-Dependent Perceptual Quantisation for Visually Lossless\n  Compression Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The default quantisation algorithms in the state-of-the-art High Efficiency\nVideo Coding (HEVC) standard, namely Uniform Reconstruction Quantisation (URQ)\nand Rate-Distortion Optimised Quantisation (RDOQ), do not take into account the\nperceptual relevance of individual transform coefficients. In this paper, a\nFrequency-Dependent Perceptual Quantisation (FDPQ) technique for HEVC is\nproposed. FDPQ exploits the well-established Modulation Transfer Function (MTF)\ncharacteristics of the linear transformation basis functions by taking into\naccount the Euclidean distance of an AC transform coefficient from the DC\ncoefficient. As such, in luma and chroma Cb and Cr Transform Blocks (TBs), FDPQ\nquantises more coarsely the least perceptually relevant transform coefficients\n(i.e., the high frequency AC coefficients). Conversely, FDPQ preserves the\nintegrity of the DC coefficient and the very low frequency AC coefficients.\nCompared with RDOQ, which is the most widely used transform coefficient-level\nquantisation technique in video coding, FDPQ successfully achieves bitrate\nreductions of up to 41%. Furthermore, the subjective evaluations confirm that\nthe FDPQ-coded video data is perceptually indistinguishable (i.e., visually\nlossless) from the raw video data for a given Quantisation Parameter (QP).\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 06:03:09 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Prangnell", "Lee", ""]]}, {"id": "1906.03940", "submitter": "Md. Iftekhar Tanveer", "authors": "Md Iftekhar Tanveer, Md Kamrul Hassan, Daniel Gildea, M. Ehsan Hoque", "title": "Predicting TED Talk Ratings from Language and Prosody", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.08392", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the largest open repository of public speaking---TED Talks---to\npredict the ratings of the online viewers. Our dataset contains over 2200 TED\nTalk transcripts (includes over 200 thousand sentences), audio features and the\nassociated meta information including about 5.5 Million ratings from\nspontaneous visitors of the website. We propose three neural network\narchitectures and compare with statistical machine learning. Our experiments\nreveal that it is possible to predict all the 14 different ratings with an\naverage AUC of 0.83 using the transcripts and prosody features only. The\ndataset and the complete source code is available for further analysis.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 00:38:33 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tanveer", "Md Iftekhar", ""], ["Hassan", "Md Kamrul", ""], ["Gildea", "Daniel", ""], ["Hoque", "M. Ehsan", ""]]}, {"id": "1906.05165", "submitter": "Jiahua Xu", "authors": "Zhibo Chen, Jiahua Xu, Chaoyi Lin and Wei Zhou", "title": "Stereoscopic Omnidirectional Image Quality Assessment Based on\n  Predictive Coding Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective quality assessment of stereoscopic omnidirectional images is a\nchallenging problem since it is influenced by multiple aspects such as\nprojection deformation, field of view (FoV) range, binocular vision, visual\ncomfort, etc. Existing studies show that classic 2D or 3D image quality\nassessment (IQA) metrics are not able to perform well for stereoscopic\nomnidirectional images. However, very few research works have focused on\nevaluating the perceptual visual quality of omnidirectional images, especially\nfor stereoscopic omnidirectional images. In this paper, based on the predictive\ncoding theory of the human vision system (HVS), we propose a stereoscopic\nomnidirectional image quality evaluator (SOIQE) to cope with the\ncharacteristics of 3D 360-degree images. Two modules are involved in SOIQE:\npredictive coding theory based binocular rivalry module and multi-view fusion\nmodule. In the binocular rivalry module, we introduce predictive coding theory\nto simulate the competition between high-level patterns and calculate the\nsimilarity and rivalry dominance to obtain the quality scores of viewport\nimages. Moreover, we develop the multi-view fusion module to aggregate the\nquality scores of viewport images with the help of both content weight and\nlocation weight. The proposed SOIQE is a parametric model without necessary of\nregression learning, which ensures its interpretability and generalization\nperformance. Experimental results on our published stereoscopic omnidirectional\nimage quality assessment database (SOLID) demonstrate that our proposed SOIQE\nmethod outperforms state-of-the-art metrics. Furthermore, we also verify the\neffectiveness of each proposed module on both public stereoscopic image\ndatasets and panoramic image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:25:28 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chen", "Zhibo", ""], ["Xu", "Jiahua", ""], ["Lin", "Chaoyi", ""], ["Zhou", "Wei", ""]]}, {"id": "1906.05268", "submitter": "Jeff Yan", "authors": "Aur\\'elien Bourquard, Jeff Yan", "title": "Differential Imaging Forensics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce some new forensics based on differential imaging, where a novel\ncategory of visual evidence created via subtle interactions of light with a\nscene, such as dim reflections, can be computationally extracted and amplified\nfrom an image of interest through a comparative analysis with an additional\nreference baseline image acquired under similar conditions. This paradigm of\ndifferential imaging forensics (DIF) enables forensic examiners for the first\ntime to retrieve the said visual evidence that is readily available in an image\nor video footage but would otherwise remain faint or even invisible to a human\nobserver. We demonstrate the relevance and effectiveness of our approach\nthrough practical experiments. We also show that DIF provides a novel method\nfor detecting forged images and video clips, including deep fakes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:48:47 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Bourquard", "Aur\u00e9lien", ""], ["Yan", "Jeff", ""]]}, {"id": "1906.05378", "submitter": "Leo Furkan Isikdogan", "authors": "Leo F. Isikdogan, Timo Gerasimow, Gilad Michael", "title": "Eye Contact Correction using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical video conferencing setup, it is hard to maintain eye contact\nduring a call since it requires looking into the camera rather than the\ndisplay. We propose an eye contact correction model that restores the eye\ncontact regardless of the relative position of the camera and display. Unlike\nprevious solutions, our model redirects the gaze from an arbitrary direction to\nthe center without requiring a redirection angle or camera/display/user\ngeometry as inputs. We use a deep convolutional neural network that inputs a\nmonocular image and produces a vector field and a brightness map to correct the\ngaze. We train this model in a bi-directional way on a large set of\nsynthetically generated photorealistic images with perfect labels. The learned\nmodel is a robust eye contact corrector which also predicts the input gaze\nimplicitly at no additional cost. Our system is primarily designed to improve\nthe quality of video conferencing experience. Therefore, we use a set of\ncontrol mechanisms to prevent creepy results and to ensure a smooth and natural\nvideo conferencing experience. The entire eye contact correction system runs\nend-to-end in real-time on a commodity CPU and does not require any dedicated\nhardware, making our solution feasible for a variety of devices.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 21:15:12 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 17:59:44 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Isikdogan", "Leo F.", ""], ["Gerasimow", "Timo", ""], ["Michael", "Gilad", ""]]}, {"id": "1906.05538", "submitter": "Wei Cai", "authors": "Tian Min and Wei Cai", "title": "A Security Case Study for Blockchain Games", "comments": null, "journal-ref": "IEEE Games Entertainment & Media Conference 2019 (GEM 2019), New\n  Haven, Connecticut, United States, June 19-22, 2019", "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain gaming is an emerging entertainment paradigm. However, blockchain\ngames are still suffering from security issues, due to the immature blockchain\ntechnologies and its unsophisticated developers. In this work, we analyzed the\nblockchain game architecture and reveal the possible penetration methods of\ncracking. We scanned more than 600 commercial blockchain games to summarize a\nsecurity overview from the perspective of the web server and smart contract,\nrespectively. We also conducted three case studies for blockchain games to show\ndetailed vulnerability detection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 08:11:55 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Min", "Tian", ""], ["Cai", "Wei", ""]]}, {"id": "1906.05558", "submitter": "Wei Cai", "authors": "Tian Min, Hanyi Wang, Yaoze Guo and Wei Cai", "title": "Blockchain Games: A Survey", "comments": null, "journal-ref": "IEEE Conference on Games (CoG 2019), London, United Kingdom, Aug\n  20-23, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the support of the blockchain systems, the cryptocurrency has changed\nthe world of virtual assets. Digital games, especially those with massive\nmulti-player scenarios, will be significantly impacted by this novel\ntechnology. However, there are insufficient academic studies on this topic. In\nthis work, we filled the blank by surveying the state-of-the-art blockchain\ngames. We discuss the blockchain integration for games and then categorize\nexisting blockchain games from the aspects of their genres and technical\nplatforms. Moreover, by analyzing the industrial trend with a statistical\napproach, we envision the future of blockchain games from technological and\ncommercial perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:16:19 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Min", "Tian", ""], ["Wang", "Hanyi", ""], ["Guo", "Yaoze", ""], ["Cai", "Wei", ""]]}, {"id": "1906.05925", "submitter": "Kevin VanHorn", "authors": "Kevin C. VanHorn, Meyer Zinn, Murat Can Cobanoglu", "title": "Deep Learning Development Environment in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) offers immersive visualization and intuitive\ninteraction. We leverage VR to enable any biomedical professional to deploy a\ndeep learning (DL) model for image classification. While DL models can be\npowerful tools for data analysis, they are also challenging to understand and\ndevelop. To make deep learning more accessible and intuitive, we have built a\nvirtual reality-based DL development environment. Within our environment, the\nuser can move tangible objects to construct a neural network only using their\nhands. Our software automatically translates these configurations into a\ntrainable model and then reports its resulting accuracy on a test dataset in\nreal-time. Furthermore, we have enriched the virtual objects with\nvisualizations of the model's components such that users can achieve insight\nabout the DL models that they are developing. With this approach, we bridge the\ngap between professionals in different fields of expertise while offering a\nnovel perspective for model analysis and data interaction. We further suggest\nthat techniques of development and visualization in deep learning can benefit\nby integrating virtual reality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 20:53:33 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["VanHorn", "Kevin C.", ""], ["Zinn", "Meyer", ""], ["Cobanoglu", "Murat Can", ""]]}, {"id": "1906.06147", "submitter": "Yasufumi Moriya", "authors": "Yasufumi Moriya, Ramon Sanabria, Florian Metze, Gareth J. F. Jones", "title": "Grounding Object Detections With Transcriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vast amount of audio-visual data is available on the Internet thanks to\nvideo streaming services, to which users upload their content. However, there\nare difficulties in exploiting available data for supervised statistical models\ndue to the lack of labels. Unfortunately, generating labels for such amount of\ndata through human annotation can be expensive, time-consuming and prone to\nannotation errors. In this paper, we propose a method to automatically extract\nentity-video frame pairs from a collection of instruction videos by using\nspeech transcriptions and videos. We conduct experiments on image recognition\nand visual grounding tasks on the automatically constructed entity-video frame\ndataset of How2. The models will be evaluated on new manually annotated portion\nof How2 dev5 and val set and on the Flickr30k dataset. This work constitutes a\nfirst step towards meta-algorithms capable of automatically construct\ntask-specific training sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 02:12:01 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 20:09:00 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Moriya", "Yasufumi", ""], ["Sanabria", "Ramon", ""], ["Metze", "Florian", ""], ["Jones", "Gareth J. F.", ""]]}, {"id": "1906.06184", "submitter": "Christian Esteve Rothenberg", "authors": "Samira Afzal and Vanessa Testoni and Christian Esteve Rothenberg and\n  Prakash Kolan and Imed Bouazizi", "title": "A Holistic Survey of Wireless Multipath Video Streaming", "comments": "42 pages. 13 figures. 9 Tables. Accepted for publication in IEEE\n  Communications Surveys and Tutorials, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of today's mobile devices are equipped with multiple network interfaces\nand one of the main bandwidth-hungry applications that would benefit from\nmultipath communications is wireless video streaming. However, most of current\ntransport protocols do not match the requirements of video streaming\napplications or are not designed to address relevant issues, such as delay\nconstraints, networks heterogeneity, and head-of-line blocking issues. This\narticle provides a holistic survey of multipath wireless video streaming,\nshedding light on the different alternatives from an end-to-end layered stack\nperspective, unveiling trade-offs of each approach and presenting a suitable\ntaxonomy to classify the state-of-the-art. Finally, we discuss open issues and\navenues for future work.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 12:55:47 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Afzal", "Samira", ""], ["Testoni", "Vanessa", ""], ["Rothenberg", "Christian Esteve", ""], ["Kolan", "Prakash", ""], ["Bouazizi", "Imed", ""]]}, {"id": "1906.06526", "submitter": "Simone Santini", "authors": "Simone Santini", "title": "Relevance Feedback with Latent Variables in Riemann spaces", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop and evaluate two methods for relevance feedback\nbased on endowing a suitable \"semantic query space\" with a Riemann metric\nderived from the probability distribution of the positive samples of the\nfeedback. The first method uses a Gaussian distribution to model the data,\nwhile the second uses a more complex Latent Semantic variable model. A mixed\n(discrete-continuous) version of the Expectation-Maximization algorithm is\ndeveloped for this model.\n  We motivate the need for the semantic query space by analyzing in some depth\nthree well-known relevance feedback methods, and we develop a new experimental\nmethodology to evaluate these methods and compare their performance in a\nneutral way, that is, without making assumptions on the system in which they\nwill be embedded.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 11:07:55 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Santini", "Simone", ""]]}, {"id": "1906.06892", "submitter": "Yaxian Xia", "authors": "Yaxian Xia and Lun Huang and Wenmin Wang and Xiaoyong Wei and Wenmin\n  Wang", "title": "ParNet: Position-aware Aggregated Relation Network for Image-Text\n  matching", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring fine-grained relationship between entities(e.g. objects in image or\nwords in sentence) has great contribution to understand multimedia content\nprecisely. Previous attention mechanism employed in image-text matching either\ntakes multiple self attention steps to gather correspondences or uses image\nobjects (or words) as context to infer image-text similarity. However, they\nonly take advantage of semantic information without considering that objects'\nrelative position also contributes to image understanding. To this end, we\nintroduce a novel position-aware relation module to model both the semantic and\nspatial relationship simultaneously for image-text matching in this paper.\nGiven an image, our method utilizes the location of different objects to\ncapture spatial relationship innovatively. With the combination of semantic and\nspatial relationship, it's easier to understand the content of different\nmodalities (images and sentences) and capture fine-grained latent\ncorrespondences of image-text pairs. Besides, we employ a two-step aggregated\nrelation module to capture interpretable alignment of image-text pairs. The\nfirst step, we call it intra-modal relation mechanism, in which we computes\nresponses between different objects in an image or different words in a\nsentence separately; The second step, we call it inter-modal relation\nmechanism, in which the query plays a role of textual context to refine the\nrelationship among object proposals in an image. In this way, our\nposition-aware aggregated relation network (ParNet) not only knows which\nentities are relevant by attending on different objects (words) adaptively, but\nalso adjust the inter-modal correspondence according to the latent alignments\naccording to query's content. Our approach achieves the state-of-the-art\nresults on MS-COCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 08:26:43 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Xia", "Yaxian", ""], ["Huang", "Lun", ""], ["Wang", "Wenmin", ""], ["Wei", "Xiaoyong", ""], ["Wang", "Wenmin", ""]]}, {"id": "1906.07901", "submitter": "Shruti Palaskar", "authors": "Shruti Palaskar, Jindrich Libovick\\'y, Spandana Gella and Florian\n  Metze", "title": "Multimodal Abstractive Summarization for How2 Videos", "comments": "To appear in ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study abstractive summarization for open-domain videos.\nUnlike the traditional text news summarization, the goal is less to \"compress\"\ntext information but rather to provide a fluent textual summary of information\nthat has been collected and fused from different source modalities, in our case\nvideo and audio transcripts (or text). We show how a multi-source\nsequence-to-sequence model with hierarchical attention can integrate\ninformation from different modalities into a coherent output, compare various\nmodels trained with different modalities and present pilot experiments on the\nHow2 corpus of instructional videos. We also propose a new evaluation metric\n(Content F1) for abstractive summarization task that measures semantic adequacy\nrather than fluency of the summaries, which is covered by metrics like ROUGE\nand BLEU.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 03:52:42 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Palaskar", "Shruti", ""], ["Libovick\u00fd", "Jindrich", ""], ["Gella", "Spandana", ""], ["Metze", "Florian", ""]]}, {"id": "1906.08415", "submitter": "Zhihao Du", "authors": "Yue Gu, Zhihao Du, Hui Zhang, Xueliang Zhang", "title": "A Monaural Speech Enhancement Method for Robust Small-Footprint Keyword\n  Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness against noise is critical for keyword spotting (KWS) in real-world\nenvironments. To improve the robustness, a speech enhancement front-end is\ninvolved. Instead of treating the speech enhancement as a separated\npreprocessing before the KWS system, in this study, a pre-trained speech\nenhancement front-end and a convolutional neural networks (CNNs) based KWS\nsystem are concatenated, where a feature transformation block is used to\ntransform the output from the enhancement front-end into the KWS system's\ninput. The whole model is trained jointly, thus the linguistic and other useful\ninformation from the KWS system can be back-propagated to the enhancement\nfront-end to improve its performance. To fit the small-footprint device, a\nnovel convolution recurrent network is proposed, which needs fewer parameters\nand computation and does not degrade performance. Furthermore, by changing the\ninput features from the power spectrogram to Mel-spectrogram, less computation\nand better performance are obtained. our experimental results demonstrate that\nthe proposed method significantly improves the KWS system with respect to noise\nrobustness.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 02:25:45 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Gu", "Yue", ""], ["Du", "Zhihao", ""], ["Zhang", "Hui", ""], ["Zhang", "Xueliang", ""]]}, {"id": "1906.08575", "submitter": "Chenglin Li", "authors": "Junni Zou, Chenglin Li, Chengming Liu, Qin Yang, Hongkai Xiong,\n  Eckehard Steinbach", "title": "Probabilistic Tile Visibility-Based Server-Side Rate Adaptation for\n  Adaptive 360-Degree Video Streaming", "comments": "33 pages (single column and double space) with 15 figures, submitted\n  to IEEE Journal of Selected Topics in Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the server-side rate adaptation problem for streaming\ntile-based adaptive 360-degree videos to multiple users who are competing for\ntransmission resources at the network bottleneck. Specifically, we develop a\nconvolutional neural network (CNN)-based viewpoint prediction model to capture\nthe nonlinear relationship between the future and historical viewpoints. A\nLaplace distribution model is utilized to characterize the probability\ndistribution of the prediction error. Given the predicted viewpoint, we then\nmap the viewport in the spherical space into its corresponding planar\nprojection in the 2-D plane, and further derive the visibility probability of\neach tile based on the planar projection and the prediction error probability.\nAccording to the visibility probability, tiles are classified as viewport,\nmarginal and invisible tiles. The server-side tile rate allocation problem for\nmultiple users is then formulated as a non-linear discrete optimization problem\nto minimize the overall received video distortion of all users and the quality\ndifference between the viewport and marginal tiles of each user, subject to the\ntransmission capacity constraints and users' specific viewport requirements. We\ndevelop a steepest descent algorithm to solve this non-linear discrete\noptimization problem, by initializing the feasible starting point in accordance\nwith the optimal solution of its continuous relaxation. Extensive experimental\nresults show that the proposed algorithm can achieve a near-optimal solution,\nand outperforms the existing rate adaptation schemes for tile-based adaptive\n360-video streaming.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 12:27:52 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Zou", "Junni", ""], ["Li", "Chenglin", ""], ["Liu", "Chengming", ""], ["Yang", "Qin", ""], ["Xiong", "Hongkai", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1906.08595", "submitter": "Christian Otto", "authors": "Christian Otto, Matthias Springstein, Avishek Anand, Ralph Ewerth", "title": "Understanding, Categorizing and Predicting Semantic Image-Text Relations", "comments": "8 pages, 8 Figures, 5 tables", "journal-ref": "In Proceedings of the 2019 on International Conference on\n  Multimedia Retrieval (ICMR '19). ACM, New York, NY, USA, 168-176", "doi": "10.1145/3323873.3325049", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two modalities are often used to convey information in a complementary and\nbeneficial manner, e.g., in online news, videos, educational resources, or\nscientific publications. The automatic understanding of semantic correlations\nbetween text and associated images as well as their interplay has a great\npotential for enhanced multimodal web search and recommender systems. However,\nautomatic understanding of multimodal information is still an unsolved research\nproblem. Recent approaches such as image captioning focus on precisely\ndescribing visual content and translating it to text, but typically address\nneither semantic interpretations nor the specific role or purpose of an\nimage-text constellation. In this paper, we go beyond previous work and\ninvestigate, inspired by research in visual communication, useful semantic\nimage-text relations for multimodal information retrieval. We derive a\ncategorization of eight semantic image-text classes (e.g., \"illustration\" or\n\"anchorage\") and show how they can systematically be characterized by a set of\nthree metrics: cross-modal mutual information, semantic correlation, and the\nstatus relation of image and text. Furthermore, we present a deep learning\nsystem to predict these classes by utilizing multimodal embeddings. To obtain a\nsufficiently large amount of training data, we have automatically collected and\naugmented data from a variety of data sets and web resources, which enables\nfuture research on this topic. Experimental results on a demanding test set\ndemonstrate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 13:20:22 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Otto", "Christian", ""], ["Springstein", "Matthias", ""], ["Anand", "Avishek", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1906.08615", "submitter": "Jeong Choi", "authors": "Jeong Choi, Jongpil Lee, Jiyoung Park and Juhan Nam", "title": "Zero-shot Learning and Knowledge Transfer in Music Classification and\n  Tagging", "comments": "International Conference on Machine Learning (ICML) 2019, Machine\n  Learning for Music Discovery Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music classification and tagging is conducted through categorical supervised\nlearning with a fixed set of labels. In principle, this cannot make predictions\non unseen labels. Zero-shot learning is an approach to solve the problem by\nusing side information about the semantic labels. We recently investigated this\nconcept of zero-shot learning in music classification and tagging task by\nprojecting both audio and label space on a single semantic space. In this work,\nwe extend the work to verify the generalization ability of zero-shot learning\nmodel by conducting knowledge transfer to different music corpora.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 13:45:17 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Choi", "Jeong", ""], ["Lee", "Jongpil", ""], ["Park", "Jiyoung", ""], ["Nam", "Juhan", ""]]}, {"id": "1906.08673", "submitter": "Wei Song", "authors": "Wei Song, Yan Wang, Dongmei Huang, Antonio Liotta, Cristian Perra", "title": "Enhancement of Underwater Images with Statistical Model of Background\n  Light and Optimization of Transmission Map", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater images often have severe quality degradation and distortion due to\nlight absorption and scattering in the water medium. A hazed image formation\nmodel is widely used to restore the image quality. It depends on two optical\nparameters: the background light and the transmission map. Underwater images\ncan also be enhanced by color and contrast correction from the perspective of\nimage processing. In this paper, we propose an effective underwater image\nenhancement method for underwater images in composition of underwater image\nrestoration and color correction. Firstly, a manually annotated background\nlights (MABLs) database is developed. With reference to the relationship\nbetween MABLs and the histogram distributions of various underwater images,\nrobust statistical models of BLs estimation are provided. Next, the TM of R\nchannel is roughly estimated based on the new underwater dark channel prior via\nthe statistic of clear and high resolution underwater images, then a scene\ndepth map based on the underwater light attenuation prior and an adjusted\nreversed saturation map are applied to compensate and modify the coarse TM of R\nchannel. Next, TMs of G-B channels are estimated based on the difference of\nattenuation ratios between R channel and G-B channels. Finally, to improve the\ncolor and contrast of the restored image with a natural appearance, a variation\nof white balance is introduced as post-processing. In order to guide the\npriority of underwater image enhancement, sufficient evaluations are conducted\nto discuss the impacts of the key parameters including BL and TM, and the\nimportance of the color correction. Comparisons with other state-of-the-art\nmethods demonstrate that our proposed underwater image enhancement method can\nachieve higher accuracy of estimated BLs, less computation time, more superior\nperformance, and more valuable information retention.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 12:19:45 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Song", "Wei", ""], ["Wang", "Yan", ""], ["Huang", "Dongmei", ""], ["Liotta", "Antonio", ""], ["Perra", "Cristian", ""]]}, {"id": "1906.09086", "submitter": "Fatima Haouari", "authors": "Fatima Haouari, Emna Baccour, Aiman Erbad, Amr Mohamed, and Mohsen\n  Guizani", "title": "QoE-Aware Resource Allocation for Crowdsourced Live Streaming: A Machine\n  Learning Approach", "comments": "This paper was accepted in the Proceedings of ICC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG cs.MM cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the tremendous technological advancement of personal devices and\nthe prevalence of wireless mobile network accesses, the world has witnessed an\nexplosion in crowdsourced live streaming. Ensuring a better viewers quality of\nexperience (QoE) is the key to maximize the audiences number and increase\nstreaming providers' profits. This can be achieved by advocating a\ngeo-distributed cloud infrastructure to allocate the multimedia resources as\nclose as possible to viewers, in order to minimize the access delay and video\nstalls. Moreover, allocating the exact needed resources beforehand avoids\nover-provisioning, which may lead to significant costs by the service\nproviders. In the contrary, under-provisioning might cause significant delays\nto the viewers. In this paper, we introduce a prediction driven resource\nallocation framework, to maximize the QoE of viewers and minimize the resource\nallocation cost. First, by exploiting the viewers locations available in our\nunique dataset, we implement a machine learning model to predict the viewers\nnumber near each geo-distributed cloud site. Second, based on the predicted\nresults that showed to be close to the actual values, we formulate an\noptimization problem to proactively allocate resources at the viewers\nproximity. Additionally, we will present a trade-off between the video access\ndelay and the cost of resource allocation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 10:57:06 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Haouari", "Fatima", ""], ["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1906.09334", "submitter": "Vincent Lostanlen", "authors": "Vincent Lostanlen, Florian Hecker", "title": "The Shape of RemiXXXes to Come: Audio Texture Synthesis with\n  Time-frequency Scattering", "comments": "8 pages, 3 figures. To appear in the proceedings of the International\n  Conference on Digital Audio Effects (DAFX-19), Birmingham, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explains how to apply time--frequency scattering, a\nconvolutional operator extracting modulations in the time--frequency domain at\ndifferent rates and scales, to the re-synthesis and manipulation of audio\ntextures. After implementing phase retrieval in the scattering network by\ngradient backpropagation, we introduce scale--rate DAFx, a class of audio\ntransformations expressed in the domain of time--frequency scattering\ncoefficients. One example of scale--rate DAFx is chirp rate inversion, which\ncauses each sonic event to be locally reversed in time while leaving the arrow\nof time globally unchanged. Over the past two years, our work has led to the\ncreation of four electroacoustic pieces: ``FAVN''; ``Modulator (Scattering\nTransform)''; ``Experimental Palimpsest''; ``Inspection''; and a remix of\nLorenzo Senni's ``XAllegroX'', released by Warp Records on a vinyl entitled\n``The Shape of RemiXXXes to Come''. The source code to reproduce experiments\nand figures is made freely available at:\nhttps://github.com/lostanlen/scattering.m. A companion website containing demos\nis at: https://lostanlen.com/pubs/dafx2019\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 21:25:55 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 14:25:26 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lostanlen", "Vincent", ""], ["Hecker", "Florian", ""]]}, {"id": "1906.09779", "submitter": "Niklas Carlsson", "authors": "Niklas Carlsson and Derek Eager", "title": "Had You Looked Where I'm Looking: Cross-user Similarities in Viewing\n  Behavior for 360$^{\\circ}$ Video and Caching Implications", "comments": "13+ pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand and usage of 360$^{\\circ}$ video services are expected to\nincrease. However, despite these services being highly bandwidth intensive, not\nmuch is known about the potential value that basic bandwidth saving techniques\nsuch as server or edge-network on-demand caching (e.g., in a CDN) could have\nwhen used for delivery of such services. This problem is both important and\ncomplicated as client-side solutions have been developed that split the full\n360$^{\\circ}$ view into multiple tiles, and adapt the quality of the downloaded\ntiles based on the user's expected viewing direction and bandwidth conditions.\nTo better understand the potential bandwidth savings that caching-based\ntechniques may offer for this context, this paper presents the first\ncharacterization of the similarities in the viewing directions of users\nwatching the same 360$^{\\circ}$ video, the overlap in viewports of these users\n(the area of the full 360$^{\\circ}$ view they actually see), and the potential\ncache hit rates for different video categories, network conditions, and\naccuracy levels in the prediction of future viewing direction when prefetching.\nThe results provide substantial insight into the conditions under which overlap\ncan be considerable and caching effective, and can inform the design of new\ncaching system policies tailored for 360$^{\\circ}$ video.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 08:31:03 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Carlsson", "Niklas", ""], ["Eager", "Derek", ""]]}, {"id": "1906.09884", "submitter": "Yan Niu", "authors": "Niu Yan, Jihong Ouyang", "title": "Channel-by-Channel Demosaicking Networks with Embedded Spectral\n  Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demosaicking is standardly the first step in today's Image Signal Processing\n(ISP) pipeline of digital cameras. It reconstructs image RGB values from the\nspatially and spectrally sparse Color Filter Array (CFA) samples, which are the\noriginal raw data digitized from electrical signals. High quality and low cost\ndemosaicking is not only necessary for photography, but also fundamental for\nmany machine vision tasks. This paper proposes an accurate and fast\ndemosaicking model based on Convolutional Neural Networks (CNN) for the Bayer\nCFA, which is the most popular color filter arrangement adopted by digital\ncamera manufacturers. Observing that each channel has different estimation\ncomplexity, we reconstruct each channel by an individual sub-network. Moreover,\ninstead of directly estimating the red and blue values, our model infers the\ngreen-red and green-blue color difference. This strategy allows recovering the\nmost complex channel by a light weight network. Although the total size of our\nmodel is significantly smaller than the state of the art demosaicking networks,\nit achieves substantially higher performance in both demosaicking quality and\ncomputational cost, as validated by extensive experiments. Source code will be\nreleased along with paper publication.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 12:33:49 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 11:55:00 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 07:31:17 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Yan", "Niu", ""], ["Ouyang", "Jihong", ""]]}, {"id": "1906.10428", "submitter": "Dorien Herremans", "authors": "Kat Agres, Simon Lui, Dorien Herremans", "title": "A novel music-based game with motion capture to support cognitive and\n  motor function in the elderly", "comments": null, "journal-ref": "IEEE Conference on Games 2019, London, UK", "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel game prototype that uses music and motion\ndetection as preventive medicine for the elderly. Given the aging populations\naround the globe, and the limited resources and staff able to care for these\npopulations, eHealth solutions are becoming increasingly important, if not\ncrucial, additions to modern healthcare and preventive medicine. Furthermore,\nbecause compliance rates for performing physical exercises are often quite low\nin the elderly, systems able to motivate and engage this population are a\nnecessity. Our prototype uses music not only to engage listeners, but also to\nleverage the efficacy of music to improve mental and physical wellness. The\ngame is based on a memory task to stimulate cognitive function, and requires\nusers to perform physical gestures to mimic the playing of different musical\ninstruments. To this end, the Microsoft Kinect sensor is used together with a\nnewly developed gesture detection module in order to process users' gestures.\nThe resulting prototype system supports both cognitive functioning and physical\nstrengthening in the elderly.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 10:02:35 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Agres", "Kat", ""], ["Lui", "Simon", ""], ["Herremans", "Dorien", ""]]}, {"id": "1906.11525", "submitter": "Marc Chaumont", "authors": "Ahmad Zakaria, Marc Chaumont and G\\'erard Subsol", "title": "Pooled Steganalysis in JPEG: how to deal with the spreading strategy?", "comments": "Ahmad Zakaria, Marc Chaumont, Gerard Subsol, \" Pooled Steganalysis in\n  JPEG: how to deal with the spreading strategy? \", WIFS'2019, IEEE\n  International Workshop on Information Forensics and Security, December 9-12,\n  2019, Delft, The Netherlands, 6 pages, Acceptance rate = 30%", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image pooled steganalysis, a steganalyst, Eve, aims to detect if a set of\nimages sent by a steganographer, Alice, to a receiver, Bob, contains a hidden\nmessage. We can reasonably assess that the steganalyst does not know the\nstrategy used to spread the payload across images. To the best of our\nknowledge, in this case, the most appropriate solution for pooled steganalysis\nis to use a Single-Image Detector (SID) to estimate/quantify if an image is\ncover or stego, and to average the scores obtained on the set of images.\n  In such a scenario, where Eve does not know the spreading strategies, we\nexperimentally show that if Eve can discriminate among few well-known spreading\nstrategies, she can improve her steganalysis performances compared to a simple\naveraging or maximum pooled approach. Our discriminative approach allows\nobtaining steganalysis efficiencies comparable to those obtained by a\nclairvoyant, Eve, who knows the Alice spreading strategy. Another interesting\nobservation is that DeLS spreading strategy behaves really better than all the\nother spreading strategies.\n  Those observations results in the experimentation with six different\nspreading strategies made on Jpeg images with J-UNIWARD, a state-of-the-art\nSingle-Image-Detector, and a discriminative architecture that is invariant to\nthe individual payload in each image, invariant to the size of the analyzed set\nof images, and build on a binary detector (for the pooling) that is able to\ndeal with various spreading strategies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 09:55:05 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 13:51:44 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zakaria", "Ahmad", ""], ["Chaumont", "Marc", ""], ["Subsol", "G\u00e9rard", ""]]}, {"id": "1906.11620", "submitter": "Jie Wang", "authors": "Wenhao Bian, Jie Wang, Bojin Zhuang, Jiankui Yang, Shaojun Wang and\n  Jing Xiao", "title": "Audio-Based Music Classification with DenseNet And Data Augmentation", "comments": "accepted by The 16th Pacific Rim International Conference on AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning technique has received intense attention owing\nto its great success in image recognition. A tendency of adaption of deep\nlearning in various information processing fields has formed, including music\ninformation retrieval (MIR). In this paper, we conduct a comprehensive study on\nmusic audio classification with improved convolutional neural networks (CNNs).\nTo the best of our knowledge, this the first work to apply Densely Connected\nConvolutional Networks (DenseNet) to music audio tagging, which has been\ndemonstrated to perform better than Residual neural network (ResNet).\nAdditionally, two specific data augmentation approaches of time overlapping and\npitch shifting have been proposed to address the deficiency of labelled data in\nthe MIR. Moreover, an ensemble learning of stacking is employed based on SVM.\nWe believe that the proposed combination of strong representation of DenseNet\nand data augmentation can be adapted to other audio processing tasks.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 09:16:08 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Bian", "Wenhao", ""], ["Wang", "Jie", ""], ["Zhuang", "Bojin", ""], ["Yang", "Jiankui", ""], ["Wang", "Shaojun", ""], ["Xiao", "Jing", ""]]}, {"id": "1906.11783", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Jiyoung Park, Juhan Nam", "title": "Representation Learning of Music Using Artist, Album, and Track\n  Information", "comments": "International Conference on Machine Learning (ICML) 2019, Machine\n  Learning for Music Discovery Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised music representation learning has been performed mainly using\nsemantic labels such as music genres. However, annotating music with semantic\nlabels requires time and cost. In this work, we investigate the use of factual\nmetadata such as artist, album, and track information, which are naturally\nannotated to songs, for supervised music representation learning. The results\nshow that each of the metadata has individual concept characteristics, and\nusing them jointly improves overall performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 16:42:40 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lee", "Jongpil", ""], ["Park", "Jiyoung", ""], ["Nam", "Juhan", ""]]}, {"id": "1906.11871", "submitter": "Ahmet Karak\\\"u\\c{c}\\\"uk", "authors": "Ahmet Karak\\\"u\\c{c}\\\"uk and Ahmet Emir Dirik", "title": "PRNU Based Source Camera Attribution for Image Sets Anonymized with\n  Patch-Match Algorithm", "comments": "Dataset avaiable on https://github.com/akarakucuk/2019_PM_SCI_DATA/", "journal-ref": null, "doi": "10.1016/j.diin.2019.06.001", "report-no": null, "categories": "eess.IV cs.MM eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Patch-Match is an efficient algorithm used for structural image editing and\navailable as a tool on popular commercial photo-editing software. The tool\nallows users to insert or remove objects from photos using information from\nsimilar scene content. Recently, a modified version of this algorithm was\nproposed as a counter-measure against Photo-Response Non-Uniformity (PRNU)\nbased Source Camera Identification (SCI). The algorithm can provide anonymity\nat a great rate (97\\%) and impede PRNU based SCI without the need of any other\ninformation, hence leaving no-known recourse for the PRNU-based SCI. In this\npaper, we propose a method to identify sources of the Patch-Match-applied\nimages by using randomized subsets of images and the traditional PRNU based SCI\nmethods. We evaluate the proposed method on two forensics scenarios in which an\nadversary makes use of the Patch-Match algorithm and distorts the PRNU noise\npattern in the incriminating images he took with his camera. Our results show\nthat it is possible to link sets of Patch-Match-applied images back to their\nsource camera even in the presence of images that come from unknown cameras. To\nour best knowledge, the proposed method represents the very first\ncounter-measure against the usage of of Patch-Match in the digital forensics\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 18:40:35 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Karak\u00fc\u00e7\u00fck", "Ahmet", ""], ["Dirik", "Ahmet Emir", ""]]}, {"id": "1906.12088", "submitter": "Chao Shi", "authors": "Chao Shi", "title": "Non-user Inclusive Design for Maintaining Harmony of Real-Virtual Human\n  Interaction in Augmented Reality", "comments": "9 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality enables the illusion of contents such as objects and humans\nin the virtual world co-existing with users in the real world. However,\nnon-users who are not aware of the presence of the virtual world and\ndynamically move nearby might either cause a conflict by directly breaking into\nspace where a user is talking to a Virtual Human (VH), or be troubled when try\nto avoid disturbing the user. To maintain harmony and keep both the user's and\nnon-users' comfort, we propose a method that controls the VH to adjust its own\nposition to avoid such potential conflict. The difficulty to address this\nproblem is that the agent must avoid potential conflict in a natural way to\nkeep the user away from feeling unnatural. Our idea is to endow the VH with\nthree capabilities: anticipating non-users walking around, understanding how to\nestablish and maintain proper formation to adapt to the environment, and\nplanning to avoid conflicts by shifting formation in advance. We develop a\nnon-user inclusive spatial formation model that realizes natural arrangement\nshift corresponding to the environment based on theoretical sources from\nliterature. We implemented our proposed model into a VH behavior planning\nsystem to achieve natural conflict avoidance. Evaluation experiments showed\nthat it successfully reduces potential conflicts caused by non-users.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 08:32:22 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Shi", "Chao", ""]]}, {"id": "1906.12324", "submitter": "Jie Wang", "authors": "Haiqian Gu, Jie Wang, Ziwen Wang, Bojin Zhuang, Wenhao Bian, Fei Su", "title": "Cross-Platform Modeling of Users' Behavior on Social Media", "comments": "Published in IEEE International Conference on Data Mining Workshops\n  (ICDMW) 2018", "journal-ref": "2018 IEEE International Conference on Data Mining Workshops\n  (ICDMW) (2018): 183-190", "doi": "10.1109/icdmw.2018.00035", "report-no": null, "categories": "cs.SI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the booming development and popularity of mobile applications, different\nverticals accumulate abundant data of user information and social behavior,\nwhich are spontaneous, genuine and diversified. However, each platform\ndescribes user's portraits in only certain aspect, resulting in difficult\ncombination of those internet footprints together. In our research, we proposed\na modeling approach to analyze user's online behavior across different social\nmedia platforms. Structured and unstructured data of same users shared by\nNetEase Music and Sina Weibo have been collected for cross-platform analysis of\ncorrelations between music preference and other users' characteristics. Based\non music tags of genre and mood, genre cluster of five groups and mood cluster\nof four groups have been formed by computing their collected song lists with\nK-means method. Moreover, with the help of user data of Weibo, correlations\nbetween music preference (i.e. genre, mood) and Big Five personalities (BFPs)\nand basic information (e.g. gender, resident region, tags) have been\ncomprehensively studied, building up full-scale user portraits with finer\ngrain. Our findings indicate that people's music preference could be linked\nwith their real social activities. For instance, people living in mountainous\nareas generally prefer folk music, while those in urban areas like pop music\nmore. Interestingly, dog lovers could love sad music more than cat lovers.\nMoreover, our proposed cross-platform modeling approach could be adapted to\nother verticals, providing an online automatic way for profiling users in a\nmore precise and comprehensive way.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 23:09:25 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Gu", "Haiqian", ""], ["Wang", "Jie", ""], ["Wang", "Ziwen", ""], ["Zhuang", "Bojin", ""], ["Bian", "Wenhao", ""], ["Su", "Fei", ""]]}]