[{"id": "1509.00464", "submitter": "Laura Toni", "authors": "Laura Toni, Gene Cheung, and Pascal Frossard", "title": "In-Network View Synthesis for Interactive Multiview Video Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable Interactive multiview video systems with a minimum view-switching\ndelay, multiple camera views are sent to the users, which are used as reference\nimages to synthesize additional virtual views via depth-image-based rendering.\nIn practice, bandwidth constraints may however restrict the number of reference\nviews sent to clients per time unit, which may in turn limit the quality of the\nsynthesized viewpoints. We argue that the reference view selection should\nideally be performed close to the users, and we study the problem of in-network\nreference view synthesis such that the navigation quality is maximized at the\nclients. We consider a distributed cloud network architecture where data stored\nin a main cloud is delivered to end users with the help of cloudlets, i.e.,\nresource-rich proxies close to the users. In order to satisfy last-hop\nbandwidth constraints from the cloudlet to the users, a cloudlet re-samples\nviewpoints of the 3D scene into a discrete set of views (combination of\nreceived camera views and virtual views synthesized) to be used as reference\nfor the synthesis of additional virtual views at the client. This in-network\nsynthesis leads to better viewpoint sampling given a bandwidth constraint\ncompared to simple selection of camera views, but it may however carry a\ndistortion penalty in the cloudlet-synthesized reference views. We therefore\ncast a new reference view selection problem where the best subset of views is\ndefined as the one minimizing the distortion over a view navigation window\ndefined by the user under some transmission bandwidth constraints. We show that\nthe view selection problem is NP-hard, and propose an effective polynomial time\nalgorithm using dynamic programming to solve the optimization problem.\nSimulation results finally confirm the performance gain offered by virtual view\nsynthesis in the network.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 19:57:33 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Toni", "Laura", ""], ["Cheung", "Gene", ""], ["Frossard", "Pascal", ""]]}, {"id": "1509.00511", "submitter": "Xitong Yang", "authors": "Xitong Yang, Yuncheng Li, Jiebo Luo", "title": "Pinterest Board Recommendation for Twitter Users", "comments": null, "journal-ref": null, "doi": "10.1145/2733373.2806375", "report-no": null, "categories": "cs.SI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pinboard on Pinterest is an emerging media to engage online social media\nusers, on which users post online images for specific topics. Regardless of its\nsignificance, there is little previous work specifically to facilitate\ninformation discovery based on pinboards. This paper proposes a novel pinboard\nrecommendation system for Twitter users. In order to associate contents from\nthe two social media platforms, we propose to use MultiLabel classification to\nmap Twitter user followees to pinboard topics and visual diversification to\nrecommend pinboards given user interested topics. A preliminary experiment on a\ndataset with 2000 users validated our proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 21:42:27 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Yang", "Xitong", ""], ["Li", "Yuncheng", ""], ["Luo", "Jiebo", ""]]}, {"id": "1509.02630", "submitter": "Satish Bhalshankar Mr", "authors": "Satish Bhalshankar and Avinash K. Gulve", "title": "Audio Steganography: LSB Technique Using a Pyramid Structure and Range\n  of Bytes", "comments": "12 page, 16 Figures", "journal-ref": "International Journal of Advanced Computer Research (IJACR),\n  Volume-5, Issue-20, September-2015 ,pp.233-248", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The demand for keeping the information secure and confidential simultaneously\nhas been progressively increasing. Among various techniques- Audio\nSteganography, a technique of embedding information transparently in a digital\nmedia thereby restricting the access to such information has been prominently\ndeveloped. Imperceptibility, robustness, and payload or hiding capacity are the\nmain character for it. In earlier, LSB techniques increased payload capacity\nwould hamper robustness as well as imperceptibility of the cover media and vice\nversa. The proposed technique overcomes the problem. It provides relatively\ngood improvement in the payload capacity by dividing the bytes of cover media\ninto ranges to hide the bits of secret message appropriately. As well as due to\nthe use of ranges of bytes the robustness of cover media has maintained and\nimperceptibility preserved by using a pyramid structure.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 04:00:59 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Bhalshankar", "Satish", ""], ["Gulve", "Avinash K.", ""]]}, {"id": "1509.02995", "submitter": "Gene Cheung", "authors": "Wei Dai, Gene Cheung, Ngai-Man Cheung, Antonio Ortega, Oscar C. Au", "title": "Merge Frame Design for Video Stream Switching using Piecewise Constant\n  Functions", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2571564", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to efficiently switch from one pre-encoded video stream to\nanother (e.g., for bitrate adaptation or view switching) is important for many\ninteractive streaming applications. Recently, stream-switching mechanisms based\non distributed source coding (DSC) have been proposed. In order to reduce the\noverall transmission rate, these approaches provide a \"merge\" mechanism, where\ninformation is sent to the decoder such that the exact same frame can be\nreconstructed given that any one of a known set of side information (SI) frames\nis available at the decoder (e.g., each SI frame may correspond to a different\nstream from which we are switching). However, the use of bit-plane coding and\nchannel coding in many DSC approaches leads to complex coding and decoding. In\nthis paper, we propose an alternative approach for merging multiple SI frames,\nusing a piecewise constant (PWC) function as the merge operator. In our\napproach, for each block to be reconstructed, a series of parameters of these\nPWC merge functions are transmitted in order to guarantee identical\nreconstruction given the known side information blocks. We consider two\ndifferent scenarios. In the first case, a target frame is first given, and then\nmerge parameters are chosen so that this frame can be reconstructed exactly at\nthe decoder. In contrast, in the second scenario, the reconstructed frame and\nmerge parameters are jointly optimized to meet a rate-distortion criteria.\nExperiments show that for both scenarios, our proposed merge techniques can\noutperform both a recent approach based on DSC and the SP-frame approach in\nH.264, in terms of compression efficiency and decoder complexity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 03:27:33 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Dai", "Wei", ""], ["Cheung", "Gene", ""], ["Cheung", "Ngai-Man", ""], ["Ortega", "Antonio", ""], ["Au", "Oscar C.", ""]]}, {"id": "1509.03278", "submitter": "Arash Saboori", "authors": "Arash Saboori, S. Abolfazl Hosseini", "title": "A New Method For Digital Watermarking Based on Combination of DCT and\n  PCA", "comments": "Telecommunications Forum Telfor (TELFOR), 2014 22nd", "journal-ref": null, "doi": "10.1109/TELFOR.2014.7034461", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the digital watermarking with DCT method,the watermark is located within a\nrange of DCT coefficients of the cover image. In this paper to use the\nlow-frequency band, a new method is proposed by using a combination of the DCT\nand PCA transform. The proposed method is compared to other DCT methods, our\nmethod is robust and keeps the quality of cover image, also increases capacity\nof the watermarking.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 19:01:14 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Saboori", "Arash", ""], ["Hosseini", "S. Abolfazl", ""]]}, {"id": "1509.03565", "submitter": "Michael Kirsche", "authors": "Zhongliang Zhao, Denis Rosario, Torsten Braun and Eduardo Cerqueira", "title": "A Tutorial of the Mobile Multimedia Wireless Sensor Network OMNeT++\n  Framework", "comments": "Published in: A. F\\\"orster, C. Minkenberg, G. R. Herrera, M. Kirsche\n  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,\n  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015", "journal-ref": null, "doi": null, "report-no": "OMNET/2015/19", "categories": "cs.NI cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we will give a detailed tutorial instruction about how to use\nthe Mobile Multi-Media Wireless Sensor Networks (M3WSN) simulation framework.\nThe M3WSN framework has been published as a scientific paper in the 6th\nInternational Workshop on OMNeT++ (2013). M3WSN framework enables the\nmultimedia transmission of real video sequence. Therefore, a set of multimedia\nalgorithms, protocols, and services can be evaluated by using QoE metrics.\nMoreover, key video-related information, such as frame types, GoP length and\nintra-frame dependency can be used for creating new assessment and optimization\nsolutions. To support mobility, M3WSN utilizes different mobility traces to\nenable the understanding of how the network behaves under mobile situations.\nThis tutorial will cover how to install and configure the M3WSN framework,\nsetting and running the experiments, creating mobility and video traces, and\nhow to evaluate the performance of different protocols. The tutorial will be\ngiven in an environment of Ubuntu 12.04 LTS and OMNeT++ 4.2.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 15:46:45 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Zhao", "Zhongliang", ""], ["Rosario", "Denis", ""], ["Braun", "Torsten", ""], ["Cerqueira", "Eduardo", ""]]}, {"id": "1509.03836", "submitter": "Kota Naga Srinivasarao Batta", "authors": "Batta Kota Naga Srinivasarao and Indrajit Chakrabarti", "title": "Hardware Implementation of Compressed Sensing based Low Complex Video\n  Encoder", "comments": "Submitted in IEEE transactions on VLSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a memory efficient VLSI architecture of low complex video\nencoder using three dimensional (3-D) wavelet and Compressed Sensing (CS) is\nproposed for space and low power video applications. Majority of the\nconventional video coding schemes are based on hybrid model, which requires\ncomplex operations like transform coding (DCT), motion estimation and\ndeblocking filter at the encoder. Complexity of the proposed encoder is reduced\nby replacing those complex operations by 3-D DWT and CS at the encoder. The\nproposed architecture uses 3-D DWT to enable the scalability with levels of\nwavelet decomposition and also to exploit the spatial and the temporal\nredundancies. CS provides the good error resilience and coding efficiency. At\nthe first stage of the proposed architecture for encoder, 3-D DWT has been\napplied (Lifting based 2-D DWT in spatial domain and Haar wavelet in temporal\ndomain) on each frame of the group of frames (GOF), and in the second stage CS\nmodule exploits the sparsity of the wavelet coefficients. Small set of linear\nmeasurements are extracted by projecting the sparse 3-D wavelet coefficients\nonto random Bernoulli matrix at the encoder. Compared with the best existing\n3-D DWT architectures, the proposed architecture for 3-D DWT requires less\nmemory and provide high throughput. For an N?N image, the proposed 3-D DWT\narchitecture consumes a total of only 2?(3N +40P) words of on-chip memory for\nthe one level of decomposition. The proposed architecture for an encoder is\nfirst of its kind and to the best of my knowledge, no architecture is noted for\ncomparison. The proposed VLSI architecture of the encoder has been synthesized\non 90-nm CMOS process technology and results show that it consumes 90.08 mW\npower and occupies an area equivalent to 416.799 K equivalent gate at frequency\nof 158 MHz.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 11:39:09 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Srinivasarao", "Batta Kota Naga", ""], ["Chakrabarti", "Indrajit", ""]]}, {"id": "1509.03844", "submitter": "Yiannis Andreopoulos", "authors": "Alhabib Abbas, Nikos Deligiannis and Yiannis Andreopoulos", "title": "Vectors of Locally Aggregated Centers for Compact Video Representation", "comments": "Proc. IEEE International Conference on Multimedia and Expo, ICME\n  2015, Torino, Italy", "journal-ref": null, "doi": "10.1109/ICME.2015.7177501", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel vector aggregation technique for compact video\nrepresentation, with application in accurate similarity detection within large\nvideo datasets. The current state-of-the-art in visual search is formed by the\nvector of locally aggregated descriptors (VLAD) of Jegou et. al. VLAD generates\ncompact video representations based on scale-invariant feature transform (SIFT)\nvectors (extracted per frame) and local feature centers computed over a\ntraining set. With the aim to increase robustness to visual distortions, we\npropose a new approach that operates at a coarser level in the feature\nrepresentation. We create vectors of locally aggregated centers (VLAC) by first\nclustering SIFT features to obtain local feature centers (LFCs) and then\nencoding the latter with respect to given centers of local feature centers\n(CLFCs), extracted from a training set. The sum-of-differences between the LFCs\nand the CLFCs are aggregated to generate an extremely-compact video description\nused for accurate video segment similarity detection. Experimentation using a\nvideo dataset, comprising more than 1000 minutes of content from the Open Video\nProject, shows that VLAC obtains substantial gains in terms of mean Average\nPrecision (mAP) against VLAD and the hyper-pooling method of Douze et. al.,\nunder the same compaction factor and the same set of distortions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 13:06:36 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Abbas", "Alhabib", ""], ["Deligiannis", "Nikos", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1509.04387", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, Sushmit Mallik, Ankit Chadha, Ravdeep Johar and M. Mani\n  Roja", "title": "Dual-Layer Video Encryption using RSA Algorithm", "comments": "arXiv admin note: text overlap with arXiv:1104.0800, arXiv:1112.0836\n  by other authors", "journal-ref": "International Journal of Computer Applications 116(1):33-40, April\n  2015", "doi": "10.5120/20302-2341", "report-no": null, "categories": "cs.CR cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a video encryption algorithm using RSA and Pseudo Noise\n(PN) sequence, aimed at applications requiring sensitive video information\ntransfers. The system is primarily designed to work with files encoded using\nthe Audio Video Interleaved (AVI) codec, although it can be easily ported for\nuse with Moving Picture Experts Group (MPEG) encoded files. The audio and video\ncomponents of the source separately undergo two layers of encryption to ensure\na reasonable level of security. Encryption of the video component involves\napplying the RSA algorithm followed by the PN-based encryption. Similarly, the\naudio component is first encrypted using PN and further subjected to encryption\nusing the Discrete Cosine Transform. Combining these techniques, an efficient\nsystem, invulnerable to security breaches and attacks with favorable values of\nparameters such as encryption/decryption speed, encryption/decryption ratio and\nvisual degradation; has been put forth. For applications requiring encryption\nof sensitive data wherein stringent security requirements are of prime concern,\nthe system is found to yield negligible similarities in visual perception\nbetween the original and the encrypted video sequence. For applications wherein\nvisual similarity is not of major concern, we limit the encryption task to a\nsingle level of encryption which is accomplished by using RSA, thereby\nquickening the encryption process. Although some similarity between the\noriginal and encrypted video is observed in this case, it is not enough to\ncomprehend the happenings in the video.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 06:52:20 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Chadha", "Aman", ""], ["Mallik", "Sushmit", ""], ["Chadha", "Ankit", ""], ["Johar", "Ravdeep", ""], ["Roja", "M. Mani", ""]]}, {"id": "1509.04581", "submitter": "Zhen Liu", "authors": "Zhen Liu", "title": "Kernelized Deep Convolutional Neural Network for Describing Complex\n  Images", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the impressive capability to capture visual content, deep convolutional\nneural networks (CNN) have demon- strated promising performance in various\nvision-based ap- plications, such as classification, recognition, and objec- t\ndetection. However, due to the intrinsic structure design of CNN, for images\nwith complex content, it achieves lim- ited capability on invariance to\ntranslation, rotation, and re-sizing changes, which is strongly emphasized in\nthe s- cenario of content-based image retrieval. In this paper, to address this\nproblem, we proposed a new kernelized deep convolutional neural network. We\nfirst discuss our motiva- tion by an experimental study to demonstrate the\nsensitivi- ty of the global CNN feature to the basic geometric trans-\nformations. Then, we propose to represent visual content with approximate\ninvariance to the above geometric trans- formations from a kernelized\nperspective. We extract CNN features on the detected object-like patches and\naggregate these patch-level CNN features to form a vectorial repre- sentation\nwith the Fisher vector model. The effectiveness of our proposed algorithm is\ndemonstrated on image search application with three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 14:35:11 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Liu", "Zhen", ""]]}, {"id": "1509.04751", "submitter": "Cheng-I Wang", "authors": "Tammuz Dubnov and Cheng-i Wang", "title": "Free-body Gesture Tracking and Augmented Reality Improvisation for Floor\n  and Aerial Dance", "comments": "8 pages. Technical paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an updated interactive performance system for floor and\nAerial Dance that controls visual and sonic aspects of the presentation via a\ndepth sensing camera (MS Kinect). In order to detect, measure and track free\nmovement in space, 3 degree of freedom (3-DOF) tracking in space (on the ground\nand in the air) is performed using IR markers with a method for multi target\ntracking capabilities added and described in detail. An improved gesture\ntracking and recognition system, called Action Graph (AG), is described in the\npaper. Action Graph uses an efficient incremental construction from a single\nlong sequence of movement features and automatically captures repeated\nsub-segments in the movement from start to finish with no manual interaction\nneeded with other advanced capabilities discussed as well. By using the new\nmodel for the gesture we can unify an entire choreography piece by dynamically\ntracking and recognizing gestures and sub-portions of the piece. This gives the\nperformer the freedom to improvise based on a set of recorded gestures/portions\nof the choreography and have the system dynamically respond in relation to the\nperformer within a set of related rehearsed actions, an ability that has not\nbeen seen in any other system to date.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 21:54:21 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Dubnov", "Tammuz", ""], ["Wang", "Cheng-i", ""]]}, {"id": "1509.05671", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, Yang Cong, Tao Mei, Jiebo Luo", "title": "User-Curated Image Collections: Modeling and Recommendation", "comments": "in IEEE BigData 2015", "journal-ref": null, "doi": "10.1109/BigData.2015.7363803", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art image retrieval and recommendation systems\npredominantly focus on individual images. In contrast, socially curated image\ncollections, condensing distinctive yet coherent images into one set, are\nlargely overlooked by the research communities. In this paper, we aim to design\na novel recommendation system that can provide users with image collections\nrelevant to individual personal preferences and interests. To this end, two key\nissues need to be addressed, i.e., image collection modeling and similarity\nmeasurement. For image collection modeling, we consider each image collection\nas a whole in a group sparse reconstruction framework and extract concise\ncollection descriptors given the pretrained dictionaries. We then consider\nimage collection recommendation as a dynamic similarity measurement problem in\nresponse to user's clicked image set, and employ a metric learner to measure\nthe similarity between the image collection and the clicked image set. As there\nis no previous work directly comparable to this study, we implement several\ncompetitive baselines and related methods for comparison. The evaluations on a\nlarge scale Pinterest data set have validated the effectiveness of our proposed\nmethods for modeling and recommending image collections.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 15:45:41 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Li", "Yuncheng", ""], ["Cong", "Yang", ""], ["Mei", "Tao", ""], ["Luo", "Jiebo", ""]]}, {"id": "1509.05849", "submitter": "Frank Ong", "authors": "Frank Ong, Sameer Pawar, and Kannan Ramchandran", "title": "Fast and Efficient Sparse 2D Discrete Fourier Transform using\n  Sparse-Graph Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm, named the 2D-FFAST, to compute a sparse\n2D-Discrete Fourier Transform (2D-DFT) featuring both low sample complexity and\nlow computational complexity. The proposed algorithm is based on mixed concepts\nfrom signal processing (sub-sampling and aliasing), coding theory (sparse-graph\ncodes) and number theory (Chinese-remainder-theorem) and generalizes the\n1D-FFAST 2 algorithm recently proposed by Pawar and Ramchandran [1] to the 2D\nsetting. Concretely, our proposed 2D-FFAST algorithm computes a k-sparse\n2D-DFT, with a uniformly random support, of size N = Nx x Ny using O(k)\nnoiseless spatial-domain measurements in O(k log k) computational time. Our\nresults are attractive when the sparsity is sub-linear with respect to the\nsignal dimension, that is, when k -> infinity and k/N -> 0. For the case when\nthe spatial-domain measurements are corrupted by additive noise, our 2D-FFAST\nframework extends to a noise-robust version in sub-linear time of O(k log4 N )\nusing O(k log3 N ) measurements. Simulation results, on synthetic images as\nwell as real-world magnetic resonance images, are provided in Section VII and\ndemonstrate the empirical performance of the proposed 2D-FFAST algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 05:56:00 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Ong", "Frank", ""], ["Pawar", "Sameer", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1509.06086", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Yu-Gang Jiang, Xi Wang, Hao Ye, Xiangyang Xue, Jun Wang", "title": "Fusing Multi-Stream Deep Networks for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies deep network architectures to address the problem of video\nclassification. A multi-stream framework is proposed to fully utilize the rich\nmultimodal information in videos. Specifically, we first train three\nConvolutional Neural Networks to model spatial, short-term motion and audio\nclues respectively. Long Short Term Memory networks are then adopted to explore\nlong-term temporal dynamics. With the outputs of the individual streams, we\npropose a simple and effective fusion method to generate the final predictions,\nwhere the optimal fusion weights are learned adaptively for each class, and the\nlearning process is regularized by automatically estimated class relationships.\nOur contributions are two-fold. First, the proposed multi-stream framework is\nable to exploit multimodal features that are more comprehensive than those\npreviously attempted. Second, we demonstrate that the adaptive fusion method\nusing the class relationship as a regularizer outperforms traditional\nalternatives that estimate the weights in a \"free\" fashion. Our framework\nproduces significantly better results than the state of the arts on two popular\nbenchmarks, 92.2\\% on UCF-101 (without using audio) and 84.9\\% on Columbia\nConsumer Videos.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 00:38:54 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 01:29:44 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Wu", "Zuxuan", ""], ["Jiang", "Yu-Gang", ""], ["Wang", "Xi", ""], ["Ye", "Hao", ""], ["Xue", "Xiangyang", ""], ["Wang", "Jun", ""]]}, {"id": "1509.06792", "submitter": "Abbas Soltanian", "authors": "Abbas Soltanian, Mohammad A. Salahuddin, Halima Elbiaze, Roch Glitho", "title": "A Resource Allocation Mechanism for Video Mixing as a Cloud Computing\n  Service in Multimedia Conferencing Applications", "comments": "6 pages, CNSM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia conferencing is the conversational exchange of multimedia content\nbetween multiple parties. It has a wide range of applications (e.g. Massively\nMultiplayer Online Games (MMOGs) and distance learning). Many multimedia\nconferencing applications use video extensively, thus video mixing in\nconferencing settings is of critical importance. Cloud computing is a\ntechnology that can solve the scalability issue in multimedia conferencing,\nwhile bringing other benefits, such as, elasticity, efficient use of resources,\nrapid development, and introduction of new applications. However, proposed\ncloud-based multimedia conferencing approaches so far have several deficiencies\nwhen it comes to efficient resource usage while meeting Quality of Service\n(QoS) requirements. We propose a solution to optimize resource allocation for\ncloud-based video mixing service in multimedia conferencing applications, which\ncan support scalability in terms of number of users, while guaranteeing QoS. We\nformulate the resource allocation problem mathematically as an Integer Linear\nProgramming (ILP) problem and design a heuristic for it. Simulation results\nshow that our resource allocation model can support more participants compared\nto the state-of-the-art, while honoring QoS, with respect to end-to-end delay.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 21:44:28 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Soltanian", "Abbas", ""], ["Salahuddin", "Mohammad A.", ""], ["Elbiaze", "Halima", ""], ["Glitho", "Roch", ""]]}, {"id": "1509.07627", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Kenji Iwata, Yutaka Satoh", "title": "Feature Evaluation of Deep Convolutional Neural Networks for Object\n  Recognition and Detection", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate convolutional neural network (CNN) features using\nthe AlexNet architecture and very deep convolutional network (VGGNet)\narchitecture. To date, most CNN researchers have employed the last layers\nbefore output, which were extracted from the fully connected feature layers.\nHowever, since it is unlikely that feature representation effectiveness is\ndependent on the problem, this study evaluates additional convolutional layers\nthat are adjacent to fully connected layers, in addition to executing simple\ntuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and\ntransformation, using tools such as principal component analysis. In our\nexperiments, we carried out detection and classification tasks using the\nCaltech 101 and Daimler Pedestrian Benchmark Datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 08:26:53 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Iwata", "Kenji", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1509.08743", "submitter": "Debajit Sensarma", "authors": "Debajit Sensarma and Samar Sen Sarma", "title": "Data Hiding using Graphical Code based Steganography Technique", "comments": "5 pages, 3 figures, 2 tables, International Journal of Engineering\n  Trends and Technology (IJETT),Volume 27 Number 3, September 2015", "journal-ref": null, "doi": "10.14445/22315381/IJETT-V27P225", "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data hiding has received much attention due to rapid development of internet\nand multimedia technologies where security of information is a very important\nconcern. This is achieved by Steganography, which is the art or science of\nhiding data into another data, so that human eyes cannot catch the hidden\ninformation easily. There are many ways to hide information-like inside an\nimage, text, audio/ video etc. Among them image steganography is a very\nattractive research area. The goal is to transmit a data within a modified\nimage (called stego-image)by minimizing the number of bit flips. In this paper,\na new steganography technique has been proposed using Graphical codes and also\ncomparison with steganography technique using BCH codes has been studied.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 13:31:08 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Sensarma", "Debajit", ""], ["Sarma", "Samar Sen", ""]]}]