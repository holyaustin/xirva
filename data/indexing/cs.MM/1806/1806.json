[{"id": "1806.00257", "submitter": "Ligang Zhang", "authors": "Ligang Zhang and Jiulong Zhang", "title": "Synchronous Prediction of Arousal and Valence Using LSTM Network for\n  Affective Video Content Analysis", "comments": "pages 689-694, proceedings of ICNC-FSKD 2017", "journal-ref": "2017 13th International Conference on Natural Computation, Fuzzy\n  Systems and Knowledge Discovery (ICNC-FSKD 2017)", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The affect embedded in video data conveys high-level semantic information\nabout the content and has direct impact on the understanding and perception of\nreviewers, as well as their emotional responses. Affective Video Content\nAnalysis (AVCA) attempts to generate a direct mapping between video content and\nthe corresponding affective states such as arousal and valence dimensions. Most\nexisting studies establish the mapping for each dimension separately using\nknowledge-based rules or traditional classifiers such as Support Vector Machine\n(SVM). The inherent correlations between affective dimensions have largely been\nunexploited, which are anticipated to include important information for\naccurate prediction of affective dimensions. To address this issue, this paper\npresents an approach to predict arousal and valance dimensions synchronously\nusing the Long Short Term Memory (LSTM) network. The approach extracts a set of\nlow-level audio and visual features from video data and projects them\nsynchronously into pairs of arousal and valence values using the LSTM network\nwhich automatically incorporates the correlations between arousal and valance\ndimensions. We evaluate the performance of the proposed approach on a dataset\ncomprising video clips segmented from real-world resources such as film, drama,\nand news, and demonstrate its superior performance over the traditional SVM\nbased method. The results provide one of the earliest preliminary evidence to\nthe benefit of considering correlations between affective dimensions towards\naccurate AVCA.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 09:44:56 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Zhang", "Ligang", ""], ["Zhang", "Jiulong", ""]]}, {"id": "1806.00263", "submitter": "Fabio Calefato", "authors": "Fabio Calefato and Giovanna Castellano and Veronica Rossano", "title": "A Revision Control System for Image Editing in Collaborative Multimedia\n  Design", "comments": "pp. 512-517 (6 pages)", "journal-ref": "Proc. 22nd Int'l Conf. on Information Visualisation (iV2018),\n  Salerno, Italy, 10-13 July 2018", "doi": "10.1109/iV.2018.00095", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revision control is a vital component in the collaborative development of\nartifacts such as software code and multimedia. While revision control has been\nwidely deployed for text files, very few attempts to control the versioning of\nbinary files can be found in the literature. This can be inconvenient for\ngraphics applications that use a significant amount of binary data, such as\nimages, videos, meshes, and animations. Existing strategies such as storing\nwhole files for individual revisions or simple binary deltas, respectively\nconsume significant storage and obscure semantic information. To overcome these\nlimitations, in this paper we present a revision control system for digital\nimages that stores revisions in form of graphs. Besides, being integrated with\nGit, our revision control system also facilitates artistic creation processes\nin common image editing and digital painting workflows. A preliminary user\nstudy demonstrates the usability of the proposed system.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 10:06:51 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 16:04:05 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Calefato", "Fabio", ""], ["Castellano", "Giovanna", ""], ["Rossano", "Veronica", ""]]}, {"id": "1806.00571", "submitter": "Chengyuan Zhang", "authors": "Jun Long, Lei Zhu, Chengyuan Zhang, Zhan Yang, Yunwu Lin and Ruipeng\n  Chen", "title": "Efficient Interactive Search for Geo-tagged Multimedia Data", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advances in mobile computing and multimedia techniques, there are\nvast amount of multimedia data with geographical information collected in\nmultifarious applications. In this paper, we propose a novel type of image\nsearch named interactive geo-tagged image search which aims to find out a set\nof images based on geographical proximity and similarity of visual content, as\nwell as the preference of users. Existing approaches for spatial keyword query\nand geo-image query cannot address this problem effectively since they do not\nconsider these three type of information together for query. In order to solve\nthis challenge efficiently, we propose the definition of interactive top-$k$\ngeo-tagged image query and then present a framework including candidate search\nstage , interaction stage and termination stage. To enhance the searching\nefficiency in a large-scale database, we propose the candidate search algorithm\nnamed GI-SUPER Search based on a new notion called superior relationship and\nGIR-Tree, a novel index structure. Furthermore, two candidate selection methods\nare proposed for learning the preferences of the user during the interaction.\nAt last, the termination procedure and estimation procedure are introduced in\nbrief. Experimental evaluation on real multimedia dataset demonstrates that our\nsolution has a really high performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 02:11:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Long", "Jun", ""], ["Zhu", "Lei", ""], ["Zhang", "Chengyuan", ""], ["Yang", "Zhan", ""], ["Lin", "Yunwu", ""], ["Chen", "Ruipeng", ""]]}, {"id": "1806.00874", "submitter": "Chieh-Chi Kao", "authors": "Chieh-Chi Kao, Yuxiang Wang, Jonathan Waltman, Pradeep Sen", "title": "Patch-Based Image Hallucination for Super Resolution with Detail\n  Reconstruction from Similar Sample Images", "comments": "13 pages, 8 figures, submitted to IEEE Transactions on Multimedia,\n  under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hallucination and super-resolution have been studied for decades, and\nmany approaches have been proposed to upsample low-resolution images using\ninformation from the images themselves, multiple example images, or large image\ndatabases. However, most of this work has focused exclusively on small\nmagnification levels because the algorithms simply sharpen the blurry edges in\nthe upsampled images - no actual new detail is typically reconstructed in the\nfinal result. In this paper, we present a patch-based algorithm for image\nhallucination which, for the first time, properly synthesizes novel high\nfrequency detail. To do this, we pose the synthesis problem as a patch-based\noptimization which inserts coherent, high-frequency detail from\ncontextually-similar images of the same physical scene/subject provided from\neither a personal image collection or a large online database. The resulting\nimage is visually plausible and contains coherent high frequency information.\nWe demonstrate the robustness of our algorithm by testing it on a large number\nof images and show that its performance is considerably superior to all\nstate-of-the-art approaches, a result that is verified to be statistically\nsignificant through a randomized user study.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 20:59:43 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kao", "Chieh-Chi", ""], ["Wang", "Yuxiang", ""], ["Waltman", "Jonathan", ""], ["Sen", "Pradeep", ""]]}, {"id": "1806.01081", "submitter": "Nattachai Watcharapinchai", "authors": "Nattachai Watcharapinchai, Sitapa Rujikietgumjorn, Sanparith Marukatat", "title": "Sloth Search System at the Video Browser Showdown 2018 - Final Notes", "comments": "Final note paper about the Sloth Search System at the VBS 2018\n  competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper provides further details of the Sloth Search System, which\nwas developed by the NECTEC team for the Video Browser Showdown (VBS) 2018.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 12:49:03 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Watcharapinchai", "Nattachai", ""], ["Rujikietgumjorn", "Sitapa", ""], ["Marukatat", "Sanparith", ""]]}, {"id": "1806.01126", "submitter": "Tobias Hossfeld", "authors": "Tobias Hossfeld, Poul E. Heegaard, Martin Varela, Lea Skorin-Kapov", "title": "Confidence Interval Estimators for MOS Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the quantification of QoE, subjects often provide individual rating\nscores on certain rating scales which are then aggregated into Mean Opinion\nScores (MOS). From the observed sample data, the expected value is to be\nestimated. While the sample average only provides a point estimator, confidence\nintervals (CI) are an interval estimate which contains the desired expected\nvalue with a given confidence level. In subjective studies, the number of\nsubjects performing the test is typically small, especially in lab\nenvironments. The used rating scales are bounded and often discrete like the\n5-point ACR rating scale. Therefore, we review statistical approaches in the\nliterature for their applicability in the QoE domain for MOS interval\nestimation (instead of having only a point estimator, which is the MOS). We\nprovide a conservative estimator based on the SOS hypothesis and binomial\ndistributions and compare its performance (CI width, outlier ratio of CI\nviolating the rating scale bounds) and coverage probability with well known CI\nestimators. We show that the provided CI estimator works very well in practice\nfor MOS interval estimators, while the commonly used studentized CIs suffer\nfrom a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.\nAs an alternative, bootstrapping, i.e., random sampling of the subjective\nratings with replacement, is an efficient CI estimator leading to typically\nsmaller CIs, but lower coverage than the proposed estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:59:55 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hossfeld", "Tobias", ""], ["Heegaard", "Poul E.", ""], ["Varela", "Martin", ""], ["Skorin-Kapov", "Lea", ""]]}, {"id": "1806.01180", "submitter": "Kyungyun Lee", "authors": "Kyungyun Lee, Keunwoo Choi, Juhan Nam", "title": "Revisiting Singing Voice Detection: a Quantitative Review and the Future\n  Outlook", "comments": "Accepted to the 19th International Society of Music Information\n  Retrieval (ISMIR) Conference, Paris, France, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the vocal component plays a crucial role in popular music, singing\nvoice detection has been an active research topic in music information\nretrieval. Although several proposed algorithms have shown high performances,\nwe argue that there still is a room to improve to build a more robust singing\nvoice detection system. In order to identify the area of improvement, we first\nperform an error analysis on three recent singing voice detection systems.\nBased on the analysis, we design novel methods to test the systems on multiple\nsets of internally curated and generated data to further examine the pitfalls,\nwhich are not clearly revealed with the current datasets. From the experiment\nresults, we also propose several directions towards building a more robust\nsinging voice detector.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:25:37 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Lee", "Kyungyun", ""], ["Choi", "Keunwoo", ""], ["Nam", "Juhan", ""]]}, {"id": "1806.01571", "submitter": "Pengpeng Yang", "authors": "Pengpeng Yang, Rongrong Ni, and Yao Zhao", "title": "Double JPEG Compression Detection by Exploring the Correlations in DCT\n  Domain", "comments": "5 pages, submitted to APSIPA ASC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of digital image processing, JPEG image compression technique\nhas been widely applied. And numerous image processing software suppose this.\nIt is likely for the images undergoing double JPEG compression to be tampered.\nTherefore, double JPEG compression detection schemes can provide an important\nclue for image forgery detection. In this paper, we propose an effective\nalgorithm to detect double JPEG compression with different quality factors.\nFirstly, the quantized DCT coefficients with same frequency are extracted to\nbuild the new data matrices. Then, considering the direction effect on the\ncorrelation between the adjacent positions in DCT domain, twelve kinds of\nhigh-pass filter templates with different directions are executed and the\ntranslation probability matrix is calculated for each filtered data.\nFurthermore, principal component analysis and support vector machine technique\nare applied to reduce the feature dimension and train a classifier,\nrespectively. Experimental results have demonstrated that the proposed method\nis effective and has comparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 09:20:40 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Yang", "Pengpeng", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""]]}, {"id": "1806.02803", "submitter": "Vaneet Aggarwal", "authors": "Anis Elgabli and Vaneet Aggarwal", "title": "FastScan: Robust Low-Complexity Rate Adaptation Algorithm for Video\n  Streaming over HTTP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and evaluates a novel algorithm for streaming video over\nHTTP. The problem is formulated as a non-convex optimization problem which is\nconstrained by the predicted available bandwidth, chunk deadlines, available\nvideo rates, and buffer occupancy. The objective is to optimize a QoE metric\nthat maintains a tradeoff between maximizing the playback rate of every chunk\nand ensuring fairness among different chunks for the minimum re-buffering time.\nWe propose FastScan, a low complexity algorithm that solves the problem. Online\nadaptations for dynamic bandwidth environments are proposed with imperfect\navailable bandwidth prediction. Results of experiments driven by Variable Bit\nRate (VBR) encoded video, video platform system (dash.js), and cellular\nbandwidth traces of a public dataset reveal the robustness of the online\nversion of FastScan algorithm and demonstrate its significant performance\nimprovement as compared to the considered state-of-the-art video streaming\nalgorithms. For example, on an experiment conducted over 100 real cellular\navailable bandwidth traces of a public dataset that spans different available\nbandwidth regimes, our proposed algorithm (FastScan) achieves the minimum\nre-buffering (stall) time and the maximum average playback rate in every single\ntrace as compared to Bola, Festive, BBA, RB, and FastMPC, and Pensieve\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:27:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 16:17:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Elgabli", "Anis", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1806.02941", "submitter": "Yadong Mu", "authors": "Xinyu Weng, Yongzhi Li, Lu Chi, and Yadong Mu", "title": "Convolutional Video Steganography with Temporal Residual Modeling", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography represents the art of unobtrusively concealing a secrete\nmessage within some cover data. The key scope of this work is about visual\nsteganography techniques that hide a full-sized color image / video within\nanother. A majority of existing works are devoted to the image case, where both\nsecret and cover data are images. We empirically validate that image\nsteganography model does not naturally extend to the video case (i.e., hiding a\nvideo into another video), mainly because it completely ignores the temporal\nredundancy within consecutive video frames. Our work proposes a novel solution\nto the problem of video steganography. The technical contributions are\ntwo-fold: first, the residual between two consecutive frames tends to zero at\nmost pixels. Hiding such highly-sparse data is significantly easier than hiding\nthe original frames. Motivated by this fact, we propose to explicitly consider\ninter-frame residuals rather than blindly applying image steganography model on\nevery video frame. Specifically, our model contains two branches, one of which\nis specially designed for hiding inter-frame difference into a cover video\nframe and the other instead hides the original secret frame. A simple\nthresholding method determines which branch a secret video frame shall choose.\nWhen revealing the concealed secret video, two decoders are devised, revealing\ndifference or frame respectively. Second, we develop the model based on deep\nconvolutional neural networks, which is the first of its kind in the literature\nof video steganography. In experiments, comprehensive evaluations are conducted\nto compare our model with both classic least significant bit (LSB) method and\npure image steganography models. All results strongly suggest that the proposed\nmodel enjoys advantages over previous methods. We also carefully investigate\nkey factors in the success of our deep video steganography model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:52:36 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Weng", "Xinyu", ""], ["Li", "Yongzhi", ""], ["Chi", "Lu", ""], ["Mu", "Yadong", ""]]}, {"id": "1806.03483", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Ruipeng Chen, Lei Zhu, Anfeng Liu, Yunwu Lin and Fang\n  Huang", "title": "Hierarchical Information Quadtree: Efficient Spatial Temporal Image\n  Search for Multimedia Stream", "comments": "Published at Multimedia Tools and Applications. arXiv admin note:\n  text overlap with arXiv:1805.02009", "journal-ref": null, "doi": "10.1007/s11042-018-6284-y", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amount of multimedia data that contain times- tamps and geographical\ninformation are being generated at an unprecedented scale in many emerging\napplications such as photo sharing web site and social networks applications.\nDue to their importance, a large body of work has focused on efficiently\ncomputing various spatial image queries. In this paper,we study the spatial\ntemporal image query which considers three important constraints during the\nsearch including time recency, spatial proximity and visual relevance. A novel\nindex structure, namely Hierarchical Information Quadtree(\\hiq), to efficiently\ninsert/delete spatial temporal images with high arrive rates. Base on \\hiq an\nefficient algorithm is developed to support spatial temporal image query. We\nshow via extensive experimentation with real spatial databases clearly\ndemonstrate the efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 14:53:07 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 07:53:15 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Chen", "Ruipeng", ""], ["Zhu", "Lei", ""], ["Liu", "Anfeng", ""], ["Lin", "Yunwu", ""], ["Huang", "Fang", ""]]}, {"id": "1806.03618", "submitter": "Yan Ke", "authors": "Yan Ke, Jia Liu, Min-qing Zhang, Ting-ting Su, Xiao-yuan Yang", "title": "Steganography Security: Principle and Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on several theoretical issues and principles in\nsteganography security, and defines four security levels by analyzing the\ncorresponding algorithm instances. In the theoretical analysis, we discuss the\ndifferences between steganography security and watermarking security. The two\nnecessary conditions for the steganography security are obtained. Under the\ncurrent technology situation, we then analyze the indistinguishability of the\ncover and stego-cover, and consider that the steganography security should rely\non the key secrecy with algorithms open. By specifying the role of key in\nsteganography, the necessary conditions for a secure steganography algorithm in\ntheory are formally presented. When analyzing the security instances, we have\nclassified the steganalysis attacks according to their variable access to the\nsteganography system, and then defined the four security levels. The higher\nlevel security one has, the higher level attacks one can resist. We have also\npresented algorithm instances based on current technical conditions, and\nanalyzed their data hiding process, security level, and practice requirements.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 09:23:10 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 11:15:23 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Ke", "Yan", ""], ["Liu", "Jia", ""], ["Zhang", "Min-qing", ""], ["Su", "Ting-ting", ""], ["Yang", "Xiao-yuan", ""]]}, {"id": "1806.03787", "submitter": "Warit Sirichotedumrong", "authors": "Warit Sirichotedumrong, Tatsuya Chuman, Shoko Imaizumi, Hitoshi Kiya", "title": "Grayscale-based Block Scrambling Image Encryption for Social Networking\n  Services", "comments": "To be appeared in ICME2018 proceedings, 23rd July, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new block scrambling encryption scheme that enhances\nthe security of encryption-then-compression (EtC) systems for JPEG images,\nwhich are used, for example, to securely transmit images through an untrusted\nchannel provider. The proposed method allows the use of a smaller block size\nand a larger number of blocks than the conventional ones. Moreover, images\nencrypted using proposed scheme include less color information due to the use\nof grayscale even when the original image has three color channels. These\nfeatures enhance security against various attacks such as jigsaw puzzle solver\nand brute-force attacks. The results of an experiment in which encrypted images\nwere uploaded to and then downloaded from Twitter and Facebook demonstrated the\neffectiveness of the proposed scheme for EtC systems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:17:57 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sirichotedumrong", "Warit", ""], ["Chuman", "Tatsuya", ""], ["Imaizumi", "Shoko", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1806.04012", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Mohamad Baydoun, Damian Campo, Pablo Marin, David\n  Martin, Lucio Marcenaro, Carlo S. Regazzoni", "title": "Hierarchy of GANs for learning embodied self-awareness model", "comments": "2018 IEEE International Conference on Image Processing - ICIP'18.\n  arXiv admin note: text overlap with arXiv:1806.02609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years several architectures have been proposed to learn embodied\nagents complex self-awareness models. In this paper, dynamic incremental\nself-awareness (SA) models are proposed that allow experiences done by an agent\nto be modeled in a hierarchical fashion, starting from more simple situations\nto more structured ones. Each situation is learned from subsets of private\nagent perception data as a model capable to predict normal behaviors and detect\nabnormalities. Hierarchical SA models have been already proposed using low\ndimensional sensorial inputs. In this work, a hierarchical model is introduced\nby means of a cross-modal Generative Adversarial Networks (GANs) processing\nhigh dimensional visual data. Different levels of the GANs are detected in a\nself-supervised manner using GANs discriminators decision boundaries. Real\nexperiments on semi-autonomous ground vehicles are presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:24:57 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Baydoun", "Mohamad", ""], ["Campo", "Damian", ""], ["Marin", "Pablo", ""], ["Martin", "David", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo S.", ""]]}, {"id": "1806.04284", "submitter": "Chenhui Chu", "authors": "Chenhui Chu, Mayu Otani and Yuta Nakashima", "title": "iParaphrasing: Extracting Visually Grounded Paraphrases via an Image", "comments": "COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A paraphrase is a restatement of the meaning of a text in other words.\nParaphrases have been studied to enhance the performance of many natural\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\nto extract visually grounded paraphrases (VGPs), which are different phrasal\nexpressions describing the same visual concept in an image. These extracted\nVGPs have the potential to improve language and image multimodal tasks such as\nvisual question answering and image captioning. How to model the similarity\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\nwell as propose a novel neural network-based method with image attention, and\nreport the results of the first attempt toward iParaphrasing.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 00:58:59 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Chu", "Chenhui", ""], ["Otani", "Mayu", ""], ["Nakashima", "Yuta", ""]]}, {"id": "1806.05147", "submitter": "Frederik Pahde", "authors": "Frederik Pahde, Patrick J\\\"ahnichen, Tassilo Klein, Moin Nabi", "title": "Cross-modal Hallucination for Few-shot Fine-grained Recognition", "comments": "CVPR 2018 Workshop on Fine-Grained Visual Categorization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning algorithms generally require large amounts of\ndata for model training. Lack thereof can severely deteriorate the performance,\nparticularly in scenarios with fine-grained boundaries between categories. To\nthis end, we propose a multimodal approach that facilitates bridging the\ninformation gap by means of meaningful joint embeddings. Specifically, we\npresent a benchmark that is multimodal during training (i.e. images and texts)\nand single-modal in testing time (i.e. images), with the associated task to\nutilize multimodal data in base classes (with many samples), to learn explicit\nvisual classifiers for novel classes (with few samples). Next, we propose a\nframework built upon the idea of cross-modal data hallucination. In this\nregard, we introduce a discriminative text-conditional GAN for sample\ngeneration with a simple self-paced strategy for sample selection. We show the\nresults of our proposed discriminative hallucinated method for 1-, 2-, and 5-\nshot learning on the CUB dataset, where the accuracy is improved by employing\nmultimodal data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 17:06:10 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 09:22:20 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Pahde", "Frederik", ""], ["J\u00e4hnichen", "Patrick", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1806.05781", "submitter": "John See", "authors": "Yee-Hui Oh, John See, Anh Cat Le Ngo, Raphael Chung-Wei Phan, Vishnu\n  Monn Baskaran", "title": "A Survey of Automatic Facial Micro-expression Analysis: Databases,\n  Methods and Challenges", "comments": "45 pages, single column preprint version. Submitted: 2 December 2017,\n  Accepted: 12 June 2018 to Frontiers in Psychology", "journal-ref": null, "doi": "10.3389/fpsyg.2018.01128", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, automatic facial micro-expression analysis has\ngarnered increasing attention from experts across different disciplines because\nof its potential applications in various fields such as clinical diagnosis,\nforensic investigation and security systems. Advances in computer algorithms\nand video acquisition technology have rendered machine analysis of facial\nmicro-expressions possible today, in contrast to decades ago when it was\nprimarily the domain of psychiatrists where analysis was largely manual.\nIndeed, although the study of facial micro-expressions is a well-established\nfield in psychology, it is still relatively new from the computational\nperspective with many interesting problems. In this survey, we present a\ncomprehensive review of state-of-the-art databases and methods for\nmicro-expressions spotting and recognition. Individual stages involved in the\nautomation of these tasks are also described and reviewed at length. In\naddition, we also deliberate on the challenges and future directions in this\ngrowing field of automatic facial micro-expression analysis.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 01:42:33 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Oh", "Yee-Hui", ""], ["See", "John", ""], ["Ngo", "Anh Cat Le", ""], ["Phan", "Raphael Chung-Wei", ""], ["Baskaran", "Vishnu Monn", ""]]}, {"id": "1806.06357", "submitter": "Yang Yang", "authors": "Pin Wu, Yang Yang and Xiaoqiang Li", "title": "StegNet: Mega Image Steganography Capacity with Deep Convolutional\n  Network", "comments": "https://github.com/adamcavendish/StegNet-Mega-Image-Steganography-Capacity-with-Deep-Convolutional-Network", "journal-ref": null, "doi": "10.3390/fi10060054", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image steganography often leans interests towards safely\nembedding hidden information into cover images with payload capacity almost\nneglected. This paper combines recent deep convolutional neural network methods\nwith image-into-image steganography. It successfully hides the same size images\nwith a decoding rate of 98.2% or bpp (bits per pixel) of 23.57 by changing only\n0.76% of the cover image on average. Our method directly learns end-to-end\nmappings between the cover image and the embedded image and between the hidden\nimage and the decoded image. We~further show that our embedded image, while\nwith mega payload capacity, is still robust to statistical analysis.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 10:06:29 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Wu", "Pin", ""], ["Yang", "Yang", ""], ["Li", "Xiaoqiang", ""]]}, {"id": "1806.06650", "submitter": "Nitin Khanna Dr.", "authors": "Sharad Joshi and Nitin Khanna", "title": "Source Printer Classification using Printer Specific Local Texture\n  Descriptor", "comments": "22 pages in main paper and 2 pages of supplementary material", "journal-ref": null, "doi": "10.1109/TIFS.2019.2919869", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge of source printer can help in printed text document\nauthentication, copyright ownership, and provide important clues about the\nauthor of a fraudulent document along with his/her potential means and motives.\nDevelopment of automated systems for classifying printed documents based on\ntheir source printer, using image processing techniques, is gaining a lot of\nattention in multimedia forensics. Currently, state-of-the-art systems require\nthat the font of letters present in test documents of unknown origin must be\navailable in those used for training the classifier. In this work, we attempt\nto take the first step towards overcoming this limitation. Specifically, we\nintroduce a novel printer specific local texture descriptor. The highlight of\nour technique is the use of encoding and regrouping strategy based on small\nlinear-shaped structures composed of pixels having similar intensity and\ngradient. The results of experiments performed on two separate datasets show\nthat: 1) on a publicly available dataset, the proposed method outperforms\nstate-of-the-art algorithms for characters printed in the same font, and 2) on\nanother dataset\\footnote{Code and dataset will be made publicly available with\npublished version of this paper.} having documents printed in four different\nfonts, the proposed method correctly classifies all test samples when\nsufficient training data is available in same font setup. In addition, it\noutperforms state-of-the-art methods for cross font experiments. Moreover, it\nreduces the confusion between the printers of same brand and model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 13:29:26 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Joshi", "Sharad", ""], ["Khanna", "Nitin", ""]]}, {"id": "1806.07008", "submitter": "Sifeng Xia", "authors": "Sifeng Xia, Wenhan Yang, Yueyu Hu, Siwei Ma and Jiaying Liu", "title": "A Group Variational Transformation Neural Network for Fractional\n  Interpolation of Video Coding", "comments": "DCC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion compensation is an important technology in video coding to remove the\ntemporal redundancy between coded video frames. In motion compensation,\nfractional interpolation is used to obtain more reference blocks at sub-pixel\nlevel. Existing video coding standards commonly use fixed interpolation filters\nfor fractional interpolation, which are not efficient enough to handle diverse\nvideo signals well. In this paper, we design a group variational transformation\nconvolutional neural network (GVTCNN) to improve the fractional interpolation\nperformance of the luma component in motion compensation. GVTCNN infers samples\nat different sub-pixel positions from the input integer-position sample. It\nfirst extracts a shared feature map from the integer-position sample to infer\nvarious sub-pixel position samples. Then a group variational transformation\ntechnique is used to transform a group of copied shared feature maps to samples\nat different sub-pixel positions. Experimental results have identified the\ninterpolation efficiency of our GVTCNN. Compared with the interpolation method\nof High Efficiency Video Coding, our method achieves 1.9% bit saving on average\nand up to 5.6% bit saving under low-delay P configuration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 01:58:09 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Xia", "Sifeng", ""], ["Yang", "Wenhan", ""], ["Hu", "Yueyu", ""], ["Ma", "Siwei", ""], ["Liu", "Jiaying", ""]]}, {"id": "1806.07840", "submitter": "Xu Chen", "authors": "En Li and Zhi Zhou and Xu Chen", "title": "Edge Intelligence: On-Demand Deep Learning Model Co-Inference with\n  Device-Edge Synergy", "comments": "ACM SIGCOMM Workshop on Mobile Edge Communications, Budapest,\n  Hungary, August 21-23, 2018. https://dl.acm.org/authorize?N665473", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the backbone technology of machine learning, deep neural networks (DNNs)\nhave have quickly ascended to the spotlight. Running DNNs on\nresource-constrained mobile devices is, however, by no means trivial, since it\nincurs high performance and energy overhead. While offloading DNNs to the cloud\nfor execution suffers unpredictable performance, due to the uncontrolled long\nwide-area network latency. To address these challenges, in this paper, we\npropose Edgent, a collaborative and on-demand DNN co-inference framework with\ndevice-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that\nadaptively partitions DNN computation between device and edge, in order to\nleverage hybrid computation resources in proximity for real-time DNN inference.\n(2) DNN right-sizing that accelerates DNN inference through early-exit at a\nproper intermediate DNN layer to further reduce the computation latency. The\nprototype implementation and extensive evaluations based on Raspberry Pi\ndemonstrate Edgent's effectiveness in enabling on-demand low-latency edge\nintelligence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:56:54 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 03:36:27 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 02:37:07 GMT"}, {"version": "v4", "created": "Thu, 27 Dec 2018 11:49:55 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "En", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1806.08332", "submitter": "Luciano Abriata", "authors": "Luciano A. Abriata", "title": "Towards Commodity, Web-Based Augmented Reality Applications for Research\n  and Education in Chemistry and Structural Biology", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.ET cs.MM physics.bio-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reports prototype web apps that use commodity, open-source\ntechnologies for augmented and virtual reality to provide immersive,\ninteractive human-computer interfaces for chemistry, structural biology and\nrelated disciplines. The examples, which run in any standard web browser and\nare accessible at\nhttps://lucianoabriata.altervista.org/jsinscience/arjs/armodeling/ together\nwith demo videos, showcase applications that could go well beyond pedagogy,\ni.e. advancing actual utility in research settings: molecular visualization at\natomistic and coarse-grained levels in interactive immersive 3D, coarse-grained\nmodeling of molecular physics and chemistry, and on-the-fly calculation of\nexperimental observables and overlay onto experimental data. From this\nplayground, I depict perspectives on how these emerging technologies might\ncouple in the future to neural network-based quantum mechanical calculations,\nadvanced forms of human-computer interaction such as speech-based\ncommunication, and sockets for concurrent collaboration through the internet\n-all technologies that are today maturing in web browsers- to deliver the next\ngeneration of tools for truly interactive, immersive molecular modeling that\ncan streamline human thought and intent with the numerical processing power of\ncomputers.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:21:17 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 22:12:45 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 21:15:42 GMT"}, {"version": "v4", "created": "Tue, 24 Jul 2018 18:33:06 GMT"}, {"version": "v5", "created": "Fri, 10 Aug 2018 08:17:53 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Abriata", "Luciano A.", ""]]}, {"id": "1806.08904", "submitter": "Chengyuan Zhang", "authors": "Jun Long, Lei Zhu, Zhan Yang, Chengyuan Zhang, Xinpan Yuan", "title": "Temporal Activity Path Based Character Correction in Social Networks", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast amount of multimedia data contains massive and multifarious social\ninformation which is used to construct large-scale social networks. In a\ncomplex social network, a character should be ideally denoted by one and only\none vertex. However, it is pervasive that a character is denoted by two or more\nvertices with different names, thus it is usually considered as multiple,\ndifferent characters. This problem causes incorrectness of results in network\nanalysis and mining. The factual challenge is that character uniqueness is hard\nto correctly confirm due to lots of complicated factors, e.g. name changing and\nanonymization, leading to character duplication. Early, limited research has\nshown that previous methods depended overly upon supplementary attribute\ninformation from databases. In this paper, we propose a novel method to merge\nthe character vertices which refer to as the same entity but are denoted with\ndifferent names. With this method, we firstly build the relationship network\namong characters based on records of social activities participated, which are\nextracted from multimedia sources. Then define temporal activity paths (TAPs)\nfor each character over time. After that, we measure similarity of the TAPs for\nany two characters. If the similarity is high enough, the two vertices should\nbe considered to the same character. Based on TAPs, we can determine whether to\nmerge the two character vertices. Our experiments shown that this solution can\naccurately confirm character uniqueness in large-scale social network.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 04:08:47 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Long", "Jun", ""], ["Zhu", "Lei", ""], ["Yang", "Zhan", ""], ["Zhang", "Chengyuan", ""], ["Yuan", "Xinpan", ""]]}, {"id": "1806.09594", "submitter": "Carl Vondrick", "authors": "Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama,\n  Kevin Murphy", "title": "Tracking Emerges by Colorizing Videos", "comments": "ECCV 2018. Blog post:\n  https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use large amounts of unlabeled video to learn models for visual tracking\nwithout manual human supervision. We leverage the natural temporal coherency of\ncolor to create a model that learns to colorize gray-scale videos by copying\ncolors from a reference frame. Quantitative and qualitative experiments suggest\nthat this task causes the model to automatically learn to track visual regions.\nAlthough the model is trained without any ground-truth labels, our method\nlearns to track well enough to outperform the latest methods based on optical\nflow. Moreover, our results suggest that failures to track are correlated with\nfailures to colorize, indicating that advancing video colorization may further\nimprove self-supervised visual tracking.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:44:40 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 22:38:39 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Vondrick", "Carl", ""], ["Shrivastava", "Abhinav", ""], ["Fathi", "Alireza", ""], ["Guadarrama", "Sergio", ""], ["Murphy", "Kevin", ""]]}, {"id": "1806.10853", "submitter": "Vaneet Aggarwal", "authors": "Eric Friedlander and Vaneet Aggarwal", "title": "Generalization of LRU Cache Replacement Policy with Applications to\n  Video Streaming", "comments": "Accepted to ACM TOMPECS, Jun 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching plays a crucial role in networking systems to reduce the load on the\nnetwork and is commonly employed by content delivery networks (CDNs) in order\nto improve performance. One of the commonly used mechanisms, Least Recently\nUsed (LRU), works well for identical file sizes. However, for asymmetric file\nsizes, the performance deteriorates. This paper proposes an adaptation to the\nLRU strategy, called gLRU, where the file is sub-divided into equal-sized\nchunks. In this strategy, a chunk of the newly requested file is added in the\ncache, and a chunk of the least-recently-used file is removed from the cache.\nEven though approximate analysis for the hit rate has been studied for LRU, the\nanalysis does not extend to gLRU since the metric of interest is no longer the\nhit rate as the cache has partial files. This paper provides a novel\napproximation analysis for this policy where the cache may have partial file\ncontents. The approximation approach is validated by simulations. Further, gLRU\noutperforms the LRU strategy for a Zipf file popularity distribution and\ncensored Pareto file size distribution for the file download times. Video\nstreaming applications can further use the partial cache contents to help the\nstall duration significantly, and the numerical results indicate significant\nimprovements (32\\%) in stall duration using the gLRU strategy as compared to\nthe LRU strategy. Furthermore, the gLRU replacement policy compares favorably\nto two other cache replacement policies when simulated on MSR Cambridge Traces\nobtained from the SNIA IOTTA repository.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 09:44:56 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 01:56:07 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Friedlander", "Eric", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1806.10923", "submitter": "Alexandre Benoit", "authors": "A Benoit (LISTIC), Leonel Cuevas, Jean-Baptiste Thomas (Le2i)", "title": "Deep learning for dehazing: Comparison and analysis", "comments": null, "journal-ref": "Colour and Visual Computing Symposium (CVCS), Sep 2018,\n  Gj{{\\o}}vik, Norway", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare a recent dehazing method based on deep learning, Dehazenet, with\ntraditional state-of-the-art approaches , on benchmark data with reference.\nDehazenet estimates the depth map from transmission factor on a single color\nimage, which is used to inverse the Koschmieder model of imaging in the\npresence of haze. In this sense, the solution is still attached to the\nKoschmieder model. We demonstrate that the transmission is very well estimated\nby the network, but also that this method exhibits the same limitation than\nothers due to the use of the same imaging model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 12:37:54 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Benoit", "A", "", "LISTIC"], ["Cuevas", "Leonel", "", "Le2i"], ["Thomas", "Jean-Baptiste", "", "Le2i"]]}]