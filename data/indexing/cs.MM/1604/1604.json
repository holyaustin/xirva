[{"id": "1604.00192", "submitter": "Yukara Ikemiya", "authors": "Yukara Ikemiya, Katsutoshi Itoyama and Kazuyoshi Yoshii", "title": "Singing Voice Separation and Vocal F0 Estimation based on Mutual\n  Combination of Robust Principal Component Analysis and Subharmonic Summation", "comments": "11 pages", "journal-ref": null, "doi": "10.1109/TASLP.2016.2577879", "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method of singing voice analysis that performs\nmutually-dependent singing voice separation and vocal fundamental frequency\n(F0) estimation. Vocal F0 estimation is considered to become easier if singing\nvoices can be separated from a music audio signal, and vocal F0 contours are\nuseful for singing voice separation. This calls for an approach that improves\nthe performance of each of these tasks by using the results of the other. The\nproposed method first performs robust principal component analysis (RPCA) for\nroughly extracting singing voices from a target music audio signal. The F0\ncontour of the main melody is then estimated from the separated singing voices\nby finding the optimal temporal path over an F0 saliency spectrogram. Finally,\nthe singing voices are separated again more accurately by combining a\nconventional time-frequency mask given by RPCA with another mask that passes\nonly the harmonic structures of the estimated F0s. Experimental results showed\nthat the proposed method significantly improved the performances of both\nsinging voice separation and vocal F0 estimation. The proposed method also\noutperformed all the other methods of singing voice separation submitted to an\ninternational music analysis competition called MIREX 2014.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 10:28:51 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ikemiya", "Yukara", ""], ["Itoyama", "Katsutoshi", ""], ["Yoshii", "Kazuyoshi", ""]]}, {"id": "1604.00233", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k", "title": "Building an Internet Radio System with Interdisciplinary factored system\n  for automatic content recommendation", "comments": "ISBN: 978-3-659-41584-5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic systems for music content recommendation have assumed a new role in\nrecent years. These systems have transformed from being just a convenient,\nstandalone tool into an inseparable element of modern living. In addition, not\nonly do these systems strongly influence human moods and feelings with the\nselection of proper music content, but they also provide significant commercial\nand advertising opportunities. This research aims to examine and implement two\nsuch systems available for the automatic recognition and recommendation of\nmusic and advertisement content for Internet radio. Through analysis of the\npractical issues of application fields and spheres of influence, conclusions\nwill be drawn about the possible perspectives on and future role of such\nsystems. Other content adaptation that is based on music genres will be\ndiscussed, as wellAnother aim of this study is to provide an innovative\nInternet radio implementation as compared to traditional radio and other\nInternet broadcast solutions. This will include automatic content\nrecommendation systems for listeners and marketing companies, as well as the\nusage of a voice synthesizer in in automatic program scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 13:25:43 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""]]}, {"id": "1604.00493", "submitter": "Minati Mishra Dr.", "authors": "Sanjeeb Kumar Behera, Minati Mishra", "title": "Steganography -- A Game of Hide and Seek in Information Communication", "comments": "5 pages, 4 figures, National Conference on Recent Innovations in\n  Engineering and Management Sciences (RIEMS-2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of communication over computer networks, how to maintain the\nconfidentiality and security of transmitted information have become some of the\nimportant issues. In order to transfer data securely to the destination without\nunwanted disclosure or damage, nature inspired hide and seek tricks such as,\ncryptography and Steganography are heavily in use. Just like the Chameleon and\nmany other bio-species those change their body color and hide themselves in the\nbackground in order to protect them from external attacks, Cryptography and\nSteganography are techniques those are used to encrypt and hide the secret data\ninside other media to ensure data security. This paper discusses the concept of\na simple spatial domain LSB Steganography that encrypts the secrets using\nFibonacci- Lucas transformation, before hiding, for better security.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 12:21:52 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Behera", "Sanjeeb Kumar", ""], ["Mishra", "Minati", ""]]}, {"id": "1604.00790", "submitter": "Cheng Wang", "authors": "Cheng Wang, Haojin Yang, Christian Bartz, Christoph Meinel", "title": "Image Captioning with Deep Bidirectional LSTMs", "comments": "accepted by ACMMM 2016 as full paper and oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an end-to-end trainable deep bidirectional LSTM\n(Long-Short Term Memory) model for image captioning. Our model builds on a deep\nconvolutional neural network (CNN) and two separate LSTM networks. It is\ncapable of learning long term visual-language interactions by making use of\nhistory and future context information at high level semantic space. Two novel\ndeep bidirectional variant models, in which we increase the depth of\nnonlinearity transition in different way, are proposed to learn hierarchical\nvisual-language embeddings. Data augmentation techniques such as multi-crop,\nmulti-scale and vertical mirror are proposed to prevent overfitting in training\ndeep models. We visualize the evolution of bidirectional LSTM internal states\nover time and qualitatively analyze how our models \"translate\" image to\nsentence. Our proposed models are evaluated on caption generation and\nimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,\nFlickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models\nachieve highly competitive performance to the state-of-the-art results on\ncaption generation even without integrating additional mechanism (e.g. object\ndetection, attention model etc.) and significantly outperform recent methods on\nretrieval task.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 09:43:04 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 07:45:25 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 14:19:37 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Wang", "Cheng", ""], ["Yang", "Haojin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1604.01219", "submitter": "Yuting Qaing", "authors": "Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou and Leonid Sigal", "title": "Learning to Generate Posters of Scientific Papers", "comments": "in Proceedings of the 30th AAAI Conference on Artificial Intelligence\n  (AAAI'16), Phoenix, AZ, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often summarize their work in the form of posters. Posters\nprovide a coherent and efficient way to convey core ideas from scientific\npapers. Generating a good scientific poster, however, is a complex and time\nconsuming cognitive task, since such posters need to be readable, informative,\nand visually aesthetic. In this paper, for the first time, we study the\nchallenging problem of learning to generate posters from scientific papers. To\nthis end, a data-driven framework, that utilizes graphical models, is proposed.\nSpecifically, given content to display, the key elements of a good poster,\nincluding panel layout and attributes of each panel, are learned and inferred\nfrom data. Then, given inferred layout and attributes, composition of graphical\nelements within each panel is synthesized. To learn and validate our model, we\ncollect and make public a Poster-Paper dataset, which consists of scientific\npapers and corresponding posters with exhaustively labelled panels and\nattributes. Qualitative and quantitative results indicate the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 11:18:04 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Qiang", "Yuting", ""], ["Fu", "Yanwei", ""], ["Guo", "Yanwen", ""], ["Zhou", "Zhi-Hua", ""], ["Sigal", "Leonid", ""]]}, {"id": "1604.01335", "submitter": "Brendan Jou", "authors": "Brendan Jou and Shih-Fu Chang", "title": "Deep Cross Residual Learning for Multitask Visual Recognition", "comments": "10 pages, 6 figures, To appear in ACM Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual learning has recently surfaced as an effective means of constructing\nvery deep neural networks for object recognition. However, current incarnations\nof residual networks do not allow for the modeling and integration of complex\nrelations between closely coupled recognition tasks or across domains. Such\nproblems are often encountered in multimedia applications involving large-scale\ncontent recognition. We propose a novel extension of residual learning for deep\nnetworks that enables intuitive learning across multiple related tasks using\ncross-connections called cross-residuals. These cross-residuals connections can\nbe viewed as a form of in-network regularization and enables greater network\ngeneralization. We show how cross-residual learning (CRL) can be integrated in\nmultitask networks to jointly train and detect visual concepts across several\ntasks. We present a single multitask cross-residual network with >40% less\nparameters that is able to achieve competitive, or even better, detection\nperformance on a visual sentiment concept detection problem normally requiring\nmultiple specialized single-task networks. The resulting multitask\ncross-residual network also achieves better detection performance by about\n10.4% over a standard multitask residual network without cross-residuals with\neven a small amount of cross-task weighting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 17:08:14 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 01:55:12 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1604.01720", "submitter": "Eric Wengrowski", "authors": "Eric Wengrowski, Kristin Dana, Marco Gruteser, and Narayan Mandayam", "title": "Reading Between the Pixels: Photographic Steganography for Camera\n  Display Messaging", "comments": "16 pages with references 8 tables and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit human color metamers to send light-modulated messages less visible\nto the human eye, but recoverable by cameras. These messages are a key\ncomponent to camera-display messaging, such as handheld smartphones capturing\ninformation from electronic signage. Each color pixel in the display image is\nmodified by a particular color gradient vector. The challenge is to find the\ncolor gradient that maximizes camera response, while minimizing human response.\nThe mismatch in human spectral and camera sensitivity curves creates an\nopportunity for hidden messaging. Our approach does not require knowledge of\nthese sensitivity curves, instead we employ a data-driven method. We learn an\nellipsoidal partitioning of the six-dimensional space of colors and color\ngradients. This partitioning creates metamer sets defined by the base color at\nthe display pixel and the color gradient direction for message encoding. We\nsample from the resulting metamer sets to find color steps for each base color\nto embed a binary message into an arbitrary image with reduced visible\nartifacts. Unlike previous methods that rely on visually obtrusive intensity\nmodulation, we embed with color so that the message is more hidden. Ordinary\ndisplays and cameras are used without the need for expensive LEDs or high speed\ndevices. The primary contribution of this work is a framework to map the pixels\nin an arbitrary image to a metamer pair for steganographic photo messaging.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 18:43:18 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Wengrowski", "Eric", ""], ["Dana", "Kristin", ""], ["Gruteser", "Marco", ""], ["Mandayam", "Narayan", ""]]}, {"id": "1604.02235", "submitter": "Yaser Sadra", "authors": "Sodeif Ahadpour, Yaser Sadra, Meisam Sadeghi", "title": "Image Encryption Based On Gradient Haar Wavelet and Rational Order\n  Chaotic Maps", "comments": "8 pages, 6 figures, and 3 tables", "journal-ref": "Annals. Computer Science Series. 14(1): 59- 66, 2016", "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haar wavelet is one of the best mathematical tools in image cryptography and\nanalysis. Because of the specific structure, this wavelet has the ability which\nis combined with other mathematical tools such as chaotic maps. The rational\norder chaotic maps are one of clusters of chaotic maps which their\ndeterministic behaviors have high sensitivity. In this paper, we propose a\nnovel method of gradient Haar wavelet transform for image encryption. This\nmethod use linearity properties of the scaling function of the gradient Haar\nwavelet and deterministic behaviors of rational order chaotic maps in order to\ngenerate encrypted images with high security factor. The security of the\nencrypted images is evaluated by the key space analysis, the correlation\ncoefficient analysis, and differential attack. The method could be used in\nother fields such as image and signal processing.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 06:20:18 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Ahadpour", "Sodeif", ""], ["Sadra", "Yaser", ""], ["Sadeghi", "Meisam", ""]]}, {"id": "1604.02546", "submitter": "Lorenzo Baraldi", "authors": "Lorenzo Baraldi, Costantino Grana and Rita Cucchiara", "title": "Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic\n  Deep Features", "comments": "ICMR 2016", "journal-ref": null, "doi": "10.1145/2911996.2912012", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel retrieval pipeline for video collections, which\naims to retrieve the most significant parts of an edited video for a given\nquery, and represent them with thumbnails which are at the same time\nsemantically meaningful and aesthetically remarkable. Videos are first\nsegmented into coherent and story-telling scenes, then a retrieval algorithm\nbased on deep learning is proposed to retrieve the most significant scenes for\na textual query. A ranking strategy based on deep features is finally used to\ntackle the problem of visualizing the best thumbnail. Qualitative and\nquantitative experiments are conducted on a collection of edited videos to\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 09:41:14 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Baraldi", "Lorenzo", ""], ["Grana", "Costantino", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1604.02778", "submitter": "James Collins", "authors": "James Collins and Sos Agaian", "title": "Trends toward real-time network data steganography", "comments": "20 pages introducing the concept of real-time network steganography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network steganography has been a well-known covert data channeling method for\nover three decades. The basic set of techniques and implementation tools have\nnot changed significantly since their introduction in the early 1980's. In this\npaper, we review the predominant methods of classical network steganography,\ndescribing the detailed operations and resultant challenges involved in\nembedding data in the network transport domain. We also consider the various\ncyber threat vectors of network steganography and point out the major\ndifferences between classical network steganography and the widely known\nend-point multimedia embedding techniques, which focus exclusively on static\ndata modification for data hiding. We then challenge the security community by\nintroducing an entirely new network dat hiding methodology, which we refer to\nas real-time network data steganography. Finally we provide the groundwork for\nthis fundamental change of covert network data embedding by forming a basic\nframework for real-time network data operations that will open the path for\neven further advances in computer network security.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 02:58:06 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Collins", "James", ""], ["Agaian", "Sos", ""]]}, {"id": "1604.02797", "submitter": "Lakshmanan S", "authors": "M.MaryShanthi Rani and S.Lakshmanan", "title": "An Integrated Method of Data Hiding and Compression of Medical Images", "comments": null, "journal-ref": null, "doi": "10.5121/ijait.2016.6104", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A new technique for embedding data into an image coupled with compression has\nbeen proposed in this paper. A fast and efficient coding algorithms are needed\nfor effective storage and transmission, due to the popularity of telemedicine\nand the use of digital medical images. Medical images are produced and\ntransferred between hospitals for review by physicians who are geographically\napart. Such image data need to be stored for future reference of patients as\nwell. This necessitates compact storage of medical images before being\ntransmitted over Internet. Moreover, as the patient information is also\nembedded within the medical images, it is very important to maintain the\nconfidentiality of patient data. Hence, this article aims at hiding patient\ninformation as well, within the medical image followed by joint compression.\nThe hidden data and the host image are absolutely recoverable from the embedded\nimage without any loss.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 05:51:22 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Rani", "M. MaryShanthi", ""], ["Lakshmanan", "S.", ""]]}, {"id": "1604.03276", "submitter": "Zhaofeng Zhang", "authors": "Zhaofeng Zhang, Xiong Xiao, Longbiao Wang, EngSiong Chng, and Haizhou\n  Li", "title": "Noise Robust Speech Recognition Using Multi-Channel Based Channel\n  Selection And ChannelWeighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study several microphone channel selection and weighting\nmethods for robust automatic speech recognition (ASR) in noisy conditions. For\nchannel selection, we investigate two methods based on the maximum likelihood\n(ML) criterion and minimum autoencoder reconstruction criterion, respectively.\nFor channel weighting, we produce enhanced log Mel filterbank coefficients as a\nweighted sum of the coefficients of all channels. The weights of the channels\nare estimated by using the ML criterion with constraints. We evaluate the\nproposed methods on the CHiME-3 noisy ASR task. Experiments show that channel\nweighting significantly outperforms channel selection due to its higher\nflexibility. Furthermore, on real test data in which different channels have\ndifferent gains of the target signal, the channel weighting method performs\nequally well or better than the MVDR beamforming, despite the fact that the\nchannel weighting does not make use of the phase delay information which is\nnormally used in beamforming.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 07:52:27 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Zhang", "Zhaofeng", ""], ["Xiao", "Xiong", ""], ["Wang", "Longbiao", ""], ["Chng", "EngSiong", ""], ["Li", "Haizhou", ""]]}, {"id": "1604.03489", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Victor Campos, Brendan Jou and Xavier Giro-i-Nieto", "title": "From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment\n  Prediction", "comments": "Accepted for publication in Image and Vision Computing. Models and\n  source code available at https://github.com/imatge-upc/sentiment-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual multimedia have become an inseparable part of our digital social\nlives, and they often capture moments tied with deep affections. Automated\nvisual sentiment analysis tools can provide a means of extracting the rich\nfeelings and latent dispositions embedded in these media. In this work, we\nexplore how Convolutional Neural Networks (CNNs), a now de facto computational\nmachine learning tool particularly in the area of Computer Vision, can be\nspecifically applied to the task of visual sentiment prediction. We accomplish\nthis through fine-tuning experiments using a state-of-the-art CNN and via\nrigorous architecture analysis, we present several modifications that lead to\naccuracy improvements over prior art on a dataset of images from a popular\nsocial media platform. We additionally present visualizations of local patterns\nthat the network learned to associate with image sentiment for insight into how\nvisual positivity (or negativity) is perceived by the model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 17:24:39 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 18:02:16 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Campos", "Victor", ""], ["Jou", "Brendan", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1604.03688", "submitter": "Niall Robinson PhD", "authors": "Niall H. Robinson, Rachel Prudden, Alberto Arribas", "title": "A Practical Approach to Spatiotemporal Data Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets representing the world around us are becoming ever more unwieldy as\ndata volumes grow. This is largely due to increased measurement and modelling\nresolution, but the problem is often exacerbated when data are stored at\nspuriously high precisions. In an effort to facilitate analysis of these\ndatasets, computationally intensive calculations are increasingly being\nperformed on specialised remote servers before the reduced data are transferred\nto the consumer. Due to bandwidth limitations, this often means data are\ndisplayed as simple 2D data visualisations, such as scatter plots or images. We\npresent here a novel way to efficiently encode and transmit 4D data fields\non-demand so that they can be locally visualised and interrogated. This nascent\n\"4D video\" format allows us to more flexibly move the boundary between data\nserver and consumer client. However, it has applications beyond purely\nscientific visualisation, in the transmission of data to virtual and augmented\nreality.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 08:33:36 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 07:47:54 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Robinson", "Niall H.", ""], ["Prudden", "Rachel", ""], ["Arribas", "Alberto", ""]]}, {"id": "1604.04653", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Amaia Salvador, Kevin McGuinness, Ferran Marques, Noel\n  E. O'Connor and Xavier Giro-i-Nieto", "title": "Bags of Local Convolutional Features for Scalable Instance Search", "comments": "Preprint of a short paper accepted in the ACM International\n  Conference on Multimedia Retrieval (ICMR) 2016 (New York City, NY, USA)", "journal-ref": null, "doi": "10.1145/2911996.2912061", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a simple instance retrieval pipeline based on encoding the\nconvolutional features of CNN using the bag of words aggregation scheme (BoW).\nAssigning each local array of activations in a convolutional layer to a visual\nword produces an \\textit{assignment map}, a compact representation that relates\nregions of an image with a visual word. We use the assignment map for fast\nspatial reranking, obtaining object localizations that are used for query\nexpansion. We demonstrate the suitability of the BoW representation based on\nlocal CNN features for instance retrieval, achieving competitive performance on\nthe Oxford and Paris buildings benchmarks. We show that our proposed system for\nCNN feature aggregation with BoW outperforms state-of-the-art techniques using\nsum pooling at a subset of the challenging TRECVid INS benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 22:02:22 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Mohedano", "Eva", ""], ["Salvador", "Amaia", ""], ["McGuinness", "Kevin", ""], ["Marques", "Ferran", ""], ["O'Connor", "Noel E.", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1604.04984", "submitter": "Hanzhou Wu", "authors": "Han-Zhou Wu, Hong-Xia Wang and Yun-Qing Shi", "title": "Prediction-error of Prediction Error (PPE)-based Reversible Data Hiding", "comments": "There has no technical difference to previous versions, but rather\n  some minor word corrections. A 2-page summary of this paper was accepted by\n  ACM IH&MMSec'16 \"Ongoing work session\". My homepage: hzwu.github.io", "journal-ref": null, "doi": "10.1145/2909827.2933196", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel reversible data hiding (RDH) algorithm for\ngray-scaled images, in which the prediction-error of prediction error (PPE) of\na pixel is used to carry the secret data. In the proposed method, the pixels to\nbe embedded are firstly predicted with their neighboring pixels to obtain the\ncorresponding prediction errors (PEs). Then, by exploiting the PEs of the\nneighboring pixels, the prediction of the PEs of the pixels can be determined.\nAnd, a sorting technique based on the local complexity of a pixel is used to\ncollect the PPEs to generate an ordered PPE sequence so that, smaller PPEs will\nbe processed first for data embedding. By reversibly shifting the PPE histogram\n(PPEH) with optimized parameters, the pixels corresponding to the altered PPEH\nbins can be finally modified to carry the secret data. Experimental results\nhave implied that the proposed method can benefit from the prediction procedure\nof the PEs, sorting technique as well as parameters selection, and therefore\noutperform some state-of-the-art works in terms of payload-distortion\nperformance when applied to different images.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 04:52:27 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 17:45:57 GMT"}, {"version": "v3", "created": "Tue, 10 May 2016 22:28:48 GMT"}, {"version": "v4", "created": "Thu, 12 May 2016 00:24:09 GMT"}, {"version": "v5", "created": "Thu, 22 Sep 2016 22:01:31 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Wu", "Han-Zhou", ""], ["Wang", "Hong-Xia", ""], ["Shi", "Yun-Qing", ""]]}, {"id": "1604.05358", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler", "title": "Text-based LSTM networks for Automatic Music Composition", "comments": "Accepted in the 1st Conference on Computer Simulation of Musical\n  Creativity, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce new methods and discuss results of text-based\nLSTM (Long Short-Term Memory) networks for automatic music composition. The\nproposed network is designed to learn relationships within text documents that\nrepresent chord progressions and drum tracks in two case studies. In the\nexperiments, word-RNNs (Recurrent Neural Networks) show good results for both\ncases, while character-based RNNs (char-RNNs) only succeed to learn chord\nprogressions. The proposed system can be used for fully automatic composition\nor as semi-automatic systems that help humans to compose music by controlling a\ndiversity parameter of the model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 21:43:44 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""]]}, {"id": "1604.05799", "submitter": "Andrew Connor", "authors": "Matthew Martin, James Charlton and Andy M. Connor", "title": "Mainstreaming video annotation software for critical video analysis", "comments": null, "journal-ref": "Journal of Technologies and Human Usability, 11(3), 1-13 (2015)", "doi": null, "report-no": null, "categories": "cs.MM cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The range of video annotation software currently available is set within\ncommercially specialized professions, distributed via outdated sources or\nthrough online video hosting services. As video content becomes an increasingly\nsignificant tool for analysis, there is a demand for appropriate digital\nannotation techniques that offer equivalent functionality to tools used for\nannotation of text based literature sources. This paper argues for the\nimportance of video annotating as an effective method for research that is as\naccessible as literature annotation is. Video annotation has been shown to\ntrigger higher learning and engagement but research struggles to explain the\nabsence of video annotation in contemporary structures of education practice.\nIn both academic and informal settings the use of video playback as a\nmeaningful tool of analysis is apparent, yet the availability of supplementary\nannotation software is not within obvious grasp or even prevalent in\nstandardized computer software. Practical software tools produced by the\nresearcher have demonstrated effective video annotation in a short development\ntime. With software design programs available for rapid application creation,\nthis paper also highlights the absence of a development community. This paper\nargues that video annotation is an accessible tool, not just for academic\ncontexts, but also for wider practical video analysis applications, potentially\nbecoming a mainstream learning tool. This paper thus presents a practical\nmultimodal public approach to video research that potentially affords a deeper\nanalysis of media content. This is supported by an in-depth consideration of\nthe motivation for undertaking video annotation and a critical analysis of\ncurrently available tools.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 02:58:36 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Martin", "Matthew", ""], ["Charlton", "James", ""], ["Connor", "Andy M.", ""]]}, {"id": "1604.06480", "submitter": "Yannis Kalantidis", "authors": "Yannis Kalantidis, Lyndon Kennedy, Huy Nguyen, Clayton Mellina, David\n  A. Shamma", "title": "LOH and behold: Web-scale visual search, recommendation and clustering\n  using Locally Optimized Hashing", "comments": "Accepted for publication at the 4th Workshop on Web-scale Vision and\n  Social Media (VSM), ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hashing-based matching scheme, called Locally Optimized\nHashing (LOH), based on a state-of-the-art quantization algorithm that can be\nused for efficient, large-scale search, recommendation, clustering, and\ndeduplication. We show that matching with LOH only requires set intersections\nand summations to compute and so is easily implemented in generic distributed\ncomputing systems. We further show application of LOH to: a) large-scale search\ntasks where performance is on par with other state-of-the-art hashing\napproaches; b) large-scale recommendation where queries consisting of thousands\nof images can be used to generate accurate recommendations from collections of\nhundreds of millions of images; and c) efficient clustering with a graph-based\nalgorithm that can be scaled to massive collections in a distributed\nenvironment or can be used for deduplication for small collections, like search\nresults, performing better than traditional hashing approaches while only\nrequiring a few milliseconds to run. In this paper we experiment on datasets of\nup to 100 million images, but in practice our system can scale to larger\ncollections and can be used for other types of data that have a vector\nrepresentation in a Euclidean space.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 20:23:55 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 02:34:52 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Kalantidis", "Yannis", ""], ["Kennedy", "Lyndon", ""], ["Nguyen", "Huy", ""], ["Mellina", "Clayton", ""], ["Shamma", "David A.", ""]]}, {"id": "1604.07051", "submitter": "Saeed Ranjbar Alvar", "authors": "Saeed Ranjbar Alvar and Fatih Kamisli", "title": "Lossless Intra Coding in HEVC with Adaptive 3-tap Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pixel-by-pixel spatial prediction methods for lossless intra coding, the\nprediction is obtained by a weighted sum of neighbouring pixels. The proposed\nprediction approach in this paper uses a weighted sum of three neighbor pixels\naccording to a two-dimensional correlation model. The weights are obtained\nafter a three step optimization procedure. The first two stages are offline\nprocedures where the computed prediction weights are obtained offline from\ntraining sequences. The third stage is an online optimization procedure where\nthe offline obtained prediction weights are further fine-tuned and adapted to\neach encoded block during encoding using a rate-distortion optimized method and\nthe modification in this third stage is transmitted to the decoder as side\ninformation. The results of the simulations show average bit rate reductions of\n12.02% and 3.28% over the default lossless intra coding in HEVC and the\nwell-known Sample-based Angular Prediction (SAP) method, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 16:58:08 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Alvar", "Saeed Ranjbar", ""], ["Kamisli", "Fatih", ""]]}, {"id": "1604.07160", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Michael Gygli, Beat Pfister, Luc Van Gool", "title": "Deep Convolutional Neural Networks and Data Augmentation for Acoustic\n  Event Detection", "comments": "Presented in INTERSPEECH 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for Acoustic Event Detection (AED). In contrast to\nspeech, sounds coming from acoustic events may be produced by a wide variety of\nsources. Furthermore, distinguishing them often requires analyzing an extended\ntime period due to the lack of a clear sub-word unit. In order to incorporate\nthe long-time frequency structure for AED, we introduce a convolutional neural\nnetwork (CNN) with a large input field. In contrast to previous works, this\nenables to train audio event detection end-to-end. Our architecture is inspired\nby the success of VGGNet and uses small, 3x3 convolutions, but more depth than\nprevious methods in AED. In order to prevent over-fitting and to take full\nadvantage of the modeling capabilities of our network, we further propose a\nnovel data augmentation method to introduce data variation. Experimental\nresults show that our CNN significantly outperforms state of the art methods\nincluding Bag of Audio Words (BoAW) and classical CNNs, achieving a 16%\nabsolute improvement.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 08:25:03 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 04:28:16 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Takahashi", "Naoya", ""], ["Gygli", "Michael", ""], ["Pfister", "Beat", ""], ["Van Gool", "Luc", ""]]}, {"id": "1604.07211", "submitter": "Edip Demirbilek", "authors": "Edip Demirbilek and Jean-Charles Gr\\'egoire", "title": "Towards Reduced Reference Parametric Models for Estimating Audiovisual\n  Quality in Multimedia Services", "comments": "Accepted to ICC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed reduced reference parametric models for estimating\nperceived quality in audiovisual multimedia services. We have created 144\nunique configurations for audiovisual content including various application and\nnetwork parameters such as bitrates and distortions in terms of bandwidth,\npacket loss rate and jitter. To generate the data needed for model training and\nvalidation we have tasked 24 subjects, in a controlled environment, to rate the\noverall audiovisual quality on the absolute category rating (ACR) 5-level\nquality scale. We have developed models using Random Forest and Neural Network\nbased machine learning methods in order to estimate Mean Opinion Scores (MOS)\nvalues. We have used information retrieved from the packet headers and side\ninformation provided as network parameters for model training. Random Forest\nbased models have performed better in terms of Root Mean Square Error (RMSE)\nand Pearson correlation coefficient. The side information proved to be very\neffective in developing the model. We have found that, while the model\nperformance might be improved by replacing the side information with more\naccurate bit stream level measurements, they are performing well in estimating\nperceived quality in audiovisual multimedia services.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 11:43:09 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Demirbilek", "Edip", ""], ["Gr\u00e9goire", "Jean-Charles", ""]]}, {"id": "1604.07322", "submitter": "Maria Torres Vega", "authors": "Maria Torres Vega, Decebal Constantin Mocanu and Antonio Liotta", "title": "Predictive No-Reference Assessment of Video Quality", "comments": "13 pages, 8 figures, IEEE Selected Topics on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the various means to evaluate the quality of video streams,\nNo-Reference (NR) methods have low computation and may be executed on thin\nclients. Thus, NR algorithms would be perfect candidates in cases of real-time\nquality assessment, automated quality control and, particularly, in adaptive\nmobile streaming. Yet, existing NR approaches are often inaccurate, in\ncomparison to Full-Reference (FR) algorithms, especially under lossy network\nconditions. In this work, we present an NR method that combines machine\nlearning with simple NR metrics to achieve a quality index comparably as\naccurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method\nis tested in an extensive dataset (960 videos), under lossy network conditions\nand considering nine different machine learning algorithms. Overall, we achieve\nan over 97% correlation with VQM, while allowing real-time assessment of video\nquality of experience in realistic streaming scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 16:34:17 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 06:16:40 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Vega", "Maria Torres", ""], ["Mocanu", "Decebal Constantin", ""], ["Liotta", "Antonio", ""]]}, {"id": "1604.07339", "submitter": "Ivan Bajic", "authors": "Sayed Hossein Khatoonabadi, Ivan V. Bajic, Yufeng Shan", "title": "Compressed-domain visual saliency models: A comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational modeling of visual saliency has become an important research\nproblem in recent years, with applications in video quality estimation, video\ncompression, object tracking, retargeting, summarization, and so on. While most\nvisual saliency models for dynamic scenes operate on raw video, several models\nhave been developed for use with compressed-domain information such as motion\nvectors and transform coefficients. This paper presents a comparative study of\neleven such models as well as two high-performing pixel-domain saliency models\non two eye-tracking datasets using several comparison metrics. The results\nindicate that highly accurate saliency estimation is possible based only on a\npartially decoded video bitstream. The strategies that have shown success in\ncompressed-domain saliency modeling are highlighted, and certain challenges are\nidentified as potential avenues for further improvement.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 17:39:25 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Khatoonabadi", "Sayed Hossein", ""], ["Bajic", "Ivan V.", ""], ["Shan", "Yufeng", ""]]}, {"id": "1604.07519", "submitter": "Balasubramanyam Appina Mr", "authors": "Manasa K, Balasubramanyam Appina, and Sumohana S. Channappayya", "title": "Subjective Assessment of H.264 Compressed Stereoscopic Video", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous growth in 3D (stereo) imaging and display technologies has led\nto stereoscopic content (video and image) becoming increasingly popular.\nHowever, both the subjective and the objective evaluation of stereoscopic video\ncontent has not kept pace with the rapid growth of the content. Further, the\navailability of standard stereoscopic video databases is also quite limited. In\nthis work, we attempt to alleviate these shortcomings. We present a\nstereoscopic video database and its subjective evaluation. We have created a\ndatabase containing a set of 144 distorted videos. We limit our attention to\nH.264 compression artifacts. The distorted videos were generated using 6\nuncompressed pristine videos of left and right views originally created by\nGoldmann et al. at EPFL [1]. Further, 19 subjects participated in the\nsubjective assessment task. Based on the subjective study, we have formulated a\nrelation between the 2D and stereoscopic subjective scores as a function of\ncompression rate and depth range. We have also evaluated the performance of\npopular 2D and 3D image/video quality assessment (I/VQA) algorithms on our\ndatabase.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 05:02:33 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["K", "Manasa", ""], ["Appina", "Balasubramanyam", ""], ["Channappayya", "Sumohana S.", ""]]}, {"id": "1604.07547", "submitter": "Conrad Sanderson", "authors": "Johanna Carvajal, Arnold Wiliem, Conrad Sanderson, Brian Lovell", "title": "Towards Miss Universe Automatic Prediction: The Evening Gown Competition", "comments": null, "journal-ref": "International Conference on Pattern Recognition, 2016", "doi": "10.1109/ICPR.2016.7899781", "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we predict the winner of Miss Universe after watching how they stride\ndown the catwalk during the evening gown competition? Fashion gurus say they\ncan! In our work, we study this question from the perspective of computer\nvision. In particular, we want to understand whether existing computer vision\napproaches can be used to automatically extract the qualities exhibited by the\nMiss Universe winners during their catwalk. This study can pave the way towards\nnew vision-based applications for the fashion industry. To this end, we propose\na novel video dataset, called the Miss Universe dataset, comprising 10 years of\nthe evening gown competition selected between 1996-2010. We further propose two\nranking-related problems: (1) Miss Universe Listwise Ranking and (2) Miss\nUniverse Pairwise Ranking. In addition, we also develop an approach that\nsimultaneously addresses the two proposed problems. To describe the videos we\nemploy the recently proposed Stacked Fisher Vectors in conjunction with robust\nlocal spatio-temporal features. From our evaluation we found that although the\naddressed problems are extremely challenging, the proposed system is able to\nrank the winner in the top 3 best predicted scores for 5 out of 10 Miss\nUniverse competitions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 07:02:16 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 03:22:01 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Carvajal", "Johanna", ""], ["Wiliem", "Arnold", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian", ""]]}, {"id": "1604.07593", "submitter": "Muhammad Fahad Khan", "authors": "Saira Beg, M. Fahad Khan, Faisal Baig", "title": "Compress Voice Transference over low Signal Strength in Satellite\n  Communication", "comments": "11 pages, 8 figures, International Journal", "journal-ref": "International Journal of System of Systems Engineering 4.2 (2013):\n  174-186", "doi": "10.1504/IJSSE.2013.056303", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the comparison of compression algorithms for voice\ntransferring method over SMS in satellite communication. Voice transferring\nmethod over SMS is useful in situations when signal strength is low and due to\npoor signal strength voice call connection is not possible to initiate or\nsignal dropped during voice call. This method has one serious flaw that it\nproduces large number of SMS while converting voice into SMS. Such issue is\ncatered to some extend by employing any compression algorithm. In this paper\nour major aim is to find best compression scheme for said method, for that\npurpose we compare 6 different types of compression algorithms which are; LZW\n(Lempel-Ziv-Welch), Huffman coding, PPM (Prediction by partial matching),\nArithmetic Coding (AC), BWT (Burrows-Wheeler-Transform), LZMA\n(Lempel-Ziv-Markov chain). This comparison shows that PPM compression method\noffers better compression ratio and produce small number of SMS. For\nexperimentation we use Thuraya SG-2520 satellite phone. Moreover, we develop an\napplication using J2ME platform[Ref:a]. We tested that application more than\n100 times and then we compare the result in terms of compression ratio of each\nalgorithm and number of connected SMS produce after each compression method.\nThe result of this study will help developers to choose better compression\nscheme for their respective applications.\nhttp://www.learnrnd.com/news.php?id=ISSUES_IN_MOLECULAR_COMMUNICATIONS\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 09:57:37 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Beg", "Saira", ""], ["Khan", "M. Fahad", ""], ["Baig", "Faisal", ""]]}, {"id": "1604.07741", "submitter": "Tavi Halperin", "authors": "Tavi Halperin, Yair Poleg, Chetan Arora, Shmuel Peleg", "title": "EgoSampling: Wide View Hyperlapse from Egocentric Videos", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2651051", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibility of sharing one's point of view makes use of wearable cameras\ncompelling. These videos are often long, boring and coupled with extreme shake,\nas the camera is worn on a moving person. Fast forwarding (i.e. frame sampling)\nis a natural choice for quick video browsing. However, this accentuates the\nshake caused by natural head motion in an egocentric video, making the fast\nforwarded video useless. We propose EgoSampling, an adaptive frame sampling\nthat gives stable, fast forwarded, hyperlapse videos. Adaptive frame sampling\nis formulated as an energy minimization problem, whose optimal solution can be\nfound in polynomial time. We further turn the camera shake from a drawback into\na feature, enabling the increase in field-of-view of the output video. This is\nobtained when each output frame is mosaiced from several input frames. The\nproposed technique also enables the generation of a single hyperlapse video\nfrom multiple egocentric videos, allowing even faster video consumption.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 16:25:24 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 15:12:19 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Halperin", "Tavi", ""], ["Poleg", "Yair", ""], ["Arora", "Chetan", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1604.07939", "submitter": "Andre Araujo", "authors": "Andre Araujo, Jason Chaves, Haricharan Lakshman, Roland Angst, Bernd\n  Girod", "title": "Large-Scale Query-by-Image Video Retrieval Using Bloom Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using image queries to retrieve videos from a\ndatabase. Our focus is on large-scale applications, where it is infeasible to\nindex each database video frame independently. Our main contribution is a\nframework based on Bloom filters, which can be used to index long video\nsegments, enabling efficient image-to-video comparisons. Using this framework,\nwe investigate several retrieval architectures, by considering different types\nof aggregation and different functions to encode visual information -- these\nplay a crucial role in achieving high performance. Extensive experiments show\nthat the proposed technique improves mean average precision by 24% on a public\ndataset, while being 4X faster, compared to the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 05:46:52 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 17:58:16 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Araujo", "Andre", ""], ["Chaves", "Jason", ""], ["Lakshman", "Haricharan", ""], ["Angst", "Roland", ""], ["Girod", "Bernd", ""]]}, {"id": "1604.08001", "submitter": "Amin Zheng", "authors": "Amin Zheng, Gene Cheung and Dinei Florencio", "title": "Context Tree based Image Contour Coding using A Geometric Prior", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2627813", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If object contours in images are coded efficiently as side information, then\nthey can facilitate advanced image / video coding techniques, such as graph\nFourier transform coding or motion prediction of arbitrarily shaped pixel\nblocks. In this paper, we study the problem of lossless and lossy compression\nof detected contours in images. Specifically, we first convert a detected\nobject contour composed of contiguous between-pixel edges to a sequence of\ndirectional symbols drawn from a small alphabet. To encode the symbol sequence\nusing arithmetic coding, we compute an optimal variable-length context tree\n(VCT) $\\mathcal{T}$ via a maximum a posterior (MAP) formulation to estimate\nsymbols' conditional probabilities. MAP prevents us from overfitting given a\nsmall training set $\\mathcal{X}$ of past symbol sequences by identifying a VCT\n$\\mathcal{T}$ that achieves a high likelihood $P(\\mathcal{X}|\\mathcal{T})$ of\nobserving $\\mathcal{X}$ given $\\mathcal{T}$, and a large geometric prior\n$P(\\mathcal{T})$ stating that image contours are more often straight than\ncurvy. For the lossy case, we design efficient dynamic programming (DP)\nalgorithms that optimally trade off coding rate of an approximate contour\n$\\hat{\\mathbf{x}}$ given a VCT $\\mathcal{T}$ with two notions of distortion of\n$\\hat{\\mathbf{x}}$ with respect to the original contour $\\mathbf{x}$. To reduce\nthe size of the DP tables, a total suffix tree is derived from a given VCT\n$\\mathcal{T}$ for compact table entry indexing, reducing complexity.\nExperimental results show that for lossless contour coding, our proposed\nalgorithm outperforms state-of-the-art context-based schemes consistently for\nboth small and large training datasets. For lossy contour coding, our\nalgorithms outperform comparable schemes in the literature in rate-distortion\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 10:00:41 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Zheng", "Amin", ""], ["Cheung", "Gene", ""], ["Florencio", "Dinei", ""]]}, {"id": "1604.08088", "submitter": "Xirong Li", "authors": "Xirong Li and Yujia Huo and Jieping Xu and Qin Jin", "title": "Detecting Violence in Video using Subclasses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attacks the challenging problem of violence detection in videos.\nDifferent from existing works focusing on combining multi-modal features, we go\none step further by adding and exploiting subclasses visually related to\nviolence. We enrich the MediaEval 2015 violence dataset by \\emph{manually}\nlabeling violence videos with respect to the subclasses. Such fine-grained\nannotations not only help understand what have impeded previous efforts on\nlearning to fuse the multi-modal features, but also enhance the generalization\nability of the learned fusion to novel test data. The new subclass based\nsolution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,\noutperforms several state-of-the-art alternatives. Notice that our solution\ndoes not require fine-grained annotations on the test set, so it can be\ndirectly applied on novel and fully unlabeled videos. Interestingly, our study\nshows that motion related features, though being essential part in previous\nsystems, are dispensable.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 14:32:16 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Li", "Xirong", ""], ["Huo", "Yujia", ""], ["Xu", "Jieping", ""], ["Jin", "Qin", ""]]}, {"id": "1604.08245", "submitter": "Muhammad Fahad Khan", "authors": "Saira Beg, M. Fahad Khan, Faisal Baig", "title": "Text writing in the air", "comments": "19 pages, 19 figures,2 tables. see\n  http://www.tandfonline.com/doi/abs/10.1080/15980316.2013.860928?journalCode=tjid20", "journal-ref": null, "doi": "10.1080/15980316.2013.860928", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a real time video based pointing method which allows\nsketching and writing of English text over air in front of mobile camera.\nProposed method have two main tasks: first it track the colored finger tip in\nthe video frames and then apply English OCR over plotted images in order to\nrecognize the written characters. Moreover, proposed method provides a natural\nhuman-system interaction in such way that it do not require keypad, stylus, pen\nor glove etc for character input. For the experiments, we have developed an\napplication using OpenCv with JAVA language. We tested the proposed method on\nSamsung Galaxy3 android mobile. Results show that proposed algorithm gains the\naverage accuracy of 92.083% when tested for different shaped alphabets. Here,\nmore than 3000 different Magnetic 3D shaped characters were used [Ref:\nhttp://learnrnd.com/news.php?id=Magnetic_3D_Bio_Printing]. Our proposed system\nis the software based approach and relevantly very simple, fast and easy. It\ndoes not require sensors or any hardware rather than camera and red tape.\nMoreover, proposed methodology can be applicable for all disconnected languages\nbut having one issue that it is color sensitive in such a way that existence of\nany red color in the background before starting the character writing can lead\nto false results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 21:11:22 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Beg", "Saira", ""], ["Khan", "M. Fahad", ""], ["Baig", "Faisal", ""]]}]