[{"id": "1507.01209", "submitter": "Raghvendra Kannao", "authors": "Raghvendra Kannao and Prithwijit Guha", "title": "TV News Commercials Detection using Success based Locally Weighted\n  Kernel Combination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial detection in news broadcast videos involves judicious selection of\nmeaningful audio-visual feature combinations and efficient classifiers. And,\nthis problem becomes much simpler if these combinations can be learned from the\ndata. To this end, we propose an Multiple Kernel Learning based method for\nboosting successful kernel functions while ignoring the irrelevant ones. We\nadopt a intermediate fusion approach where, a SVM is trained with a weighted\nlinear combination of different kernel functions instead of single kernel\nfunction. Each kernel function is characterized by a feature set and kernel\ntype. We identify the feature sub-space locations of the prediction success of\na particular classifier trained only with particular kernel function. We\npropose to estimate a weighing function using support vector regression (with\nRBF kernel) for each kernel function which has high values (near 1.0) where the\nclassifier learned on kernel function succeeded and lower values (nearly 0.0)\notherwise. Second contribution of this work is TV News Commercials Dataset of\n150 Hours of News videos. Classifier trained with our proposed scheme has\noutperformed the baseline methods on 6 of 8 benchmark dataset and our own TV\ncommercials dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 12:01:34 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Kannao", "Raghvendra", ""], ["Guha", "Prithwijit", ""]]}, {"id": "1507.01673", "submitter": "Junhui Hou", "authors": "Junhui Hou, Lap-Pui Chau, Nadia Magnenat-Thalmann, Ying He", "title": "SLRMA: Sparse Low-Rank Matrix Approximation for Data Compression", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix approximation (LRMA) is a powerful technique for signal\nprocessing and pattern analysis. However, its potential for data compression\nhas not yet been fully investigated in the literature. In this paper, we\npropose sparse low-rank matrix approximation (SLRMA), an effective\ncomputational tool for data compression. SLRMA extends the conventional LRMA by\nexploring both the intra- and inter-coherence of data samples simultaneously.\nWith the aid of prescribed orthogonal transforms (e.g., discrete cosine/wavelet\ntransform and graph transform), SLRMA decomposes a matrix into a product of two\nsmaller matrices, where one matrix is made of extremely sparse and orthogonal\ncolumn vectors, and the other consists of the transform coefficients.\nTechnically, we formulate SLRMA as a constrained optimization problem, i.e.,\nminimizing the approximation error in the least-squares sense regularized by\n$\\ell_0$-norm and orthogonality, and solve it using the inexact augmented\nLagrangian multiplier method. Through extensive tests on real-world data, such\nas 2D image sets and 3D dynamic meshes, we observe that (i) SLRMA empirically\nconverges well; (ii) SLRMA can produce approximation error comparable to LRMA\nbut in a much sparse form; (iii) SLRMA-based compression schemes significantly\noutperform the state-of-the-art in terms of rate-distortion performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 04:36:19 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 08:10:47 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""], ["Magnenat-Thalmann", "Nadia", ""], ["He", "Ying", ""]]}, {"id": "1507.04831", "submitter": "Yongtao Hu", "authors": "Yongtao Hu, Jimmy Ren, Jingwen Dai, Chang Yuan, Li Xu, and Wenping\n  Wang", "title": "Deep Multimodal Speaker Naming", "comments": null, "journal-ref": null, "doi": "10.1145/2733373.2806293", "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speaker naming is the problem of localizing as well as identifying\neach speaking character in a TV/movie/live show video. This is a challenging\nproblem mainly attributes to its multimodal nature, namely face cue alone is\ninsufficient to achieve good performance. Previous multimodal approaches to\nthis problem usually process the data of different modalities individually and\nmerge them using handcrafted heuristics. Such approaches work well for simple\nscenes, but fail to achieve high performance for speakers with large appearance\nvariations. In this paper, we propose a novel convolutional neural networks\n(CNN) based learning framework to automatically learn the fusion function of\nboth face and audio cues. We show that without using face tracking, facial\nlandmark localization or subtitle/transcript, our system with robust multimodal\nfeature extraction is able to achieve state-of-the-art speaker naming\nperformance evaluated on two diverse TV series. The dataset and implementation\nof our algorithm are publicly available online.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 04:13:12 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Hu", "Yongtao", ""], ["Ren", "Jimmy", ""], ["Dai", "Jingwen", ""], ["Yuan", "Chang", ""], ["Xu", "Li", ""], ["Wang", "Wenping", ""]]}, {"id": "1507.04913", "submitter": "Weiyao Lin", "authors": "Xintong Han, Chongyang Zhang, Weiyao Lin, Mingliang Xu, Bin Sheng, Tao\n  Mei", "title": "Tree-based Visualization and Optimization for Image Collection", "comments": "This manuscript is the accepted version for T-CYB (IEEE Transactions\n  on Cybernetics) IEEE Trans. Cybernetics, 2015", "journal-ref": null, "doi": "10.1109/TCYB.2015.2448236", "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visualization of an image collection is the process of displaying a\ncollection of images on a screen under some specific layout requirements. This\npaper focuses on an important problem that is not well addressed by the\nprevious methods: visualizing image collections into arbitrary layout shapes\nwhile arranging images according to user-defined semantic or visual\ncorrelations (e.g., color or object category). To this end, we first propose a\nproperty-based tree construction scheme to organize images of a collection into\na tree structure according to user-defined properties. In this way, images can\nbe adaptively placed with the desired semantic or visual correlations in the\nfinal visualization layout. Then, we design a two-step visualization\noptimization scheme to further optimize image layouts. As a result, multiple\nlayout effects including layout shape and image overlap ratio can be\neffectively controlled to guarantee a satisfactory visualization. Finally, we\nalso propose a tree-transfer scheme such that visualization layouts can be\nadaptively changed when users select different \"images of interest\". We\ndemonstrate the effectiveness of our proposed approach through the comparisons\nwith state-of-the-art visualization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:45:26 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Han", "Xintong", ""], ["Zhang", "Chongyang", ""], ["Lin", "Weiyao", ""], ["Xu", "Mingliang", ""], ["Sheng", "Bin", ""], ["Mei", "Tao", ""]]}, {"id": "1507.05150", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O. Nwana and Tshuan Chen", "title": "Towards Understanding User Preferences from User Tagging Behavior for\n  Personalization", "comments": "6 pages", "journal-ref": null, "doi": "10.1109/ISM.2015.79", "report-no": null, "categories": "cs.MM cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalizing image tags is a relatively new and growing area of research,\nand in order to advance this research community, we must review and challenge\nthe de-facto standard of defining tag importance. We believe that for greater\nprogress to be made, we must go beyond tags that merely describe objects that\nare visually represented in the image, towards more user-centric and subjective\nnotions such as emotion, sentiment, and preferences.\n  We focus on the notion of user preferences and show that the order that users\nlist tags on images is correlated to the order of preference over the tags that\nthey provided for the image. While this observation is not completely\nsurprising, to our knowledge, we are the first to explore this aspect of user\ntagging behavior systematically and report empirical results to support this\nobservation. We argue that this observation can be exploited to help advance\nthe image tagging (and related) communities.\n  Our contributions include: 1.) conducting a user study demonstrating this\nobservation, 2.) collecting a dataset with user tag preferences explicitly\ncollected.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 05:55:37 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 19:56:36 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Nwana", "Amandianeze O.", ""], ["Chen", "Tshuan", ""]]}, {"id": "1507.05174", "submitter": "Xiaoyan Gao", "authors": "Jasmin Fantel and Yan Gao", "title": "Joint Data Scheduling and FEC Coding for Multihomed Wireless Video\n  Delivery", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of mobile video delivery in heterogenous\nwireless networks from a server to multihomed device. Most existing works only\nconsider delivering video streaming on single path which bandwidth is limited\ncausing ultimate video transmission rate. To solve this live video streaming\ntransmission bottleneck problem, we propose a novel solution named Joint Data\nAllocation and Fountain Coding (JDAFC) method that contain below characters:\n(1) path selection, (2) dynamic data allocation, and (3) fountain coding. We\nevaluate the performance of JDAFC by simulation experiments using Exata and\nJVSM and compare it with some reference solutions. Experimental results\nrepresent that JDAFC outperforms the competing solutions in improving the video\npeak signal-to-noise ratio as well as reducing the end-to-end delay.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 11:45:44 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Fantel", "Jasmin", ""], ["Gao", "Yan", ""]]}, {"id": "1507.05242", "submitter": "Jayati Ghosh Dastidar", "authors": "Subhashri Acharya, Pramita Srimany, Sanchari Kundu, JayatiGhosh\n  Dastidar", "title": "Data Hiding in Video using Triangularization LSB Technique", "comments": null, "journal-ref": "International Journal of Advanced Trends in Computer Science and\n  Engineering, Volume 4, No.3, May - June 2015", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of data hiding in the field of Information Technology is a\nwidely accepted. The challenge is to be able to pass information in a manner\nthat the very existence of the message is unknown in order to repel attention\nof the potential attacker. Steganography is a technique that has been widely\nused to achieve this objective. However Steganography is often found to be\nlacking when it comes to hiding bulk data. Attempting to hide data in a video\novercomes this problem because of the large sized cover object (video) as\ncompared to an image in the case of steganography. This paper attempts to\npropose a scheme using which data can be hidden in a video. We focus on the\nTriangularization method and make use of the Least Significant Bit (LSB)\ntechnique in hiding messages in a video.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 03:05:26 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Acharya", "Subhashri", ""], ["Srimany", "Pramita", ""], ["Kundu", "Sanchari", ""], ["Dastidar", "JayatiGhosh", ""]]}, {"id": "1507.08075", "submitter": "Xiaochao Qu", "authors": "Xiaochao Qu, Suah Kim, Run Cui, Hyoung Joong Kim", "title": "Low Bit-Rate and High Fidelity Reversible Data Hiding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate predictor is crucial for histogram-shifting (HS) based reversible\ndata hiding methods. The embedding capacity is increased and the embedding\ndistortion is decreased simultaneously if the predictor can generate accurate\npredictions. In this paper, we propose an accurate linear predictor based on\nweighted least squares (WLS) estimation. The robustness of WLS helps the\nproposed predictor generate accurate predictions, especially in complex texture\nareas of an image, where other predictors usually fail. To further reduce the\nembedding distortion, we propose a new embedding method called dynamic\nhistogram shifting with pixel selection (DHS-PS) that selects not only the\nproper histogram bins but also the proper pixel locations to embed the given\ndata. As a result, the proposed method can obtain very high fidelity marked\nimages with low bit-rate data embedded. The experimental results show that the\nproposed method outperforms the state-of-the-art low bit-rate reversible data\nhiding method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:25:44 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Qu", "Xiaochao", ""], ["Kim", "Suah", ""], ["Cui", "Run", ""], ["Kim", "Hyoung Joong", ""]]}, {"id": "1507.08861", "submitter": "Muhammet Bastan", "authors": "Fatih Calisir, Muhammet Bastan, Ozgur Ulusoy, Ugur Gudukbay", "title": "Mobile Multi-View Object Image Search", "comments": "Multimedia Tools and Applications, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  High user interaction capability of mobile devices can help improve the\naccuracy of mobile visual search systems. At query time, it is possible to\ncapture multiple views of an object from different viewing angles and at\ndifferent scales with the mobile device camera to obtain richer information\nabout the object compared to a single view and hence return more accurate\nresults. Motivated by this, we developed a mobile multi-view object image\nsearch system, using a client-server architecture. Multi-view images of objects\nacquired by the mobile clients are processed and local features are sent to the\nserver, which combines the query image representations with early/late fusion\nmethods based on bag-of-visual-words and sends back the query results. We\nperformed a comprehensive analysis of early and late fusion approaches using\nvarious similarity functions, on an existing single view and a new multi-view\nobject image database. The experimental results show that multi-view search\nprovides significantly better retrieval accuracy compared to single view\nsearch.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 13:02:23 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 14:23:01 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Calisir", "Fatih", ""], ["Bastan", "Muhammet", ""], ["Ulusoy", "Ozgur", ""], ["Gudukbay", "Ugur", ""]]}]