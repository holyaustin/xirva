[{"id": "1510.00561", "submitter": "Stamos Katsigiannis", "authors": "Stamos Katsigiannis, Georgios Papaioannou, Dimitris Maroulis", "title": "CVC: The Contourlet Video Compression algorithm for real-time\n  applications", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, real-time video communication over the internet through video\nconferencing applications has become an invaluable tool in everyone's\nprofessional and personal life. This trend underlines the need for video coding\nalgorithms that provide acceptable quality on low bitrates and can support\nvarious resolutions inside the same stream in order to cope with limitations on\ncomputational resources and network bandwidth. In this work, a novel scalable\nvideo coding algorithm based on the contourlet transform is presented. The\nalgorithm utilizes both lossy and lossless methods in order to achieve\ncompression. One of its most notable features is that due to the transform\nutilised, it does not suffer from blocking artifacts that occur with many\nwidely adopted compression algorithms. The proposed algorithm takes advantage\nof the vast computational capabilities of modern GPUs, in order to achieve\nreal-time performance and provide satisfactory encoding and decoding times at\nrelatively low cost, making it suitable for applications like video\nconferencing. Experiments show that the proposed algorithm performs\nsatisfactorily in terms of compression ratio and speed, while it outperforms\nstandard methods in terms of perceptual quality on lower bitrates.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 11:08:45 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Katsigiannis", "Stamos", ""], ["Papaioannou", "Georgios", ""], ["Maroulis", "Dimitris", ""]]}, {"id": "1510.01134", "submitter": "Christoph Bachhuber", "authors": "Christoph Bachhuber and Eckehard Steinbach", "title": "A System for Precise End-to-End Delay Measurements in Video\n  Communication", "comments": "5 pages, 4 figures, IEEE International Conference on Image Processing\n  (ICIP 2016), Phoenix, AZ, USA, 2016", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532735", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low delay video transmission is becoming increasingly important. Delay\ncritical, video enabled applications range from teleoperation scenarios such as\ncontrolling drones or telesurgery to autonomous control through computer vision\nalgorithms applied on real-time video. To judge the quality of the video\ntransmission in such a system, it is important to be able to precisely measure\nthe end-to-end (E2E) delay of the transmitted video. We present a\nlow-complexity system that automatically takes pairwise independent\nmeasurements of E2E delay. The precision can be far below the millisecond\norder, mainly limited by the sampling rate of the measurement system. In our\nimplementation, we achieve a precision of 0.5 milliseconds with a sampling rate\nof 2kHz.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 13:04:55 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 13:36:58 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Bachhuber", "Christoph", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1510.02177", "submitter": "Khan Muhammad", "authors": "Khan Muhammad, Irfan Mehmood, Mi Young Lee, Su Mi Ji, Sung Wook Baik", "title": "Ontology-based Secure Retrieval of Semantically Significant Visual\n  Contents", "comments": "A short paper of 11 pages for secure visual contents retrieval.The\n  original version can be accessed at this link:\n  http://www.kingpc.or.kr/inc_html/index.html", "journal-ref": "Khan Muhammad, Irfan Mehmood, Mi Young Lee, Su Mi Ji, Sung Wook\n  Baik, \"Ontology-based Secure Retrieval of Semantically Significant Visual\n  Contents,\" JOURNAL OF KOREAN INSTITUTE OF NEXT GENERATION COMPUTING, vol. 11,\n  pp. 87-96, 2015", "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is an enthusiastic research field where large amount of\nimage data is classified into various classes based on their visual contents.\nResearchers have presented various low-level features-based techniques for\nclassifying images into different categories. However, efficient and effective\nclassification and retrieval is still a challenging problem due to complex\nnature of visual contents. In addition, the traditional information retrieval\ntechniques are vulnerable to security risks, making it easy for attackers to\nretrieve personal visual contents such as patients records and law enforcement\nagencies databases. Therefore, we propose a novel ontology-based framework\nusing image steganography for secure image classification and information\nretrieval. The proposed framework uses domain-specific ontology for mapping the\nlow-level image features to high-level concepts of ontologies which\nconsequently results in efficient classification. Furthermore, the proposed\nmethod utilizes image steganography for hiding the image semantics as a secret\nmessage inside them, making the information retrieval process secure from third\nparties. The proposed framework minimizes the computational complexity of\ntraditional techniques, increasing its suitability for secure and real-time\nvisual contents retrieval from personalized image databases. Experimental\nresults confirm the efficiency, effectiveness, and security of the proposed\nframework as compared with other state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 01:09:32 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Muhammad", "Khan", ""], ["Mehmood", "Irfan", ""], ["Lee", "Mi Young", ""], ["Ji", "Su Mi", ""], ["Baik", "Sung Wook", ""]]}, {"id": "1510.02834", "submitter": "Mauricio Toro", "authors": "Mauricio Toro and Camilo Rueda and Carlos Ag\\'on and G\\'erard Assayag", "title": "NTCCRT: A concurrent constraint framework for real-time interaction\n  (extended version)", "comments": "12 pages, short version published in the International Computer Music\n  Conference (ICMC), 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing multimedia interaction systems is not easy. Their concurrent\nprocesses usually access shared resources in a non-deterministic order, often\nleading to unpredictable behavior. Using Pure Data (Pd) and Max/MSP is possible\nto program concurrency, however, it is difficult to synchronize processes based\non multiple criteria. Process calculi such as the Non-deterministic Timed\nConcurrent Constraint (ntcc) calculus, overcome that problem by representing\nmultiple criteria as constraints. We propose using our framework Ntccrt to\nmanage concurrency in Pd and Max. Ntccrt is a real-time capable inter- preter\nfor ntcc. Using Ntccrt externals (binary plugins) in Pd we ran models for\nmachine improvisation and signal processing.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 21:53:12 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Toro", "Mauricio", ""], ["Rueda", "Camilo", ""], ["Ag\u00f3n", "Carlos", ""], ["Assayag", "G\u00e9rard", ""]]}, {"id": "1510.02899", "submitter": "Xirong Li", "authors": "Masoud Mazloom and Xirong Li and Cees G. M. Snoek", "title": "TagBook: A Semantic Video Representation without Supervision for Event\n  Detection", "comments": "accepted for publication as a regular paper in the IEEE Transactions\n  on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of event detection in video for scenarios where only\nfew, or even zero examples are available for training. For this challenging\nsetting, the prevailing solutions in the literature rely on a semantic video\nrepresentation obtained from thousands of pre-trained concept detectors.\nDifferent from existing work, we propose a new semantic video representation\nthat is based on freely available social tagged videos only, without the need\nfor training any intermediate concept detectors. We introduce a simple\nalgorithm that propagates tags from a video's nearest neighbors, similar in\nspirit to the ones used for image retrieval, but redesign it for video event\ndetection by including video source set refinement and varying the video tag\nassignment. We call our approach TagBook and study its construction,\ndescriptiveness and detection performance on the TRECVID 2013 and 2014\nmultimedia event detection datasets and the Columbia Consumer Video dataset.\nDespite its simple nature, the proposed TagBook video representation is\nremarkably effective for few-example and zero-example event detection, even\noutperforming very recent state-of-the-art alternatives building on supervised\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 09:28:56 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 13:23:03 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Mazloom", "Masoud", ""], ["Li", "Xirong", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1510.03050", "submitter": "Nikolaos Efthymiopoulos", "authors": "Nikolaos Efthymiopoulos, Athanasios Christakidis, Maria\n  Efthymiopoulou, Loris Corazza, Spyros Denazis", "title": "Congestion Control for P2P Live Streaming", "comments": "21 pages", "journal-ref": null, "doi": "10.5121/ijp2p.2015.6201", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years, research efforts tried to exploit peer-to-peer (P2P) systems\nin order to provide Live Streaming (LS) and Video-on-Demand (VoD) services.\nMost of these research efforts focus on the development of distributed P2P\nblock schedulers for content exchange among the participating peers and on the\ncharacteristics of the overlay graph (P2P overlay) that interconnects the set\nof these peers. Currently, researchers try to combine peer-to-peer systems with\ncloud infrastructures. They developed monitoring and control architectures that\nuse resources from the cloud in order to enhance QoS and achieve an attractive\ntrade-off between stability and low cost operation. However, there is a lack of\nresearch effort on the congestion control of these systems and the existing\ncongestion control architectures are not suitable for P2P live streaming\ntraffic (small sequential non persistent traffic towards multiple network\nlocations). This paper proposes a P2P live streaming traffic aware congestion\ncontrol protocol that: i) is capable to manage sequential traffic heading to\nmultiple network destinations , ii) efficiently exploits the available\nbandwidth, iii) accurately measures the idle peer resources, iv) avoids network\ncongestion, and v) is friendly to traditional TCP generated traffic. The\nproposed P2P congestion control has been implemented, tested and evaluated\nthrough a series of real experiments powered across the BonFIRE infrastructure.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 13:26:53 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Efthymiopoulos", "Nikolaos", ""], ["Christakidis", "Athanasios", ""], ["Efthymiopoulou", "Maria", ""], ["Corazza", "Loris", ""], ["Denazis", "Spyros", ""]]}, {"id": "1510.03057", "submitter": "Mauricio Toro", "authors": "Mauricio Toro", "title": "Towards non-threaded Concurrent Constraint Programming for implementing\n  multimedia interaction systems", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explain the implementation of event-driven real-time\ninterpreters for the Concurrent Constraint Programming (CCP) and\nNon-deterministic Timed Concurrent Constraint (NTCC) for- malisms. The CCP\ninterpreter was tested with a program to find, concurrently, paths in a graph\nand it will be used in the future to find musical sequences in the music\nimprovisation software Omax, developed by the French Acoustics/Music Research\nInstitute (IRCAM). In the other hand, the NTCC interpreter was tested with a\nmusic improvisation system based on NTCC (CCFOMI), developed by the AVISPA\nresearch group and IRCAM. Additionally, we present GECOL 2, a wrapper for the\nGeneric Constraints Development Environment (GECODE) to Common LISP, de-\nveloped to port the interpreters to Common LISP in the future. We concluded\nthat using GECODE for the concurrency control avoids the need of having threads\nand synchronizing them, leading to a simple and efficient implementation of CCP\nand NTCC. We also noticed that the time units in NTCC interpreter do not\nrepresent discrete time units, because when we simulate the NTCC specifications\nin the interpreter, the time units have different durations. In the future, we\npropose forcing the duration of each time unit to a fix time, that way we would\nbe able to reason about NTCC time units as we do with discrete time units.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 14:15:47 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Toro", "Mauricio", ""]]}, {"id": "1510.03090", "submitter": "Mauricio Toro", "authors": "Mauricio Toro and Myriam Desainte-Catherine and Julien Castet", "title": "An Extension of Interactive Scores for Multimedia Scenarios with\n  Temporal Relations for Micro and Macro Controls", "comments": "extended version of article presented in the Sound and Music\n  Computing Conference 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software to design multimedia scenarios is usually based either on a fixed\ntimeline or on cue lists, but both models are unrelated temporally. On the\ncontrary, the formalism of interactive scores can describe multimedia scenarios\nwith flexible and fixed temporal relations among the objects of the scenario,\nbut cannot express neither temporal relations for micro controls nor signal\nprocessing. We extend interactive scores with such relations and with sound\nprocessing. We show some applications and we describe how they can be\nimplemented in Pure Data. Our implementation has low average relative jitter\neven under high cpu load.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 19:15:56 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Toro", "Mauricio", ""], ["Desainte-Catherine", "Myriam", ""], ["Castet", "Julien", ""]]}, {"id": "1510.03971", "submitter": "Mostafa Zaman Chowdhury", "authors": "Mostafa Zaman Chowdhury and Yeong Min Jang", "title": "Quality-Aware Popularity Based Bandwidth Allocation for Scalable Video\n  Broadcast over Wireless Access Networks", "comments": "9 pages, journal paper (Accepted) in Journal of Internet Technology,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video broadcast/multicast over wireless access networks is an attractive\nresearch issue in the field of wireless communication. With the rapid\nimprovement of various wireless network technologies, it is now possible to\nprovide high quality video transmission over wireless networks. The high\nquality video streams need higher bandwidth. Hence, during the video\ntransmission through wireless networks, it is very important to make the best\nutilization of the limited bandwidth. Therefore, when many broadcasting video\nsessions are active, the bandwidth per video session can be allocated based on\npopularity of the video sessions (programs). Instead of allocating equal\nbandwidth to each of them, our proposed scheme allocates bandwidth per\nbroadcasting video session based on popularity of the video program. When the\nsystem bandwidth is not sufficient to allocate the demanded bandwidth for all\nthe active video sessions, our proposed scheme efficiently allocates the total\nsystem bandwidth among all the scalable active video sessions in such a way\nthat higher bandwidth is allocated to higher popularity one. Using the\nmathematical and simulation analyses, we show that the proposed scheme\nmaximizes the average user satisfaction level and achieves the best utilization\nof bandwidth. The simulation results indicate that a large number of\nsubscribers can receive a significantly improved quality of video. To improve\nthe video quality for large number of subscribers, the only tradeoff is that a\nvery few subscribers receive slightly degraded video quality.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 06:01:53 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Chowdhury", "Mostafa Zaman", ""], ["Jang", "Yeong Min", ""]]}, {"id": "1510.03973", "submitter": "Mostafa Zaman Chowdhury", "authors": "Mostafa Zaman Chowdhury, Mohammad Arif Hossain, Shakil Ahmed, and\n  Yeong Min Jang", "title": "Radio Resource Management Based on Reused Frequency Allocation for\n  Dynamic Channel Borrowing Scheme in Wireless Networks", "comments": "Journal paper (Wireless Networks, Online published)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modern era, cellular communication consumers are exponentially\nincreasing as they find the system more user-friendly. Due to enormous users\nand their numerous demands, it has become a mandate to make the best use of the\nlimited radio resources that assures the highest standard of Quality of Service\n(QoS). To reach the guaranteed level of QoS for the maximum number of users,\nmaximum utilization of bandwidth is not only the key issue to be considered,\nrather some other factors like interference, call blocking probability etc. are\nalso needed to keep under deliberation. The lower performances of these factors\nmay retrograde the overall cellular networks performances. Keeping these\ndifficulties under consideration, we propose an effective dynamic channel\nborrowing model that safeguards better QoS, other factors as well. The proposed\nscheme reduces the excessive overall call blocking probability and does\ninterference mitigation without sacrificing bandwidth utilization. The proposed\nscheme is modeled in such a way that the cells are bifurcated after the channel\nborrowing process if the borrowed channels have the same type of frequency band\n(i.e. reused frequency). We also propose that the unoccupied interfering\nchannels of adjacent cells can also be inactivated, instead of cell bifurcation\nfor interference mitigation. The simulation endings show satisfactory\nperformances in terms of overall call blocking probability and bandwidth\nutilization that are compared to the conventional scheme without channel\nborrowing. Furthermore, signal to interference plus noise ratio (SINR) level,\ncapacity, and outage probability are compared to the conventional scheme\nwithout interference mitigation after channel borrowing that may attract the\nconsiderable concentration to the operators.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 06:08:51 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Chowdhury", "Mostafa Zaman", ""], ["Hossain", "Mohammad Arif", ""], ["Ahmed", "Shakil", ""], ["Jang", "Yeong Min", ""]]}, {"id": "1510.04389", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Kota Ito, Yuji Aramaki, Toshihiko Yamasaki, Kiyoharu\n  Aizawa", "title": "Sketch-based Manga Retrieval using Manga109 Dataset", "comments": "13 pages", "journal-ref": "Multimedia Tools and Applications, Volume 76, Issue 20, 2017", "doi": "10.1007/s11042-016-4020-z", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manga (Japanese comics) are popular worldwide. However, current e-manga\narchives offer very limited search support, including keyword-based search by\ntitle or author, or tag-based categorization. To make the manga search\nexperience more intuitive, efficient, and enjoyable, we propose a content-based\nmanga retrieval system. First, we propose a manga-specific image-describing\nframework. It consists of efficient margin labeling, edge orientation histogram\nfeature description, and approximate nearest-neighbor search using product\nquantization. Second, we propose a sketch-based interface as a natural way to\ninteract with manga content. The interface provides sketch-based querying,\nrelevance feedback, and query retouch. For evaluation, we built a novel dataset\nof manga images, Manga109, which consists of 109 comic books of 21,142 pages\ndrawn by professional manga artists. To the best of our knowledge, Manga109 is\ncurrently the biggest dataset of manga images available for research. We\nconducted a comparative study, a localization evaluation, and a large-scale\nqualitative study. From the experiments, we verified that: (1) the retrieval\naccuracy of the proposed method is higher than those of previous methods; (2)\nthe proposed method can localize an object instance with reasonable runtime and\naccuracy; and (3) sketch querying is useful for manga search.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 03:47:46 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Matsui", "Yusuke", ""], ["Ito", "Kota", ""], ["Aramaki", "Yuji", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1510.04413", "submitter": "Khan Muhammad", "authors": "Khan Muhammad, Jamil Ahmad, Muhammad Sajjad, Muhammad Zubair", "title": "Secure Image Steganography using Cryptography and Image Transposition", "comments": "A simple but effective image steganographic method, providing secure\n  transmission of secret data over Internet. The final published version of the\n  paper can be downloaded from the link:\n  (http://www.neduet.edu.pk/NED-Journal/2015/15vol4paper3.html). Please contact\n  me at khan.muhammad.icp@gmail.com if you need the final formatted published\n  version of the paper", "journal-ref": "NED University Journal of Research 12.4 (2015): 81-91", "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information security is one of the most challenging problems in today's\ntechnological world. In order to secure the transmission of secret data over\nthe public network (Internet), various schemes have been presented over the\nlast decade. Steganography combined with cryptography, can be one of the best\nchoices for solving this problem. This paper proposes a new steganographic\nmethod based on gray-level modification for true colour images using image\ntransposition, secret key and cryptography. Both the secret key and secret\ninformation are initially encrypted using multiple encryption algorithms\n(bitxor operation, bits shuffling, and stego key-based encryption); these are,\nsubsequently, hidden in the host image pixels. In addition, the input image is\ntransposed before data hiding. Image transposition, bits shuffling, bitxoring,\nstego key-based encryption, and gray-level modification introduce five\ndifferent security levels to the proposed scheme, making the data recovery\nextremely difficult for attackers. The proposed technique is evaluated by\nobjective analysis using various image quality assessment metrics, producing\npromising results in terms of imperceptibility and security. Moreover, the high\nquality stego images and its minimal histogram changeability, also validate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 06:27:29 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Muhammad", "Khan", ""], ["Ahmad", "Jamil", ""], ["Sajjad", "Muhammad", ""], ["Zubair", "Muhammad", ""]]}, {"id": "1510.04825", "submitter": "Mira Sarkis", "authors": "Mira Sarkis (LTCI), Cyril Concolato (LTCI), Jean-Claude Dufourd (LTCI)", "title": "MSoS: A Multi-Screen-Oriented Web Page Segmentation Approach", "comments": "DocEng'14: ACM Symposium on Document Engineering, ACM, 2015", "journal-ref": null, "doi": "10.1145/2682571.2797090", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a multiscreen-oriented approach for segmenting web\npages. The segmentation is an automatic and hybrid visual and structural\nmethod. It aims at creating coherent blocks which have different functions\ndetermined by the multiscreen environment. It is also characterized by a\ndynamic adaptation to the page content. Experiments are conducted on a set of\nexisting applications that contain multimedia elements, in particular YouTube\nand video player pages. Results are compared with one seg-mentation method from\nthe literature and with a ground truth manually created. With a 75% precision,\nthe MSoS is a promising method that is capable of producing good segmentation\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 09:43:12 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Sarkis", "Mira", "", "LTCI"], ["Concolato", "Cyril", "", "LTCI"], ["Dufourd", "Jean-Claude", "", "LTCI"]]}, {"id": "1510.04861", "submitter": "Tomislav Petkovi\\'c", "authors": "Martin Bla\\v{z}evi\\'c, Karla Brki\\'c, Tomislav Hrka\\'c", "title": "Towards Reversible De-Identification in Video Sequences Using 3D Avatars\n  and Steganography", "comments": "Part of the Proceedings of the Croatian Computer Vision Workshop,\n  CCVW 2015, Year 3", "journal-ref": null, "doi": null, "report-no": "UniZg-CRV-CCVW/2015/0011", "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a de-identification pipeline that protects the privacy of humans\nin video sequences by replacing them with rendered 3D human models, hence\nconcealing their identity while retaining the naturalness of the scene. The\noriginal images of humans are steganographically encoded in the carrier image,\ni.e. the image containing the original scene and the rendered 3D human models.\nWe qualitatively explore the feasibility of our approach, utilizing the Kinect\nsensor and its libraries to detect and localize human joints. A 3D avatar is\nrendered into the scene using the obtained joint positions, and the original\nhuman image is steganographically encoded in the new scene. Our qualitative\nevaluation shows reasonably good results that merit further exploration.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 12:31:29 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Bla\u017eevi\u0107", "Martin", ""], ["Brki\u0107", "Karla", ""], ["Hrka\u0107", "Tomislav", ""]]}, {"id": "1510.05405", "submitter": "Mira Sarkis", "authors": "Mira Sarkis (LTCI), Cyril Concolato (LTCI), Jean-Claude Dufourd (LTCI)", "title": "The Virtual Splitter: Refactoring Web Applications for the Multiscreen\n  Environment", "comments": null, "journal-ref": "DocEng'14: ACM Symposium on Document Engineering, ACM, 2014,\n  pp.Pages139-142 \\&lt;10.1145/2644866.2644893\\&gt;", "doi": "10.1145/2644866.2644893", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating web applications for the multiscreen environment is still a\nchallenge. One approach is to transform existing single-screen applications but\nthis has not been done yet automatically or generically. This paper proposes a\nrefactor-ing system. It consists of a generic and extensible mapping phase that\nautomatically analyzes the application content based on a semantic or a visual\ncriterion determined by the author or the user, and prepares it for the\nsplitting process. The system then splits the application and as a result\ndelivers two instrumented applications ready for distribution across devices.\nDuring runtime, the system uses a mirroring phase to maintain the functionality\nof the distributed application and to support a dynamic splitting process.\nDeveloped as a Chrome extension, our approach is validated on several web\napplications, including a YouTube page and a video application from Mozilla.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 09:40:17 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Sarkis", "Mira", "", "LTCI"], ["Concolato", "Cyril", "", "LTCI"], ["Dufourd", "Jean-Claude", "", "LTCI"]]}, {"id": "1510.06659", "submitter": "Eirina Bourtsoulatze", "authors": "Eirina Bourtsoulatze, Nikolaos Thomos, Jonnahtan Saltarin and Torsten\n  Braun", "title": "Content-Aware Delivery of Scalable Video in Network Coding Enabled Named\n  Data Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel network coding enabled NDN architecture for\nthe delivery of scalable video. Our scheme utilizes network coding in order to\naddress the problem that arises in the original NDN protocol, where optimal use\nof the bandwidth and caching resources necessitates the coordination of the\nforwarding decisions. To optimize the performance of the proposed network\ncoding based NDN protocol and render it appropriate for transmission of\nscalable video, we devise a novel rate allocation algorithm that decides on the\noptimal rates of Interest messages sent by clients and intermediate nodes. This\nalgorithm guarantees that the achieved flow of Data objects will maximize the\naverage quality of the video delivered to the client population. To support the\nhandling of Interest messages and Data objects when intermediate nodes perform\nnetwork coding, we modify the standard NDN protocol and introduce the use of\nBloom filters, which store efficiently additional information about the\nInterest messages and Data objects. The proposed architecture is evaluated for\ntransmission of scalable video over PlanetLab topologies. The evaluation shows\nthat the proposed scheme performs very close to the optimal performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 15:36:27 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Bourtsoulatze", "Eirina", ""], ["Thomos", "Nikolaos", ""], ["Saltarin", "Jonnahtan", ""], ["Braun", "Torsten", ""]]}, {"id": "1510.08893", "submitter": "Lorenzo Baraldi", "authors": "Lorenzo Baraldi, Costantino Grana and Rita Cucchiara", "title": "A Deep Siamese Network for Scene Detection in Broadcast Videos", "comments": "ACM Multimedia 2015", "journal-ref": null, "doi": "10.1145/2733373.2806316", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model that automatically divides broadcast videos into coherent\nscenes by learning a distance measure between shots. Experiments are performed\nto demonstrate the effectiveness of our approach by comparing our algorithm\nagainst recent proposals for automatic scene segmentation. We also propose an\nimproved performance measure that aims to reduce the gap between numerical\nevaluation and expected results, and propose and release a new benchmark\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:34:15 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Baraldi", "Lorenzo", ""], ["Grana", "Costantino", ""], ["Cucchiara", "Rita", ""]]}]