[{"id": "1812.00551", "submitter": "Junghyuk Lee", "authors": "Junghyuk Lee and Jong-Seok Lee", "title": "Music Popularity: Metrics, Characteristics, and Audio-Based Prediction", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, vol. 20, no. 11, pp. 3173-3182,\n  Nov. 2018", "doi": "10.1109/TMM.2018.2820903", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding music popularity is important not only for the artists who\ncreate and perform music but also for the music-related industry. It has not\nbeen studied well how music popularity can be defined, what its characteristics\nare, and whether it can be predicted, which are addressed in this paper. We\nfirst define eight popularity metrics to cover multiple aspects of popularity.\nThen, the analysis of each popularity metric is conducted with long-term\nreal-world chart data to deeply understand the characteristics of music\npopularity in the real world. We also build classification models for\npredicting popularity metrics using acoustic data. In particular, we focus on\nevaluating features describing music complexity together with other\nconventional acoustic features including MPEG-7 and Mel-frequency cepstral\ncoefficient (MFCC) features. The results show that, although room still exists\nfor improvement, it is feasible to predict the popularity metrics of a song\nsignificantly better than random chance based on its audio signal, particularly\nusing both the complexity and MFCC features.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 04:47:18 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lee", "Junghyuk", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1812.00816", "submitter": "Vaneet Aggarwal", "authors": "Arnob Ghosh and Vaneet Aggarwal and Feng Qian", "title": "A Robust Algorithm for Tile-based 360-degree Video Streaming with\n  Uncertain FoV Estimation", "comments": "arXiv admin note: text overlap with arXiv:1704.08215", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust scheme for streaming 360-degree immersive videos to\nmaximize the quality of experience (QoE). Our streaming approach introduces a\nholistic analytical framework built upon the formal method of stochastic\noptimization. We propose a robust algorithm which provides a streaming rate\nsuch that the video quality degrades below that rate with very low probability\neven in presence of uncertain head movement, and bandwidth. It assumes the\nknowledge of the viewing probability of different portions (tiles) of a\npanoramic scene. Such probabilities can be easily derived from crowdsourced\nmeasurements performed by 360 video content providers. We then propose\nefficient methods to solve the problem at runtime while achieving a bounded\noptimality gap (in terms of the QoE). We implemented our proposed approaches\nusing emulation. Using real users' head movement traces and real cellular\nbandwidth traces, we show that our algorithms significantly outperform the\nbaseline algorithms by at least in $30\\%$ in the QoE metric. Our algorithm\ngives a streaming rate which is $50\\%$ higher compared to the baseline\nalgorithms when the prediction error is high.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:04:56 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ghosh", "Arnob", ""], ["Aggarwal", "Vaneet", ""], ["Qian", "Feng", ""]]}, {"id": "1812.00828", "submitter": "Md Sahidullah", "authors": "Arnab Poddar, Md Sahidullah, Goutam Saha", "title": "Novel Quality Metric for Duration Variability Compensation in Speaker\n  Verification using i-Vectors", "comments": "Accepted and presented in ICAPR 2017, Bangalore, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speaker verification (ASV) is the process to recognize persons\nusing voice as biometric. The ASV systems show considerable recognition\nperformance with sufficient amount of speech from matched condition. One of the\ncrucial challenges of ASV technology is to improve recognition performance with\nspeech segments of short duration. In short duration condition, the model\nparameters are not properly estimated due to inadequate speech information, and\nthis results poor recognition accuracy even with the state-of-the-art i-vector\nbased ASV system. We hypothesize that considering the estimation quality during\nrecognition process would help to improve the ASV performance. This can be\nincorporated as a quality measure during fusion of ASV systems. This paper\ninvestigates a new quality measure for i-vector representation of speech\nutterances computed directly from Baum-Welch statistics. The proposed metric is\nsubsequently used as quality measure during fusion of ASV systems. In\nexperiments with the NIST SRE 2008 corpus, We have shown that inclusion of\nproposed quality metric exhibits considerable improvement in speaker\nverification performance. The results also indicate the potentiality of the\nproposed method in real-world scenario with short test utterances.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 15:20:59 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Poddar", "Arnab", ""], ["Sahidullah", "Md", ""], ["Saha", "Goutam", ""]]}, {"id": "1812.00874", "submitter": "Chiranjoy Chattopadhyay", "authors": "Shreya Goyal, Satya Bhavsar, Shreya Patel, Chiranjoy Chattopadhyay,\n  Gaurav Bhatnagar", "title": "SUGAMAN: Describing Floor Plans for Visually Impaired by Annotation\n  Learning and Proximity based Grammar", "comments": "19 pages, 20 figures, Under review in IET Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose SUGAMAN (Supervised and Unified framework using\nGrammar and Annotation Model for Access and Navigation). SUGAMAN is a Hindi\nword meaning \"easy passage from one place to another\". SUGAMAN synthesizes\ntextual description from a given floor plan image for the visually impaired. A\nvisually impaired person can navigate in an indoor environment using the\ntextual description generated by SUGAMAN. With the help of a text reader\nsoftware, the target user can understand the rooms within the building and\narrangement of furniture to navigate. SUGAMAN is the first framework for\ndescribing a floor plan and giving direction for obstacle-free movement within\na building. We learn $5$ classes of room categories from $1355$ room image\nsamples under a supervised learning paradigm. These learned annotations are fed\ninto a description synthesis framework to yield a holistic description of a\nfloor plan image. We demonstrate the performance of various supervised\nclassifiers on room learning. We also provide a comparative analysis of system\ngenerated and human written descriptions. SUGAMAN gives state of the art\nperformance on challenging, real-world floor plan images. This work can be\napplied to areas like understanding floor plans of historical monuments,\nstability analysis of buildings, and retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 05:38:40 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Goyal", "Shreya", ""], ["Bhavsar", "Satya", ""], ["Patel", "Shreya", ""], ["Chattopadhyay", "Chiranjoy", ""], ["Bhatnagar", "Gaurav", ""]]}, {"id": "1812.01516", "submitter": "Pawe{\\l} Korus", "authors": "Pawel Korus, Nasir Memon", "title": "Content Authentication for Neural Imaging Pipelines: End-to-end\n  Optimization of Photo Provenance in Complex Distribution Channels", "comments": "Camera ready + supplement, CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic analysis of digital photo provenance relies on intrinsic traces left\nin the photograph at the time of its acquisition. Such analysis becomes\nunreliable after heavy post-processing, such as down-sampling and\nre-compression applied upon distribution in the Web. This paper explores\nend-to-end optimization of the entire image acquisition and distribution\nworkflow to facilitate reliable forensic analysis at the end of the\ndistribution channel. We demonstrate that neural imaging pipelines can be\ntrained to replace the internals of digital cameras, and jointly optimized for\nhigh-fidelity photo development and reliable provenance analysis. In our\nexperiments, the proposed approach increased image manipulation detection\naccuracy from 45% to over 90%. The findings encourage further research towards\nbuilding more reliable imaging pipelines with explicit provenance-guaranteeing\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:38:47 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 16:46:19 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Korus", "Pawel", ""], ["Memon", "Nasir", ""]]}, {"id": "1812.01593", "submitter": "Yi Zhu", "authors": "Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam,\n  Andrew Tao and Bryan Catanzaro", "title": "Improving Semantic Segmentation via Video Propagation and Label\n  Relaxation", "comments": "CVPR 2019 Oral. Code link:\n  https://github.com/NVIDIA/semantic-segmentation. YouTube link:\n  https://www.youtube.com/watch?v=aEbXjGZDZSQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation requires large amounts of pixel-wise annotations to\nlearn accurate models. In this paper, we present a video prediction-based\nmethodology to scale up training sets by synthesizing new training samples in\norder to improve the accuracy of semantic segmentation networks. We exploit\nvideo prediction models' ability to predict future frames in order to also\npredict future labels. A joint propagation strategy is also proposed to\nalleviate mis-alignments in synthesized samples. We demonstrate that training\nsegmentation models on datasets augmented by the synthesized samples leads to\nsignificant improvements in accuracy. Furthermore, we introduce a novel\nboundary label relaxation technique that makes training robust to annotation\nnoise and propagation artifacts along object boundaries. Our proposed methods\nachieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our\nsingle model, without model ensembles, achieves 72.8% mIoU on the KITTI\nsemantic segmentation test set, which surpasses the winning entry of the ROB\nchallenge 2018. Our code and videos can be found at\nhttps://nv-adlr.github.io/publication/2018-Segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:49:54 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 18:56:34 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 03:16:39 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zhu", "Yi", ""], ["Sapra", "Karan", ""], ["Reda", "Fitsum A.", ""], ["Shih", "Kevin J.", ""], ["Newsam", "Shawn", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1812.01748", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, Julian\n  McAuley", "title": "Complete the Look: Scene-based Complementary Product Recommendation", "comments": "Accepted to CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling fashion compatibility is challenging due to its complexity and\nsubjectivity. Existing work focuses on predicting compatibility between product\nimages (e.g. an image containing a t-shirt and an image containing a pair of\njeans). However, these approaches ignore real-world 'scene' images (e.g.\nselfies); such images are hard to deal with due to their complexity, clutter,\nvariations in lighting and pose (etc.) but on the other hand could potentially\nprovide key context (e.g. the user's body type, or the season) for making more\naccurate recommendations. In this work, we propose a new task called 'Complete\nthe Look', which seeks to recommend visually compatible products based on scene\nimages. We design an approach to extract training data for this task, and\npropose a novel way to learn the scene-product compatibility from fashion or\ninterior design images. Our approach measures compatibility both globally and\nlocally via CNNs and attention mechanisms. Extensive experiments show that our\nmethod achieves significant performance gains over alternative systems. Human\nevaluation and qualitative analysis are also conducted to further understand\nmodel behavior. We hope this work could lead to useful applications which link\nlarge corpora of real-world scenes with shoppable products.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 23:30:22 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 21:32:32 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Kim", "Eric", ""], ["Leskovec", "Jure", ""], ["Rosenberg", "Charles", ""], ["McAuley", "Julian", ""]]}, {"id": "1812.01777", "submitter": "Huanle Zhang", "authors": "Huanle Zhang, Ahmed Elmokashfi, Zhicheng Yang, Prasant Mohapatra", "title": "Wireless Access to Ultimate Virtual Reality 360-Degree Video At Home", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality 360-degree videos will become the first prosperous online VR\napplication. VR 360 videos are data-hungry and latency-sensitive that pose\nunique challenges to the networking infrastructure. In this paper, we focus on\nthe ultimate VR 360 that satisfies human eye fidelity. The ultimate VR 360\nrequires downlink 1.5 Gbps for viewing and uplink 6.6 Gbps for live\nbroadcasting, with round-trip time of less than 8.3 ms. On the other hand,\nwireless access to VR 360 services is preferred over wire-line transmission\nbecause of the better user experience and the safety concern (e.g., tripping\nhazard). We explore in this paper whether the most advanced wireless\ntechnologies from both cellular communications and WiFi communications support\nthe ultimate VR 360. Specifically, we consider 5G in cellular communications,\nIEEE 802.11ac (operating in 5GHz) and IEEE 802.11ad (operating in 60GHz) in\nWiFi communications. According to their performance specified in their\nstandards and/or empirical measurements, we have the following findings: (1)\nOnly 5G has the potential to support both the the ultimate VR 360 viewing and\nlive broadcasting. However, it is difficult for 5G to support multiple users of\nthe ultimate VR live broadcasting at home; (2) IEEE 802.11ac supports the\nultimate VR 360 viewing but fails to support the ultimate VR 360 live\nbroadcasting because it does not meet the data rate requirement of the ultimate\nVR 360 live broadcasting; (3) IEEE 802.11ad fails to support the ultimate VR\n360, because its current implementation incurs very high latency. Our\npreliminary results indicate that more advanced wireless technologies are\nneeded to fully support multiple ultimate VR 360 users at home.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 01:54:53 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 21:44:59 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Zhang", "Huanle", ""], ["Elmokashfi", "Ahmed", ""], ["Yang", "Zhicheng", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1812.01973", "submitter": "Martin Engilberge", "authors": "Romain Cohendet, Claire-H\\'el\\`ene Demarty, Ngoc Q. K. Duong, Martin\n  Engilberge", "title": "VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term\n  Video Memorability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans share a strong tendency to memorize/forget some of the visual\ninformation they encounter. This paper focuses on providing computational\nmodels for the prediction of the intrinsic memorability of visual content. To\naddress this new challenge, we introduce a large scale dataset (VideoMem)\ncomposed of 10,000 videos annotated with memorability scores. In contrast to\nprevious work on image memorability -- where memorability was measured a few\nminutes after memorization -- memory performance is measured twice: a few\nminutes after memorization and again 24-72 hours later. Hence, the dataset\ncomes with short-term and long-term memorability annotations. After an in-depth\nanalysis of the dataset, we investigate several deep neural network based\nmodels for the prediction of video memorability. Our best model using a ranking\nloss achieves a Spearman's rank correlation of 0.494 for short-term\nmemorability prediction, while our proposed model with attention mechanism\nprovides insights of what makes a content memorable. The VideoMem dataset with\npre-extracted features is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 13:03:11 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Cohendet", "Romain", ""], ["Demarty", "Claire-H\u00e9l\u00e8ne", ""], ["Duong", "Ngoc Q. K.", ""], ["Engilberge", "Martin", ""]]}, {"id": "1812.02137", "submitter": "Thorsten Laude", "authors": "Felix Haub, Thorsten Laude, J\\\"orn Ostermann", "title": "HEVC Inter Coding Using Deep Recurrent Neural Networks and Artificial\n  Reference Pictures", "comments": "7 pages, 4 figures, under review for ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of motion compensated prediction in modern video codecs highly\ndepends on the available reference pictures. Occlusions and non-linear motion\npose challenges for the motion compensation and often result in high bit rates\nfor the prediction error. We propose the generation of artificial reference\npictures using deep recurrent neural networks. Conceptually, a reference\npicture at the time instance of the currently coded picture is generated from\npreviously reconstructed conventional reference pictures. Based on these\nartificial reference pictures, we propose a complete coding pipeline based on\nHEVC. By using the artificial reference pictures for motion compensated\nprediction, average BD-rate gains of 1.5% over HEVC are achieved.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:52:16 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Haub", "Felix", ""], ["Laude", "Thorsten", ""], ["Ostermann", "J\u00f6rn", ""]]}, {"id": "1812.02982", "submitter": "Radu Arsinte", "authors": "Radu Arsinte, Eugen Lupu", "title": "An Experimental Evaluation Of Analog TV Cable Services Distributed In\n  GPON Architecture", "comments": "4pages, 7 figures", "journal-ref": "Acta technica Napocensis, Electronics and telecommunications, ISSN\n  1221-6542, Volume 57, Number 4/2016, pp.11-14", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the benefits of GPON (Gigabit Passive Optical Networks)\nin analog TV services. Analog TV service is still present in the standard\ntriple play distribution architectures, as an effect of unique advantages:\nsimple distribution in an apartment via standard RF splitters, unlimited number\nof viewing sites, real time behavior by lack of encoding/decoding processes. Of\ncourse, the quality is still limited by analogic standards, but the\nprice/performance ratio is unbeatable. The most important parameters\ncharacterizing the analog TV performance are described, the network\narchitecture is emphasized. To have a reference, the test architecture is a\nsub-network of a commercial telecom company. In this case only the quality of\nTV reception is performed. Using open-source software, and the integrated TV\ntuner board makes possible to test the levels and S/N ratio for all the analog\nTV channels of the spectrum. The system opens the possibility to do the same\nfor digital channels in DVB-C standard, with minimal changes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 11:18:16 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Arsinte", "Radu", ""], ["Lupu", "Eugen", ""]]}, {"id": "1812.03322", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Jannick P. Rolland", "title": "Scene Synchronization for Real-Time Interaction in Distributed Mixed\n  Reality and Virtual Reality Environments", "comments": null, "journal-ref": "Special Issue: Collaborative Virtual Environments (2004),\n  PRESENCE, Vol. 13(3), pp. 315-327 (ISSN 1054-7460)", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in computer networks and rendering systems facilitate the creation\nof distributed collaborative environments in which the distribution of\ninformation at remote locations allows efficient communication. One of the\nchallenges in networked virtual environments is maintaining a consistent view\nof the shared state in the presence of inevitable network latency and jitter. A\nconsistent view in a shared scene may significantly increase the sense of\npresence among participants and facilitate their interactivity. The dynamic\nshared state is directly affected by the frequency of actions applied on the\nobjects in the scene. Mixed Reality (MR) and Virtual Reality (VR) environments\ncontain several types of action producers including human users, a wide range\nof electronic motion sensors, and haptic devices. In this paper, the authors\npropose a novel criterion for categorization of distributed MR/VR systems and\npresent an adaptive synchronization algorithm for distributed MR/VR\ncollaborative environments. In spite of significant network latency, results\nshow that for low levels of update frequencies the dynamic shared state can be\nmaintained consistent at multiple remotely located sites.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 14:01:49 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Rolland", "Jannick P.", ""]]}, {"id": "1812.06071", "submitter": "Naji Khosravan", "authors": "Naji Khosravan, Shervin Ardeshir, Rohit Puri", "title": "On Attention Modules for Audio-Visual Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of media and networking technologies, multimedia\napplications ranging from feature presentation in a cinema setting to video on\ndemand to interactive video conferencing are in great demand. Good\nsynchronization between audio and video modalities is a key factor towards\ndefining the quality of a multimedia presentation. The audio and visual signals\nof a multimedia presentation are commonly managed by independent workflows -\nthey are often separately authored, processed, stored and even delivered to the\nplayback system. This opens up the possibility of temporal misalignment between\nthe two modalities - such a tendency is often more pronounced in the case of\nproduced content (such as movies).\n  To judge whether audio and video signals of a multimedia presentation are\nsynchronized, we as humans often pay close attention to discriminative\nspatio-temporal blocks of the video (e.g. synchronizing the lip movement with\nthe utterance of words, or the sound of a bouncing ball at the moment it hits\nthe ground). At the same time, we ignore large portions of the video in which\nno discriminative sounds exist (e.g. background music playing in a movie).\nInspired by this observation, we study leveraging attention modules for\nautomatically detecting audio-visual synchronization. We propose neural network\nbased attention modules, capable of weighting different portions\n(spatio-temporal blocks) of the video based on their respective discriminative\npower. Our experiments indicate that incorporating attention modules yields\nstate-of-the-art results for the audio-visual synchronization classification\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:37:12 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Khosravan", "Naji", ""], ["Ardeshir", "Shervin", ""], ["Puri", "Rohit", ""]]}, {"id": "1812.06501", "submitter": "Emna Baccour", "authors": "Emna Baccour, Aiman Erbad, Amr Mohamed, Kashif Bilal, Mohsen Guizani", "title": "Proactive Video Chunks Caching and Processing for Latency and Cost\n  Minimization in Edge Networks", "comments": "Submitted to International Conference on Wireless Communications and\n  Networking (WCNC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the growing demand for rich multimedia content such as Video on\nDemand (VoD) has made the data transmission from content delivery networks\n(CDN) to end-users quite challenging. Edge networks have been proposed as an\nextension to CDN networks to alleviate this excessive data transfer through\ncaching and to delegate the computation tasks to edge servers. To maximize the\ncaching efficiency in the edge networks, different Mobile Edge Computing (MEC)\nservers assist each others to effectively select which content to store and the\nappropriate computation tasks to process. In this paper, we adopt a\ncollaborative caching and transcoding model for VoD in MEC networks. However,\nunlike other models in the literature, different chunks of the same video are\nnot fetched and cached in the same MEC server. Instead, neighboring servers\nwill collaborate to store and transcode different video chunks and consequently\noptimize the limited resources usage. Since we are dealing with chunks caching\nand processing, we propose to maximize the edge efficiency by studying the\nviewers watching pattern and designing a probabilistic model where chunks\npopularities are evaluated. Based on this model, popularity-aware policies,\nnamely Proactive caching policy (PcP) and Cache replacement Policy (CrP), are\nintroduced to cache only highest probably requested chunks. In addition to PcP\nand CrP, an online algorithm (PCCP) is proposed to schedule the collaborative\ncaching and processing. The evaluation results prove that our model and\npolicies give better performance than approaches using conventional replacement\npolicies. This improvement reaches up to 50% in some cases.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 17:03:08 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Bilal", "Kashif", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1812.06673", "submitter": "Zhao Kang", "authors": "Zhao Kang, Haiqi Pan, Steven C.H. Hoi, Zenglin Xu", "title": "Robust Graph Learning from Noisy Data", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2018.2887094", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning graphs from data automatically has shown encouraging performance on\nclustering and semisupervised learning tasks. However, real data are often\ncorrupted, which may cause the learned graph to be inexact or unreliable. In\nthis paper, we propose a novel robust graph learning scheme to learn reliable\ngraphs from real-world noisy data by adaptively removing noise and errors in\nthe raw data. We show that our proposed model can also be viewed as a robust\nversion of manifold regularized robust PCA, where the quality of the graph\nplays a critical role. The proposed model is able to boost the performance of\ndata clustering, semisupervised classification, and data recovery\nsignificantly, primarily due to two key factors: 1) enhanced low-rank recovery\nby exploiting the graph smoothness assumption, 2) improved graph construction\nby exploiting clean data recovered by robust PCA. Thus, it boosts the\nclustering, semi-supervised classification, and data recovery performance\noverall. Extensive experiments on image/document clustering, object\nrecognition, image shadow removal, and video background subtraction reveal that\nour model outperforms the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 10:01:59 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Kang", "Zhao", ""], ["Pan", "Haiqi", ""], ["Hoi", "Steven C. H.", ""], ["Xu", "Zenglin", ""]]}, {"id": "1812.06713", "submitter": "Xiaoda Jiang", "authors": "Xiaoda Jiang, Hancheng Lu, Chang Wen Chen, Feng Wu", "title": "Receiver-driven Video Multicast over NOMA Systems in Heterogeneous\n  Environments", "comments": "9 pages, 6 figures. This paper has already been accepted by IEEE\n  INFOCOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-orthogonal multiple access (NOMA) has shown potential for scalable\nmulticast of video data. However, one key drawback for NOMA-based video\nmulticast is the limited number of layers allowed by the embedded successive\ninterference cancellation algorithm, failing to meet satisfaction of\nheterogeneous receivers. We propose a novel receiver-driven superposed video\nmulticast (Supcast) scheme by integrating Softcast, an analog-like transmission\nscheme, into the NOMA-based system to achieve high bandwidth efficiency as well\nas gradual decoding quality proportional to channel conditions at receivers.\nAlthough Softcast allows gradual performance by directly transmitting\npower-scaled transformation coefficients of frames, it suffers performance\ndegradation due to discarding coefficients under insufficient bandwidth and its\npower allocation strategy cannot be directly applied in NOMA due to\ninterference. In Supcast, coefficients are grouped into chunks, which are basic\nunits for power allocation and superposition scheduling. By bisecting chunks\ninto base-layer chunks and enhanced-layer chunks, the joint power allocation\nand chunk scheduling is formulated as a distortion minimization problem. A\ntwo-stage power allocation strategy and a near-optimal low-complexity algorithm\nfor chunk scheduling based on the matching theory are proposed. Simulation\nresults have shown the advantage of Supcast against Softcast as well as the\nreference scheme in NOMA under various practical scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 11:36:52 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 06:13:39 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Jiang", "Xiaoda", ""], ["Lu", "Hancheng", ""], ["Chen", "Chang Wen", ""], ["Wu", "Feng", ""]]}, {"id": "1812.06799", "submitter": "Warit Sirichotedumrong", "authors": "Warit Sirichotedumrong, Tatsuya Chuman, Hitoshi Kiya", "title": "Grayscale-Based Image Encryption Considering Color Sub-sampling\n  Operation for Encryption-then-Compression Systems", "comments": "Accepted in 2018 IEEE 7th Global Conference on Consumer Electronics\n  (GCCE 2018). arXiv admin note: text overlap with arXiv:1810.13067", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new grayscale-based block scrambling image encryption scheme is presented\nto enhance the security of Encryption-then-Compression (EtC) systems, which are\nused to securely transmit images through an untrusted channel provider. The\nproposed scheme enables the use of a smaller block size and a larger number of\nblocks than the conventional scheme. Images encrypted using the proposed scheme\ninclude less color information due to the use of grayscale images even when the\noriginal image has three color channels. These features enhance security\nagainst various attacks, such as jigsaw puzzle solver and brute-force attacks.\nMoreover, it allows the use of color sub-sampling, which can improve the\ncompression performance, although the encrypted images have no color\ninformation. In an experiment, encrypted images were uploaded to and then\ndownloaded from Facebook and Twitter, and the results demonstrated that the\nproposed scheme is effective for EtC systems, while maintaining a high\ncompression performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 05:15:25 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sirichotedumrong", "Warit", ""], ["Chuman", "Tatsuya", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1812.07126", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Wei Chu, Sam Young, and Xin Chen", "title": "BandNet: A Neural Network-based, Multi-Instrument Beatles-Style MIDI\n  Music Composition Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a recurrent neural network (RNN)-based MIDI music\ncomposition machine that is able to learn musical knowledge from existing\nBeatles' songs and generate music in the style of the Beatles with little human\nintervention. In the learning stage, a sequence of stylistically uniform,\nmultiple-channel music samples was modeled by a RNN. In the composition stage,\na short clip of randomly-generated music was used as a seed for the RNN to\nstart music score prediction. To form structured music, segments of generated\nmusic from different seeds were concatenated together. To improve the quality\nand structure of the generated music, we integrated music theory knowledge into\nthe model, such as controlling the spacing of gaps in the vocal melody,\nnormalizing the timing of chord changes, and requiring notes to be related to\nthe song's key (C major, for example). This integration improved the quality of\nthe generated music as verified by a professional composer. We also conducted a\nsubjective listening test that showed our generated music was close to original\nmusic by the Beatles in terms of style similarity, professional quality, and\ninterestingness. Generated music samples are at https://goo.gl/uaLXoB.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 01:26:13 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zhou", "Yichao", ""], ["Chu", "Wei", ""], ["Young", "Sam", ""], ["Chen", "Xin", ""]]}, {"id": "1812.07200", "submitter": "Bibliotheque Universitaire Deposants Hal-Avignon", "authors": "Xavier Bost (LIA), Georges Linares (LIA)", "title": "D{\\'e}tection de locuteurs dans les s{\\'e}ries TV", "comments": "in French", "journal-ref": "Coria 2015, Mar 2015, Paris, France", "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization of audio streams turns out to be particularly challenging\nwhen applied to fictional films, where many characters talk in various acoustic\nconditions (background music, sound effects, variations in intonation...).\nDespite this acoustic variability, such movies exhibit specific visual\npatterns, particularly within dialogue scenes. In this paper, we introduce a\ntwo-step method to achieve speaker diarization in TV series: speaker\ndiarization is first performed locally within scenes visually identified as\ndialogues; then, the hypothesized local speakers are compared to each other\nduring a second clustering process in order to detect recurring speakers: this\nsecond stage of clustering is subject to the constraint that the different\nspeakers involved in the same dialogue have to be assigned to different\nclusters. The performances of our approach are compared to those obtained by\nstandard speaker diarization tools applied to the same data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:11:19 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "1812.07205", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Georges Linar\\`es (LIA), Serigne Gueye (LIA)", "title": "Audiovisual speaker diarization of TV series", "comments": null, "journal-ref": "2015 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Apr 2015, Brisbane, Australia. IEEE, pp.4799-4803, 2015", "doi": "10.1109/ICASSP.2015.7178882", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization may be difficult to achieve when applied to narrative\nfilms, where speakers usually talk in adverse acoustic conditions: background\nmusic, sound effects, wide variations in intonation may hide the inter-speaker\nvariability and make audio-based speaker diarization approaches error prone. On\nthe other hand, such fictional movies exhibit strong regularities at the image\nlevel, particularly within dialogue scenes. In this paper, we propose to\nperform speaker diarization within dialogue scenes of TV series by combining\nthe audio and video modalities: speaker diarization is first performed by using\neach modality, the two resulting partitions of the instance set are then\noptimally matched, before the remaining instances, corresponding to cases of\ndisagreement between both modalities, are finally processed. The results\nobtained by applying such a multi-modal approach to fictional films turn out to\noutperform those obtained by relying on a single modality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:21:36 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 14:59:28 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Linar\u00e8s", "Georges", "", "LIA"], ["Gueye", "Serigne", "", "LIA"]]}, {"id": "1812.07209", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Georges Linares (LIA)", "title": "Constrained speaker diarization of TV series based on visual patterns", "comments": null, "journal-ref": "2014 IEEE Spoken Language Technology Workshop (SLT), Dec 2014,\n  South Lake Tahoe, United States. IEEE, pp.390-395, 2014", "doi": "10.1109/SLT.2014.7078606", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization, usually denoted as the ''who spoke when'' task, turns\nout to be particularly challenging when applied to fictional films, where many\ncharacters talk in various acoustic conditions (background music, sound\neffects...). Despite this acoustic variability , such movies exhibit specific\nvisual patterns in the dialogue scenes. In this paper, we introduce a two-step\nmethod to achieve speaker diarization in TV series: a speaker diarization is\nfirst performed locally in the scenes detected as dialogues; then, the\nhypothesized local speakers are merged in a second agglomerative clustering\nprocess, with the constraint that speakers locally hypothesized to be distinct\nmust not be assigned to the same cluster. The performances of our approach are\ncompared to those obtained by standard speaker diarization tools applied to the\nsame data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:29:27 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 15:04:31 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "1812.07277", "submitter": "Niklas Carlsson", "authors": "Mathias Almquist, Viktor Almquist, Vengatanathan Krishnamoorthi,\n  Niklas Carlsson, and Derek Eager", "title": "The Prefetch Aggressiveness Tradeoff in 360$^{\\circ}$ Video Streaming", "comments": "This paper is an extended version of our original ACM MMSys 2018\n  paper. Please cite our original paper (with the same title) published in ACM\n  Multimedia Systems (MMSys), Amsterdam, Netherlands, June 2018, pp. 258-269", "journal-ref": null, "doi": "10.1145/3204949.3204970", "report-no": null, "categories": "cs.MM cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With 360$^{\\circ}$ video, only a limited fraction of the full view is\ndisplayed at each point in time. This has prompted the design of streaming\ndelivery techniques that allow alternative playback qualities to be delivered\nfor each candidate viewing direction. However, while prefetching based on the\nuser's expected viewing direction is best done close to playback deadlines,\nlarge buffers are needed to protect against shortfalls in future available\nbandwidth. This results in conflicting goals and an important prefetch\naggressiveness tradeoff problem regarding how far ahead in time from the\ncurrent playpoint prefetching should be done. This paper presents the first\ncharacterization of this tradeoff. The main contributions include an empirical\ncharacterization of head movement behavior based on data from viewing sessions\nof four different categories of 360$^{\\circ}$ video, an optimization-based\ncomparison of the prefetch aggressiveness tradeoffs seen for these video\ncategories, and a data-driven discussion of further optimizations, which\ninclude a novel system design that allows both tradeoff objectives to be\ntargeted simultaneously. By qualitatively and quantitatively analyzing the\nabove tradeoffs, we provide insights into how to best design tomorrow's\ndelivery systems for 360$^{\\circ}$ videos, allowing content providers to reduce\nbandwidth costs and improve users' playback experiences.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 10:28:35 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Almquist", "Mathias", ""], ["Almquist", "Viktor", ""], ["Krishnamoorthi", "Vengatanathan", ""], ["Carlsson", "Niklas", ""], ["Eager", "Derek", ""]]}, {"id": "1812.07660", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu, Josef Kittler", "title": "Discriminative Supervised Hashing for Cross-Modal similarity Search", "comments": "7 pages,3 figures,4 tables;The paper is under consideration at Image\n  and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advantage of low storage cost and high retrieval efficiency, hashing\ntechniques have recently been an emerging topic in cross-modal similarity\nsearch. As multiple modal data reflect similar semantic content, many\nresearches aim at learning unified binary codes. However, discriminative\nhashing features learned by these methods are not adequate. This results in\nlower accuracy and robustness. We propose a novel hashing learning framework\nwhich jointly performs classifier learning, subspace learning and matrix\nfactorization to preserve class-specific semantic content, termed\nDiscriminative Supervised Hashing (DSH), to learn the discrimative unified\nbinary codes for multi-modal data. Besides, reducing the loss of information\nand preserving the non-linear structure of data, DSH non-linearly projects\ndifferent modalities into the common space in which the similarity among\nheterogeneous data points can be measured. Extensive experiments conducted on\nthe three publicly available datasets demonstrate that the framework proposed\nin this paper outperforms several state-of -the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 04:28:31 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 08:53:49 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 02:54:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1812.09127", "submitter": "Trung Nguyen", "authors": "Trung Nguyen, Barth Lakshmanan, Weihua Sheng", "title": "A Smart Security System with Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based technology has improved drastically in the past decade. As a\nresult, security technology has become a major help to protect our daily life.\nIn this paper, we propose a robust security based on face recognition system\n(SoF). In particular, we develop this system to giving access into a home for\nauthenticated users. The classifier is trained by using a new adaptive learning\nmethod. The training data are initially collected from social networks. The\naccuracy of the classifier is incrementally improved as the user starts using\nthe system. A novel method has been introduced to improve the classifier model\nby human interaction and social media. By using a deep learning framework -\nTensorFlow, it will be easy to reuse the framework to adopt with many devices\nand applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 07:57:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Nguyen", "Trung", ""], ["Lakshmanan", "Barth", ""], ["Sheng", "Weihua", ""]]}, {"id": "1812.09443", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang, Zhibo Chen, Jianxin Lin, Weiping Li", "title": "Learned Scalable Image Compression with Bidirectional Context\n  Disentanglement Network", "comments": "IEEE International Conference on Multimedia and Expo (ICME2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learned scalable/progressive image compression\nscheme based on deep neural networks (DNN), named Bidirectional Context\nDisentanglement Network (BCD-Net). For learning hierarchical representations,\nwe first adopt bit-plane decomposition to decompose the information coarsely\nbefore the deep-learning-based transformation. However, the information carried\nby different bit-planes is not only unequal in entropy but also of different\nimportance for reconstruction. We thus take the hidden features corresponding\nto different bit-planes as the context and design a network topology with\nbidirectional flows to disentangle the contextual information for more\neffective compressed representations. Our proposed scheme enables us to obtain\nthe compressed codes with scalable rates via a one-pass encoding-decoding.\nExperiment results demonstrate that our proposed model outperforms the\nstate-of-the-art DNN-based scalable image compression methods in both PSNR and\nMS-SSIM metrics. In addition, our proposed model achieves higher performance in\nMS-SSIM metric than conventional scalable image codecs. Effectiveness of our\ntechnical components is also verified through sufficient ablation experiments.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 03:25:27 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 14:54:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Chen", "Zhibo", ""], ["Lin", "Jianxin", ""], ["Li", "Weiping", ""]]}, {"id": "1812.09499", "submitter": "Zhaoxia Yin", "authors": "Youzhi Xiang, Zhaoxia Yin, Xinpeng Zhang", "title": "Reversible Data Hiding in Encrypted Images based on MSB Prediction and\n  Huffman Coding", "comments": "20 pages, 8 figures, to submit to IEEE Trans. Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of cloud storage and privacy protection, reversible data\nhiding in encrypted images (RDHEI) has attracted increasing attention as a\ntechnology that can embed additional data in the encryption domain. In general,\nan RDHEI method embeds secret data in an encrypted image while ensuring that\nthe embedded data can be extracted error-free and the original image can be\nrestored lossless. In this paper, A high-capacity RDHEI algorithm is proposed.\nAt first, the Most Significant Bits (MSB) of each pixel was predicted\nadaptively and marked by Huffman coding in the original image. Then, the image\nwas encrypted by a stream cipher method. At last, the vacated space can be used\nto embed additional data. Experimental results show that our method achieved\nhigher embedding capacity while comparing with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 10:57:48 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Xiang", "Youzhi", ""], ["Yin", "Zhaoxia", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "1812.09681", "submitter": "Zhuoqian Yang", "authors": "Zhuoqian Yang, Zengchang Qin, Jing Yu, Yue Hu", "title": "Scene Graph Reasoning with Prior Visual Relationship for Visual Question\n  Answering", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key issues of Visual Question Answering (VQA) is to reason with\nsemantic clues in the visual content under the guidance of the question, how to\nmodel relational semantics still remains as a great challenge. To fully capture\nvisual semantics, we propose to reason over a structured visual representation\n- scene graph, with embedded objects and inter-object relationships. This shows\ngreat benefit over vanilla vector representations and implicit visual\nrelationship learning. Based on existing visual relationship models, we propose\na visual relationship encoder that projects visual relationships into a learned\ndeep semantic space constrained by visual context and language priors. Upon the\nconstructed graph, we propose a Scene Graph Convolutional Network (SceneGCN) to\njointly reason the object properties and relational semantics for the correct\nanswer. We demonstrate the model's effectiveness and interpretability on the\nchallenging GQA dataset and the classical VQA 2.0 dataset, remarkably achieving\nstate-of-the-art 54.56% accuracy on GQA compared to the existing best model.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 09:59:49 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 16:42:04 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Yang", "Zhuoqian", ""], ["Qin", "Zengchang", ""], ["Yu", "Jing", ""], ["Hu", "Yue", ""]]}, {"id": "1812.09900", "submitter": "Yipeng Sun", "authors": "Yipeng Sun, Chengquan Zhang, Zuming Huang, Jiaming Liu, Junyu Han, and\n  Errui Ding", "title": "TextNet: Irregular Text Reading from Images with an End-to-End Trainable\n  Network", "comments": "Asian conference on computer vision, 2018, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading text from images remains challenging due to multi-orientation,\nperspective distortion and especially the curved nature of irregular text. Most\nof existing approaches attempt to solve the problem in two or multiple stages,\nwhich is considered to be the bottleneck to optimize the overall performance.\nTo address this issue, we propose an end-to-end trainable network architecture,\nnamed TextNet, which is able to simultaneously localize and recognize irregular\ntext from images. Specifically, we develop a scale-aware attention mechanism to\nlearn multi-scale image features as a backbone network, sharing fully\nconvolutional features and computation for localization and recognition. In\ntext detection branch, we directly generate text proposals in quadrangles,\ncovering oriented, perspective and curved text regions. To preserve text\nfeatures for recognition, we introduce a perspective RoI transform layer, which\ncan align quadrangle proposals into small feature maps. Furthermore, in order\nto extract effective features for recognition, we propose to encode the aligned\nRoI features by RNN into context information, combining spatial attention\nmechanism to generate text sequences. This overall pipeline is capable of\nhandling both regular and irregular cases. Finally, text localization and\nrecognition tasks can be jointly trained in an end-to-end fashion with designed\nmulti-task loss. Experiments on standard benchmarks show that the proposed\nTextNet can achieve state-of-the-art performance, and outperform existing\napproaches on irregular datasets by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 11:44:55 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sun", "Yipeng", ""], ["Zhang", "Chengquan", ""], ["Huang", "Zuming", ""], ["Liu", "Jiaming", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""]]}, {"id": "1812.10067", "submitter": "Tianyu He", "authors": "Zhibo Chen, Tianyu He", "title": "Learning based Facial Image Compression with Semantic Fidelity Metric", "comments": "Accepted by Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2019.01.086", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance and security scenarios usually require high efficient facial\nimage compression scheme for face recognition and identification. While either\ntraditional general image codecs or special facial image compression schemes\nonly heuristically refine codec separately according to face verification\naccuracy metric. We propose a Learning based Facial Image Compression (LFIC)\nframework with a novel Regionally Adaptive Pooling (RAP) module whose\nparameters can be automatically optimized according to gradient feedback from\nan integrated hybrid semantic fidelity metric, including a successfully\nexploration to apply Generative Adversarial Network (GAN) as metric directly in\nimage compression scheme. The experimental results verify the framework's\nefficiency by demonstrating performance improvement of 71.41%, 48.28% and\n52.67% bitrate saving separately over JPEG2000, WebP and neural network-based\ncodecs under the same face verification accuracy distortion metric. We also\nevaluate LFIC's superior performance gain compared with latest specific facial\nimage codecs. Visual experiments also show some interesting insight on how LFIC\ncan automatically capture the information in critical areas based on semantic\ndistortion metrics for optimized compression, which is quite different from the\nheuristic way of optimization in traditional image compression algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 08:42:55 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 11:54:39 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Chen", "Zhibo", ""], ["He", "Tianyu", ""]]}, {"id": "1812.11735", "submitter": "Feng Yu", "authors": "Xinhui Gong, Feng Yu, Xiaohong Zhao, Shihong Wang", "title": "Security analysis of a self-embedding fragile image watermark scheme", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a self-embedding fragile watermark scheme based on reference-bits\ninterleaving and adaptive selection of embedding mode was proposed. Reference\nbits are derived from the scrambled MSB bits of a cover image, and then are\ncombined with authentication bits to form the watermark bits for LSB embedding.\nWe find this algorithm has a feature of block independence of embedding\nwatermark such that it is vulnerable to a collage attack. In addition, because\nthe generation of authentication bits via hash function operations is not\nrelated to secret keys, we analyze this algorithm by a multiple stego-image\nattack. We find that the cost of obtaining all the permutation relations of\n$l\\cdot b^2$ watermark bits of each block (i.e., equivalent permutation keys)\nis about $(l\\cdot b^2)!$ for the embedding mode $(m, l)$, where $m$ MSB layers\nof a cover image are used for generating reference bits and $l$ LSB layers for\nembedding watermark, and $b\\times b$ is the size of image block. The simulation\nresults and the statistical results demonstrate our analysis is effective.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:37:36 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 09:26:00 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Gong", "Xinhui", ""], ["Yu", "Feng", ""], ["Zhao", "Xiaohong", ""], ["Wang", "Shihong", ""]]}]