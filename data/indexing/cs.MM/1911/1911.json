[{"id": "1911.00227", "submitter": "Yuma Kinoshita", "authors": "Ayana Kawamura, Yuma Kinoshita, and Hitoshi Kiya", "title": "Privacy-Preserving Machine Learning Using EtC Images", "comments": "to be presented at IWAIT2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel privacy-preserving machine learning scheme\nwith encrypted images, called EtC (Encryption-then-Compression) images. Using\nmachine learning algorithms in cloud environments has been spreading in many\nfields. However, there are serious issues with it for end users, due to\nsemi-trusted cloud providers. Accordingly, we propose using EtC images, which\nhave been proposed for EtC systems with JPEG compression. In this paper, a\nnovel property of EtC images is considered under the use of z-score\nnormalization. It is demonstrated that the use of EtC images allows us not only\nto protect visual information of images, but also to preserve both the\nEuclidean distance and the inner product between vectors. In addition,\ndimensionality reduction is shown to can be applied to EtC images for fast and\naccurate matching. In an experiment, the proposed scheme is applied to a facial\nrecognition algorithm with classifiers for confirming the effectiveness of the\nscheme under the use of support vector machine (SVM) with the kernel trick.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 06:54:27 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Kawamura", "Ayana", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1911.00334", "submitter": "Zhesong Yu", "authors": "Zhesong Yu, Xiaoshuo Xu, Xiaoou Chen, Deshun Yang", "title": "Learning a Representation for Cover Song Identification Using\n  Convolutional Neural Network", "comments": "MIREX2020-Cover Song Identification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cover song identification represents a challenging task in the field of Music\nInformation Retrieval (MIR) due to complex musical variations between query\ntracks and cover versions. Previous works typically utilize hand-crafted\nfeatures and alignment algorithms for the task. More recently, further\nbreakthroughs are achieved employing neural network approaches. In this paper,\nwe propose a novel Convolutional Neural Network (CNN) architecture based on the\ncharacteristics of the cover song task. We first train the network through\nclassification strategies; the network is then used to extract music\nrepresentation for cover song identification. A scheme is designed to train\nrobust models against tempo changes. Experimental results show that our\napproach outperforms state-of-the-art methods on all public datasets, improving\nthe performance especially on the large dataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 12:32:40 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Yu", "Zhesong", ""], ["Xu", "Xiaoshuo", ""], ["Chen", "Xiaoou", ""], ["Yang", "Deshun", ""]]}, {"id": "1911.00382", "submitter": "Shadrokh Samavi", "authors": "Hamidreza Zarrabi, Ali Emami, Pejman Khadivi, Nader Karimi, Shadrokh\n  Samavi", "title": "BlessMark: A Blind Diagnostically-Lossless Watermarking Framework for\n  Medical Applications Based on Deep Neural Networks", "comments": "Drs. Soroushmehr and Najarian declared that they had not\n  contributions to the paper. I removed their names", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the development of public network usage, medical information\nis transmitted throughout the hospitals. The watermarking system can help for\nthe confidentiality of medical information distributed over the internet. In\nmedical images, regions-of-interest (ROI) contain diagnostic information. The\nwatermark should be embedded only into non-regions-of-interest (NROI) to keep\ndiagnostic information without distortion. Recently, ROI based watermarking has\nattracted the attention of the medical research community. The ROI map can be\nused as an embedding key for improving confidentiality protection purposes.\nHowever, in most existing works, the ROI map that is used for the embedding\nprocess must be sent as side-information along with the watermarked image. This\nside information is a disadvantage and makes the extraction process non-blind.\nAlso, most existing algorithms do not recover NROI of the original cover image\nafter the extraction of the watermark. In this paper, we propose a framework\nfor blind diagnostically-lossless watermarking, which iteratively embeds only\ninto NROI. The significance of the proposed framework is in satisfying the\nconfidentiality of the patient information through a blind watermarking system,\nwhile it preserves diagnostic/medical information of the image throughout the\nwatermarking process. A deep neural network is used to recognize the ROI map in\nthe embedding, extraction, and recovery processes. In the extraction process,\nthe same ROI map of the embedding process is recognized without requiring any\nadditional information. Hence, the watermark is blindly extracted from the\nNROI.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:56:52 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 21:17:09 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zarrabi", "Hamidreza", ""], ["Emami", "Ali", ""], ["Khadivi", "Pejman", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1911.00639", "submitter": "Minhao Tang", "authors": "Minhao Tang, Jiangtao Wen, Yuxing Han", "title": "A Generalized Rate-Distortion-${\\lambda}$ Model Based HEVC Rate Control\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The High Efficiency Video Coding (HEVC/H.265) standard doubles the\ncompression efficiency of the widely used H.264/AVC standard. For practical\napplications, rate control (RC) algorithms for HEVC need to be developed. Based\non the R-Q, R-${\\rho}$ or R-${\\lambda}$ models, rate control algorithms aim at\nencoding a video clip/segment to a target bit rate accurately with high video\nquality after compression. Among the various models used by HEVC rate control\nalgorithms, the R-${\\lambda}$ model performs the best in both coding efficiency\nand rate control accuracy. However, compared with encoding with a fixed\nquantization parameter (QP), even the best rate control algorithm [1] still\nunder-performs when comparing the video quality achieved at identical average\nbit rates. In this paper, we propose a novel generalized\nrate-distortion-${\\lambda}$ (R-D-${\\lambda}$) model for the relationship\nbetween rate (R), distortion (D) and the Lagrangian multiplier (${\\lambda}$) in\nrate-distortion (RD) optimized encoding. In addition to the well designed\nhierarchical initialization and coefficient update scheme, a new model based\nrate allocation scheme composed of amortization, smooth window and consistency\ncontrol is proposed for a better rate allocation. Experimental results\nimplementing the proposed algorithm in the HEVC reference software HM-16.9 show\nthat the proposed rate control algorithm is able to achieve an average of BDBR\nsaving of 6.09%, 3.15% and 4.03% for random access (RA), low delay P (LDP) and\nlow delay B (LDB) configurations respectively as compared with the\nR-${\\lambda}$ model based RC algorithm [1] implemented in HM. The proposed\nalgorithm also outperforms the state-of-the-art algorithms, while rate control\naccuracy and encoding speed are hardly impacted.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 03:44:10 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Tang", "Minhao", ""], ["Wen", "Jiangtao", ""], ["Han", "Yuxing", ""]]}, {"id": "1911.00682", "submitter": "Yang Hao", "authors": "Hao Yang, ZhongLiang Yang, YongJian Bao, Sheng Liu, YongFeng Huang", "title": "FCEM: A Novel Fast Correlation Extract Model For Real Time Steganalysis\n  of VoIP Stream via Multi-head Attention", "comments": "5 pages, 2 figures. accepted by ICASSP'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting correlation features between codes-words with high computational\nefficiency is crucial to steganalysis of Voice over IP (VoIP) streams. In this\npaper, we utilized attention mechanisms, which have recently attracted enormous\ninterests due to their highly parallelizable computation and flexibility in\nmodeling correlation in sequence, to tackle steganalysis problem of\nQuantization Index Modulation (QIM) based steganography in compressed VoIP\nstream. We design a light-weight neural network named Fast Correlation Extract\nModel (FCEM) only based on a variant of attention called multi-head attention\nto extract correlation features from VoIP frames. Despite its simple form, FCEM\noutperforms complicated Recurrent Neural Networks (RNNs) and Convolutional\nNeural Networks (CNNs) models on both prediction accuracy and time efficiency.\nIt significantly improves the best result in detecting both low embedded rates\nand short samples recently. Besides, the proposed model accelerates the\ndetection speed as twice as before when the sample length is as short as 0.1s,\nmaking it a excellent method for online services.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 08:51:46 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 05:20:55 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Yang", "Hao", ""], ["Yang", "ZhongLiang", ""], ["Bao", "YongJian", ""], ["Liu", "Sheng", ""], ["Huang", "YongFeng", ""]]}, {"id": "1911.00713", "submitter": "Hao Zhou", "authors": "Hao Zhou, Chongyang Zhang, Chuanping Hu", "title": "Visual Relationship Detection with Relative Location Mining", "comments": "Accepted to ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3351024", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection, as a challenging task used to find and\ndistinguish the interactions between object pairs in one image, has received\nmuch attention recently. In this work, we propose a novel visual relationship\ndetection framework by deeply mining and utilizing relative location of\nobject-pair in every stage of the procedure. In both the stages, relative\nlocation information of each object-pair is abstracted and encoded as auxiliary\nfeature to improve the distinguishing capability of object-pairs proposing and\npredicate recognition, respectively; Moreover, one Gated Graph Neural\nNetwork(GGNN) is introduced to mine and measure the relevance of predicates\nusing relative location. With the location-based GGNN, those non-exclusive\npredicates with similar spatial position can be clustered firstly and then be\nsmoothed with close classification scores, thus the accuracy of top $n$ recall\ncan be increased further. Experiments on two widely used datasets VRD and VG\nshow that, with the deeply mining and exploiting of relative location\ninformation, our proposed model significantly outperforms the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 13:33:15 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhou", "Hao", ""], ["Zhang", "Chongyang", ""], ["Hu", "Chuanping", ""]]}, {"id": "1911.00753", "submitter": "Mohamed Hamidi", "authors": "Mohamed Hamidi and Mohamed El Haziti and Hocine Cherifi and Mohammed\n  El Hassouni", "title": "Hybrid blind robust image watermarking technique based on DFT-DCT and\n  Arnold transform", "comments": "34 page, 17 figures, published in Multimedia Tools and Applications\n  Springer, 2018", "journal-ref": "Multimedia Tools and Applications, 77(20), 27181-27214 (2018)", "doi": "10.1007/s11042-018-5913-9", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust blind image watermarking method is proposed for\ncopyright protection of digital images. This hybrid method relies on combining\ntwo well-known transforms that are the discrete Fourier transform (DFT) and the\ndiscrete cosine transform (DCT). The motivation behind this combination is to\nenhance the imperceptibility and the robustness. The imperceptibility\nrequirement is achieved by using magnitudes of DFT coefficients while the\nrobustness improvement is ensured by applying DCT to the DFT coefficients\nmagnitude. The watermark is embedded by modifying the coefficients of the\nmiddle band of the DCT using a secret key. The security of the proposed method\nis enhanced by applying Arnold transform (AT) to the watermark before\nembedding. Experiments were conducted on natural and textured images. Results\nshow that, compared with state-of-the-art methods, the proposed method is\nrobust to a wide range of attacks while preserving high imperceptibility.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 16:58:15 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Hamidi", "Mohamed", ""], ["Haziti", "Mohamed El", ""], ["Cherifi", "Hocine", ""], ["Hassouni", "Mohammed El", ""]]}, {"id": "1911.00772", "submitter": "Shadrokh Samavi", "authors": "Maedeh Jamali, Mahnoosh Bagheri, Nader Karimi, Shadrokh Samavi", "title": "Robustness and Imperceptibility Enhancement in Watermarked Images by\n  Color Transformation", "comments": "5 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the effective methods for the preservation of copyright ownership of\ndigital media is watermarking. Different watermarking techniques try to set a\ntradeoff between robustness and transparency of the process. In this research\nwork, we have used color space conversion and frequency transform to achieve\nhigh robustness and transparency. Due to the distribution of image information\nin the RGB domain, we use the YUV color space, which concentrates the visual\ninformation in the Y channel. Embedding of the watermark is performed in the\nDCT coefficients of the specific wavelet subbands. Experimental results show\nhigh transparency and robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 19:19:24 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Jamali", "Maedeh", ""], ["Bagheri", "Mahnoosh", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1911.00812", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini", "title": "Adaptive Rate Allocation for View-Aware Point-Cloud Streaming", "comments": "Technical Report, University of Illinois at Urbana-Champaign (UIUC),\n  September 2017, 5 pages", "journal-ref": null, "doi": "10.13140/RG.2.2.23436.26244", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of view-dependent point-cloud streaming in a scene, our rate\nallocation is \"adaptive\" in the sense that it priorities the point-cloud models\ndepending on the camera view and the visibility of the objects and their\ndistance as described. The algorithm delivers higher bitrate to the point-cloud\nmodels which are inside user's viewport, more likely for the user to look at,\nor are closer to the view camera or, while delivers lower quality level to the\npoint-cloud models outside of a user's immediate viewport or farther away from\nthe camera. For that purpose, we hereby explain the rate allocation problem\nwithin the context of multi-point-cloud streaming where multiple point-cloud\nmodels are aimed to be streamed to the target device, and propose a rate\nallocation heuristic algorithm to enable the adaptations within this context.\nTo the best of our knowledge, this is the first work to mathematically model,\nand propose a rate allocation heuristic algorithm within the context of\npoint-cloud streaming.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 02:56:24 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Hosseini", "Mohammad", ""]]}, {"id": "1911.01355", "submitter": "Li Li", "authors": "Li Li, Zhu Li, Shan Liu, Houqiang Li", "title": "Video-based compression for plenoptic point clouds", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plenoptic point cloud that has multiple colors from various directions,\nis a more complete representation than the general point cloud that usually has\nonly one color. It is more realistic but also brings a larger volume of data\nthat needs to be compressed efficiently. The state-of-the-art method to\ncompress the plenoptic point cloud is an extension of the region-based adaptive\nhierarchical transform (RAHT). As far as we can see, in addition to RAHT, the\nvideo-based point cloud compression (V-PCC) is also an efficient point cloud\ncompression method. However, to the best of our knowledge, no works have used a\nvideo-based solution to compress the plenoptic point cloud yet. In this paper,\nwe first extend the V-PCC to support the plenoptic point cloud compression by\ngenerating multiple attribute videos. Then based on the observation that these\nvideos from multiple views have very high correlations, we propose encoding\nthem using multiview high efficiency video coding. We further propose a\nblock-based padding method that unifies the unoccupied attribute pixels from\ndifferent views to reduce their bit cost. The proposed algorithms are\nimplemented in the V-PCC reference software. The experimental results show that\nthe proposed algorithms can bring significant bitrate savings compared with the\nstate-of-the-art method for plenoptic point cloud compression.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:36:30 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Li", "Li", ""], ["Li", "Zhu", ""], ["Liu", "Shan", ""], ["Li", "Houqiang", ""]]}, {"id": "1911.01699", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Yinyin Peng, and Youzhi Xiang", "title": "Reversible Data Hiding in Encrypted Images based on Pixel Prediction and\n  Bit-plane Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible data hiding in encrypted images (RDHEI) receives growing attention\nbecause it protects the content of the original image while the embedded data\ncan be accurately extracted and the original image can be reconstructed\nlossless. To make full use of the correlation of the adjacent pixels, this\npaper proposes an RDHEI scheme based on pixel prediction and bit-plane\ncompression. Firstly, to vacate room for data embedding, the prediction error\nof the original image is calculated and used for bit-plane rearrangement and\ncompression. Then, the image after vacating room is encrypted by a stream\ncipher. Finally, the additional data is embedded in the vacated room by\nmulti-LSB substitution. Experimental results show that the embedding capacity\nof the proposed method outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 10:19:07 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Peng", "Yinyin", ""], ["Xiang", "Youzhi", ""]]}, {"id": "1911.01806", "submitter": "Zhiyuan Peng", "authors": "Zhiyuan Peng, Siyuan Feng, Tan Lee", "title": "Mixture factorized auto-encoder for unsupervised hierarchical deep\n  factorization of speech signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech signal is constituted and contributed by various informative factors,\nsuch as linguistic content and speaker characteristic. There have been notable\nrecent studies attempting to factorize speech signal into these individual\nfactors without requiring any annotation. These studies typically assume\ncontinuous representation for linguistic content, which is not in accordance\nwith general linguistic knowledge and may make the extraction of speaker\ninformation less successful. This paper proposes the mixture factorized\nauto-encoder (mFAE) for unsupervised deep factorization. The encoder part of\nmFAE comprises a frame tokenizer and an utterance embedder. The frame tokenizer\nmodels linguistic content of input speech with a discrete categorical\ndistribution. It performs frame clustering by assigning each frame a soft\nmixture label. The utterance embedder generates an utterance-level vector\nrepresentation. A frame decoder serves to reconstruct speech features from the\nencoders'outputs. The mFAE is evaluated on speaker verification (SV) task and\nunsupervised subword modeling (USM) task. The SV experiments on VoxCeleb 1 show\nthat the utterance embedder is capable of extracting speaker-discriminative\nembeddings with performance comparable to a x-vector baseline. The USM\nexperiments on ZeroSpeech 2017 dataset verify that the frame tokenizer is able\nto capture linguistic content and the utterance embedder can acquire\nspeaker-related information.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 08:54:34 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Peng", "Zhiyuan", ""], ["Feng", "Siyuan", ""], ["Lee", "Tan", ""]]}, {"id": "1911.01840", "submitter": "Fu Song", "authors": "Guangke Chen, Sen Chen, Lingling Fan, Xiaoning Du, Zhe Zhao, Fu Song,\n  Yang Liu", "title": "Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems", "comments": "IEEE Symposium on Security and Privacy 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CR cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker recognition (SR) is widely used in our daily life as a biometric\nauthentication or identification mechanism. The popularity of SR brings in\nserious security concerns, as demonstrated by recent adversarial attacks.\nHowever, the impacts of such threats in the practical black-box setting are\nstill open, since current attacks consider the white-box setting only. In this\npaper, we conduct the first comprehensive and systematic study of the\nadversarial attacks on SR systems (SRSs) to understand their security weakness\nin the practical blackbox setting. For this purpose, we propose an adversarial\nattack, named FAKEBOB, to craft adversarial samples. Specifically, we formulate\nthe adversarial sample generation as an optimization problem, incorporated with\nthe confidence of adversarial samples and maximal distortion to balance between\nthe strength and imperceptibility of adversarial voices. One key contribution\nis to propose a novel algorithm to estimate the score threshold, a feature in\nSRSs, and use it in the optimization problem to solve the optimization problem.\nWe demonstrate that FAKEBOB achieves 99% targeted attack success rate on both\nopen-source and commercial systems. We further demonstrate that FAKEBOB is also\neffective on both open-source and commercial systems when playing over the air\nin the physical world. Moreover, we have conducted a human study which reveals\nthat it is hard for human to differentiate the speakers of the original and\nadversarial voices. Last but not least, we show that four promising defense\nmethods for adversarial attack from the speech recognition domain become\nineffective on SRSs against FAKEBOB, which calls for more effective defense\nmethods. We highlight that our study peeks into the security implications of\nadversarial attacks on SRSs, and realistically fosters to improve the security\nrobustness of SRSs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 16:50:13 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 02:10:01 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Chen", "Guangke", ""], ["Chen", "Sen", ""], ["Fan", "Lingling", ""], ["Du", "Xiaoning", ""], ["Zhao", "Zhe", ""], ["Song", "Fu", ""], ["Liu", "Yang", ""]]}, {"id": "1911.02103", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Alba Herrera-Palacio, Carles Ventura, Carina Silberer, Ionut-Teodor\n  Sorodoc, Gemma Boleda and Xavier Giro-i-Nieto", "title": "Recurrent Instance Segmentation using Sequences of Referring Expressions", "comments": "3rd NeurIPS Workshop on Visually Grounded Interaction and Language\n  (ViGIL, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to segment the objects in an image that are referred\nto by a sequence of linguistic descriptions (referring expressions). We propose\na deep neural network with recurrent layers that output a sequence of binary\nmasks, one for each referring expression provided by the user. The recurrent\nlayers in the architecture allow the model to condition each predicted mask on\nthe previous ones, from a spatial perspective within the same image. Our\nmultimodal approach uses off-the-shelf architectures to encode both the image\nand the referring expressions. The visual branch provides a tensor of pixel\nembeddings that are concatenated with the phrase embeddings produced by a\nlanguage encoder. Our experiments on the RefCOCO dataset for still images\nindicate how the proposed architecture successfully exploits the sequences of\nreferring expressions to solve a pixel-wise task of instance segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 21:49:55 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Herrera-Palacio", "Alba", ""], ["Ventura", "Carles", ""], ["Silberer", "Carina", ""], ["Sorodoc", "Ionut-Teodor", ""], ["Boleda", "Gemma", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1911.02360", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Hua Wang, Li Chen, Jie Wang and Weiming Zhang", "title": "Reversible Adversarial Attack based on Reversible Image Transformation", "comments": "2021 International Workshop on Safety & Security of Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to prevent illegal or unauthorized access of image data such as\nhuman faces and ensure legitimate users can use authorization-protected data,\nreversible adversarial attack technique is rise. Reversible adversarial\nexamples (RAE) get both attack capability and reversibility at the same time.\nHowever, the existing technique can not meet application requirements because\nof serious distortion and failure of image recovery when adversarial\nperturbations get strong. In this paper, we take advantage of Reversible Image\nTransformation technique to generate RAE and achieve reversible adversarial\nattack. Experimental results show that proposed RAE generation scheme can\nensure imperceptible image distortion and the original image can be\nreconstructed error-free. What's more, both the attack ability and the image\nquality are not limited by the perturbation amplitude.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:15:32 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 12:04:48 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 14:21:11 GMT"}, {"version": "v4", "created": "Sun, 10 May 2020 15:49:06 GMT"}, {"version": "v5", "created": "Tue, 9 Mar 2021 07:55:50 GMT"}, {"version": "v6", "created": "Wed, 12 May 2021 05:55:19 GMT"}, {"version": "v7", "created": "Tue, 25 May 2021 15:11:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Wang", "Hua", ""], ["Chen", "Li", ""], ["Wang", "Jie", ""], ["Zhang", "Weiming", ""]]}, {"id": "1911.03100", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, P{\\aa}l Halvorsen, Hugo Hammer, Michael Riegler,\n  Trine B. Haugen", "title": "Extracting temporal features into a spatial domain using autoencoders\n  for sperm video analysis", "comments": "3 pages, 1 figure, MediaEval 19, 27-29 October 2019, Sophia\n  Antipolis, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a two-step deep learning method that is used to\npredict sperm motility and morphology-based on video recordings of human\nspermatozoa. First, we use an autoencoder to extract temporal features from a\ngiven semen video and plot these into image-space, which we call\nfeature-images. Second, these feature-images are used to perform transfer\nlearning to predict the motility and morphology values of human sperm. The\npresented method shows it's capability to extract temporal information into\nspatial domain feature-images which can be used with traditional convolutional\nneural networks. Furthermore, the accuracy of the predicted motility of a given\nsemen sample shows that a deep learning-based model can capture the temporal\ninformation of microscopic recordings of human semen.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 07:29:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Thambawita", "Vajira", ""], ["Halvorsen", "P\u00e5l", ""], ["Hammer", "Hugo", ""], ["Riegler", "Michael", ""], ["Haugen", "Trine B.", ""]]}, {"id": "1911.03793", "submitter": "Mohamed Hamidi", "authors": "Mohamed Hamidi, Mohamed El Haziti, Hocine Cherifi, Driss Aboutajdine", "title": "A Robust Blind 3-D Mesh Watermarking based on Wavelet Transform for\n  Copyright Protection", "comments": "6 pages, 3 figures, International Conference on Advanced Technologies\n  for Signal and Image Processing (ATSIP'2017)", "journal-ref": null, "doi": "10.1109/ATSIP.2017.8075525", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, three-dimensional meshes have been extensively used in several\napplications such as, industrial, medical, computer-aided design (CAD) and\nentertainment due to the processing capability improvement of computers and the\ndevelopment of the network infrastructure. Unfortunately, like digital images\nand videos, 3-D meshes can be easily modified, duplicated and redistributed by\nunauthorized users. Digital watermarking came up while trying to solve this\nproblem. In this paper, we propose a blind robust watermarking scheme for\nthree-dimensional semiregular meshes for Copyright protection. The watermark is\nembedded by modifying the norm of the wavelet coefficient vectors associated\nwith the lowest resolution level using the edge normal norms as synchronizing\nprimitives. The experimental results show that in comparison with alternative\n3-D mesh watermarking approaches, the proposed method can resist to a wide\nrange of common attacks, such as similarity transformations including\ntranslation, rotation, uniform scaling and their combination, noise addition,\nLaplacian smoothing, quantization, while preserving high imperceptibility.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 22:48:35 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Hamidi", "Mohamed", ""], ["Haziti", "Mohamed El", ""], ["Cherifi", "Hocine", ""], ["Aboutajdine", "Driss", ""]]}, {"id": "1911.03974", "submitter": "Antonio Busson", "authors": "Pedro V. A. de Freitas, Paulo R. C. Mendes, Gabriel N. P. dos Santos,\n  Antonio Jos\\'e G. Busson, \\'Alan Livio Guedes, S\\'ergio Colcher, Ruy Luiz\n  Milidi\\'u", "title": "A Multimodal CNN-based Tool to Censure Inappropriate Video Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the extensive use of video-sharing platforms and services for their\nstorage, the amount of such media on the internet has become massive. This\nvolume of data makes it difficult to control the kind of content that may be\npresent in such video files. One of the main concerns regarding the video\ncontent is if it has an inappropriate subject matter, such as nudity, violence,\nor other potentially disturbing content. More than telling if a video is either\nappropriate or inappropriate, it is also important to identify which parts of\nit contain such content, for preserving parts that would be discarded in a\nsimple broad analysis. In this work, we present a multimodal~(using audio and\nimage features) architecture based on Convolutional Neural Networks (CNNs) for\ndetecting inappropriate scenes in video files. In the task of classifying video\nfiles, our model achieved 98.95\\% and 98.94\\% of F1-score for the appropriate\nand inappropriate classes, respectively. We also present a censoring tool that\nautomatically censors inappropriate segments of a video file.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 18:26:24 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["de Freitas", "Pedro V. A.", ""], ["Mendes", "Paulo R. C.", ""], ["Santos", "Gabriel N. P. dos", ""], ["Busson", "Antonio Jos\u00e9 G.", ""], ["Guedes", "\u00c1lan Livio", ""], ["Colcher", "S\u00e9rgio", ""], ["Milidi\u00fa", "Ruy Luiz", ""]]}, {"id": "1911.04139", "submitter": "Yu Guan", "authors": "Yu Guan, Chengyuan Zheng, Zongming Guo, Xinggong Zhang, Junchen Jiang", "title": "Pano: Optimizing 360{\\deg} Video Streaming with a Better Understanding\n  of Quality Perception", "comments": "16 pages, 18 figures, Sigcomm conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming 360{\\deg} videos requires more bandwidth than non-360{\\deg} videos.\nThis is because current solutions assume that users perceive the quality of\n360{\\deg} videos in the same way they perceive the quality of non-360{\\deg}\nvideos. This means the bandwidth demand must be proportional to the size of the\nuser's field of view. However, we found several qualitydetermining factors\nunique to 360{\\deg}videos, which can help reduce the bandwidth demand. They\ninclude the moving speed of a user's viewpoint (center of the user's field of\nview), the recent change of video luminance, and the difference in\ndepth-of-fields of visual objects around the viewpoint. This paper presents\nPano, a 360{\\deg} video streaming system that leverages the 360{\\deg}\nvideo-specific factors. We make three contributions. (1) We build a new quality\nmodel for 360{\\deg} videos that captures the impact of the 360{\\deg}\nvideo-specific factors. (2) Pano proposes a variable-sized tiling scheme in\norder to strike a balance between the perceived quality and video encoding\nefficiency. (3) Pano proposes a new qualityadaptation logic that maximizes\n360{\\deg} video user-perceived quality and is readily deployable. Our\nevaluation (based on user study and trace analysis) shows that compared with\nstate-of-the-art techniques, Pano can save 41-46% bandwidth without any drop in\nthe perceived quality, or it can raise the perceived quality (user rating) by\n25%-142% without using more bandwidth.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 08:50:41 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Guan", "Yu", ""], ["Zheng", "Chengyuan", ""], ["Guo", "Zongming", ""], ["Zhang", "Xinggong", ""], ["Jiang", "Junchen", ""]]}, {"id": "1911.04657", "submitter": "Shunquan Tan", "authors": "Shunquan Tan, Weilong Wu, Zilong Shao, Qiushi Li, Bin Li, Jiwu Huang", "title": "CALPA-NET: Channel-pruning-assisted Deep Residual Network for\n  Steganalysis of Digital Images", "comments": "Accepted by IEEE Transactions on Information Forensics & Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, detection performance improvements of deep-learning\nbased steganalyzers have been usually achieved through structure expansion.\nHowever, excessive expanded structure results in huge computational cost,\nstorage overheads, and consequently difficulty in training and deployment. In\nthis paper we propose CALPA-NET, a ChAnneL-Pruning-Assisted deep residual\nnetwork architecture search approach to shrink the network structure of\nexisting vast, over-parameterized deep-learning based steganalyzers. We observe\nthat the broad inverted-pyramid structure of existing deep-learning based\nsteganalyzers might contradict the well-established model diversity oriented\nphilosophy, and therefore is not suitable for steganalysis. Then a hybrid\ncriterion combined with two network pruning schemes is introduced to adaptively\nshrink every involved convolutional layer in a data-driven manner. The\nresulting network architecture presents a slender bottleneck-like structure. We\nhave conducted extensive experiments on BOSSBase+BOWS2 dataset, more diverse\nALASKA dataset and even a large-scale subset extracted from ImageNet CLS-LOC\ndataset. The experimental results show that the model structure generated by\nour proposed CALPA-NET can achieve comparative performance with less than two\npercent of parameters and about one third FLOPs compared to the original\nsteganalytic model. The new model possesses even better adaptivity,\ntransferability, and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:39:42 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 02:21:32 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Tan", "Shunquan", ""], ["Wu", "Weilong", ""], ["Shao", "Zilong", ""], ["Li", "Qiushi", ""], ["Li", "Bin", ""], ["Huang", "Jiwu", ""]]}, {"id": "1911.05033", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Jun Feng, Yang Gao, Ting Lei, Xiaocong Yuan", "title": "Visual cryptography in single-pixel imaging", "comments": null, "journal-ref": null, "doi": "10.1364/OE.383240", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two novel visual cryptography (VC) schemes are proposed by combining VC with\nsingle-pixel imaging (SPI) for the first time. It is pointed out that the\noverlapping of visual key images in VC is similar to the superposition of pixel\nintensities by a single-pixel detector in SPI. In the first scheme, QR-code VC\nis designed by using opaque sheets instead of transparent sheets. The secret\nimage can be recovered when identical illumination patterns are projected onto\nmultiple visual key images and a single detector is used to record the total\nlight intensities. In the second scheme, the secret image is shared by multiple\nillumination pattern sequences and it can be recovered when the visual key\npatterns are projected onto identical items. The application of VC can be\nextended to more diversified scenarios by our proposed schemes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 17:49:50 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Jiao", "Shuming", ""], ["Feng", "Jun", ""], ["Gao", "Yang", ""], ["Lei", "Ting", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1911.05609", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Shangfei Wang, Mohammad Soleymani, Dhiraj Joshi, Qiang\n  Ji", "title": "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A\n  Survey", "comments": "Accepted by ACM TOMM", "journal-ref": null, "doi": "10.1145/3363560", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide popularity of digital photography and social networks has generated\na rapidly growing volume of multimedia data (i.e., image, music, and video),\nresulting in a great demand for managing, retrieving, and understanding these\ndata. Affective computing (AC) of these data can help to understand human\nbehaviors and enable wide applications. In this article, we survey the\nstate-of-the-art AC technologies comprehensively for large-scale heterogeneous\nmultimedia data. We begin this survey by introducing the typical emotion\nrepresentation models from psychology that are widely employed in AC. We\nbriefly describe the available datasets for evaluating AC algorithms. We then\nsummarize and compare the representative methods on AC of different multimedia\ntypes, i.e., images, music, videos, and multimodal data, with the focus on both\nhandcrafted features-based methods and deep learning methods. Finally, we\ndiscuss some challenges and future directions for multimedia affective\ncomputing.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 21:22:47 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhao", "Sicheng", ""], ["Wang", "Shangfei", ""], ["Soleymani", "Mohammad", ""], ["Joshi", "Dhiraj", ""], ["Ji", "Qiang", ""]]}, {"id": "1911.05833", "submitter": "Khaled Koutini", "authors": "Khaled Koutini, Shreyan Chowdhury, Verena Haunschmid, Hamid\n  Eghbal-zadeh, Gerhard Widmer", "title": "Emotion and Theme Recognition in Music with Frequency-Aware\n  RF-Regularized CNNs", "comments": "MediaEval`19, 27-29 October 2019, Sophia Antipolis, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CP-JKU submission to MediaEval 2019; a Receptive\nField-(RF)-regularized and Frequency-Aware CNN approach for tagging music with\nemotion/mood labels. We perform an investigation regarding the impact of the RF\nof the CNNs on their performance on this dataset. We observe that ResNets with\nsmaller receptive fields -- originally adapted for acoustic scene\nclassification -- also perform well in the emotion tagging task. We improve the\nperformance of such architectures using techniques such as Frequency Awareness\nand Shake-Shake regularization, which were used in previous work on general\nacoustic recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 10:19:55 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Koutini", "Khaled", ""], ["Chowdhury", "Shreyan", ""], ["Haunschmid", "Verena", ""], ["Eghbal-zadeh", "Hamid", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1911.06245", "submitter": "Zhenyu Tang", "authors": "Zhenyu Tang, Nicholas J. Bryan, Dingzeyu Li, Timothy R. Langlois,\n  Dinesh Manocha", "title": "Scene-Aware Audio Rendering via Deep Acoustic Analysis", "comments": "Accepted to IEEE VR 2020 Journal Track (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to capture the acoustic characteristics of real-world\nrooms using commodity devices, and use the captured characteristics to generate\nsimilar sounding sources with virtual models. Given the captured audio and an\napproximate geometric model of a real-world room, we present a novel\nlearning-based method to estimate its acoustic material properties. Our\napproach is based on deep neural networks that estimate the reverberation time\nand equalization of the room from recorded audio. These estimates are used to\ncompute material properties related to room reverberation using a novel\nmaterial optimization objective. We use the estimated acoustic material\ncharacteristics for audio rendering using interactive geometric sound\npropagation and highlight the performance on many real-world scenarios. We also\nperform a user study to evaluate the perceptual similarity between the recorded\nsounds and our rendered audio.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 17:04:00 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 19:20:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Tang", "Zhenyu", ""], ["Bryan", "Nicholas J.", ""], ["Li", "Dingzeyu", ""], ["Langlois", "Timothy R.", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1911.06727", "submitter": "Katerine Romeo", "authors": "Katerine Romeo (LITIS), Edwige Pissaloux (LITIS), Fr\\'ed\\'eric Serin\n  (LITIS)", "title": "Accessibility to textual and visual information on websites for visually\n  impaired persons", "comments": "in French. Handicap 2018, Jun 2018, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to textual and visual information for visually impaired persons\nbecomes very difficult with screen readers which are not adapted to different\nwebsites.This paper analyses the use of different technologies for access\ndigital content and to establish some ameliorations to the existing\nrecommendations to accessible website conception for all. The preliminary\nevaluation results with visually impaired people of our website ACCESSPACE\nwhich is constructed with the existing recommendations, confirm the project's\nrelevance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:23:46 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Romeo", "Katerine", "", "LITIS"], ["Pissaloux", "Edwige", "", "LITIS"], ["Serin", "Fr\u00e9d\u00e9ric", "", "LITIS"]]}, {"id": "1911.06981", "submitter": "Hilmi Enes Egilmez", "authors": "Hilmi E. Egilmez, Oguzhan Teke, Amir Said, Vadim Seregin, Marta\n  Karczewicz", "title": "Parametric Graph-based Separable Transforms for Video Coding", "comments": "5 pages, submitted to IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many video coding systems, separable transforms (such as two-dimensional\nDCT-2) have been used to code block residual signals obtained after prediction.\nThis paper proposes a parametric approach to build graph-based separable\ntransforms (GBSTs) for video coding. Specifically, a GBST is derived from a\npair of line graphs, whose weights are determined based on two non-negative\nparameters. As certain choices of those parameters correspond to the discrete\nsine and cosine transform types used in recent video coding standards\n(including DCT-2, DST-7 and DCT-8), this paper further optimizes these graph\nparameters to better capture residual block statistics and improve video coding\nefficiency. The proposed GBSTs are tested on the Versatile Video Coding (VVC)\nreference software, and the experimental results show that about 0.4% average\ncoding gain is achieved over the existing set of separable transforms\nconstructed based on DCT-2, DST-7 and DCT-8 in VVC.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 07:24:20 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:13:52 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 12:42:11 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Egilmez", "Hilmi E.", ""], ["Teke", "Oguzhan", ""], ["Said", "Amir", ""], ["Seregin", "Vadim", ""], ["Karczewicz", "Marta", ""]]}, {"id": "1911.07036", "submitter": "Shishun Tian", "authors": "Shishun Tian, Lu Zhang, Wenbin Zou, Xia Li, Ting Su, Luce Morin, and\n  Olivier Deforges", "title": "Quality Assessment of DIBR-synthesized views: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Depth-Image-Based-Rendering (DIBR) is one of the main fundamental\ntechnique to generate new views in 3D video applications, such as Multi-View\nVideos (MVV), Free-Viewpoint Videos (FVV) and Virtual Reality (VR). However,\nthe quality assessment of DIBR-synthesized views is quite different from the\ntraditional 2D images/videos. In recent years, several efforts have been made\ntowards this topic, but there {is a lack of} detailed survey in {the}\nliterature. In this paper, we provide a comprehensive survey on various current\napproaches for DIBR-synthesized views. The current accessible datasets of\nDIBR-synthesized views are firstly reviewed{, followed} by a summary analysis\nof the representative state-of-the-art objective metrics. Then, the\nperformances of different objective metrics are evaluated and discussed on all\navailable datasets. Finally, we discuss the potential challenges and suggest\npossible directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 14:13:56 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 08:18:10 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Tian", "Shishun", ""], ["Zhang", "Lu", ""], ["Zou", "Wenbin", ""], ["Li", "Xia", ""], ["Su", "Ting", ""], ["Morin", "Luce", ""], ["Deforges", "Olivier", ""]]}, {"id": "1911.07253", "submitter": "Suping Zhou", "authors": "Suping Zhou, Jia Jia, Yufeng Yin, Xiang Li, Yang Yao, Ying Zhang,\n  Zeyang Ye, Kehua Lei, Yan Huang, Jialie Shen", "title": "Understanding the Teaching Styles by an Attention based Multi-task\n  Cross-media Dimensional modelling", "comments": "ACM Muitimedia 2019", "journal-ref": null, "doi": "10.1145/3343031.3351059", "report-no": null, "categories": "cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching style plays an influential role in helping students to achieve\nacademic success. In this paper, we explore a new problem of effectively\nunderstanding teachers' teaching styles. Specifically, we study 1) how to\nquantitatively characterize various teachers' teaching styles for various\nteachers and 2) how to model the subtle relationship between cross-media\nteaching related data (speech, facial expressions and body motions, content et\nal.) and teaching styles. Using the adjectives selected from more than 10,000\nfeedback questionnaires provided by an educational enterprise, a novel concept\ncalled Teaching Style Semantic Space (TSSS) is developed based on the\npleasure-arousal dimensional theory to describe teaching styles quantitatively\nand comprehensively. Then a multi-task deep learning based model,\nAttention-based Multi-path Multi-task Deep Neural Network (AMMDNN), is proposed\nto accurately and robustly capture the internal correlations between\ncross-media features and TSSS. Based on the benchmark dataset, we further\ndevelop a comprehensive data set including 4,541 full-annotated cross-modality\nteaching classes. Our experimental results demonstrate that the proposed AMMDNN\noutperforms (+0.0842 in terms of the concordance correlation coefficient (CCC)\non average) baseline methods. To further demonstrate the advantages of the\nproposed TSSS and our model, several interesting case studies are carried out,\nsuch as teaching styles comparison among different teachers and courses, and\nleveraging the proposed method for teaching quality analysis.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 15:05:39 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhou", "Suping", ""], ["Jia", "Jia", ""], ["Yin", "Yufeng", ""], ["Li", "Xiang", ""], ["Yao", "Yang", ""], ["Zhang", "Ying", ""], ["Ye", "Zeyang", ""], ["Lei", "Kehua", ""], ["Huang", "Yan", ""], ["Shen", "Jialie", ""]]}, {"id": "1911.07421", "submitter": "Xiaofeng Liu", "authors": "Tong Che, Xiaofeng Liu, Site Li, Yubin Ge, Ruixiang Zhang, Caiming\n  Xiong, Yoshua Bengio", "title": "Deep Verifier Networks: Verification of Deep Discriminative Models with\n  Deep Generative Models", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI Safety is a major concern in many deep learning applications such as\nautonomous driving. Given a trained deep learning model, an important natural\nproblem is how to reliably verify the model's prediction. In this paper, we\npropose a novel framework -- deep verifier networks (DVN) to verify the inputs\nand outputs of deep discriminative models with deep generative models. Our\nproposed model is based on conditional variational auto-encoders with\ndisentanglement constraints. We give both intuitive and theoretical\njustifications of the model. Our verifier network is trained independently with\nthe prediction model, which eliminates the need of retraining the verifier\nnetwork for a new model. We test the verifier network on out-of-distribution\ndetection and adversarial example detection problems, as well as anomaly\ndetection problems in structured prediction tasks such as image caption\ngeneration. We achieve state-of-the-art results in all of these problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:23:12 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 03:10:15 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 21:08:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Che", "Tong", ""], ["Liu", "Xiaofeng", ""], ["Li", "Site", ""], ["Ge", "Yubin", ""], ["Zhang", "Ruixiang", ""], ["Xiong", "Caiming", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1911.07844", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Clinton Fookes, Simon Denman, Sridha Sridharan", "title": "Exploiting Human Social Cognition for the Detection of Fake and\n  Fraudulent Faces via Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in computer vision have brought us to the point where we have the\nability to synthesise realistic fake content. Such approaches are seen as a\nsource of disinformation and mistrust, and pose serious concerns to governments\naround the world. Convolutional Neural Networks (CNNs) demonstrate encouraging\nresults when detecting fake images that arise from the specific type of\nmanipulation they are trained on. However, this success has not transitioned to\nunseen manipulation types, resulting in a significant gap in the\nline-of-defense. We propose a Hierarchical Memory Network (HMN) architecture,\nwhich is able to successfully detect faked faces by utilising knowledge stored\nin neural memories as well as visual cues to reason about the perceived face\nand anticipate its future semantic embeddings. This renders a generalisable\nface tampering detection framework. Experimental results demonstrate the\nproposed approach achieves superior performance for fake and fraudulent face\ndetection compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 23:20:23 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Fernando", "Tharindu", ""], ["Fookes", "Clinton", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1911.07848", "submitter": "Sijie Mai", "authors": "Sijie Mai and Haifeng Hu and Songlong Xing", "title": "Modality to Modality Translation: An Adversarial Representation Learning\n  and Graph Fusion Network for Multimodal Fusion", "comments": "Accepted by AAAI-2020; code is available at:\n  https://github.com/TmacMai/ARGF_multimodal_fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning joint embedding space for various modalities is of vital importance\nfor multimodal fusion. Mainstream modality fusion approaches fail to achieve\nthis goal, leaving a modality gap which heavily affects cross-modal fusion. In\nthis paper, we propose a novel adversarial encoder-decoder-classifier framework\nto learn a modality-invariant embedding space. Since the distributions of\nvarious modalities vary in nature, to reduce the modality gap, we translate the\ndistributions of source modalities into that of target modality via their\nrespective encoders using adversarial training. Furthermore, we exert\nadditional constraints on embedding space by introducing reconstruction loss\nand classification loss. Then we fuse the encoded representations using\nhierarchical graph neural network which explicitly explores unimodal, bimodal\nand trimodal interactions in multi-stage. Our method achieves state-of-the-art\nperformance on multiple datasets. Visualization of the learned embeddings\nsuggests that the joint embedding space learned by our method is\ndiscriminative. code is available at:\n\\url{https://github.com/TmacMai/ARGF_multimodal_fusion}\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 08:29:20 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 08:34:28 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 15:43:06 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 01:52:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mai", "Sijie", ""], ["Hu", "Haifeng", ""], ["Xing", "Songlong", ""]]}, {"id": "1911.07923", "submitter": "Lu Wang", "authors": "Lu Wang, Jie Yang", "title": "Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search", "comments": "13 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale cross-modal hashing similarity retrieval has attracted more and\nmore attention in modern search applications such as search engines and\nautopilot, showing great superiority in computation and storage. However,\ncurrent unsupervised cross-modal hashing methods still have some limitations:\n(1)many methods relax the discrete constraints to solve the optimization\nobjective which may significantly degrade the retrieval performance;(2)most\nexisting hashing model project heterogenous data into a common latent space,\nwhich may always lose sight of diversity in heterogenous data;(3)transforming\nreal-valued data point to binary codes always results in abundant loss of\ninformation, producing the suboptimal continuous latent space. To overcome\nabove problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)\nmethod is proposed. Specifically, CUH jointly performs the multi-view\nclustering that projects the original data points from different modalities\ninto its own low-dimensional latent semantic space and finds the cluster\ncentroid points and the common clustering indicators in its own low-dimensional\nspace, and learns the compact hash codes and the corresponding linear hash\nfunctions. An discrete optimization framework is developed to learn the unified\nbinary codes across modalities under the guidance cluster-wise code-prototypes.\nThe reasonableness and effectiveness of CUH is well demonstrated by\ncomprehensive experiments on diverse benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 05:50:01 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 11:51:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wang", "Lu", ""], ["Yang", "Jie", ""]]}, {"id": "1911.07944", "submitter": "Zhengfang Duanmu", "authors": "Zhengfang Duanmu, Wentao Liu, Diqi Chen, Zhuoran Li, Zhou Wang, Yizhou\n  Wang, Wen Gao", "title": "A Knowledge-Driven Quality-of-Experience Model for Adaptive Streaming\n  Videos", "comments": "12 pages, 5 figures, Journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental conflict between the enormous space of adaptive streaming\nvideos and the limited capacity for subjective experiment casts significant\nchallenges to objective Quality-of-Experience (QoE) prediction. Existing\nobjective QoE models exhibit complex functional form, failing to generalize\nwell in diverse streaming environments. In this study, we propose an objective\nQoE model namely knowledge-driven streaming quality index (KSQI) to integrate\nprior knowledge on the human visual system and human annotated data in a\nprincipled way. By analyzing the subjective characteristics towards streaming\nvideos from a corpus of subjective studies, we show that a family of QoE\nfunctions lies in a convex set. Using a variant of projected gradient descent,\nwe optimize the objective QoE model over a database of training videos. The\nproposed KSQI demonstrates strong generalizability to diverse streaming\nenvironments, evident by state-of-the-art performance on four publicly\navailable benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 20:45:42 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Duanmu", "Zhengfang", ""], ["Liu", "Wentao", ""], ["Chen", "Diqi", ""], ["Li", "Zhuoran", ""], ["Wang", "Zhou", ""], ["Wang", "Yizhou", ""], ["Gao", "Wen", ""]]}, {"id": "1911.07982", "submitter": "Qian Wang", "authors": "Qian Wang, Toby P. Breckon", "title": "Unsupervised Domain Adaptation via Structured Prediction Based Selective\n  Pseudo-Labeling", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to address the problem of classifying\nunlabeled samples from the target domain whilst labeled samples are only\navailable from the source domain and the data distributions are different in\nthese two domains. As a result, classifiers trained from labeled samples in the\nsource domain suffer from significant performance drop when directly applied to\nthe samples from the target domain. To address this issue, different approaches\nhave been proposed to learn domain-invariant features or domain-specific\nclassifiers. In either case, the lack of labeled samples in the target domain\ncan be an issue which is usually overcome by pseudo-labeling. Inaccurate\npseudo-labeling, however, could result in catastrophic error accumulation\nduring learning. In this paper, we propose a novel selective pseudo-labeling\nstrategy based on structured prediction. The idea of structured prediction is\ninspired by the fact that samples in the target domain are well clustered\nwithin the deep feature space so that unsupervised clustering analysis can be\nused to facilitate accurate pseudo-labeling. Experimental results on four\ndatasets (i.e. Office-Caltech, Office31, ImageCLEF-DA and Office-Home) validate\nour approach outperforms contemporary state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:21:47 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Wang", "Qian", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1911.07990", "submitter": "Qian Wang", "authors": "Qian Wang, Toby P. Breckon", "title": "Crowd Counting via Segmentation Guided Attention Networks and Curriculum\n  Loss", "comments": "Technical Report, Durham University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic crowd behaviour analysis is an important task for intelligent\ntransportation systems to enable effective flow control and dynamic route\nplanning for varying road participants. Crowd counting is one of the keys to\nautomatic crowd behaviour analysis. Crowd counting using deep convolutional\nneural networks (CNN) has achieved encouraging progress in recent years.\nResearchers have devoted much effort to the design of variant CNN architectures\nand most of them are based on the pre-trained VGG16 model. Due to the\ninsufficient expressive capacity, the backbone network of VGG16 is usually\nfollowed by another cumbersome network specially designed for good counting\nperformance. Although VGG models have been outperformed by Inception models in\nimage classification tasks, the existing crowd counting networks built with\nInception modules still only have a small number of layers with basic types of\nInception modules. To fill in this gap, in this paper, we firstly benchmark the\nbaseline Inception-v3 model on commonly used crowd counting datasets and\nachieve surprisingly good performance comparable with or better than most\nexisting crowd counting models. Subsequently, we push the boundary of this\ndisruptive work further by proposing a Segmentation Guided Attention Network\n(SGANet) with Inception-v3 as the backbone and a novel curriculum loss for\ncrowd counting. We conduct thorough experiments to compare the performance of\nour SGANet with prior arts and the proposed model can achieve state-of-the-art\nperformance with MAE of 57.6, 6.3 and 87.6 on ShanghaiTechA, ShanghaiTechB and\nUCF\\_QNRF, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:40:13 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 21:06:44 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Qian", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1911.08199", "submitter": "Zhijie Lin", "authors": "Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang and Huasheng Liu", "title": "Weakly-Supervised Video Moment Retrieval via Semantic Completion Network", "comments": "Accepted by AAAI 2020 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video moment retrieval is to search the moment that is most relevant to the\ngiven natural language query. Existing methods are mostly trained in a\nfully-supervised setting, which requires the full annotations of temporal\nboundary for each query. However, manually labeling the annotations is actually\ntime-consuming and expensive. In this paper, we propose a novel\nweakly-supervised moment retrieval framework requiring only coarse video-level\nannotations for training. Specifically, we devise a proposal generation module\nthat aggregates the context information to generate and score all candidate\nproposals in one single pass. We then devise an algorithm that considers both\nexploitation and exploration to select top-K proposals. Next, we build a\nsemantic completion module to measure the semantic similarity between the\nselected proposals and query, compute reward and provide feedbacks to the\nproposal generation module for scoring refinement. Experiments on the\nActivityCaptions and Charades-STA demonstrate the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 10:31:43 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 12:40:37 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 11:09:43 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Lin", "Zhijie", ""], ["Zhao", "Zhou", ""], ["Zhang", "Zhu", ""], ["Wang", "Qi", ""], ["Liu", "Huasheng", ""]]}, {"id": "1911.08217", "submitter": "Huizhou Li", "authors": "Chao Yang, Huizhou Li, Fangting Lin, Bin Jiang, Hao Zhao", "title": "Constrained R-CNN: A general image manipulation detection model", "comments": "Accepted to IEEE International Conference on Multimedia and Expo\n  (ICME2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning-based models have exhibited remarkable performance\nfor image manipulation detection. However, most of them suffer from poor\nuniversality of handcrafted or predetermined features. Meanwhile, they only\nfocus on manipulation localization and overlook manipulation classification. To\naddress these issues, we propose a coarse-to-fine architecture named\nConstrained R-CNN for complete and accurate image forensics. First, the\nlearnable manipulation feature extractor learns a unified feature\nrepresentation directly from data. Second, the attention region proposal\nnetwork effectively discriminates manipulated regions for the next manipulation\nclassification and coarse localization. Then, the skip structure fuses\nlow-level and high-level information to refine the global manipulation\nfeatures. Finally, the coarse localization information guides the model to\nfurther learn the finer local features and segment out the tampered region.\nExperimental results show that our model achieves state-of-the-art performance.\nEspecially, the F1 score is increased by 28.4%, 73.2%, 13.3% on the NIST16,\nCOVERAGE, and Columbia dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 12:12:20 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 12:14:58 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 11:01:38 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Yang", "Chao", ""], ["Li", "Huizhou", ""], ["Lin", "Fangting", ""], ["Jiang", "Bin", ""], ["Zhao", "Hao", ""]]}, {"id": "1911.08588", "submitter": "Qilei Chen", "authors": "Qilei Chen, Xinzi Sun, Ning Zhang, Yu Cao, Benyuan Liu", "title": "Mini Lesions Detection on Diabetic Retinopathy Images via Large Scale\n  CNN Features", "comments": "diabetic retinopathy, mini lesion detection, FPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is\na primary cause of blindness in working-age people and it is estimated that 3\nto 4 million people with diabetes are blinded by DR every year worldwide. Early\ndiagnosis have been considered an effective way to mitigate such problem. The\nultimate goal of our research is to develop novel machine learning techniques\nto analyze the DR images generated by the fundus camera for automatically DR\ndiagnosis. In this paper, we focus on identifying small lesions on DR fundus\nimages. The results from our analysis, which include the lesion category and\ntheir exact locations in the image, can be used to facilitate the determination\nof DR severity (indicated by DR stages). Different from traditional object\ndetection for natural images, lesion detection for fundus images have unique\nchallenges. Specifically, the size of a lesion instance is usually very small,\ncompared with the original resolution of the fundus images, making them\ndiffcult to be detected. We analyze the lesion-vs-image scale carefully and\npropose a large-size feature pyramid network (LFPN) to preserve more image\ndetails for mini lesion instance detection. Our method includes an effective\nregion proposal strategy to increase the sensitivity. The experimental results\nshow that our proposed method is superior to the original feature pyramid\nnetwork (FPN) method and Faster RCNN.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:06:50 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Chen", "Qilei", ""], ["Sun", "Xinzi", ""], ["Zhang", "Ning", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "1911.08618", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Anupriy and Vinay P. Namboodiri", "title": "Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA", "comments": "AAAI-2020(Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we aim to obtain improved attention for a visual question\nanswering (VQA) task. It is challenging to provide supervision for attention.\nAn observation we make is that visual explanations as obtained through class\nactivation mappings (specifically Grad-CAM) that are meant to explain the\nperformance of various networks could form a means of supervision. However, as\nthe distributions of attention maps and that of Grad-CAMs differ, it would not\nbe suitable to directly use these as a form of supervision. Rather, we propose\nthe use of a discriminator that aims to distinguish samples of visual\nexplanation and attention maps. The use of adversarial training of the\nattention regions as a two-player game between attention and explanation serves\nto bring the distributions of attention maps and visual explanations closer.\nSignificantly, we observe that providing such a means of supervision also\nresults in attention maps that are more closely related to human attention\nresulting in a substantial improvement over baseline stacked attention network\n(SAN) models. It also results in a good improvement in rank correlation metric\non the VQA task. This method can also be combined with recent MCB based methods\nand results in consistent improvement. We also provide comparisons with other\nmeans for learning distributions such as based on Correlation Alignment\n(Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and\nobserve that the adversarial loss outperforms the other forms of learning the\nattention maps. Visualization of the results also confirms our hypothesis that\nattention maps improve using this form of supervision.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 22:30:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Patro", "Badri N.", ""], ["Anupriy", "", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1911.08764", "submitter": "Matteo Testa", "authors": "Matteo Testa, Arslan Ali, Tiziano Bianchi, Enrico Magli", "title": "Learning mappings onto regularized latent spaces for biometric\n  authentication", "comments": "Accepted at IEEE MMSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture for generic biometric authentication based on\ndeep neural networks: RegNet. Differently from other methods, RegNet learns a\nmapping of the input biometric traits onto a target distribution in a\nwell-behaved space in which users can be separated by means of simple and\ntunable boundaries. More specifically, authorized and unauthorized users are\nmapped onto two different and well behaved Gaussian distributions. The novel\napproach of learning the mapping instead of the boundaries further avoids the\nproblem encountered in typical classifiers for which the learnt boundaries may\nbe complex and difficult to analyze. RegNet achieves high performance in terms\nof security metrics such as Equal Error Rate (EER), False Acceptance Rate (FAR)\nand Genuine Acceptance Rate (GAR). The experiments we conducted on publicly\navailable datasets of face and fingerprint confirm the effectiveness of the\nproposed system.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 08:40:44 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Testa", "Matteo", ""], ["Ali", "Arslan", ""], ["Bianchi", "Tiziano", ""], ["Magli", "Enrico", ""]]}, {"id": "1911.08854", "submitter": "Krzysztof Domino", "authors": "Agnieszka A. Tomaka, Leszek Luchowski, Dariusz Pojda, Micha{\\l}\n  Tarnawski, Krzysztof Domino", "title": "The dynamics of the stomatognathic system from 4D multimodal data", "comments": "Chapter 3 in A.Gadomski (ed.): Multiscale Locomotion: Its\n  Active-Matter Addressing Physical Principles; UTP University of Science &\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this chapter is to discuss methods of acquisition,\nvisualization and analysis of the dynamics of a complex biomedical system,\nillustrated by the human stomatognathic system. The stomatognathic system\nconsists of the teeth and the skull bones with the maxilla and the mandible.\nIts dynamics can be described by the change of mutual position of the\nlower/mandibular part versus the upper/maxillary one due to the physiological\nmotion of opening, chewing and swallowing. In order to analyse the dynamics of\nthe stomatognathic system its morphology and motion has to be digitized, which\nis done using static and dynamic multimodal imagery like CBCT and 3D scans data\nand temporal measurements of motion. The integration of multimodal data\nincorporates different direct and indirect methods of registration - aligning\nof all the data in the same coordinate system. The integrated sets of data form\n4D multimodal data which can be further visualized, modeled, and subjected to\nmultivariate time series analysis. Example results are shown. Although there is\nno direct method of imaging the TMJ motion, the integration of multimodal data\nforms an adequate tool. As medical imaging becomes ever more diverse and ever\nmore accessible, organizing the imagery and measurements into unified,\ncomprehensive records can deliver to the doctor the most information in the\nmost accessible form, creating a new quality in data simulation, analysis and\ninterpretation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 12:25:58 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Tomaka", "Agnieszka A.", ""], ["Luchowski", "Leszek", ""], ["Pojda", "Dariusz", ""], ["Tarnawski", "Micha\u0142", ""], ["Domino", "Krzysztof", ""]]}, {"id": "1911.08864", "submitter": "Jaafar Elmirghani", "authors": "Hatem A. Alharbi, Taisir E.H. Elgorashi and Jaafar M.H. Elmirghani", "title": "Impact of the Net Neutrality Repeal on Communication Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Network neutrality (net neutrality) is the principle of treating equally all\nInternet traffic regardless of its source, destination, content, application or\nother related distinguishing metrics. Under net neutrality, ISPs are compelled\nto charge all content providers (CPs) the same per Gbps rate despite the\ngrowing profit achieved by CPs. In this paper, we study the impact of the\nrepeal of net neutrality on communication networks by developing a\ntechno-economic Mixed Integer Linear Programming (MILP) model to maximize the\npotential profit ISPs can achieve by offering their services to CPs. We focus\non video delivery as video traffic accounts for 78% of the cloud traffic. We\nconsider an ISP that offers CPs different classes of service representing\ntypical video content qualities including standard definition (SD), high\ndefinition (HD) and ultra-high definition (UHD) video. The MILP model maximizes\nthe ISP profit by optimizing the prices of the different classes according to\nthe users demand sensitivity to the change in price, referred to as Price\nElasticity of Demand (PED). We analyze how PED impacts the profit in different\nCP delivery scenarios in cloud-fog architectures. The results show that the\nrepeal of net neutrality can potentially increase ISPs profit by a factor of 8\nwith a pricing scheme that discriminates against data intensive content. Also,\nthe repeal of net neutrality positively impacts the network energy efficiency\nby reducing the core network power consumption by 55% as a result of\nsuppressing data intensive content compared to the net neutrality scenario.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 12:48:59 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Alharbi", "Hatem A.", ""], ["Elgorashi", "Taisir E. H.", ""], ["Elmirghani", "Jaafar M. H.", ""]]}, {"id": "1911.09349", "submitter": "Di Xie", "authors": "Jiaxu Chen and Jing Hao and Kai Chen and Di Xie and Shicai Yang and\n  Shiliang Pu", "title": "An End-to-End Audio Classification System based on Raw Waveforms and\n  Mix-Training Strategy", "comments": "InterSpeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio classification can distinguish different kinds of sounds, which is\nhelpful for intelligent applications in daily life. However, it remains a\nchallenging task since the sound events in an audio clip is probably multiple,\neven overlapping. This paper introduces an end-to-end audio classification\nsystem based on raw waveforms and mix-training strategy. Compared to\nhuman-designed features which have been widely used in existing research, raw\nwaveforms contain more complete information and are more appropriate for\nmulti-label classification. Taking raw waveforms as input, our network consists\nof two variants of ResNet structure which can learn a discriminative\nrepresentation. To explore the information in intermediate layers, a\nmulti-level prediction with attention structure is applied in our model.\nFurthermore, we design a mix-training strategy to break the performance\nlimitation caused by the amount of training data. Experiments show that the\nmean average precision of the proposed audio classification system on Audio Set\ndataset is 37.2%. Without using extra training data, our system exceeds the\nstate-of-the-art multi-level attention model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 08:54:48 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chen", "Jiaxu", ""], ["Hao", "Jing", ""], ["Chen", "Kai", ""], ["Xie", "Di", ""], ["Yang", "Shicai", ""], ["Pu", "Shiliang", ""]]}, {"id": "1911.09606", "submitter": "Guilherme Lima", "authors": "Guilherme Lima, Rodrigo Costa, Marcio Ferreira Moreno", "title": "An Introduction to Symbolic Artificial Intelligence Applied to\n  Multimedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we give an introduction to symbolic artificial intelligence\n(AI) and discuss its relation and application to multimedia. We begin by\ndefining what symbolic AI is, what distinguishes it from non-symbolic\napproaches, such as machine learning, and how it can used in the construction\nof advanced multimedia applications. We then introduce description logic (DL)\nand use it to discuss symbolic representation and reasoning. DL is the logical\nunderpinning of OWL, the most successful family of ontology languages. After\ndiscussing DL, we present OWL and related Semantic Web technologies, such as\nRDF and SPARQL. We conclude the chapter by discussing a hybrid model for\nmultimedia representation, called Hyperknowledge. Throughout the text, we make\nreferences to technologies and extensions specifically designed to solve the\nkinds of problems that arise in multimedia representation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 17:01:36 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 13:11:21 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Lima", "Guilherme", ""], ["Costa", "Rodrigo", ""], ["Moreno", "Marcio Ferreira", ""]]}, {"id": "1911.09857", "submitter": "Chao Liu", "authors": "Chao Liu, Heming Sun, Junan Chen, Zhengxue Cheng, Masaru Takeuchi,\n  Jiro Katto, Xiaoyang Zeng and Yibo Fan", "title": "Dual Learning-based Video Coding with Inception Dense Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a dual learning-based method in intra coding is introduced for\nPCS Grand Challenge. This method is mainly composed of two parts: intra\nprediction and reconstruction filtering. They use different network structures,\nthe neural network-based intra prediction uses the full-connected network to\npredict the block while the neural network-based reconstruction filtering\nutilizes the convolutional networks. Different with the previous filtering\nworks, we use a network with more powerful feature extraction capabilities in\nour reconstruction filtering network. And the filtering unit is the block-level\nso as to achieve a more accurate filtering compensation. To our best knowledge,\namong all the learning-based methods, this is the first attempt to combine two\ndifferent networks in one application, and we achieve the state-of-the-art\nperformance for AI configuration on the HEVC Test sequences. The experimental\nresult shows that our method leads to significant BD-rate saving for provided 8\nsequences compared to HM-16.20 baseline (average 10.24% and 3.57% bitrate\nreductions for all-intra and random-access coding, respectively). For HEVC test\nsequences, our model also achieved a 9.70% BD-rate saving compared to HM-16.20\nbaseline for all-intra configuration.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 04:57:44 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Liu", "Chao", ""], ["Sun", "Heming", ""], ["Chen", "Junan", ""], ["Cheng", "Zhengxue", ""], ["Takeuchi", "Masaru", ""], ["Katto", "Jiro", ""], ["Zeng", "Xiaoyang", ""], ["Fan", "Yibo", ""]]}, {"id": "1911.09882", "submitter": "Nikki Lijing Kuang", "authors": "Nikki Lijing Kuang and Clement H.C. Leung", "title": "Analysis of Evolutionary Behavior in Self-Learning Media Search Engines", "comments": "IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of intrinsic qualities of multimedia entities tends to impede\ntheir effective retrieval. In a SelfLearning Search Engine architecture, the\nsubtle nuances of human perceptions and deep knowledge are taught and captured\nthrough unsupervised reinforcement learning, where the degree of reinforcement\nmay be suitably calibrated. Such architectural paradigm enables indexes to\nevolve naturally while accommodating the dynamic changes of user interests. It\noperates by continuously constructing indexes over time, while injecting\nprogressive improvement in search performance. For search operations to be\neffective, convergence of index learning is of crucial importance to ensure\nefficiency and robustness. In this paper, we develop a Self-Learning Search\nEngine architecture based on reinforcement learning using a Markov Decision\nProcess framework. The balance between exploration and exploitation is achieved\nthrough evolutionary exploration Strategies. The evolutionary index learning\nbehavior is then studied and formulated using stochastic analysis. Experimental\nresults are presented which corroborate the steady convergence of the index\nevolution mechanism. Index Term\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 06:43:56 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Kuang", "Nikki Lijing", ""], ["Leung", "Clement H. C.", ""]]}, {"id": "1911.09891", "submitter": "Nikki Lijing Kuang", "authors": "Nikki Lijing Kuang and Clement H.C. Leung", "title": "Performance Effectiveness of Multimedia Information Search Using the\n  Epsilon-Greedy Algorithm", "comments": "8 pages, 10 figures. IEEE ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the search and retrieval of multimedia objects, it is impractical to\neither manually or automatically extract the contents for indexing since most\nof the multimedia contents are not machine extractable, while manual extraction\ntends to be highly laborious and time-consuming. However, by systematically\ncapturing and analyzing the feedback patterns of human users, vital information\nconcerning the multimedia contents can be harvested for effective indexing and\nsubsequent search. By learning from the human judgment and mental evaluation of\nusers, effective search indices can be gradually developed and built up, and\nsubsequently be exploited to find the most relevant multimedia objects. To\navoid hovering around a local maximum, we apply the epsilon-greedy method to\nsystematically explore the search space. Through such methodic exploration, we\nshow that the proposed approach is able to guarantee that the most relevant\nobjects can always be discovered, even though initially it may have been\noverlooked or not regarded as relevant. The search behavior of the present\napproach is quantitatively analyzed, and closed-form expressions are obtained\nfor the performance of two variants of the epsilon-greedy algorithm, namely\nEGSE-A and EGSE-B. Simulations and experiments on real data set have been\nperformed which show good agreement with the theoretical findings. The present\nmethod is able to leverage exploration in an effective way to significantly\nraise the performance of multimedia information search, and enables the certain\ndiscovery of relevant objects which may be otherwise undiscoverable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 07:12:05 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Kuang", "Nikki Lijing", ""], ["Leung", "Clement H. C.", ""]]}, {"id": "1911.10357", "submitter": "Yang Wang", "authors": "Huibing Wang, Yang Wang, Zhao Zhang, Xianping Fu, Zhuo Li, Mingliang\n  Xu, Meng Wang", "title": "Kernelized Multiview Subspace Analysis by Self-weighted Learning", "comments": "Appearing at IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2020.3032023", "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of multimedia technology, information is always\nrepresented or transmitted from multiple views. Most of the existing algorithms\nare graph-based ones to learn the complex structures within multiview data but\noverlooked the information within data representations. Furthermore, many\nexisting works treat multiple views discriminatively by introducing some\nhyperparameters, which is undesirable in practice. To this end, abundant\nmultiview based methods have been proposed for dimension reduction. However,\nthere are still no research to leverage the existing work into a unified\nframework. To address this issue, in this paper, we propose a general framework\nfor multiview data dimension reduction, named Kernelized Multiview Subspace\nAnalysis (KMSA). It directly handles the multi-view feature representation in\nthe kernel space, which provides a feasible channel for direct manipulations on\nmultiview data with different dimensions. Meanwhile, compared with those\ngraph-based methods, KMSA can fully exploit information from multiview data\nwith nothing to lose. Furthermore, since different views have different\ninfluences on KMSA, we propose a self-weighted strategy to treat different\nviews discriminatively according to their contributions. A co-regularized term\nis proposed to promote the mutual learning from multi-views. KMSA combines\nself-weighted learning with the co-regularized term to learn appropriate\nweights for all views. We also discuss the influence of the parameters in KMSA\nregarding the weights of multi-views. We evaluate our proposed framework on 6\nmultiview datasets for classification and image retrieval. The experimental\nresults validate the advantages of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 12:40:14 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 10:47:29 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 02:37:55 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wang", "Huibing", ""], ["Wang", "Yang", ""], ["Zhang", "Zhao", ""], ["Fu", "Xianping", ""], ["Li", "Zhuo", ""], ["Xu", "Mingliang", ""], ["Wang", "Meng", ""]]}, {"id": "1911.10531", "submitter": "Ruicong Xu", "authors": "Ruicong Xu, Li Niu, Jianfu Zhang, Liqing Zhang", "title": "A Proposal-based Approach for Activity Image-to-Video Retrieval", "comments": "The Thirty-Fourth AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity image-to-video retrieval task aims to retrieve videos containing the\nsimilar activity as the query image, which is a challenging task because videos\ngenerally have many background segments irrelevant to the activity. In this\npaper, we utilize R-C3D model to represent a video by a bag of activity\nproposals, which can filter out background segments to some extent. However,\nthere are still noisy proposals in each bag. Thus, we propose an Activity\nProposal-based Image-to-Video Retrieval (APIVR) approach, which incorporates\nmulti-instance learning into cross-modal retrieval framework to address the\nproposal noise issue. Specifically, we propose a Graph Multi-Instance Learning\n(GMIL) module with graph convolutional layer, and integrate this module with\nclassification loss, adversarial loss, and triplet loss in our cross-modal\nretrieval framework. Moreover, we propose geometry-aware triplet loss based on\npoint-to-subspace distance to preserve the structural information of activity\nproposals. Extensive experiments on three widely-used datasets verify the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:03:21 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Xu", "Ruicong", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Zhang", "Liqing", ""]]}, {"id": "1911.10672", "submitter": "Dongxu Wei", "authors": "Dongxu Wei, Xiaowei Xu, Haibin Shen, Kejie Huang", "title": "GAC-GAN: A General Method for Appearance-Controllable Human Video Motion\n  Transfer", "comments": "paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human video motion transfer has a wide range of applications in multimedia,\ncomputer vision and graphics. Recently, due to the rapid development of\nGenerative Adversarial Networks (GANs), there has been significant progress in\nthe field. However, almost all existing GAN-based works are prone to address\nthe mapping from human motions to video scenes, with scene appearances are\nencoded individually in the trained models. Therefore, each trained model can\nonly generate videos with a specific scene appearance, new models are required\nto be trained to generate new appearances. Besides, existing works lack the\ncapability of appearance control. For example, users have to provide video\nrecords of wearing new clothes or performing in new backgrounds to enable\nclothes or background changing in their synthetic videos, which greatly limits\nthe application flexibility. In this paper, we propose GAC-GAN, a general\nmethod for appearance-controllable human video motion transfer. To enable\ngeneral-purpose appearance synthesis, we propose to include appearance\ninformation in the conditioning inputs. Thus, once trained, our model can\ngenerate new appearances by altering the input appearance information. To\nachieve appearance control, we first obtain the appearance-controllable\nconditioning inputs and then utilize a two-stage GAC-GAN to generate the\ncorresponding appearance-controllable outputs, where we utilize an ACGAN loss\nand a shadow extraction module for output foreground and background appearance\ncontrol respectively. We further build a solo dance dataset containing a large\nnumber of dance videos for training and evaluation. Experimental results show\nthat, our proposed GAC-GAN can not only support appearance-controllable human\nvideo motion transfer but also achieve higher video quality than state-of-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 02:56:47 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 15:52:47 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wei", "Dongxu", ""], ["Xu", "Xiaowei", ""], ["Shen", "Haibin", ""], ["Huang", "Kejie", ""]]}, {"id": "1911.11378", "submitter": "Manraj Singh Grover", "authors": "Osaid Rehman Nasir, Shailesh Kumar Jha, Manraj Singh Grover, Yi Yu,\n  Ajit Kumar, Rajiv Ratn Shah", "title": "Text2FaceGAN: Face Generation from Fine Grained Textual Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful generative adversarial networks (GAN) have been developed to\nautomatically synthesize realistic images from text. However, most existing\ntasks are limited to generating simple images such as flowers from captions. In\nthis work, we extend this problem to the less addressed domain of face\ngeneration from fine-grained textual descriptions of face, e.g., \"A person has\ncurly hair, oval face, and mustache\". We are motivated by the potential of\nautomated face generation to impact and assist critical tasks such as criminal\nface reconstruction. Since current datasets for the task are either very small\nor do not contain captions, we generate captions for images in the CelebA\ndataset by creating an algorithm to automatically convert a list of attributes\nto a set of captions. We then model the highly multi-modal problem of text to\nface generation as learning the conditional distribution of faces (conditioned\non text) in same latent space. We utilize the current state-of-the-art GAN\n(DC-GAN with GAN-CLS loss) for learning conditional multi-modality. The\npresence of more fine-grained details and variable length of the captions makes\nthe problem easier for a user but more difficult to handle compared to the\nother text-to-image tasks. We flipped the labels for real and fake images and\nadded noise in discriminator. Generated images for diverse textual descriptions\nshow promising results. In the end, we show how the widely used inceptions\nscore is not a good metric to evaluate the performance of generative models\nused for synthesizing faces from text.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 07:37:47 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Nasir", "Osaid Rehman", ""], ["Jha", "Shailesh Kumar", ""], ["Grover", "Manraj Singh", ""], ["Yu", "Yi", ""], ["Kumar", "Ajit", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "1911.11631", "submitter": "Rodrigo Santos", "authors": "Marcio Ferreira Moreno, Guilherme Lima, Rodrigo Costa Mesquita Santos,\n  Roberto Azevedo, Markus Endler", "title": "Bridging the Gap between Semantics and Multimedia Processing", "comments": "1st International Workshop on Bridging the Gap between Semantics and\n  Multimedia Processing (SeMP 2019): http://semp.mybluemix.net/2019/. arXiv\n  admin note: text overlap with arXiv:1911.09606", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give an overview of the semantic gap problem in multimedia\nand discuss how machine learning and symbolic AI can be combined to narrow this\ngap. We describe the gap in terms of a classical architecture for multimedia\nprocessing and discuss a structured approach to bridge it. This approach\ncombines machine learning (for mapping signals to objects) and symbolic AI (for\nlinking objects to meanings). Our main goal is to raise awareness and discuss\nthe challenges involved in this structured approach to multimedia\nunderstanding, especially in the view of the latest developments in machine\nlearning and symbolic AI.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 13:12:35 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 12:25:22 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Moreno", "Marcio Ferreira", ""], ["Lima", "Guilherme", ""], ["Santos", "Rodrigo Costa Mesquita", ""], ["Azevedo", "Roberto", ""], ["Endler", "Markus", ""]]}, {"id": "1911.13279", "submitter": "Saikat Chakraborty", "authors": "Saikat Chakraborty", "title": "A Graph-based Ranking Approach to Extract Key-frames for Static Video\n  Summarization", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video abstraction has become one of the efficient approaches to grasp the\ncontent of a video without seeing it entirely. Key frame-based static video\nsummarization falls under this category. In this paper, we propose a\ngraph-based approach which summarizes the video with best user satisfaction. We\ntreated each video frame as a node of the graph and assigned a rank to each\nnode by our proposed VidRank algorithm. We developed three different models of\nVidRank algorithm and performed a comparative study on those models. A\ncomprehensive evaluation of 50 videos from open video database using objective\nand semi-objective measures indicates the superiority of our static video\nsummary generation method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:24:44 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Chakraborty", "Saikat", ""]]}]