[{"id": "1704.00389", "submitter": "Yi Zhu", "authors": "Yi Zhu, Zhenzhong Lan, Shawn Newsam, Alexander G. Hauptmann", "title": "Hidden Two-Stream Convolutional Networks for Action Recognition", "comments": "Accepted at ACCV 2018, camera ready. Code available at\n  https://github.com/bryanyzhu/Hidden-Two-Stream", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing videos of human actions involves understanding the temporal\nrelationships among video frames. State-of-the-art action recognition\napproaches rely on traditional optical flow estimation methods to pre-compute\nmotion information for CNNs. Such a two-stage approach is computationally\nexpensive, storage demanding, and not end-to-end trainable. In this paper, we\npresent a novel CNN architecture that implicitly captures motion information\nbetween adjacent frames. We name our approach hidden two-stream CNNs because it\nonly takes raw video frames as input and directly predicts action classes\nwithout explicitly computing optical flow. Our end-to-end approach is 10x\nfaster than its two-stage baseline. Experimental results on four challenging\naction recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show\nthat our approach significantly outperforms the previous best real-time\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 23:39:51 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 21:48:54 GMT"}, {"version": "v3", "created": "Sun, 22 Oct 2017 03:53:21 GMT"}, {"version": "v4", "created": "Tue, 30 Oct 2018 04:55:03 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Yi", ""], ["Lan", "Zhenzhong", ""], ["Newsam", "Shawn", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1704.00616", "submitter": "Mohammadreza Zolfaghari", "authors": "Mohammadreza Zolfaghari, Gabriel L. Oliveira, Nima Sedaghat, and\n  Thomas Brox", "title": "Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance\n  for Action Classification and Detection", "comments": "10 pages, 7 figures, ICCV 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General human action recognition requires understanding of various visual\ncues. In this paper, we propose a network architecture that computes and\nintegrates the most important visual cues for action recognition: pose, motion,\nand the raw images. For the integration, we introduce a Markov chain model\nwhich adds cues successively. The resulting approach is efficient and\napplicable to action classification as well as to spatial and temporal action\nlocalization. The two contributions clearly improve the performance over\nrespective baselines. The overall approach achieves state-of-the-art action\nclassification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,\nit yields state-of-the-art spatio-temporal action localization results on\nUCF101 and J-HMDB.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 14:29:40 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:40:14 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zolfaghari", "Mohammadreza", ""], ["Oliveira", "Gabriel L.", ""], ["Sedaghat", "Nima", ""], ["Brox", "Thomas", ""]]}, {"id": "1704.00631", "submitter": "Abhishek Kashyap", "authors": "Abhishek Kashyap, Megha Agarwal, Hariom Gupta", "title": "Detection of Copy-move Image forgery using SVD and Cuckoo Search\n  Algorithm", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy-move forgery is one of the simple and effective operations to create\nforged images. Recently, techniques based on singular value decomposition (SVD)\nare widely used to detect copy-move forgery (CMF). Some approaches based on SVD\nare most acceptable to detect copy-move forgery but some copy-move forgery\ndetection approaches can not produce satisfactory detection results. Sometimes\nthese approaches may even produce error results. According to our observation,\ndetection result produced using SVD depend highly on those parameters whose\nvalues are often determined with experiences. These values are only applicable\nto a few images, which limit their application. To solve this problem, a novel\napproach named as copy-move forgery detection using Cuckoo search algorithm\n(CMFD-CS) is proposed in this paper. CMFD-CS integrates the CS algorithm into\nSVD. It utilizes the CS algorithm to generate customized parameter values for\nimages, which are used CMFD under block-based framework.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:06:37 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Kashyap", "Abhishek", ""], ["Agarwal", "Megha", ""], ["Gupta", "Hariom", ""]]}, {"id": "1704.02116", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Jinwei Qi, Xin Huang and Yuxin Yuan", "title": "CCL: Cross-modal Correlation Learning with Multi-grained Fusion by\n  Hierarchical Network", "comments": "16 pages, accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval has become a highlighted research topic for retrieval\nacross multimedia data such as image and text. A two-stage learning framework\nis widely adopted by most existing methods based on Deep Neural Network (DNN):\nThe first learning stage is to generate separate representation for each\nmodality, and the second learning stage is to get the cross-modal common\nrepresentation. However, the existing methods have three limitations: (1) In\nthe first learning stage, they only model intra-modality correlation, but\nignore inter-modality correlation with rich complementary context. (2) In the\nsecond learning stage, they only adopt shallow networks with single-loss\nregularization, but ignore the intrinsic relevance of intra-modality and\ninter-modality correlation. (3) Only original instances are considered while\nthe complementary fine-grained clues provided by their patches are ignored. For\naddressing the above problems, this paper proposes a cross-modal correlation\nlearning (CCL) approach with multi-grained fusion by hierarchical network, and\nthe contributions are as follows: (1) In the first learning stage, CCL exploits\nmulti-level association with joint optimization to preserve the complementary\ncontext from intra-modality and inter-modality correlation simultaneously. (2)\nIn the second learning stage, a multi-task learning strategy is designed to\nadaptively balance the intra-modality semantic category constraints and\ninter-modality pairwise similarity constraints. (3) CCL adopts multi-grained\nmodeling, which fuses the coarse-grained instances and fine-grained patches to\nmake cross-modal correlation more precise. Comparing with 13 state-of-the-art\nmethods on 6 widely-used cross-modal datasets, the experimental results show\nour CCL approach achieves the best performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 07:36:00 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 08:15:29 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 08:00:08 GMT"}, {"version": "v4", "created": "Tue, 8 Aug 2017 07:37:26 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Peng", "Yuxin", ""], ["Qi", "Jinwei", ""], ["Huang", "Xin", ""], ["Yuan", "Yuxin", ""]]}, {"id": "1704.02216", "submitter": "Ashkan Esmaeili", "authors": "Ali Mottaghi, Kayhan Behdin, Ashkan Esmaeili, Mohammadreza Heydari,\n  and Farokh Marvasti", "title": "OBTAIN: Real-Time Beat Tracking in Audio Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a system in order to perform the real-time beat\ntracking for an audio signal. We use Onset Strength Signal (OSS) to detect the\nonsets and estimate the tempos. Then, we form Cumulative Beat Strength Signal\n(CBSS) by taking advantage of OSS and estimated tempos. Next, we perform peak\ndetection by extracting the periodic sequence of beats among all CBSS peaks. In\nsimulations, we can see that our proposed algorithm, Online Beat TrAckINg\n(OBTAIN), outperforms state-of-art results in terms of prediction accuracy\nwhile maintaining comparable and practical computational complexity. The\nreal-time performance is tractable visually as illustrated in the simulations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:15:15 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 19:36:55 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Mottaghi", "Ali", ""], ["Behdin", "Kayhan", ""], ["Esmaeili", "Ashkan", ""], ["Heydari", "Mohammadreza", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1704.02223", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Xin Huang, and Yunzhen Zhao", "title": "An Overview of Cross-media Retrieval: Concepts, Methodologies,\n  Benchmarks and Challenges", "comments": "14 pages, accepted by IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia retrieval plays an indispensable role in big data utilization.\nPast efforts mainly focused on single-media retrieval. However, the\nrequirements of users are highly flexible, such as retrieving the relevant\naudio clips with one query of image. So challenges stemming from the \"media\ngap\", which means that representations of different media types are\ninconsistent, have attracted increasing attention. Cross-media retrieval is\ndesigned for the scenarios where the queries and retrieval results are of\ndifferent media types. As a relatively new research topic, its concepts,\nmethodologies and benchmarks are still not clear in the literatures. To address\nthese issues, we review more than 100 references, give an overview including\nthe concepts, methodologies, major challenges and open issues, as well as build\nup the benchmarks including datasets and experimental results. Researchers can\ndirectly adopt the benchmarks to promptly evaluate their proposed methods. This\nwill help them to focus on algorithm design, rather than the time-consuming\ncompared methods and results. It is noted that we have constructed a new\ndataset XMedia, which is the first publicly available dataset with up to five\nmedia types (text, image, video, audio and 3D model). We believe this overview\nwill attract more researchers to focus on cross-media retrieval and be helpful\nto them.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:26:10 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 12:52:59 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 12:28:48 GMT"}, {"version": "v4", "created": "Sat, 1 Jul 2017 13:29:40 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Peng", "Yuxin", ""], ["Huang", "Xin", ""], ["Zhao", "Yunzhen", ""]]}, {"id": "1704.02698", "submitter": "Umamaheswari Ganesh", "authors": "G.Umamaheswari, Dr.C.P.Sumathi", "title": "A New Steganographic Technique Matching the Secret Message and Cover\n  image Binary Value", "comments": "6 pages", "journal-ref": "International Journal of Computer Science and Information\n  Security, January 2017", "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography involves hiding a secret message or image inside another cover\nimage. Changes are made in the cover image without affecting visual quality of\nthe image. In contrast to cryptography, Steganography provides complete secrecy\nof the communication. Security of very sensitive data can be enhanced by\ncombining cryptography and steganography. A new technique that uses the concept\nof Steganography to obtain the position values from an image is suggested. This\npaper proposes a new method where no change is made to the cover image, only\nthe pixel position LSB (Least Significant Bit) values that match with the\nsecret message bit values are noted in a separate position file. At the sending\nend the position file along with the cover image is sent. At the receiving end\nthe position file is opened only with a secret key. The bit positions are taken\nfrom the position file and the LSB values from the positions are combined to\nget ASCII values and then form characters of the secret message\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 03:47:42 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Umamaheswari", "G.", ""], ["Sumathi", "Dr. C. P.", ""]]}, {"id": "1704.02754", "submitter": "Jinquan Zhang", "authors": "Zhang Jin-quan and Han Bin", "title": "A Synchronization Algorithm Based on Moving Average for Robust Audio\n  Watermarking Scheme", "comments": "23 pages, 7 tables,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A synchronization code scheme based on moving average is proposed for robust\naudio watermarking in the paper. Two proper positive integers are chosen to\ncompute the moving average sequence by sliding one sample every time. The\nsynchronization bits are embedded at crosses of the two moving average\nsequences with the quantization index modulation. The experimental results show\nthat the proposed watermarking scheme maintains high audio quality and is\nrobust to common attacks such as additive white Gaussian noise, re-sampling,\nlow-pass filtering, random cropping, MP3 compression, jitter attack and time\nscale modification. Simultaneously, the algorithm has high search efficiency\nand low false alarm rate.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 08:25:36 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Jin-quan", "Zhang", ""], ["Bin", "Han", ""]]}, {"id": "1704.02755", "submitter": "Jinquan Zhang", "authors": "Jinquan Zhang and Bin Han", "title": "Robust Audio Watermarking Algorithm Based on Moving Average and DCT", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is often brought to host audio by common signal processing operation,\nand it usually changes the high-frequency component of an audio signal. So\nembedding watermark by adjusting low-frequency coefficient can improve the\nrobustness of a watermark scheme. Moving Average sequence is a low-frequency\nfeature of an audio signal. This work proposed a method which embedding\nwatermark into the maximal coefficient in discrete cosine transform domain of a\nmoving average sequence. Subjective and objective tests reveal that the\nproposed watermarking scheme maintains highly audio quality, and\nsimultaneously, the algorithm is highly robust to common digital signal\nprocessing operations, including additive noise, sampling rate change, bit\nresolution transformation, MP3 compression, and random cropping, especially\nlow-pass filtering.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 08:26:19 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Zhang", "Jinquan", ""], ["Han", "Bin", ""]]}, {"id": "1704.02790", "submitter": "Hussein Al-Zubaidy", "authors": "Hussein Al-Zubaidy, Viktoria Fodor, Gy\\\"orgy D\\'an, Markus Flierl", "title": "Performance Analysis of Reliable Video Streaming with Strict Playout\n  Deadline in Multi-Hop Wireless Networks", "comments": "33 single column pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by emerging vision-based intelligent services, we consider the\nproblem of rate adaptation for high quality and low delay visual information\ndelivery over wireless networks using scalable video coding. Rate adaptation in\nthis setting is inherently challenging due to the interplay between the\nvariability of the wireless channels, the queuing at the network nodes and the\nframe-based decoding and playback of the video content at the receiver at very\nshort time scales. To address the problem, we propose a low-complexity,\nmodel-based rate adaptation algorithm for scalable video streaming systems,\nbuilding on a novel performance model based on stochastic network calculus. We\nvalidate the model using extensive simulations. We show that it allows fast,\nnear optimal rate adaptation for fixed transmission paths, as well as\ncross-layer optimized routing and video rate adaptation in mesh networks, with\nless than $10$\\% quality degradation compared to the best achievable\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:32:18 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Al-Zubaidy", "Hussein", ""], ["Fodor", "Viktoria", ""], ["D\u00e1n", "Gy\u00f6rgy", ""], ["Flierl", "Markus", ""]]}, {"id": "1704.03248", "submitter": "Seung-Min Mun", "authors": "Seung-Min Mun, Seung-Hun Nam, Han-Ul Jang, Dongkyu Kim, and Heung-Kyu\n  Lee", "title": "A Robust Blind Watermarking Using Convolutional Neural Network", "comments": "4 pages. We are modifying this paper to submit to SPL", "journal-ref": null, "doi": "10.1016/j.neucom.2019.01.067", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a blind watermarking based on a convolutional neural\nnetwork (CNN). We propose an iterative learning framework to secure robustness\nof watermarking. One loop of learning process consists of the following three\nstages: Watermark embedding, attack simulation, and weight update. We have\nlearned a network that can detect a 1-bit message from a image sub-block.\nExperimental results show that this learned network is an extension of the\nfrequency domain that is widely used in existing watermarking scheme. The\nproposed scheme achieved robustness against geometric and signal processing\nattacks with a learning time of one day.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 11:39:02 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Mun", "Seung-Min", ""], ["Nam", "Seung-Hun", ""], ["Jang", "Han-Ul", ""], ["Kim", "Dongkyu", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "1704.03503", "submitter": "Yi Zhu", "authors": "Yi Zhu, Shawn Newsam, Zaikun Xu", "title": "UC Merced Submission to the ActivityNet Challenge 2016", "comments": "Notebook paper for ActivityNet 2016 challenge, untrimmed video\n  classification track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper describes our system for the untrimmed classification\ntask in the ActivityNet challenge 2016. We investigate multiple\nstate-of-the-art approaches for action recognition in long, untrimmed videos.\nWe exploit hand-crafted motion boundary histogram features as well feature\nactivations from deep networks such as VGG16, GoogLeNet, and C3D. These\nfeatures are separately fed to linear, one-versus-rest support vector machine\nclassifiers to produce confidence scores for each action class. These\npredictions are then fused along with the softmax scores of the recent\nultra-deep ResNet-101 using weighted averaging.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 19:11:36 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""], ["Xu", "Zaikun", ""]]}, {"id": "1704.04133", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Alexander Wong, Graham W. Taylor", "title": "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR)\n  Approach to Understanding Deep Neural Networks", "comments": "Accepted at Computer Vision and Patter Recognition Workshop (CVPR-W)\n  on Explainable Computer Vision, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an\napproach to visualize and understand the decisions made by deep neural networks\n(DNNs) given a specific input. CLEAR facilitates the visualization of attentive\nregions and levels of interest of DNNs during the decision-making process. It\nalso enables the visualization of the most dominant classes associated with\nthese attentive regions of interest. As such, CLEAR can mitigate some of the\nshortcomings of heatmap-based methods associated with decision ambiguity, and\nallows for better insights into the decision-making process of DNNs.\nQuantitative and qualitative experiments across three different datasets\ndemonstrate the efficacy of CLEAR for gaining a better understanding of the\ninner workings of DNNs during the decision-making process.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:44:33 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 18:38:06 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Kumar", "Devinder", ""], ["Wong", "Alexander", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1704.04333", "submitter": "Yuxin Peng", "authors": "Jinwei Qi, Xin Huang, and Yuxin Peng", "title": "Cross-media Similarity Metric Learning with Unified Deep Networks", "comments": "19 pages, submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a highlighting research topic in the multimedia area, cross-media\nretrieval aims to capture the complex correlations among multiple media types.\nLearning better shared representation and distance metric for multimedia data\nis important to boost the cross-media retrieval. Motivated by the strong\nability of deep neural network in feature representation and comparison\nfunctions learning, we propose the Unified Network for Cross-media Similarity\nMetric (UNCSM) to associate cross-media shared representation learning with\ndistance metric in a unified framework. First, we design a two-pathway deep\nnetwork pretrained with contrastive loss, and employ double triplet similarity\nloss for fine-tuning to learn the shared representation for each media type by\nmodeling the relative semantic similarity. Second, the metric network is\ndesigned for effectively calculating the cross-media similarity of the shared\nrepresentation, by modeling the pairwise similar and dissimilar constraints.\nCompared to the existing methods which mostly ignore the dissimilar constraints\nand only use sample distance metric as Euclidean distance separately, our UNCSM\napproach unifies the representation learning and distance metric to preserve\nthe relative similarity as well as embrace more complex similarity functions\nfor further improving the cross-media retrieval accuracy. The experimental\nresults show that our UNCSM approach outperforms 8 state-of-the-art methods on\n4 widely-used cross-media datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 02:25:50 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Qi", "Jinwei", ""], ["Huang", "Xin", ""], ["Peng", "Yuxin", ""]]}, {"id": "1704.04886", "submitter": "Bo Zhao", "authors": "Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, Jiashi Feng", "title": "Multi-View Image Generation from a Single-View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a challenging problem -- how to generate multi-view\ncloth images from only a single view input. To generate realistic-looking\nimages with different views from the input, we propose a new image generation\nmodel termed VariGANs that combines the strengths of the variational inference\nand the Generative Adversarial Networks (GANs). Our proposed VariGANs model\ngenerates the target image in a coarse-to-fine manner instead of a single pass\nwhich suffers from severe artifacts. It first performs variational inference to\nmodel global appearance of the object (e.g., shape and color) and produce a\ncoarse image with a different view. Conditioned on the generated low resolution\nimages, it then proceeds to perform adversarial learning to fill details and\ngenerate images of consistent details with the input. Extensive experiments\nconducted on two clothing datasets, MVC and DeepFashion, have demonstrated that\nimages of a novel view generated by our model are more plausible than those\ngenerated by existing approaches, in terms of more consistent global appearance\nas well as richer and sharper details.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 06:54:34 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 06:42:24 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 03:55:09 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 02:36:32 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zhao", "Bo", ""], ["Wu", "Xiao", ""], ["Cheng", "Zhi-Qi", ""], ["Liu", "Hao", ""], ["Jie", "Zequn", ""], ["Feng", "Jiashi", ""]]}, {"id": "1704.05665", "submitter": "Qingcai Chen", "authors": "Xin Liu, Qingcai Chen, Xiangping Wu, Yan Liu, Yang Liu", "title": "CNN based music emotion classification", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music emotion recognition (MER) is usually regarded as a multi-label tagging\ntask, and each segment of music can inspire specific emotion tags. Most\nresearchers extract acoustic features from music and explore the relations\nbetween these features and their corresponding emotion tags. Considering the\ninconsistency of emotions inspired by the same music segment for human beings,\nseeking for the key acoustic features that really affect on emotions is really\na challenging task. In this paper, we propose a novel MER method by using deep\nconvolutional neural network (CNN) on the music spectrograms that contains both\nthe original time and frequency domain information. By the proposed method, no\nadditional effort on extracting specific features required, which is left to\nthe training procedure of the CNN model. Experiments are conducted on the\nstandard CAL500 and CAL500exp dataset. Results show that, for both datasets,\nthe proposed method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 09:28:39 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Liu", "Xin", ""], ["Chen", "Qingcai", ""], ["Wu", "Xiangping", ""], ["Liu", "Yan", ""], ["Liu", "Yang", ""]]}, {"id": "1704.06109", "submitter": "Yashar Deldjoo", "authors": "Yashar Deldjoo, Massimo Quadrana, Mehdi Elahi, Paolo Cremonesi", "title": "Using Mise-En-Sc\\`ene Visual Features based on MPEG-7 and Deep Learning\n  for Movie Recommendation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item features play an important role in movie recommender systems, where\nrecommendations can be generated by using explicit or implicit preferences of\nusers on traditional features (attributes) such as tag, genre, and cast.\nTypically, movie features are human-generated, either editorially (e.g., genre\nand cast) or by leveraging the wisdom of the crowd (e.g., tag), and as such,\nthey are prone to noise and are expensive to collect. Moreover, these features\nare often rare or absent for new items, making it difficult or even impossible\nto provide good quality recommendations.\n  In this paper, we show that user's preferences on movies can be better\ndescribed in terms of the mise-en-sc\\`ene features, i.e., the visual aspects of\na movie that characterize design, aesthetics and style (e.g., colors,\ntextures). We use both MPEG-7 visual descriptors and Deep Learning hidden\nlayers as example of mise-en-sc\\`ene features that can visually describe\nmovies. Interestingly, mise-en-sc\\`ene features can be computed automatically\nfrom video files or even from trailers, offering more flexibility in handling\nnew items, avoiding the need for costly and error-prone human-based tagging,\nand providing good scalability.\n  We have conducted a set of experiments on a large catalogue of 4K movies.\nResults show that recommendations based on mise-en-sc\\`ene features\nconsistently provide the best performance with respect to richer sets of more\ntraditional features, such as genre and tag.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 12:33:48 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Quadrana", "Massimo", ""], ["Elahi", "Mehdi", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1704.06192", "submitter": "Daniel Reiter Horn", "authors": "Daniel Reiter Horn, Ken Elkabany, Chris Lesniewski-Laas, Keith\n  Winstein", "title": "The Design, Implementation, and Deployment of a System to Transparently\n  Compress Hundreds of Petabytes of Image Files for a File-Storage Service", "comments": "12 pages", "journal-ref": "Proc. NSDI 2017, Boston. p1-15", "doi": null, "report-no": null, "categories": "cs.MM cs.DC cs.GR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report the design, implementation, and deployment of Lepton, a\nfault-tolerant system that losslessly compresses JPEG images to 77% of their\noriginal size on average. Lepton replaces the lowest layer of baseline JPEG\ncompression-a Huffman code-with a parallelized arithmetic code, so that the\nexact bytes of the original JPEG file can be recovered quickly. Lepton matches\nthe compression efficiency of the best prior work, while decoding more than\nnine times faster and in a streaming manner. Lepton has been released as\nopen-source software and has been deployed for a year on the Dropbox\nfile-storage backend. As of February 2017, it had compressed more than 203 PiB\nof user JPEG files, saving more than 46 PiB.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 00:38:30 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Horn", "Daniel Reiter", ""], ["Elkabany", "Ken", ""], ["Lesniewski-Laas", "Chris", ""], ["Winstein", "Keith", ""]]}, {"id": "1704.06444", "submitter": "Lun Wang", "authors": "Lun Wang, Damai Dai, Jie Jiang, Tong Yang, Xiaoke Jiang, Zekun Cai,\n  Yang Li, Xiaoming Li", "title": "FISF: Better User Experience using Smaller Bandwidth for Panoramic\n  Virtual Reality Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The panoramic video is widely used to build virtual reality (VR) and is\nexpected to be one of the next generation Killer-Apps. Transmitting panoramic\nVR videos is a challenging task because of two problems: 1) panoramic VR videos\nare typically much larger than normal videos but they need to be transmitted\nwith limited bandwidth in mobile networks. 2) high-resolution and fluent views\nshould be provided to guarantee a superior user experience and avoid\nside-effects such as dizziness and nausea. To address these two problems, we\npropose a novel interactive streaming technology, namely Focus-based\nInteractive Streaming Framework (FISF). FISF consists of three parts: 1) we use\nthe classic clustering algorithm DBSCAN to analyze real user data for Video\nFocus Detection (VFD); 2) we propose a Focus-based Interactive Streaming\nTechnology (FIST), including a static version and a dynamic version; 3) we\npropose two optimization methods: focus merging and prefetch strategy.\nExperimental results show that FISF significantly outperforms the\nstate-of-the-art. The paper is submitted to Sigcomm 2017, VR/AR Network on 31\nMar 2017 at 10:44:04am EDT.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 08:41:21 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Wang", "Lun", ""], ["Dai", "Damai", ""], ["Jiang", "Jie", ""], ["Yang", "Tong", ""], ["Jiang", "Xiaoke", ""], ["Cai", "Zekun", ""], ["Li", "Yang", ""], ["Li", "Xiaoming", ""]]}, {"id": "1704.07355", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e (Technicolor) and Anne-Marie Kermarrec (Inria) and\n  Nicolas Le Scouarnec (Technicolor)", "title": "Accelerated Nearest Neighbor Search with Quick ADC", "comments": "8 pages, 5 figures, published in Proceedings of ICMR'17, Bucharest,\n  Romania, June 06-09, 2017", "journal-ref": null, "doi": "10.1145/3078971.3078992", "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:49:37 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Andr\u00e9", "Fabien", "", "Technicolor"], ["Kermarrec", "Anne-Marie", "", "Inria"], ["Scouarnec", "Nicolas Le", "", "Technicolor"]]}, {"id": "1704.08215", "submitter": "Vaneet Aggarwal", "authors": "Arnob Ghosh and Vaneet Aggarwal and Feng Qian", "title": "A Rate Adaptation Algorithm for Tile-based 360-degree Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 360-degree immersive video, a user only views a part of the entire raw\nvideo frame based on her viewing direction. However, today's 360-degree video\nplayers always fetch the entire panoramic view regardless of users' head\nmovement, leading to significant bandwidth waste that can be potentially\navoided. In this paper, we propose a novel adaptive streaming scheme for\n360-degree videos. The basic idea is to fetch the invisible portion of a video\nat the lowest quality based on users' head movement prediction and to\nadaptively decide the video playback quality for the visible portion based on\nbandwidth prediction. Doing both in a robust manner requires overcome a series\nof challenges, such as jointly considering the spatial and temporal domains,\ntolerating prediction errors, and achieving low complexity. To overcome these\nchallenges, we first define quality of experience (QoE) metrics for adaptive\n360-degree video streaming. We then formulate an optimization problem and solve\nit at a low complexity. The algorithm strategically leverages both future\nbandwidth and the distribution of users' head positions to determine the\nquality level of each tile (i.e., a sub-area of a raw frame). We further\nprovide theoretical proof showing that our algorithm achieves optimality under\npractical assumptions. Numerical results show that our proposed algorithms\nsignificantly boost the user QoE by at least 20\\% compared to baseline\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:02:53 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Ghosh", "Arnob", ""], ["Aggarwal", "Vaneet", ""], ["Qian", "Feng", ""]]}, {"id": "1704.08292", "submitter": "Chenliang Xu", "authors": "Lele Chen, Sudhanshu Srivastava, Zhiyao Duan and Chenliang Xu", "title": "Deep Cross-Modal Audio-Visual Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal audio-visual perception has been a long-lasting topic in\npsychology and neurology, and various studies have discovered strong\ncorrelations in human perception of auditory and visual stimuli. Despite works\nin computational multimodal modeling, the problem of cross-modal audio-visual\ngeneration has not been systematically studied in the literature. In this\npaper, we make the first attempt to solve this cross-modal generation problem\nleveraging the power of deep generative adversarial training. Specifically, we\nuse conditional generative adversarial networks to achieve cross-modal\naudio-visual generation of musical performances. We explore different encoding\nmethods for audio and visual signals, and work on two scenarios:\ninstrument-oriented generation and pose-oriented generation. Being the first to\nexplore this new problem, we compose two new datasets with pairs of images and\nsounds of musical performances of different instruments. Our experiments using\nboth classification and human evaluations demonstrate that our model has the\nability to generate one modality, i.e., audio/visual, from the other modality,\ni.e., visual/audio, to a good extent. Our experiments on various design choices\nalong with the datasets will facilitate future research in this new problem\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 18:46:10 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Chen", "Lele", ""], ["Srivastava", "Sudhanshu", ""], ["Duan", "Zhiyao", ""], ["Xu", "Chenliang", ""]]}, {"id": "1704.08378", "submitter": "Guanshuo Xu", "authors": "Guanshuo Xu", "title": "Deep Convolutional Neural Network to Detect J-UNIWARD", "comments": "Accepted by IH&MMSec 2017. This is a personal copy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an empirical study on applying convolutional neural\nnetworks (CNNs) to detecting J-UNIWARD, one of the most secure JPEG\nsteganographic method. Experiments guiding the architectural design of the CNNs\nhave been conducted on the JPEG compressed BOSSBase containing 10,000 covers of\nsize 512x512. Results have verified that both the pooling method and the depth\nof the CNNs are critical for performance. Results have also proved that a\n20-layer CNN, in general, outperforms the most sophisticated feature-based\nmethods, but its advantage gradually diminishes on hard-to-detect cases. To\nshow that the performance generalizes to large-scale databases and to different\ncover sizes, one experiment has been conducted on the CLS-LOC dataset of\nImageNet containing more than one million covers cropped to unified size of\n256x256. The proposed 20-layer CNN has cut the error achieved by a CNN recently\nproposed for large-scale JPEG steganalysis by 35%. Source code is available via\nGitHub: https://github.com/GuanshuoXu/deep_cnn_jpeg_steganalysis\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 23:15:52 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Xu", "Guanshuo", ""]]}, {"id": "1704.08443", "submitter": "Ho Bae", "authors": "Ho Bae, Byunghan Lee, Sunyoung Kwon, Sungroh Yoon", "title": "DNA Steganalysis Using Deep Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in next-generation sequencing technologies have facilitated\nthe use of deoxyribonucleic acid (DNA) as a novel covert channels in\nsteganography. There are various methods that exist in other domains to detect\nhidden messages in conventional covert channels. However, they have not been\napplied to DNA steganography. The current most common detection approaches,\nnamely frequency analysis-based methods, often overlook important signals when\ndirectly applied to DNA steganography because those methods depend on the\ndistribution of the number of sequence characters. To address this limitation,\nwe propose a general sequence learning-based DNA steganalysis framework. The\nproposed approach learns the intrinsic distribution of coding and non-coding\nsequences and detects hidden messages by exploiting distribution variations\nafter hiding these messages. Using deep recurrent neural networks (RNNs), our\nframework identifies the distribution variations by using the classification\nscore to predict whether a sequence is to be a coding or non-coding sequence.\nWe compare our proposed method to various existing methods and biological\nsequence analysis methods implemented on top of our framework. According to our\nexperimental results, our approach delivers a robust detection performance\ncompared to other tools.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 06:16:30 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 14:20:46 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 04:04:58 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Bae", "Ho", ""], ["Lee", "Byunghan", ""], ["Kwon", "Sunyoung", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1704.08535", "submitter": "Chia-Wen Lin", "authors": "Chao Zhou, Chia-Wen Lin, Xinggong Zhang, and Zongming Guo", "title": "TFDASH: A Fairness, Stability, and Efficiency Aware Rate Control\n  Approach for Multiple Clients over DASH", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic adaptive streaming over HTTP (DASH) has recently been widely deployed\nin the Internet and adopted in the industry. It, however, does not impose any\nadaptation logic for selecting the quality of video fragments requested by\nclients and suffers from lackluster performance with respect to a number of\ndesirable properties: efficiency, stability, and fairness when multiple players\ncompete for a bottleneck link. In this paper, we propose a throughput-friendly\nDASH (TFDASH) rate control scheme for video streaming with multiple clients\nover DASH to well balance the trade-offs among efficiency, stability, and\nfairness. The core idea behind guaranteeing fairness and high efficiency\n(bandwidth utilization) is to avoid OFF periods during the downloading process\nfor all clients, i.e., the bandwidth is in perfect-subscription or\nover-subscription with bandwidth utilization approach to 100\\%. We also propose\na dual-threshold buffer model to solve the instability problem caused by the\nabove idea. As a result, by integrating these novel components, we also propose\na probability-driven rate adaption logic taking into account several key\nfactors that most influence visual quality, including buffer occupancy, video\nplayback quality, video bit-rate switching frequency and amplitude, to\nguarantee high-quality video streaming. Our experiments evidently demonstrate\nthe superior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 12:35:30 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 12:05:59 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zhou", "Chao", ""], ["Lin", "Chia-Wen", ""], ["Zhang", "Xinggong", ""], ["Guo", "Zongming", ""]]}, {"id": "1704.08657", "submitter": "David Barina", "authors": "David Barina and Michal Kula and Michal Matysek and Pavel Zemcik", "title": "Accelerating Discrete Wavelet Transforms on Parallel Architectures", "comments": "submitted on WSCG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2-D discrete wavelet transform (DWT) can be found in the heart of many\nimage-processing algorithms. Until recently, several studies have compared the\nperformance of such transform on various shared-memory parallel architectures,\nespecially on graphics processing units (GPUs). All these studies, however,\nconsidered only separable calculation schemes. We show that corresponding\nseparable parts can be merged into non-separable units, which halves the number\nof steps. In addition, we introduce an optional optimization approach leading\nto a reduction in the number of arithmetic operations. The discussed schemes\nwere adapted on the OpenCL framework and pixel shaders, and then evaluated\nusing GPUs of two biggest vendors. We demonstrate the performance of the\nproposed non-separable methods by comparison with existing separable schemes.\nThe non-separable schemes outperform their separable counterparts on numerous\nsetups, especially considering the pixel shaders.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:01:07 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 17:39:27 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Barina", "David", ""], ["Kula", "Michal", ""], ["Matysek", "Michal", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1704.08768", "submitter": "Li Li", "authors": "Li Li and Zhu Li and Xiang Ma and Haitao Yang and Houqiang Li", "title": "Co-projection-plane based 3-D padding for polyhedron projection for\n  360-degree video", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polyhedron projection for 360-degree video is becoming more and more\npopular since it can lead to much less geometry distortion compared with the\nequirectangular projection. However, in the polyhedron projection, we can\nobserve very obvious texture discontinuity in the area near the face boundary.\nSuch a texture discontinuity may lead to serious quality degradation when\nmotion compensation crosses the discontinuous face boundary. To solve this\nproblem, in this paper, we first propose to fill the corresponding neighboring\nfaces in the suitable positions as the extension of the current face to keep\napproximated texture continuity. Then a co-projection-plane based 3-D padding\nmethod is proposed to project the reference pixels in the neighboring face to\nthe current face to guarantee exact texture continuity. Under the proposed\nscheme, the reference pixel is always projected to the same plane with the\ncurrent pixel when performing motion compensation so that the texture\ndiscontinuity problem can be solved. The proposed scheme is implemented in the\nreference software of High Efficiency Video Coding. Compared with the existing\nmethod, the proposed algorithm can significantly improve the rate-distortion\nperformance. The experimental results obviously demonstrate that the texture\ndiscontinuity in the face boundary can be well handled by the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 22:48:18 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Li", "Li", ""], ["Li", "Zhu", ""], ["Ma", "Xiang", ""], ["Yang", "Haitao", ""], ["Li", "Houqiang", ""]]}, {"id": "1704.08822", "submitter": "Yaser Sadra", "authors": "Yaser Sadra", "title": "A new image compression by gradient Haar wavelet", "comments": "9 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of human communications the usage of Visual\nCommunications has also increased. The advancement of image compression methods\nis one of the main reasons for the enhancement. This paper first presents main\nmodes of image compression methods such as JPEG and JPEG2000 without\nmathematical details. Also, the paper describes gradient Haar wavelet\ntransforms in order to construct a preliminary image compression algorithm.\nThen, a new image compression method is proposed based on the preliminary image\ncompression algorithm that can improve standards of image compression. The new\nmethod is compared with original modes of JPEG and JPEG2000 (based on Haar\nwavelet) by image quality measures such as MAE, PSNAR, and SSIM. The image\nquality and statistical results confirm that can boost image compression\nstandards. It is suggested that the new method is used in a part or all of an\nimage compression standard.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 06:49:37 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 17:21:30 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Sadra", "Yaser", ""]]}]