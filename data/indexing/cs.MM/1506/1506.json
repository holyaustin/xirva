[{"id": "1506.00243", "submitter": "Hui Wang", "authors": "Hui Wang and Anthony TS Ho and Shujun Li", "title": "OR-Benchmark: An Open and Reconfigurable Digital Watermarking\n  Benchmarking Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarking digital watermarking algorithms is not an easy task because\ndifferent applications of digital watermarking often have very different sets\nof requirements and trade-offs between conflicting requirements. While there\nhave been some general-purpose digital watermarking benchmarking systems\navailable, they normally do not support complicated benchmarking tasks and\ncannot be easily reconfigured to work with different watermarking algorithms\nand testing conditions. In this paper, we propose OR-Benchmark, an open and\nhighly reconfigurable general-purpose digital watermarking benchmarking\nframework, which has the following two key features: 1) all the interfaces are\npublic and general enough to support all watermarking applications and\nbenchmarking tasks we can think of; 2) end users can easily extend the\nfunctionalities and freely configure what watermarking algorithms are tested,\nwhat system components are used, how the benchmarking process runs, and what\nresults should be produced. We implemented a prototype of this framework as a\nMATLAB software package and used it to benchmark a number of digital\nwatermarking algorithms involving two types of watermarks for content\nauthentication and self-restoration purposes. The benchmarking results\ndemonstrated the advantages of the proposed benchmarking framework, and also\ngave us some useful insights about existing image authentication and\nself-restoration watermarking algorithms which are an important but less\nstudied topic in digital watermarking.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 14:50:26 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 14:49:01 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Wang", "Hui", ""], ["Ho", "Anthony TS", ""], ["Li", "Shujun", ""]]}, {"id": "1506.00527", "submitter": "Gianluigi Ciocca", "authors": "Simone Bianco, Gianluigi Ciocca", "title": "User Preferences Modeling and Learning for Pleasing Photo Collage\n  Generation", "comments": "To be published in ACM Transactions on Multimedia Computing,\n  Communications, and Applications (TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider how to automatically create pleasing photo collages\ncreated by placing a set of images on a limited canvas area. The task is\nformulated as an optimization problem. Differently from existing\nstate-of-the-art approaches, we here exploit subjective experiments to model\nand learn pleasantness from user preferences. To this end, we design an\nexperimental framework for the identification of the criteria that need to be\ntaken into account to generate a pleasing photo collage. Five different\nthematic photo datasets are used to create collages using state-of-the-art\ncriteria. A first subjective experiment where several subjects evaluated the\ncollages, emphasizes that different criteria are involved in the subjective\ndefinition of pleasantness. We then identify new global and local criteria and\ndesign algorithms to quantify them. The relative importance of these criteria\nare automatically learned by exploiting the user preferences, and new collages\nare generated. To validate our framework, we performed several psycho-visual\nexperiments involving different users. The results shows that the proposed\nframework allows to learn a novel computational model which effectively encodes\nan inter-user definition of pleasantness. The learned definition of\npleasantness generalizes well to new photo datasets of different themes and\nsizes not used in the learning. Moreover, compared with two state of the art\napproaches, the collages created using our framework are preferred by the\nmajority of the users.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 15:20:29 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Bianco", "Simone", ""], ["Ciocca", "Gianluigi", ""]]}, {"id": "1506.00711", "submitter": "Babak Saleh", "authors": "Ahmed Elgammal and Babak Saleh", "title": "Quantifying Creativity in Art Networks", "comments": "This paper will be published in the sixth International Conference on\n  Computational Creativity (ICCC) June 29-July 2nd 2015, Park City, Utah, USA.\n  This arXiv version is an extended version of the conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we develop a computer algorithm that assesses the creativity of a\npainting given its context within art history? This paper proposes a novel\ncomputational framework for assessing the creativity of creative products, such\nas paintings, sculptures, poetry, etc. We use the most common definition of\ncreativity, which emphasizes the originality of the product and its influential\nvalue. The proposed computational framework is based on constructing a network\nbetween creative products and using this network to infer about the originality\nand influence of its nodes. Through a series of transformations, we construct a\nCreativity Implication Network. We show that inference about creativity in this\nnetwork reduces to a variant of network centrality problems which can be solved\nefficiently. We apply the proposed framework to the task of quantifying\ncreativity of paintings (and sculptures). We experimented on two datasets with\nover 62K paintings to illustrate the behavior of the proposed framework. We\nalso propose a methodology for quantitatively validating the results of the\nproposed algorithm, which we call the \"time machine experiment\".\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 00:20:54 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Saleh", "Babak", ""]]}, {"id": "1506.00765", "submitter": "Rongrong Ji Rongrong Ji", "authors": "Zheng Cai, Donglin Cao, Rongrong Ji", "title": "Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  With faster connection speed, Internet users are now making social network a\nhuge reservoir of texts, images and video clips (GIF). Sentiment analysis for\nsuch online platform can be used to predict political elections, evaluates\neconomic indicators and so on. However, GIF sentiment analysis is quite\nchallenging, not only because it hinges on spatio-temporal visual\ncontentabstraction, but also for the relationship between such abstraction and\nfinal sentiment remains unknown.In this paper, we dedicated to find out such\nrelationship.We proposed a SentiPairSequence basedspatiotemporal visual\nsentiment ontology, which forms the midlevel representations for GIFsentiment.\nThe establishment process of SentiPair contains two steps. First, we construct\nthe Synset Forest to define the semantic tree structure of visual sentiment\nlabel elements. Then, through theSynset Forest, we organically select and\ncombine sentiment label elements to form a mid-level visual sentiment\nrepresentation. Our experiments indicate that SentiPair outperforms other\ncompeting mid-level attributes. Using SentiPair, our analysis frameworkcan\nachieve satisfying prediction accuracy (72.6%). We also opened ourdataset\n(GSO-2015) to the research community. GSO-2015 contains more than 6,000\nmanually annotated GIFs out of more than 40,000 candidates. Each is labeled\nwith both sentiment and SentiPair Sequence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 06:31:57 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Cai", "Zheng", ""], ["Cao", "Donglin", ""], ["Ji", "Rongrong", ""]]}, {"id": "1506.01501", "submitter": "Nematollah Zarmehi", "authors": "Nematollah Zarmehi, Morteza Banagar, Mohammad Ali Akhaee", "title": "Optimum Decoder for an Additive Video Watermarking with Laplacian Noise\n  in H.264", "comments": null, "journal-ref": "2013 10th International ISC Conference on Information Security and\n  Cryptology (ISCISC),Aug. 2013, pp. 1-5", "doi": "10.1109/ISCISC.2013.6767352", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate an additive video watermarking method in H.264\nstandard in presence of the Laplacian noise. In some applications, due to the\nloss of some pixels or a region of a frame, we resort to Laplacian noise rather\nthan Gaussian one. The embedding is performed in the transform domain; while an\noptimum and a sub-optimum decoder are derived for the proposed Laplacian model.\nSimulation results show that the proposed watermarking scheme has suitable\nperformance with enough transparency required for watermarking applications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 08:10:13 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Zarmehi", "Nematollah", ""], ["Banagar", "Morteza", ""], ["Akhaee", "Mohammad Ali", ""]]}, {"id": "1506.01952", "submitter": "Esmaeil Kokabifar", "authors": "E. Kokabifar, G.B. Loghmani and A. Latif", "title": "Digital image watermarking using normal matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents techniques for digital image watermarking based on\neigenvalue decomposition of normal matrices. The introduced methods are\nconvenient and self-explanatory, achieve satisfactory results, as well as\nrequire less and easy computations compared to some current methods. Through\nthe proposed methods, host images and watermarks are transformed to the space\nof normal matrices, and the properties of spectral decompositions are dealt\nwith to obtain watermarked images. Watermark extraction is carried out via a\nprocedure similar to embedding. Experimental results are provided to illustrate\nthe reliability and robustness of the methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 12:53:38 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Kokabifar", "E.", ""], ["Loghmani", "G. B.", ""], ["Latif", "A.", ""]]}, {"id": "1506.01977", "submitter": "Tim Weninger PhD", "authors": "Maria Glenski, Thomas J. Johnston, Tim Weninger", "title": "Random Voting Effects in Social-Digital Spaces: A case study of Reddit\n  Post Submissions", "comments": "Paper preprint accepted to 2015 ACM Hypertext Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  At a time when information seekers first turn to digital sources for news and\nopinion, it is critical that we understand the role that social media plays in\nhuman behavior. This is especially true when information consumers also act as\ninformation producers and editors by their online activity. In order to better\nunderstand the effects that editorial ratings have on online human behavior, we\nreport the results of a large-scale in-vivo experiment in social media. We find\nthat small, random rating manipulations on social media submissions created\nsignificant changes in downstream ratings resulting in significantly different\nfinal outcomes. Positive treatment resulted in a positive effect that increased\nthe final rating by 11.02% on average. Compared to the control group, positive\ntreatment also increased the probability of reaching a high rating (>=2000) by\n24.6%. Contrary to the results of related work we also find that negative\ntreatment resulted in a negative effect that decreased the final rating by\n5.15% on average.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 17:11:53 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Glenski", "Maria", ""], ["Johnston", "Thomas J.", ""], ["Weninger", "Tim", ""]]}, {"id": "1506.02071", "submitter": "Krzysztof Szczypiorski", "authors": "Jason Hiney, Tejas Dakve, Krzysztof Szczypiorski, Kris Gaj", "title": "Using Facebook for Image Steganography", "comments": "6 pages, 4 figures, 2 tables. Accepted to Fourth International\n  Workshop on Cyber Crime (IWCC 2015), co-located with 10th International\n  Conference on Availability, Reliability and Security (ARES 2015), Toulouse,\n  France, 24-28 August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because Facebook is available on hundreds of millions of desktop and mobile\ncomputing platforms around the world and because it is available on many\ndifferent kinds of platforms (from desktops and laptops running Windows, Unix,\nor OS X to hand held devices running iOS, Android, or Windows Phone), it would\nseem to be the perfect place to conduct steganography. On Facebook, information\nhidden in image files will be further obscured within the millions of pictures\nand other images posted and transmitted daily. Facebook is known to alter and\ncompress uploaded images so they use minimum space and bandwidth when displayed\non Facebook pages. The compression process generally disrupts attempts to use\nFacebook for image steganography. This paper explores a method to minimize the\ndisruption so JPEG images can be used as steganography carriers on Facebook.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 21:16:22 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Hiney", "Jason", ""], ["Dakve", "Tejas", ""], ["Szczypiorski", "Krzysztof", ""], ["Gaj", "Kris", ""]]}, {"id": "1506.02100", "submitter": "Khan Muhammad", "authors": "Khan Muhammad, Muhammad Sajjad, Irfan Mehmood, Seungmin Rho, Sung Wook\n  Baik", "title": "A novel magic LSB substitution method (M-LSB-SM) using multi-level\n  encryption and achromatic component of an image", "comments": "This paper has been published in Multimedia Tools and Applications\n  Journal with impact factor=1.058. The readers can study the formatted paper\n  using the following link:\n  http://link.springer.com/article/10.1007/s11042-015-2671-9. Please use\n  sci-hub.org for downloading this paper if you are unable to access it freely\n  or email us at khan.muhammad.icp@gmail.com", "journal-ref": "Multimedia Tools and Applications, pp. 1-27, 2015", "doi": "10.1007/s11042-015-2671-9", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Steganography is a thriving research area of information security where\nsecret data is embedded in images to hide its existence while getting the\nminimum possible statistical detectability. This paper proposes a novel magic\nleast significant bit substitution method (M-LSB-SM) for RGB images. The\nproposed method is based on the achromatic component (I-plane) of the\nhue-saturation-intensity (HSI) color model and multi-level encryption (MLE) in\nthe spatial domain. The input image is transposed and converted into an HSI\ncolor space. The I-plane is divided into four sub-images of equal size,\nrotating each sub-image with a different angle using a secret key. The secret\ninformation is divided into four blocks, which are then encrypted using an MLE\nalgorithm (MLEA). Each sub-block of the message is embedded into one of the\nrotated sub-images based on a specific pattern using magic LSB substitution.\nExperimental results validate that the proposed method not only enhances the\nvisual quality of stego images but also provides good imperceptibility and\nmultiple security levels as compared to several existing prominent methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 01:50:29 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Muhammad", "Khan", ""], ["Sajjad", "Muhammad", ""], ["Mehmood", "Irfan", ""], ["Rho", "Seungmin", ""], ["Baik", "Sung Wook", ""]]}, {"id": "1506.02311", "submitter": "Krzysztof Szczypiorski", "authors": "Wojciech Fraczek, Krzysztof Szczypiorski", "title": "StegBlocks: ensuring perfect undetectability of network steganography", "comments": "6 pages, 1 figure, Accepted to Fourth International Workshop on Cyber\n  Crime (IWCC 2015), co-located with 10th International Conference on\n  Availability, Reliability and Security (ARES 2015), Toulouse, France, 24-28\n  August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents StegBlocks, which defines a new concept for performing\nundetectable hidden communication. StegBlocks is a general approach for\nconstructing methods of network steganography. In StegBlocks, one has to\ndetermine objects with defined properties which will be used to transfer hidden\nmessages. The objects are dependent on a specific network protocol (or\napplication) used as a carrier for a given network steganography method.\nMoreover, the paper presents the approach to perfect undetectability of network\nsteganography, which was developed based on the rules of undetectability for\ngeneral steganography. The approach to undetectability of network steganography\nwas used to show the possibility of developing perfectly undetectable network\nsteganography methods using the StegBlocks concept.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 20:46:27 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Fraczek", "Wojciech", ""], ["Szczypiorski", "Krzysztof", ""]]}, {"id": "1506.03681", "submitter": "Fatema Akhter", "authors": "Fatema Akhter", "title": "A Novel Approach for Image Steganography in Spatial Domain", "comments": null, "journal-ref": "Global Journal of Computer Science and Technology, Volume 13,\n  Issue 7, pp. 1-6 Year 2013", "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for hiding information in digital image in\nspatial domain. In this approach three bits of message is embedded in a pixel\nusing Lucas number system but only one bit plane is allowed for alternation.\nThe experimental results show that the proposed method has the larger capacity\nof embedding data, high peak signal to noise ratio compared to existing methods\nand is hardly detectable for steganolysis algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 14:05:45 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Akhter", "Fatema", ""]]}, {"id": "1506.04904", "submitter": "Reza Sabzevari", "authors": "Jonas Schuler, Reza Sabzevari and Davide Scaramuzza", "title": "LightPanel: Active Mobile Platform for Dense 3D Modelling", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel platform for dense 3D modelling. This\nplatform is an active image acquisition setup assisted with a set of light\nsources and a distance sensor. The hardware setup is designed for being mounted\non a mobile robot which is remotely driven to create accurate dense 3D models\nfrom out-of-reach objects. For this reason, the object is actively illuminated\nby the imaging setup and Photometric Stereo is used to recover the dense 3D\nmodel. The proposed image acquisition setup, called LightPanel, is described\nfrom design to calibration and discusses the practical challenges of using\nPhotometric Stereo under uncontrolled lighting conditions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 10:30:34 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Schuler", "Jonas", ""], ["Sabzevari", "Reza", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1506.06312", "submitter": "Junfei Huang", "authors": "Junfei Huang and Guochu Shou", "title": "A QoS Guarantee Strategy for Multimedia Conferencing based on Bayesian\n  Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service Oriented Architecture (SOA) is commonly employed in the design and\nimplementation of web service systems. The key technology to enable media\ncommunications in the context of SOA is the Service Oriented Communication. To\nexploit the advantage of SOA, we design and implement a web-based multimedia\nconferencing system that provides users with a hybrid orchestration of web and\ncommunication services. As the current SOA lacks effective QoS guarantee\nsolutions for multimedia services, the user satisfaction is greatly challenged\nwith QoS violations, e.g., low video PSNR (Peak Signal-to-Noise Ratio) and long\nplayback delay. Motivated by addressing the critical problem, we firstly employ\nthe Business Process Execution Language (BPEL) service engine for the hybrid\nservices orchestration and execution. Secondly, we propose a novel\ncontext-aware approach to quantify and leverage the causal relationships\nbetween QoS metrics and available contexts based on Bayesian networks (CABIN).\nThis approach includes three phases: (1) information discretization, (2) causal\nrelationship profiling, and (3) optimal context tuning. We implement CABIN in a\nreal-life multimedia conferencing system and compare its performance with\nexisting delay and throughput oriented schemes. Experimental results show that\nCABIN outperforms the competing approaches in improving the video quality in\nterms of PSNR. It also provides a one-stop shop controls both the web and\ncommunication services.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 02:18:34 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Huang", "Junfei", ""], ["Shou", "Guochu", ""]]}, {"id": "1506.07823", "submitter": "Ana De Abreu", "authors": "Ana De Abreu, Laura Toni, Nikolaos Thomos, Thomas Maugey, Fernando\n  Pereira, Pascal Frossard", "title": "Optimal Layered Representation for Adaptive Interactive Multiview Video\n  Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an interactive multiview video streaming (IMVS) system where\nclients select their preferred viewpoint in a given navigation window. To\nprovide high quality IMVS, many high quality views should be transmitted to the\nclients. However, this is not always possible due to the limited and\nheterogeneous capabilities of the clients. In this paper, we propose a novel\nadaptive IMVS solution based on a layered multiview representation where camera\nviews are organized into layered subsets to match the different clients\nconstraints. We formulate an optimization problem for the joint selection of\nthe views subsets and their encoding rates. Then, we propose an optimal and a\nreduced computational complexity greedy algorithms, both based on\ndynamic-programming. Simulation results show the good performance of our novel\nalgorithms compared to a baseline algorithm, proving that an effective IMVS\nadaptive solution should consider the scene content and the client capabilities\nand their preferences in navigation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 17:24:13 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["De Abreu", "Ana", ""], ["Toni", "Laura", ""], ["Thomos", "Nikolaos", ""], ["Maugey", "Thomas", ""], ["Pereira", "Fernando", ""], ["Frossard", "Pascal", ""]]}, {"id": "1506.08125", "submitter": "Zhi Wang Dr.", "authors": "Zhi Wang", "title": "Data-driven Approaches for Social Video Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has recently witnessed the convergence of online social network\nservices and online video services: users import videos from content sharing\nsites, and propagate them along the social connections by re-sharing them. Such\nsocial behaviors have dramatically reshaped how videos are disseminated, and\nthe users are now actively engaged to be part of the social ecosystem, rather\nthan being passively consumers. Despite the increasingly abundant bandwidth and\ncomputation resources, the ever increasing data volume of user generated video\ncontent and the boundless coverage of socialized sharing have presented\nunprecedented challenges. In this paper, we first presents the challenges in\nsocial-aware video delivery. Then, we present a principal framework for\ndata-driven social video delivery approaches. Moreover, we identify the unique\ncharacteristics of social-aware video access and the social content\npropagation, and closely reveal the design of individual modules and their\nintegration towards enhancing users' experience in the social network context.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 15:46:24 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Wang", "Zhi", ""]]}, {"id": "1506.08126", "submitter": "Dragomir", "authors": "Dragomir Radev and Amanda Stent and Joel Tetreault and Aasish Pappu\n  and Aikaterini Iliakopoulou and Agustin Chanfreau and Paloma de Juan and\n  Jordi Vallmitjana and Alejandro Jaimes and Rahul Jha and Bob Mankoff", "title": "Humor in Collective Discourse: Unsupervised Funniness Detection in the\n  New Yorker Cartoon Caption Contest", "comments": "10 pages, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The New Yorker publishes a weekly captionless cartoon. More than 5,000\nreaders submit captions for it. The editors select three of them and ask the\nreaders to pick the funniest one. We describe an experiment that compares a\ndozen automatic methods for selecting the funniest caption. We show that\nnegative sentiment, human-centeredness, and lexical centrality most strongly\nmatch the funniest captions, followed by positive sentiment. These results are\nuseful for understanding humor and also in the design of more engaging\nconversational agents in text and multimodal (vision+text) systems. As part of\nthis work, a large set of cartoons and captions is being made available to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 15:48:10 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Radev", "Dragomir", ""], ["Stent", "Amanda", ""], ["Tetreault", "Joel", ""], ["Pappu", "Aasish", ""], ["Iliakopoulou", "Aikaterini", ""], ["Chanfreau", "Agustin", ""], ["de Juan", "Paloma", ""], ["Vallmitjana", "Jordi", ""], ["Jaimes", "Alejandro", ""], ["Jha", "Rahul", ""], ["Mankoff", "Bob", ""]]}, {"id": "1506.08316", "submitter": "Jianshu Chao", "authors": "Jianshu Chao and Eckehard Steinbach", "title": "Keypoint Encoding for Improved Feature Extraction from Compressed Video\n  at Low Bitrates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many mobile visual analysis applications, compressed video is transmitted\nover a communication network and analyzed by a server. Typical processing steps\nperformed at the server include keypoint detection, descriptor calculation, and\nfeature matching. Video compression has been shown to have an adverse effect on\nfeature-matching performance. The negative impact of compression can be reduced\nby using the keypoints extracted from the uncompressed video to calculate\ndescriptors from the compressed video. Based on this observation, we propose to\nprovide these keypoints to the server as side information and to extract only\nthe descriptors from the compressed video. First, we introduce four different\nframe types for keypoint encoding to address different types of changes in\nvideo content. These frame types represent a new scene, the same scene, a\nslowly changing scene, or a rapidly moving scene and are determined by\ncomparing features between successive video frames. Then, we propose Intra,\nSkip and Inter modes of encoding the keypoints for different frame types. For\nexample, keypoints for new scenes are encoded using the Intra mode, and\nkeypoints for unchanged scenes are skipped. As a result, the bitrate of the\nside information related to keypoint encoding is significantly reduced.\nFinally, we present pairwise matching and image retrieval experiments conducted\nto evaluate the performance of the proposed approach using the Stanford mobile\naugmented reality dataset and 720p format videos. The results show that the\nproposed approach offers significantly improved feature matching and image\nretrieval performance at a given bitrate.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 17:33:34 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 16:43:42 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Chao", "Jianshu", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1506.08348", "submitter": "Mohammad Salahuddin", "authors": "Mohammad A. Salahuddin, Halima Elbiaze, Wessam Ajib and Roch Glitho", "title": "Social Network Analysis Inspired Content Placement with QoS in\n  Cloud-based Content Delivery Networks", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content Placement (CP) problem in Cloud-based Content Delivery Networks\n(CCDNs) leverage resource elasticity to build cost effective CDNs that\nguarantee QoS. In this paper, we present our novel CP model, which optimally\nplaces content on surrogates in the cloud, to achieve (a) minimum cost of\nleasing storage and bandwidth resources for data coming into and going out of\nthe cloud zones and regions, (b) guarantee Service Level Agreement (SLA), and\n(c) minimize degree of QoS violations. The CP problem is NP-Hard, hence we\ndesign a unique push-based heuristic, called Weighted Social Network Analysis\n(W-SNA) for CCDN providers. W-SNA is based on Betweeness Centrality (BC) from\nSNA and prioritizes surrogates based on their relationship to the other\nvertices in the network graph. To achieve our unique objectives, we further\nprioritize surrogates based on weights derived from storage cost and content\nrequests. We compare our heuristic to current state of the art Greedy Site (GS)\nand purely Social Network Analysis (SNA) heuristics, which are relevant to our\nwork. We show that W-SNA outperforms GS and SNA in minimizing cost and QoS.\nMoreover, W-SNA guarantees SLA but also minimizes the degree of QoS violations.\nTo the best of our knowledge, this is the first model and heuristic of its\nkind, which is timely and gives a fundamental pre-allocation scheme for future\nonline and dynamic resource provision for CCDNs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 03:25:27 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 16:50:07 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2015 16:05:46 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Salahuddin", "Mohammad A.", ""], ["Elbiaze", "Halima", ""], ["Ajib", "Wessam", ""], ["Glitho", "Roch", ""]]}, {"id": "1506.08811", "submitter": "Esmaeil Kokabifar", "authors": "E. Kokabifar, G.B. Loghmani and A. Latif", "title": "A new approach for image compression using normal matrices", "comments": "arXiv admin note: text overlap with arXiv:1506.01952", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present methods for image compression on the basis of\neigenvalue decomposition of normal matrices. The proposed methods are\nconvenient and self-explanatory, requiring fewer and easier computations as\ncompared to some existing methods. Through the proposed techniques, the image\nis transformed to the space of normal matrices. Then, the properties of\nspectral decomposition are dealt with to obtain compressed images. Experimental\nresults are provided to illustrate the validity of the methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 21:43:59 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Kokabifar", "E.", ""], ["Loghmani", "G. B.", ""], ["Latif", "A.", ""]]}, {"id": "1506.08898", "submitter": "Junhui Hou", "authors": "Junhui Hou, Lap-Pui Chau, Nadia Magnenat-Thalmann, Ying He", "title": "Low-latency compression of mocap data using learned spatial\n  decorrelation transform", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the growing needs of human motion capture (mocap) in movie, video\ngames, sports, etc., it is highly desired to compress mocap data for efficient\nstorage and transmission. This paper presents two efficient frameworks for\ncompressing human mocap data with low latency. The first framework processes\nthe data in a frame-by-frame manner so that it is ideal for mocap data\nstreaming and time critical applications. The second one is clip-based and\nprovides a flexible tradeoff between latency and compression performance. Since\nmocap data exhibits some unique spatial characteristics, we propose a very\neffective transform, namely learned orthogonal transform (LOT), for reducing\nthe spatial redundancy. The LOT problem is formulated as minimizing square\nerror regularized by orthogonality and sparsity and solved via alternating\niteration. We also adopt a predictive coding and temporal DCT for temporal\ndecorrelation in the frame- and clip-based frameworks, respectively.\nExperimental results show that the proposed frameworks can produce higher\ncompression performance at lower computational cost and latency than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 23:47:00 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2015 05:56:11 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 02:53:43 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""], ["Magnenat-Thalmann", "Nadia", ""], ["He", "Ying", ""]]}]