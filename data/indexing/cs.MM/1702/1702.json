[{"id": "1702.00182", "submitter": "Ryuji Hirayama", "authors": "Ryuji Hirayama, Tomotaka Suzuki, Tomoyoshi Shimobaba, Atsushi Shiraki,\n  Makoto Naruse, Hirotaka Nakayama, Takashi Kakue and Tomoyoshi Ito", "title": "Inkjet printing-based volumetric display projecting multiple full-colour\n  2D patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a method to construct a full-colour volumetric display is\npresented using a commercially available inkjet printer. Photoreactive\nluminescence materials are minutely and automatically printed as the volume\nelements, and volumetric displays are constructed with high resolution using\neasy-to-fabricate means that exploit inkjet printing technologies. The results\nexperimentally demonstrate the first prototype of an inkjet printing-based\nvolumetric display composed of multiple layers of transparent films that yield\na full-colour three-dimensional (3D) image. Moreover, we propose a design\nalgorithm with 3D structures that provide multiple different 2D full-colour\npatterns when viewed from different directions and experimentally demonstrates\nprototypes. It is considered that these types of 3D volumetric structures and\ntheir fabrication methods based on widely deployed existing printing\ntechnologies can be utilised as novel information display devices and systems,\nincluding digital signage, media art, entertainment and security.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:01:44 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Hirayama", "Ryuji", ""], ["Suzuki", "Tomotaka", ""], ["Shimobaba", "Tomoyoshi", ""], ["Shiraki", "Atsushi", ""], ["Naruse", "Makoto", ""], ["Nakayama", "Hirotaka", ""], ["Kakue", "Takashi", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1702.00817", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra", "title": "DCT-like Transform for Image Compression Requires 14 Additions Only", "comments": "6 pages, 3 figures, 1 table", "journal-ref": "Electronics Letters, Volume 48, Issue 15, pp. 919-921 (2012)", "doi": "10.1049/el.2012.1148", "report-no": null, "categories": "cs.MM cs.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-complexity 8-point orthogonal approximate DCT is introduced. The\nproposed transform requires no multiplications or bit-shift operations. The\nderived fast algorithm requires only 14 additions, less than any existing DCT\napproximation. Moreover, in several image compression scenarios, the proposed\ntransform could outperform the well-known signed DCT, as well as\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 20:08:42 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1702.01325", "submitter": "Mohsen Moradi", "authors": "Mohsen Moradi, Mohammad-Reza Sadeghi", "title": "Combining and Steganography of 3D Face Textures", "comments": "6 pages, 10 figures, 16 equations, 5 sections", "journal-ref": "Volume 5, Issue 2 - Issue Serial Number 10, Autumn 2017, Page 1-1", "doi": "10.22061/JECEI.2017.690", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the serious issues in communication between people is hiding\ninformation from others, and the best way for this, is deceiving them. Since\nnowadays face images are mostly used in three dimensional format, in this paper\nwe are going to steganography 3D face images, detecting which by curious people\nwill be impossible. As in detecting face only its texture is important, we\nseparate texture from shape matrices, for eliminating half of the extra\ninformation, steganography is done only for face texture, and for\nreconstructing 3D face, we can use any other shape. Moreover, we will indicate\nthat, by using two textures, how two 3D faces can be combined. For a complete\ndescription of the process, first, 2D faces are used as an input for building\n3D faces, and then 3D textures are hidden within other images.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 18:55:16 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 17:37:43 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Moradi", "Mohsen", ""], ["Sadeghi", "Mohammad-Reza", ""]]}, {"id": "1702.01528", "submitter": "Jinsoo Choi", "authors": "Jinsoo Choi, Tae-Hyun Oh, In So Kweon", "title": "Contextually Customized Video Summaries via Natural Language", "comments": "To appear in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best summary of a long video differs among different people due to its\nhighly subjective nature. Even for the same person, the best summary may change\nwith time or mood. In this paper, we introduce the task of generating\ncustomized video summaries through simple text. First, we train a deep\narchitecture to effectively learn semantic embeddings of video frames by\nleveraging the abundance of image-caption data via a progressive and residual\nmanner. Given a user-specific text description, our algorithm is able to select\nsemantically relevant video segments and produce a temporally aligned video\nsummary. In order to evaluate our textually customized video summaries, we\nconduct experimental comparison with baseline methods that utilize ground-truth\ninformation. Despite the challenging baselines, our method still manages to\nshow comparable or even exceeding performance. We also show that our method is\nable to generate semantically diverse video summaries by only utilizing the\nlearned visual embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 08:31:44 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 11:37:58 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 06:13:45 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Choi", "Jinsoo", ""], ["Oh", "Tae-Hyun", ""], ["Kweon", "In So", ""]]}, {"id": "1702.01805", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra, A. Edirisuriya, A. Madanayake", "title": "A Digital Hardware Fast Algorithm and FPGA-based Prototype for a Novel\n  16-point Approximate DCT for Image Compression Applications", "comments": "17 pages, 6 figures, 6 tables", "journal-ref": "Measurement Science and Technology, Volume 23, Number 11, 2012", "doi": "10.1088/0957-0233/23/11/114010", "report-no": null, "categories": "cs.MM cs.AR cs.DS cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete cosine transform (DCT) is the key step in many image and video\ncoding standards. The 8-point DCT is an important special case, possessing\nseveral low-complexity approximations widely investigated. However, 16-point\nDCT transform has energy compaction advantages. In this sense, this paper\npresents a new 16-point DCT approximation with null multiplicative complexity.\nThe proposed transform matrix is orthogonal and contains only zeros and ones.\nThe proposed transform outperforms the well-know Walsh-Hadamard transform and\nthe current state-of-the-art 16-point approximation. A fast algorithm for the\nproposed transform is also introduced. This fast algorithm is experimentally\nvalidated using hardware implementations that are physically realized and\nverified on a 40 nm CMOS Xilinx Virtex-6 XC6VLX240T FPGA chip for a maximum\nclock rate of 342 MHz. Rapid prototypes on FPGA for 8-bit input word size shows\nsignificant improvement in compressed image quality by up to 1-2 dB at the cost\nof only eight adders compared to the state-of-art 16-point DCT approximation\nalgorithm in the literature [S. Bouguezel, M. O. Ahmad, and M. N. S. Swamy. A\nnovel transform for image compression. In {\\em Proceedings of the 53rd IEEE\nInternational Midwest Symposium on Circuits and Systems (MWSCAS)}, 2010].\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 22:00:34 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Edirisuriya", "A.", ""], ["Madanayake", "A.", ""]]}, {"id": "1702.02258", "submitter": "Ehsan Jahangiri", "authors": "Ehsan Jahangiri, Alan L. Yuille", "title": "Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with\n  2D Joint Detections", "comments": "accepted to ICCV 2017 (PeopleCap)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method to generate multiple diverse and valid human pose\nhypotheses in 3D all consistent with the 2D detection of joints in a monocular\nRGB image. We use a novel generative model uniform (unbiased) in the space of\nanatomically plausible 3D poses. Our model is compositional (produces a pose by\ncombining parts) and since it is restricted only by anatomical constraints it\ncan generalize to every plausible human 3D pose. Removing the model bias\nintrinsically helps to generate more diverse 3D pose hypotheses. We argue that\ngenerating multiple pose hypotheses is more reasonable than generating only a\nsingle 3D pose based on the 2D joint detection given the depth ambiguity and\nthe uncertainty due to occlusion and imperfect 2D joint detection. We hope that\nthe idea of generating multiple consistent pose hypotheses can give rise to a\nnew line of future work that has not received much attention in the literature.\nWe used the Human3.6M dataset for empirical evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 02:54:25 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 19:39:19 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Jahangiri", "Ehsan", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1702.03790", "submitter": "Ralph Ewerth", "authors": "Markus M\\\"uhling, Manja Meister, Nikolaus Korfhage, J\\\"org Wehling,\n  Angelika H\\\"orth, Ralph Ewerth, Bernd Freisleben", "title": "Content-Based Video Retrieval in Historical Collections of the German\n  Broadcasting Archive", "comments": "TPDL 2016, Hannover, Germany. Final version is available at Springer\n  via DOI", "journal-ref": null, "doi": "10.1007/978-3-319-43997-6_6", "report-no": null, "categories": "cs.DL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The German Broadcasting Archive (DRA) maintains the cultural heritage of\nradio and television broadcasts of the former German Democratic Republic (GDR).\nThe uniqueness and importance of the video material stimulates a large\nscientific interest in the video content. In this paper, we present an\nautomatic video analysis and retrieval system for searching in historical\ncollections of GDR television recordings. It consists of video analysis\nalgorithms for shot boundary detection, concept classification, person\nrecognition, text recognition and similarity search. The performance of the\nsystem is evaluated from a technical and an archival perspective on 2,500 hours\nof GDR television recordings.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 14:42:31 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["M\u00fchling", "Markus", ""], ["Meister", "Manja", ""], ["Korfhage", "Nikolaus", ""], ["Wehling", "J\u00f6rg", ""], ["H\u00f6rth", "Angelika", ""], ["Ewerth", "Ralph", ""], ["Freisleben", "Bernd", ""]]}, {"id": "1702.05718", "submitter": "Seyed Hamid Safavi", "authors": "Seyed Hamid Safavi and Farah Torkamani-Azar", "title": "Perceptual Compressive Sensing based on Contrast Sensitivity Function:\n  Can we avoid non-visible redundancies acquisition?", "comments": "Accepted for publication in 25'th Iranian Conference on Electrical\n  Engineering (ICEE2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel CS approach in which the acquisition of\nnon-visible information is also avoided.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 08:21:20 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 09:40:44 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 08:13:53 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Safavi", "Seyed Hamid", ""], ["Torkamani-Azar", "Farah", ""]]}, {"id": "1702.05878", "submitter": "Mengfan Tang", "authors": "Mengfan Tang, Feiping Nie, Siripen Pongpaichet, Ramesh Jain", "title": "From Photo Streams to Evolving Situations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photos are becoming spontaneous, objective, and universal sources of\ninformation. This paper develops evolving situation recognition using photo\nstreams coming from disparate sources combined with the advances of deep\nlearning. Using visual concepts in photos together with space and time\ninformation, we formulate the situation detection into a semi-supervised\nlearning framework and propose new graph-based models to solve the problem. To\nextend the method for unknown situations, we introduce a soft label method\nwhich enables the traditional semi-supervised learning framework to accurately\npredict predefined labels as well as effectively form new clusters. To overcome\nthe noisy data which degrades graph quality, leading to poor recognition\nresults, we take advantage of two kinds of noise-robust norms which can\neliminate the adverse effects of outliers in visual concepts and improve the\naccuracy of situation recognition. Finally, we demonstrate the idea and the\neffectiveness of the proposed model on Yahoo Flickr Creative Commons 100\nMillion.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 06:53:21 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Tang", "Mengfan", ""], ["Nie", "Feiping", ""], ["Pongpaichet", "Siripen", ""], ["Jain", "Ramesh", ""]]}, {"id": "1702.05957", "submitter": "Qi Liu", "authors": "Shubham Goyal, Qi Liu, Khairina Tajul-Arifin, Waqas Awan, Bimlesh\n  Wadhwa, Zhenguang Liu", "title": "I Ate This: A Photo-based Food Journaling System with Expert Feedback", "comments": "5 pages, 5 figures, CHI 2016 DIY Health Workshop (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What we eat is one of the most frequent and important health decisions we\nmake in daily life, yet it remains notoriously difficult to capture and\nunderstand. Effective food journaling is thus a grand challenge in personal\nhealth informatics. In this paper we describe a system for food journaling\ncalled I Ate This, which is inspired by the Remote Food Photography Method\n(RFPM). I Ate This is simple: you use a smartphone app to take a photo and give\na very basic description of any food or beverage you are about to consume.\nLater, a qualified dietitian will evaluate your photo, giving you feedback on\nhow you did and where you can improve. The aim of I Ate This is to provide a\nconvenient, visual and reliable way to help users learn from their eating\nhabits and nudge them towards better choices each and every day. Ultimately,\nthis incremental approach can lead to long-term behaviour change. Our goal is\nto bring RFPM to a wider audience, through APIs that can be incorporated into\nother apps.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:13:53 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 06:35:34 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Goyal", "Shubham", ""], ["Liu", "Qi", ""], ["Tajul-Arifin", "Khairina", ""], ["Awan", "Waqas", ""], ["Wadhwa", "Bimlesh", ""], ["Liu", "Zhenguang", ""]]}, {"id": "1702.06151", "submitter": "Tal Yarkoni", "authors": "Quinten McNamara, Alejandro de la Vega, and Tal Yarkoni", "title": "Developing a comprehensive framework for multimodal feature extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a critical component of many applied data science\nworkflows. In recent years, rapid advances in artificial intelligence and\nmachine learning have led to an explosion of feature extraction tools and\nservices that allow data scientists to cheaply and effectively annotate their\ndata along a vast array of dimensions---ranging from detecting faces in images\nto analyzing the sentiment expressed in coherent text. Unfortunately, the\nproliferation of powerful feature extraction services has been mirrored by a\ncorresponding expansion in the number of distinct interfaces to feature\nextraction services. In a world where nearly every new service has its own API,\ndocumentation, and/or client library, data scientists who need to combine\ndiverse features obtained from multiple sources are often forced to write and\nmaintain ever more elaborate feature extraction pipelines. To address this\nchallenge, we introduce a new open-source framework for comprehensive\nmultimodal feature extraction. Pliers is an open-source Python package that\nsupports standardized annotation of diverse data types (video, images, audio,\nand text), and is expressly with both ease-of-use and extensibility in mind.\nUsers can apply a wide range of pre-existing feature extraction tools to their\ndata in just a few lines of Python code, and can also easily add their own\ncustom extractors by writing modular classes. A graph-based API enables rapid\ndevelopment of complex feature extraction pipelines that output results in a\nsingle, standardized format. We describe the package's architecture, detail its\nmajor advantages over previous feature extraction toolboxes, and use a sample\napplication to a large functional MRI dataset to illustrate how pliers can\nsignificantly reduce the time and effort required to construct sophisticated\nfeature extraction workflows while increasing code clarity and maintainability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 19:22:21 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["McNamara", "Quinten", ""], ["de la Vega", "Alejandro", ""], ["Yarkoni", "Tal", ""]]}, {"id": "1702.06228", "submitter": "Yanwei  Fu", "authors": "Yu-ting Qiang, Yanwei Fu, Xiao Yu, Yanwen Guo, Zhi-Hua Zhou and Leonid\n  Sigal", "title": "Learning to Generate Posters of Scientific Papers by Probabilistic\n  Graphical Models", "comments": "10 pages, submission to IEEE TPAMI. arXiv admin note: text overlap\n  with arXiv:1604.01219", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often summarize their work in the form of scientific posters.\nPosters provide a coherent and efficient way to convey core ideas expressed in\nscientific papers. Generating a good scientific poster, however, is a complex\nand time consuming cognitive task, since such posters need to be readable,\ninformative, and visually aesthetic. In this paper, for the first time, we\nstudy the challenging problem of learning to generate posters from scientific\npapers. To this end, a data-driven framework, that utilizes graphical models,\nis proposed. Specifically, given content to display, the key elements of a good\nposter, including attributes of each panel and arrangements of graphical\nelements are learned and inferred from data. During the inference stage, an MAP\ninference framework is employed to incorporate some design principles. In order\nto bridge the gap between panel attributes and the composition within each\npanel, we also propose a recursive page splitting algorithm to generate the\npanel layout for a poster. To learn and validate our model, we collect and\nrelease a new benchmark dataset, called NJU-Fudan Paper-Poster dataset, which\nconsists of scientific papers and corresponding posters with exhaustively\nlabelled panels and attributes. Qualitative and quantitative results indicate\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 01:02:56 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Qiang", "Yu-ting", ""], ["Fu", "Yanwei", ""], ["Yu", "Xiao", ""], ["Guo", "Yanwen", ""], ["Zhou", "Zhi-Hua", ""], ["Sigal", "Leonid", ""]]}, {"id": "1702.06277", "submitter": "Li Li", "authors": "Li Li, Zhu Li, Madhukar Budagavi, and Houqiang Li", "title": "Projection based advanced motion model for cubic mapping for 360-degree\n  video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel advanced motion model to handle the irregular\nmotion for the cubic map projection of 360-degree video. Since the irregular\nmotion is mainly caused by the projection from the sphere to the cube map, we\nfirst try to project the pixels in both the current picture and reference\npicture from unfolding cube back to the sphere. Then through utilizing the\ncharacteristic that most of the motions in the sphere are uniform, we can\nderive the relationship between the motion vectors of various pixels in the\nunfold cube. The proposed advanced motion model is implemented in the High\nEfficiency Video Coding reference software. Experimental results demonstrate\nthat quite obvious performance improvement can be achieved for the sequences\nwith obvious motions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 06:35:01 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Li", "Li", ""], ["Li", "Zhu", ""], ["Budagavi", "Madhukar", ""], ["Li", "Houqiang", ""]]}, {"id": "1702.06297", "submitter": "Li Li", "authors": "Li Li, Houqiang Li, Dong Liu, Haitao Yang, Sixin Lin, Huanbang Chen,\n  and Feng Wu", "title": "An Efficient Four-Parameter Affine Motion Model for Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a simplified affine motion model based coding\nframework to overcome the limitation of translational motion model and maintain\nlow computational complexity. The proposed framework mainly has three key\ncontributions. First, we propose to reduce the number of affine motion\nparameters from 6 to 4. The proposed four-parameter affine motion model can not\nonly handle most of the complex motions in natural videos but also save the\nbits for two parameters. Second, to efficiently encode the affine motion\nparameters, we propose two motion prediction modes, i.e., advanced affine\nmotion vector prediction combined with a gradient-based fast affine motion\nestimation algorithm and affine model merge, where the latter attempts to reuse\nthe affine motion parameters (instead of the motion vectors) of neighboring\nblocks. Third, we propose two fast affine motion compensation algorithms. One\nis the one-step sub-pixel interpolation, which reduces the computations of each\ninterpolation. The other is the interpolation-precision-based adaptive block\nsize motion compensation, which performs motion compensation at the block level\nrather than the pixel level to reduce the interpolation times. Our proposed\ntechniques have been implemented based on the state-of-the-art high efficiency\nvideo coding standard, and the experimental results show that the proposed\ntechniques altogether achieve on average 11.1% and 19.3% bits saving for random\naccess and low delay configurations, respectively, on typical video sequences\nthat have rich rotation or zooming motions. Meanwhile, the computational\ncomplexity increases of both encoder and decoder are within an acceptable\nrange.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 09:04:15 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Li", "Li", ""], ["Li", "Houqiang", ""], ["Liu", "Dong", ""], ["Yang", "Haitao", ""], ["Lin", "Sixin", ""], ["Chen", "Huanbang", ""], ["Wu", "Feng", ""]]}, {"id": "1702.06728", "submitter": "Dong Liu", "authors": "Yue Li, Dong Liu, Houqiang Li, Li Li, Feng Wu, Hong Zhang, Haitao Yang", "title": "Convolutional Neural Network-Based Block Up-sampling for Intra Frame\n  Coding", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2727682", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent advances of image super-resolution using convolutional\nneural network (CNN), we propose a CNN-based block up-sampling scheme for intra\nframe coding. A block can be down-sampled before being compressed by normal\nintra coding, and then up-sampled to its original resolution. Different from\nprevious studies on down/up-sampling-based coding, the up-sampling methods in\nour scheme have been designed by training CNN instead of hand-crafted. We\nexplore a new CNN structure for up-sampling, which features deconvolution of\nfeature maps, multi-scale fusion, and residue learning, making the network both\ncompact and efficient. We also design different networks for the up-sampling of\nluma and chroma components, respectively, where the chroma up-sampling CNN\nutilizes the luma information to boost its performance. In addition, we design\na two-stage up-sampling process, the first stage being within the\nblock-by-block coding loop, and the second stage being performed on the entire\nframe, so as to refine block boundaries. We also empirically study how to set\nthe coding parameters of down-sampled blocks for pursuing the frame-level\nrate-distortion optimization. Our proposed scheme is implemented into the High\nEfficiency Video Coding (HEVC) reference software, and a comprehensive set of\nexperiments have been performed to evaluate our methods. Experimental results\nshow that our scheme achieves significant bits saving compared with HEVC anchor\nespecially at low bit rates, leading to on average 5.5% BD-rate reduction on\ncommon test sequences and on average 9.0% BD-rate reduction on ultra high\ndefinition (UHD) test sequences.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 09:51:49 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 10:17:52 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 01:47:39 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Li", "Yue", ""], ["Liu", "Dong", ""], ["Li", "Houqiang", ""], ["Li", "Li", ""], ["Wu", "Feng", ""], ["Zhang", "Hong", ""], ["Yang", "Haitao", ""]]}, {"id": "1702.06925", "submitter": "Xiang Xiang", "authors": "Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter,\n  Gregory D. Hager, Harry Quon, Jian Cheng, Alan L. Yuille", "title": "Regularizing Face Verification Nets For Pain Intensity Regression", "comments": "5 pages, 3 figure; Camera-ready version to appear at IEEE ICIP 2017", "journal-ref": null, "doi": "10.13140/RG.2.2.20841.49765", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited labeled data are available for the research of estimating facial\nexpression intensities. For instance, the ability to train deep networks for\nautomated pain assessment is limited by small datasets with labels of\npatient-reported pain intensities. Fortunately, fine-tuning from a\ndata-extensive pre-trained domain, such as face verification, can alleviate\nthis problem. In this paper, we propose a network that fine-tunes a\nstate-of-the-art face verification network using a regularized regression loss\nand additional data with expression labels. In this way, the expression\nintensity regression task can benefit from the rich feature representations\ntrained on a huge amount of data for face verification. The proposed\nregularized deep regressor is applied to estimate the pain expression intensity\nand verified on the widely-used UNBC-McMaster Shoulder-Pain dataset, achieving\nthe state-of-the-art performance. A weighted evaluation metric is also proposed\nto address the imbalance issue of different pain intensities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:15:42 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 17:58:58 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 17:49:56 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Wang", "Feng", ""], ["Xiang", "Xiang", ""], ["Liu", "Chang", ""], ["Tran", "Trac D.", ""], ["Reiter", "Austin", ""], ["Hager", "Gregory D.", ""], ["Quon", "Harry", ""], ["Cheng", "Jian", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1702.07178", "submitter": "Zhenyu Li", "authors": "Zhenyu Li and Adrian G. Bors", "title": "Steganalysis of 3D Objects Using Statistics of Local Feature Sets", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2017.06.011", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D steganalysis aims to identify subtle invisible changes produced in\ngraphical objects through digital watermarking or steganography. Sets of\nstatistical representations of 3D features, extracted from both cover and stego\n3D mesh objects, are used as inputs into machine learning classifiers in order\nto decide whether any information was hidden in the given graphical object.\nAccording to previous studies, sets of local geometry features can be used to\ndefine the differences between stego and cover-objects. The features proposed\nin this paper include those representing the local object curvature, vertex\nnormals, the local geometry representation in the spherical coordinate system\nand are considered in various combinations with others. We also analyze the\neffectiveness of various 3D feature sets applied for steganalysis based on the\nPearson correlation coefficient. The classifiers proposed in this study for\ndiscriminating the 3D stego and cover-objects include Support Vector Machine\nand the Fisher Linear Discriminant ensemble. Three different watermarking and\nsteganographic methods are used for hiding information in the 3D objects used\nfor testing the performance of the proposed steganalysis methodology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 11:24:03 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Li", "Zhenyu", ""], ["Bors", "Adrian G.", ""]]}, {"id": "1702.07437", "submitter": "Tim Althoff", "authors": "Ali Shameli, Tim Althoff, Amin Saberi, Jure Leskovec", "title": "How Gamification Affects Physical Activity: Large-scale Analysis of\n  Walking Challenges in a Mobile Application", "comments": "WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gamification represents an effective way to incentivize user behavior across\na number of computing applications. However, despite the fact that physical\nactivity is essential for a healthy lifestyle, surprisingly little is known\nabout how gamification and in particular competitions shape human physical\nactivity. Here we study how competitions affect physical activity. We focus on\nwalking challenges in a mobile activity tracking application where multiple\nusers compete over who takes the most steps over a predefined number of days.\nWe synthesize our findings in a series of game and app design implications. In\nparticular, we analyze nearly 2,500 physical activity competitions over a\nperiod of one year capturing more than 800,000 person days of activity\ntracking. We observe that during walking competitions, the average user\nincreases physical activity by 23%. Furthermore, there are large increases in\nactivity for both men and women across all ages, and weight status, and even\nfor users that were previously fairly inactive. We also find that the\ncomposition of participants greatly affects the dynamics of the game. In\nparticular, if highly unequal participants get matched to each other, then\ncompetition suffers and the overall effect on the physical activity drops\nsignificantly. Furthermore, competitions with an equal mix of both men and\nwomen are more effective in increasing the level of activities. We leverage\nthese insights to develop a statistical model to predict whether or not a\ncompetition will be particularly engaging with significant accuracy. Our models\ncan serve as a guideline to help design more engaging competitions that lead to\nmost beneficial behavioral changes.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 01:32:02 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Shameli", "Ali", ""], ["Althoff", "Tim", ""], ["Saberi", "Amin", ""], ["Leskovec", "Jure", ""]]}, {"id": "1702.07452", "submitter": "Manabu Tsukada", "authors": "Manabu Tsukada and Keiko Ogawa and Masahiro Ikeda and Takuro Sone and\n  Kenta Niwa and Shoichiro Saito and Takashi Kasuya and Hideki Sunahara and\n  Hiroshi Esaki", "title": "Software Defined Media: Virtualization of Audio-Visual Services", "comments": "IEEE International Conference on Communications (ICC2017), Paris,\n  France, 21-25 May 2017", "journal-ref": null, "doi": "10.1109/ICC.2017.7996610", "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-native audio-visual services are witnessing rapid development. Among\nthese services, object-based audio-visual services are gaining importance. In\n2014, we established the Software Defined Media (SDM) consortium to target new\nresearch areas and markets involving object-based digital media and\nInternet-by-design audio-visual environments. In this paper, we introduce the\nSDM architecture that virtualizes networked audio-visual services along with\nthe development of smart buildings and smart cities using Internet of Things\n(IoT) devices and smart building facilities. Moreover, we design the SDM\narchitecture as a layered architecture to promote the development of innovative\napplications on the basis of rapid advancements in software-defined networking\n(SDN). Then, we implement a prototype system based on the architecture, present\nthe system at an exhibition, and provide it as an SDM API to application\ndevelopers at hackathons. Various types of applications are developed using the\nAPI at these events. An evaluation of SDM API access shows that the prototype\nSDM platform effectively provides 3D audio reproducibility and interactiveness\nfor SDM applications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 03:09:25 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tsukada", "Manabu", ""], ["Ogawa", "Keiko", ""], ["Ikeda", "Masahiro", ""], ["Sone", "Takuro", ""], ["Niwa", "Kenta", ""], ["Saito", "Shoichiro", ""], ["Kasuya", "Takashi", ""], ["Sunahara", "Hideki", ""], ["Esaki", "Hiroshi", ""]]}, {"id": "1702.07548", "submitter": "Krzysztof Wegner", "authors": "Tomasz Grajek, Jakub Stankowski, Damian Karwowski, Krzysztof\n  Klimaszewski, Olgierd Stankiewicz, Krzysztof Wegner", "title": "Analysis of video quality losses in the homogenous HEVC video\n  transcoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents quantitative analysis of the video quality losses in the\nhomogenous HEVC video transcoder. With the use of HM15.0 reference software and\na set of test video sequences, cascaded pixel domain video transcoder (CPDT)\nconcept has been used to gather all the necessary data needed for the analysis.\nThis experiment was done for wide range of source and target bitrates. The\nessential result of the work is extensive evaluation of CPDT, commonly used as\na reference in works on effective video transcoding. Until now no such\nextensively performed study have been made available in the literature. Quality\ndegradation between transcoded video and the video that would be result of\ndirect compression of the original video at the same bitrate as the transcoded\none have been reported. The dependency between quality degradation caused by\ntranscoding and the bitrate changes of the transcoded data stream are clearly\npresented on graphs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 11:53:22 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Grajek", "Tomasz", ""], ["Stankowski", "Jakub", ""], ["Karwowski", "Damian", ""], ["Klimaszewski", "Krzysztof", ""], ["Stankiewicz", "Olgierd", ""], ["Wegner", "Krzysztof", ""]]}, {"id": "1702.07627", "submitter": "Ge Ma", "authors": "Ge Ma, Zhi Wang, Miao Zhang, Jiahui Ye, Minghua Chen and Wenwu Zhu", "title": "Understanding Performance of Edge Content Caching for Mobile Video\n  Streaming", "comments": "13 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Internet has witnessed an increase in the popularity of mobile video\nstreaming, which is expected to exceed 3/4 of the global mobile data traffic by\n2019. To satisfy the considerable amount of mobile video requests, video\nservice providers have been pushing their content delivery infrastructure to\nedge networks--from regional CDN servers to peer CDN servers (e.g.,\nsmartrouters in users' homes)--to cache content and serve users with storage\nand network resources nearby. Among the edge network content caching paradigms,\nWi-Fi access point caching and cellular base station caching have become two\nmainstream solutions. Thus, understanding the effectiveness and performance of\nthese solutions for large-scale mobile video delivery is important. However,\nthe characteristics and request patterns of mobile video streaming are unclear\nin practical wireless network. In this paper, we use real-world datasets\ncontaining 50 million trace items of nearly 2 million users viewing more than\n0.3 million unique videos using mobile devices in a metropolis in China over 2\nweeks, not only to understand the request patterns and user behaviors in mobile\nvideo streaming, but also to evaluate the effectiveness of Wi-Fi and\ncellular-based edge content caching solutions. To understand performance of\nedge content caching for mobile video streaming, we first present temporal and\nspatial video request patterns, and we analyze their impacts on caching\nperformance using frequency-domain and entropy analysis approaches. We then\nstudy the behaviors of mobile video users, including their mobility and\ngeographical migration behaviors. Using trace-driven experiments, we compare\nstrategies for edge content caching including LRU and LFU, in terms of\nsupporting mobile video requests. Moreover, we design an efficient caching\nstrategy based on the measurement insights and experimentally evaluate its\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 15:28:20 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Ma", "Ge", ""], ["Wang", "Zhi", ""], ["Zhang", "Miao", ""], ["Ye", "Jiahui", ""], ["Chen", "Minghua", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1702.08680", "submitter": "Jie Zhu", "authors": "Jie Zhu, Yanwen Guo and Han Ma", "title": "A Data-driven Approach for Furniture and Indoor Scene Colorization", "comments": "13 pages, 16 figures, submission to IEEE TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven approach that colorizes 3D furniture models and\nindoor scenes by leveraging indoor images on the internet. Our approach is able\nto colorize the furniture automatically according to an example image. The core\nis to learn image-guided mesh segmentation to segment the model into different\nparts according to the image object. Given an indoor scene, the system supports\ncolorization-by-example, and has the ability to recommend the colorization\nscheme that is consistent with a user-desired color theme. The latter is\nrealized by formulating the problem as a Markov random field model that imposes\nuser input as an additional constraint. We contribute to the community a\nhierarchically organized image-model database with correspondences between each\nimage and the corresponding model at the part-level. Our experiments and a user\nstudy show that our system produces perceptually convincing results comparable\nto those generated by interior designers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 07:54:21 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Zhu", "Jie", ""], ["Guo", "Yanwen", ""], ["Ma", "Han", ""]]}]