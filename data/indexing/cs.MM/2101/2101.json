[{"id": "2101.00317", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Linghao Jin, Xu Han, Jun Lu, Jane You, Lingsheng Kong", "title": "Identity-aware Facial Expression Recognition in Compressed Video", "comments": "Accepted as the Oral paper at ICPR 2020 (<4.4%). arXiv admin note:\n  substantial text overlap with arXiv:2010.10637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper targets to explore the inter-subject variations eliminated facial\nexpression representation in the compressed video domain. Most of the previous\nmethods process the RGB images of a sequence, while the off-the-shelf and\nvaluable expression-related muscle movement already embedded in the compression\nformat. In the up to two orders of magnitude compressed domain, we can\nexplicitly infer the expression from the residual frames and possible to\nextract identity factors from the I frame with a pre-trained face recognition\nnetwork. By enforcing the marginal independent of them, the expression feature\nis expected to be purer for the expression and be robust to identity shifts. We\ndo not need the identity label or multiple expression samples from the same\nperson for identity elimination. Moreover, when the apex frame is annotated in\nthe dataset, the complementary constraint can be further added to regularize\nthe feature-level game. In testing, only the compressed residual frames are\nrequired to achieve expression prediction. Our solution can achieve comparable\nor better performance than the recent decoded image based methods on the\ntypical FER benchmarks with about 3$\\times$ faster inference with compressed\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 21:03:13 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 23:46:22 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Jin", "Linghao", ""], ["Han", "Xu", ""], ["Lu", "Jun", ""], ["You", "Jane", ""], ["Kong", "Lingsheng", ""]]}, {"id": "2101.00604", "submitter": "Jizhe Zhou", "authors": "Jizhe Zhou, Chi-Man Pun, Yu Tong", "title": "Privacy-sensitive Objects Pixelation for Live Video Streaming", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413972", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the prevailing of live video streaming, establishing an online\npixelation method for privacy-sensitive objects is an urgency. Caused by the\ninaccurate detection of privacy-sensitive objects, simply migrating the\ntracking-by-detection structure into the online form will incur problems in\ntarget initialization, drifting, and over-pixelation. To cope with the\ninevitable but impacting detection issue, we propose a novel Privacy-sensitive\nObjects Pixelation (PsOP) framework for automatic personal privacy filtering\nduring live video streaming. Leveraging pre-trained detection networks, our\nPsOP is extendable to any potential privacy-sensitive objects pixelation.\nEmploying the embedding networks and the proposed Positioned Incremental\nAffinity Propagation (PIAP) clustering algorithm as the backbone, our PsOP\nunifies the pixelation of discriminating and indiscriminating pixelation\nobjects through trajectories generation. In addition to the pixelation accuracy\nboosting, experiments on the streaming video data we built show that the\nproposed PsOP can significantly reduce the over-pixelation ratio in\nprivacy-sensitive object pixelation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 11:07:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Jizhe", ""], ["Pun", "Chi-Man", ""], ["Tong", "Yu", ""]]}, {"id": "2101.00606", "submitter": "Jizhe Zhou", "authors": "Jizhe Zhou, Chi-Man Pun, Yu Tong", "title": "News Image Steganography: A Novel Architecture Facilitates the Fake News\n  Identification", "comments": null, "journal-ref": null, "doi": "10.1109/VCIP49819.2020.9301846", "report-no": null, "categories": "cs.CV cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A larger portion of fake news quotes untampered images from other sources\nwith ulterior motives rather than conducting image forgery. Such elaborate\nengraftments keep the inconsistency between images and text reports stealthy,\nthereby, palm off the spurious for the genuine. This paper proposes an\narchitecture named News Image Steganography (NIS) to reveal the aforementioned\ninconsistency through image steganography based on GAN. Extractive\nsummarization about a news image is generated based on its source texts, and a\nlearned steganographic algorithm encodes and decodes the summarization of the\nimage in a manner that approaches perceptual invisibility. Once an encoded\nimage is quoted, its source summarization can be decoded and further presented\nas the ground truth to verify the quoting news. The pairwise encoder and\ndecoder endow images of the capability to carry along their imperceptible\nsummarization. Our NIS reveals the underlying inconsistency, thereby, according\nto our experiments and investigations, contributes to the identification\naccuracy of fake news that engrafts untampered images.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 11:12:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Jizhe", ""], ["Pun", "Chi-Man", ""], ["Tong", "Yu", ""]]}, {"id": "2101.00611", "submitter": "Xing Wei", "authors": "Xing Wei, Chenyang Yang, and Shengqian Han", "title": "Duration-Squeezing-Aware Communication and Computing for Proactive VR", "comments": "5 pages, 3 figures, submit to IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactive tile-based virtual reality video streaming computes and delivers\nthe predicted tiles to be requested before playback. All existing works\noverlook the important fact that computing and communication (CC) tasks for a\nsegment may squeeze the time for the tasks for the next segment, which will\ncause less and less available time for the latter segments. In this paper, we\njointly optimize the durations for CC tasks to maximize the completion rate of\nCC tasks under the task duration-squeezing-aware constraint. To ensure the\nlatter segments remain enough time for the tasks, the CC tasks for a segment\nare not allowed to squeeze the time for computing and delivering the subsequent\nsegment. We find the closed-form optimal solution, from which we find a\nminimum-resource-limited, an unconditional and a conditional resource-tradeoff\nregions, which are determined by the total time for proactive CC tasks and the\nplayback duration of a segment. Owing to the duration-squeezing-prohibited\nconstraints, the increase of the configured resources may not be always useful\nfor improving the completion rate of CC tasks. Numerical results validate the\nimpact of the duration-squeezing-prohibited constraints and illustrate the\nthree regions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 11:57:04 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wei", "Xing", ""], ["Yang", "Chenyang", ""], ["Han", "Shengqian", ""]]}, {"id": "2101.00690", "submitter": "Kapil Ahuja", "authors": "Rohit Agrawal and Kapil Ahuja", "title": "CSIS: compressed sensing-based enhanced-embedding capacity image\n  steganography scheme", "comments": "12 pages double-column, 7 tables, and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image steganography plays a vital role in securing secret data by embedding\nit in the cover images. Usually, these images are communicated in a compressed\nformat. Existing techniques achieve this but have low embedding capacity.\nEnhancing this capacity causes a deterioration in the visual quality of the\nstego-image. Hence, our goal here is to enhance the embedding capacity while\npreserving the visual quality of the stego-image. We also intend to ensure that\nour scheme is resistant to steganalysis attacks.\n  This paper proposes a Compressed Sensing Image Steganography (CSIS) scheme to\nachieve our goal while embedding binary data in images. The novelty of our\nscheme is the combination of three components in attaining the above-listed\ngoals. First, we use compressed sensing to sparsify cover image block-wise,\nobtain its linear measurements, and then uniquely select permissible\nmeasurements. Further, before embedding the secret data, we encrypt it using\nthe Data Encryption Standard (DES) algorithm, and finally, we embed two bits of\nencrypted data into each permissible measurement. Second, we propose a novel\ndata extraction technique, which is lossless and completely recovers our secret\ndata. Third, for the reconstruction of the stego-image, we use the least\nabsolute shrinkage and selection operator (LASSO) for the resultant\noptimization problem.\n  We perform experiments on several standard grayscale images and a color\nimage, and evaluate embedding capacity, PSNR value, mean SSIM index, NCC\ncoefficients, and entropy. We achieve 1.53 times more embedding capacity as\ncompared to the most recent scheme. We obtain an average of 37.92 dB PSNR\nvalue, and average values close to 1 for both the mean SSIM index and the NCC\ncoefficients, which are considered good. Moreover, the entropy of cover images\nand their corresponding stego-images are nearly the same.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 19:13:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Agrawal", "Rohit", ""], ["Ahuja", "Kapil", ""]]}, {"id": "2101.00820", "submitter": "Yang Liu", "authors": "Yang Liu, Keze Wang, Haoyuan Lan, Liang Lin", "title": "Temporal Contrastive Graph Learning for Video Action Recognition and\n  Retrieval", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempt to fully discover the temporal diversity and chronological\ncharacteristics for self-supervised video representation learning, this work\ntakes advantage of the temporal dependencies within videos and further proposes\na novel self-supervised method named Temporal Contrastive Graph Learning\n(TCGL). In contrast to the existing methods that ignore modeling elaborate\ntemporal dependencies, our TCGL roots in a hybrid graph contrastive learning\nstrategy to jointly regard the inter-snippet and intra-snippet temporal\ndependencies as self-supervision signals for temporal representation learning.\nTo model multi-scale temporal dependencies, our TCGL integrates the prior\nknowledge about the frame and snippet orders into graph structures, i.e., the\nintra-/inter- snippet temporal contrastive graphs. By randomly removing edges\nand masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL\ncan generate different correlated graph views. Then, specific contrastive\nlearning modules are designed to maximize the agreement between nodes in\ndifferent views. To adaptively learn the global context representation and\nrecalibrate the channel-wise features, we introduce an adaptive video snippet\norder prediction module, which leverages the relational knowledge among video\nsnippets to predict the actual snippet orders. Experimental results demonstrate\nthe superiority of our TCGL over the state-of-the-art methods on large-scale\naction recognition and video retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:11:39 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 03:33:28 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 08:51:36 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 03:43:47 GMT"}, {"version": "v5", "created": "Mon, 1 Mar 2021 08:51:25 GMT"}, {"version": "v6", "created": "Tue, 2 Mar 2021 08:34:02 GMT"}, {"version": "v7", "created": "Thu, 4 Mar 2021 13:08:41 GMT"}, {"version": "v8", "created": "Wed, 17 Mar 2021 03:32:52 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Yang", ""], ["Wang", "Keze", ""], ["Lan", "Haoyuan", ""], ["Lin", "Liang", ""]]}, {"id": "2101.01060", "submitter": "Jizhe Zhou", "authors": "Jizhe Zhou, Chi-Man Pun", "title": "Personal Privacy Protection via Irrelevant Faces Tracking and Pixelation\n  in Video Live Streaming", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2020.3029913", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To date, the privacy-protection intended pixelation tasks are still\nlabor-intensive and yet to be studied. With the prevailing of video live\nstreaming, establishing an online face pixelation mechanism during streaming is\nan urgency. In this paper, we develop a new method called Face Pixelation in\nVideo Live Streaming (FPVLS) to generate automatic personal privacy filtering\nduring unconstrained streaming activities. Simply applying multi-face trackers\nwill encounter problems in target drifting, computing efficiency, and\nover-pixelation. Therefore, for fast and accurate pixelation of irrelevant\npeople's faces, FPVLS is organized in a frame-to-video structure of two core\nstages. On individual frames, FPVLS utilizes image-based face detection and\nembedding networks to yield face vectors. In the raw trajectories generation\nstage, the proposed Positioned Incremental Affinity Propagation (PIAP)\nclustering algorithm leverages face vectors and positioned information to\nquickly associate the same person's faces across frames. Such frame-wise\naccumulated raw trajectories are likely to be intermittent and unreliable on\nvideo level. Hence, we further introduce the trajectory refinement stage that\nmerges a proposal network with the two-sample test based on the Empirical\nLikelihood Ratio (ELR) statistic to refine the raw trajectories. A Gaussian\nfilter is laid on the refined trajectories for final pixelation. On the video\nlive streaming dataset we collected, FPVLS obtains satisfying accuracy,\nreal-time efficiency, and contains the over-pixelation problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:18:26 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 14:01:09 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhou", "Jizhe", ""], ["Pun", "Chi-Man", ""]]}, {"id": "2101.01168", "submitter": "Wieslaw Kopec", "authors": "Rafa{\\l} Mas{\\l}yk, Kinga Skorupska, Piotr Gago, Marcin Niewi\\'nski,\n  Barbara Karpowicz, Anna Jaskulska, Katarzyna Abramczuk, Wies{\\l}aw Kope\\'c", "title": "Deploying Crowdsourcing for Workflow Driven Business Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this paper is to discuss how to integrate the possibilities\nof crowdsourcing platforms with systems supporting workflow to enable the\nengagement and interaction with business tasks of a wider group of people.\nThus, this work is an attempt to expand the functional capabilities of typical\nbusiness systems by allowing selected process tasks to be performed by\nunlimited human resources. Opening business tasks to crowdsourcing, within\nestablished Business Process Management Systems (BPMS) will improve the\nflexibility of company processes and allow for lower work-load and greater\nspecialization among the staff employed on-site. The presented conceptual work\nis based on the current international standards in this field, promoted by\nWorkflows Management Coalition. To this end, the functioning of business\nplatforms was analysed and their functionality was presented visually, followed\nby a proposal and a discussion of how to implement crowdsourcing into workflow\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:57:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mas\u0142yk", "Rafa\u0142", ""], ["Skorupska", "Kinga", ""], ["Gago", "Piotr", ""], ["Niewi\u0144ski", "Marcin", ""], ["Karpowicz", "Barbara", ""], ["Jaskulska", "Anna", ""], ["Abramczuk", "Katarzyna", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}, {"id": "2101.01285", "submitter": "Wieslaw Kopec", "authors": "Kinga Skorupska, Daniel Cnotkowski, Julia Paluch, Rafa{\\l} Mas{\\l}yk,\n  Anna Jaskulska, Monika Kornacka, Wies{\\l}aw Kope\\'c", "title": "All Factors Should Matter! Reference Checklist for Describing Research\n  Conditions in Pursuit of Comparable IVR Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant problem with immersive virtual reality (IVR) experiments is the\nability to compare research conditions. VR kits and IVR environments are\ncomplex and diverse but researchers from different fields, e.g. ICT,\npsychology, or marketing, often neglect to describe them with a level of detail\nsufficient to situate their research on the IVR landscape. Careful reporting of\nthese conditions may increase the applicability of research results and their\nimpact on the shared body of knowledge on HCI and IVR. Based on literature\nreview, our experience, practice and a synthesis of key IVR factors, in this\narticle we present a reference checklist for describing research conditions of\nIVR experiments. Including these in publications will contribute to the\ncomparability of IVR research and help other researchers decide to what extent\nreported results are relevant to their own research goals. The compiled\nchecklist is a ready-to-use reference tool and takes into account key hardware,\nsoftware and human factors as well as diverse factors connected to visual,\naudio, tactile, and other aspects of interaction.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 23:45:52 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 18:32:45 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Skorupska", "Kinga", ""], ["Cnotkowski", "Daniel", ""], ["Paluch", "Julia", ""], ["Mas\u0142yk", "Rafa\u0142", ""], ["Jaskulska", "Anna", ""], ["Kornacka", "Monika", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}, {"id": "2101.01368", "submitter": "Haiwen Diao", "authors": "Haiwen Diao, Ying Zhang, Lin Ma, Huchuan Lu", "title": "Similarity Reasoning and Filtration for Image-Text Matching", "comments": "14 pages, 8 figures, Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching plays a critical role in bridging the vision and\nlanguage, and great progress has been made by exploiting the global alignment\nbetween image and sentence, or local alignments between regions and words.\nHowever, how to make the most of these alignments to infer more accurate\nmatching scores is still underexplored. In this paper, we propose a novel\nSimilarity Graph Reasoning and Attention Filtration (SGRAF) network for\nimage-text matching. Specifically, the vector-based similarity representations\nare firstly learned to characterize the local and global alignments in a more\ncomprehensive manner, and then the Similarity Graph Reasoning (SGR) module\nrelying on one graph convolutional neural network is introduced to infer\nrelation-aware similarities with both the local and global alignments. The\nSimilarity Attention Filtration (SAF) module is further developed to integrate\nthese alignments effectively by selectively attending on the significant and\nrepresentative alignments and meanwhile casting aside the interferences of\nnon-meaningful alignments. We demonstrate the superiority of the proposed\nmethod with achieving state-of-the-art performances on the Flickr30K and MSCOCO\ndatasets, and the good interpretability of SGR and SAF modules with extensive\nqualitative experiments and analyses.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 06:29:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Diao", "Haiwen", ""], ["Zhang", "Ying", ""], ["Ma", "Lin", ""], ["Lu", "Huchuan", ""]]}, {"id": "2101.01404", "submitter": "Changsheng Chen", "authors": "Changsheng Chen, Shuzheng Zhang, Fengbo Lan, Jiwu Huang", "title": "Domain Generalization for Document Authentication against Practical\n  Recapturing Attacks", "comments": "13 pages, 14 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recapturing attack can be employed as a simple but effective anti-forensic\ntool for digital document images. Inspired by the document inspection process\nthat compares a questioned document against a reference sample, we proposed a\ndocument recapture detection scheme by employing Siamese network to compare and\nextract distinct features in a recapture document image. The proposed algorithm\ntakes advantages of both metric learning and image forensic techniques. Instead\nof adopting Euclidean distance-based loss function, we integrate the forensic\nsimilarity function with a triplet loss and a normalized softmax loss. After\ntraining with the proposed triplet selection strategy, the resulting feature\nembedding clusters the genuine samples near the reference while pushes the\nrecaptured samples apart. In the experiment, we consider practical domain\ngeneralization problems, such as the variations in printing/imaging devices,\nsubstrates, recapturing channels, and document types. To evaluate the\nrobustness of different approaches, we benchmark some popular off-the-shelf\nmachine learning-based approaches, a state-of-the-art document image detection\nscheme, and the proposed schemes with different network backbones under various\nexperimental protocols. Experimental results show that the proposed schemes\nwith different network backbones have consistently outperformed the\nstate-of-the-art approaches under different experimental settings.\nSpecifically, under the most challenging scenario in our experiment, i.e.,\nevaluation across different types of documents that produced by different\ndevices, we have achieved less than 5.00% APCER (Attack Presentation\nClassification Error Rate) and 5.56% BPCER (Bona Fide Presentation\nClassification Error Rate) by the proposed network with ResNeXt101 backbone at\n5% BPCER decision threshold.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:22:38 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 14:18:31 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chen", "Changsheng", ""], ["Zhang", "Shuzheng", ""], ["Lan", "Fengbo", ""], ["Huang", "Jiwu", ""]]}, {"id": "2101.01447", "submitter": "Hung-Ting Su", "authors": "Hung-Ting Su, Chen-Hsi Chang, Po-Wei Shen, Yu-Siang Wang, Ya-Liang\n  Chang, Yu-Cheng Chang, Pu-Jen Cheng and Winston H. Hsu", "title": "End-to-End Video Question-Answer Generation with Generator-Pretester\n  Network", "comments": "Accepted to TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel task, Video Question-Answer Generation (VQAG), for\nchallenging Video Question Answering (Video QA) task in multimedia. Due to\nexpensive data annotation costs, many widely used, large-scale Video QA\ndatasets such as Video-QA, MSVD-QA and MSRVTT-QA are automatically annotated\nusing Caption Question Generation (CapQG) which inputs captions instead of the\nvideo itself. As captions neither fully represent a video, nor are they always\npractically available, it is crucial to generate question-answer pairs based on\na video via Video Question-Answer Generation (VQAG). Existing video-to-text\n(V2T) approaches, despite taking a video as the input, only generate a question\nalone. In this work, we propose a novel model Generator-Pretester Network that\nfocuses on two components: (1) The Joint Question-Answer Generator (JQAG) which\ngenerates a question with its corresponding answer to allow Video Question\n\"Answering\" training. (2) The Pretester (PT) verifies a generated question by\ntrying to answer it and checks the pretested answer with both the model's\nproposed answer and the ground truth answer. We evaluate our system with the\nonly two available large-scale human-annotated Video QA datasets and achieves\nstate-of-the-art question generation performances. Furthermore, using our\ngenerated QA pairs only on the Video QA task, we can surpass some supervised\nbaselines. We apply our generated questions to Video QA applications and\nsurpasses some supervised baselines using generated questions only. As a\npre-training strategy, we outperform both CapQG and transfer learning\napproaches when employing semi-supervised (20%) or fully supervised learning\nwith annotated data. These experimental results suggest the novel perspectives\nfor Video QA training.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:46:06 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Su", "Hung-Ting", ""], ["Chang", "Chen-Hsi", ""], ["Shen", "Po-Wei", ""], ["Wang", "Yu-Siang", ""], ["Chang", "Ya-Liang", ""], ["Chang", "Yu-Cheng", ""], ["Cheng", "Pu-Jen", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2101.01484", "submitter": "Tantan Zhao", "authors": "Tantan Zhao, Lijun He, Xinyu Huang, Fan Li", "title": "QoE-driven Secure Video Transmission in Cloud-edge Collaborative\n  Networks", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video transmission over the backhaul link in cloudedge collaborative networks\nusually suffers security risks. Only a few existing studies focus on ensuring\nsecure backhaul link transmission. However, video content characteristics,\nwhich have significant effects on quality of experience (QoE), are ignored in\nthe study. In this paper, we investigate the QoE-driven crosslayer optimization\nof secure video transmission over the backhaul link in cloud-edge collaborative\nnetworks. First, we establish the secure transmission model for backhaul link\nby considering video encoding and MEC-caching in a distributed cache scenario.\nThen, based on the established model, a joint optimization problem is\nformulated with the objective of improving user QoE and reducing transmission\nlatency under the constraints of MEC capacity. To solve the optimization\nproblem, we propose two algorithms: a near optimal iterative algorithm based on\nrelaxation and branch and bound method (MC-VEB), and a greedy algorithm with\nlow computational complexity (Greedy MC-VEB). Simulation results show that our\nproposed MC-VEB can greatly improve the user QoE and reduce transmission\nlatency within security constraints, and the proposed Greedy MC-VEB can obtain\nthe tradeoff between the user QoE and the computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 12:39:47 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhao", "Tantan", ""], ["He", "Lijun", ""], ["Huang", "Xinyu", ""], ["Li", "Fan", ""]]}, {"id": "2101.01652", "submitter": "Wieslaw Kopec", "authors": "Grzegorz Pochwatko, Barbara Karpowicz, Anna Chrzanowska, Wies{\\l}aw\n  Kope\\'c", "title": "Interpersonal distance in VR: reactions of older adults to the presence\n  of a virtual agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of virtual reality technology has increased its\navailability and, consequently, increased the number of its possible\napplications. The interest in the new medium has grown due to the entertainment\nindustry (games, VR experiences and movies). The number of freely available\ntraining and therapeutic applications is also increasing. Contrary to popular\nopinion, new technologies are also adopted by older adults. Creating virtual\nenvironments tailored to the needs and capabilities of older adults requires\nintense research on the behaviour of these participants in the most common\nsituations, towards commonly used elements of the virtual environment, in\ntypical sceneries. Comfortable immersion in a virtual environment is key to\nachieving the impression of presence. Presence is, in turn, necessary to obtain\nappropriate training, persuasive and therapeutic effects. A virtual agent (a\nhumanoid representation of an algorithm or artificial intelligence) is often an\nelement of the virtual environment interface. Maintaining an appropriate\ndistance to the agent is, therefore, a key parameter for the creator of the VR\nexperience. Older (65+) participants maintain greater distance towards an agent\n(a young white male) than younger ones (25-35). It may be caused by differences\nin the level of arousal, but also cultural norms. As a consequence, VR\ndevelopers are advised to use algorithms that maintain the agent at the\nappropriate distance, depending on the user's age.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:06:03 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Pochwatko", "Grzegorz", ""], ["Karpowicz", "Barbara", ""], ["Chrzanowska", "Anna", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}, {"id": "2101.01718", "submitter": "Solomia Fedushko", "authors": "Solomiia Fedushko, Yuriy Syerov, Oleksandr Skybinskyi, Nataliya\n  Shakhovska, Zoryana Kunch", "title": "Efficiency of Using Utility for Usernames Verification in Online\n  Community Management", "comments": "10 pages, 6 figures", "journal-ref": "Efficiency of Using Utility for Username Verification in Online\n  Community Management. Proceedings of the International Workshop on Conflict\n  Management in Global Information Networks, 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study deals with the methods and means of checking the reliability of\nusernames of online communities on the basis of computer-linguistic analysis of\nthe results of their communicative interaction. The methodological basis of the\nstudy is a combination of general scientific methods and special approaches to\nthe study of the data verification of online communities in the Ukrainian\nsegment of the global information environment. The algorithm of functioning of\nthe utility Verifier of online community username is developed. The\ninformational model of the automated means of checking the usernames of online\ncommunity is designed. The utility Verifier of online community username data\nvalidation system approbation is realized in the online community. The\nindicator of the data verification system effectiveness is determined.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:42:59 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Fedushko", "Solomiia", ""], ["Syerov", "Yuriy", ""], ["Skybinskyi", "Oleksandr", ""], ["Shakhovska", "Nataliya", ""], ["Kunch", "Zoryana", ""]]}, {"id": "2101.03413", "submitter": "Soumendu Chakraborty", "authors": "Soumendu Chakraborty, Satish Kumar Singh, Kush Kumar", "title": "Facial Biometric System for Recognition using Extended LGHP Algorithm on\n  Raspberry Pi", "comments": null, "journal-ref": "IEEE Sensors Journal, vol-20, no-14, pp. 8117-8127, (2020)", "doi": "10.1109/JSEN.2020.2979907", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In todays world, where the need for security is paramount and biometric\naccess control systems are gaining mass acceptance due to their increased\nreliability, research in this area is quite relevant. Also with the advent of\nIOT devices and increased community support for cheap and small computers like\nRaspberry Pi its convenient than ever to design a complete standalone system\nfor any purpose. This paper proposes a Facial Biometric System built on the\nclient-server paradigm using Raspberry Pi 3 model B running a novel local\ndescriptor based parallel algorithm. This paper also proposes an extended\nversion of Local Gradient Hexa Pattern with improved accuracy. The proposed\nextended version of LGHP improved performance as shown in performance analysis.\nExtended LGHP shows improvement over other state-of-the-art descriptors namely\nLDP, LTrP, MLBP and LVP on the most challenging benchmark facial image\ndatabases, i.e. Cropped Extended Yale-B, CMU-PIE, color-FERET, LFW, and\nGhallager database. Proposed system is also compared with various patents\nhaving similar system design and intent to emphasize the difference and novelty\nof the system proposed.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 19:23:45 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chakraborty", "Soumendu", ""], ["Singh", "Satish Kumar", ""], ["Kumar", "Kush", ""]]}, {"id": "2101.04215", "submitter": "\\\"Omer S\\\"umer", "authors": "\\\"Omer S\\\"umer, Patricia Goldberg, Sidney D'Mello, Peter Gerjets,\n  Ulrich Trautwein, Enkelejda Kasneci", "title": "Multimodal Engagement Analysis from Facial Videos in the Classroom", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student engagement is a key construct for learning and teaching. While most\nof the literature explored the student engagement analysis on computer-based\nsettings, this paper extends that focus to classroom instruction. To best\nexamine student visual engagement in the classroom, we conducted a study\nutilizing the audiovisual recordings of classes at a secondary school over one\nand a half month's time, acquired continuous engagement labeling per student\n(N=15) in repeated sessions, and explored computer vision methods to classify\nengagement levels from faces in the classroom. We trained deep embeddings for\nattentional and emotional features, training Attention-Net for head pose\nestimation and Affect-Net for facial expression recognition. We additionally\ntrained different engagement classifiers, consisting of Support Vector\nMachines, Random Forest, Multilayer Perceptron, and Long Short-Term Memory, for\nboth features. The best performing engagement classifiers achieved AUCs of .620\nand .720 in Grades 8 and 12, respectively. We further investigated fusion\nstrategies and found score-level fusion either improves the engagement\nclassifiers or is on par with the best performing modality. We also\ninvestigated the effect of personalization and found that using only 60-seconds\nof person-specific data selected by margin uncertainty of the base classifier\nyielded an average AUC improvement of .084. 4.Our main aim with this work is to\nprovide the technical means to facilitate the manual data analysis of classroom\nvideos in research on teaching quality and in the context of teacher training.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:15:04 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 18:53:32 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Goldberg", "Patricia", ""], ["D'Mello", "Sidney", ""], ["Gerjets", "Peter", ""], ["Trautwein", "Ulrich", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2101.04475", "submitter": "Glenn Van Wallendael", "authors": "Johan De Praeter, Christopher Hollmann, Rickard Sjoberg, Glenn Van\n  Wallendael, and Peter Lambert", "title": "Network-Distributed Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, an enormous amount of videos are streamed every day to countless\nusers, all using different devices and networks. These videos must be adapted\nin order to provide users with the most suitable video representation based on\ntheir device properties and current network conditions. However, the two most\ncommon techniques for video adaptation, simulcast and transcoding, represent\ntwo extremes. The former offers excellent scalability, but requires a large\namount of storage, while the latter has a small storage cost, but is not\nscalable to many users due to the additional computing cost per requested\nrepresentation. As a third, in-between approach, network-distributed video\ncoding (NDVC) was proposed within the Moving Picture Experts Group (MPEG). The\naim of NDVC is to reduce the storage cost compared to simulcast, while\nretaining a smaller computing cost compared to transcoding. By exploring the\nproposed techniques for NDVC, we show the workings of this third option for\nvideo providers to deliver their contents to their clients.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:45:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["De Praeter", "Johan", ""], ["Hollmann", "Christopher", ""], ["Sjoberg", "Rickard", ""], ["Van Wallendael", "Glenn", ""], ["Lambert", "Peter", ""]]}, {"id": "2101.04756", "submitter": "Mohammad Akbari", "authors": "Seyedkooshan Hashemifard and Mohammad Akbari", "title": "A Compact Deep Learning Model for Face Spoofing Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, face biometric security systems are rapidly increasing,\ntherefore, the presentation attack detection (PAD) has received significant\nattention from research communities and has become a major field of research.\nResearchers have tackled the problem with various methods, from exploiting\nconventional texture feature extraction such as LBP, BSIF, and LPQ to using\ndeep neural networks with different architectures. Despite the results each of\nthese techniques has achieved for a certain attack scenario or dataset, most of\nthem still failed to generalized the problem for unseen conditions, as the\nefficiency of each is limited to certain type of presentation attacks and\ninstruments (PAI). In this paper, instead of completely extracting hand-crafted\ntexture features or relying only on deep neural networks, we address the\nproblem via fusing both wide and deep features in a unified neural\narchitecture. The main idea is to take advantage of the strength of both\nmethods to derive well-generalized solution for the problem. We also evaluated\nthe effectiveness of our method by comparing the results with each of the\nmentioned techniques separately. The procedure is done on different spoofing\ndatasets such as ROSE-Youtu, SiW and NUAA Imposter datasets.\n  In particular, we simultanously learn a low dimensional latent space\nempowered with data-driven features learnt via Convolutional Neural Network\ndesignes for spoofing detection task (i.e., deep channel) as well as leverages\nspoofing detection feature already popular for spoofing in frequency and\ntemporal dimensions ( i.e., via wide channel).\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:20:09 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hashemifard", "Seyedkooshan", ""], ["Akbari", "Mohammad", ""]]}, {"id": "2101.04790", "submitter": "Wilder Castellanos", "authors": "Wilder Castellanos", "title": "Evaluation of quality scalability techniques for video transmission", "comments": "12 pages, 11 figures. Language Spanish. This work is a Chapter of the\n  eBook: \"Avances de la Ingenieria Bonaventuriana\". ISBN 978-958-8928-85-2\n  Bogota. Colombia. 2019", "journal-ref": "Avances de la Ingenieria Bonaventuriana. Editorial Bonaventuriana.\n  ISBN 978-958-8928-85-2. Bogota. Colombia. (2019)", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The significant increase of the transmission of multimedia content over\nInternet are demanded new delivery strategies to assure a good quality of\nexperience of the users. Transmission of video over packet networks is not an\neasy task due to multiple fluctuations of the network conditions. One\npossibility to improve the quality of some video streaming services is the\ncombinate use of the scalable video coding and cross layer mechanisms that\nallow applications to adapt its traffic stream to the resources network that\nare available. In this paper, it is presented a performance evaluation of the\nthree main scalability techniques: CGS (Coarse-Grained Scalability), FGS (Fine\nGrain scalability) and MGS (Medium Grain Scalability). In particular, we focus\non determining what method is more appropriated for video transmission taking\ninto account some video quality metrics like PSNR (Peak Signal-to-Noise Ratio)\nand decoded frame rate. The results reveal that the rate-adaptive strategy and\nthe MGS technique help avoid or reduce the congestion in networks obtaining a\nbetter quality in the received videos.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:53:10 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Castellanos", "Wilder", ""]]}, {"id": "2101.04827", "submitter": "Zhinan Qiao", "authors": "Zhinan Qiao, Xiaohui Yuan", "title": "Urban land-use analysis using proximate sensing imagery: a survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban regions are complicated functional systems that are closely associated\nwith and reshaped by human activities. The propagation of online geographic\ninformation-sharing platforms and mobile devices equipped with Global\nPositioning System (GPS) greatly proliferates proximate sensing images taken\nnear or on the ground at a close distance to urban targets. Studies leveraging\nproximate sensing imagery have demonstrated great potential to address the need\nfor local data in urban land-use analysis. This paper reviews and summarizes\nthe state-of-the-art methods and publicly available datasets from proximate\nsensing to support land-use analysis. We identify several research problems in\nthe perspective of examples to support training of models and means of\nintegrating diverse data sets. Our discussions highlight the challenges,\nstrategies, and opportunities faced by the existing methods using proximate\nsensing imagery in urban land-use studies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:30:21 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 04:24:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Qiao", "Zhinan", ""], ["Yuan", "Xiaohui", ""]]}, {"id": "2101.04884", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar, Jaiden Reddy, Brendan Morris", "title": "Piano Skills Assessment", "comments": "Dataset is available from:\n  https://github.com/ParitoshParmar/Piano-Skills-Assessment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can a computer determine a piano player's skill level? Is it preferable to\nbase this assessment on visual analysis of the player's performance or should\nwe trust our ears over our eyes? Since current CNNs have difficulty processing\nlong video videos, how can shorter clips be sampled to best reflect the players\nskill level? In this work, we collect and release a first-of-its-kind dataset\nfor multimodal skill assessment focusing on assessing piano player's skill\nlevel, answer the asked questions, initiate work in automated evaluation of\npiano playing skills and provide baselines for future work. Dataset is\navailable from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:26:29 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 01:57:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Parmar", "Paritosh", ""], ["Reddy", "Jaiden", ""], ["Morris", "Brendan", ""]]}, {"id": "2101.05508", "submitter": "Pengyuan Zhou", "authors": "Pengyuan Zhou, Pranvera Kortoci, Yui-Pan Yau, Tristan Braud, Xiujun\n  Wang, Benjamin Finley, Lik-Hang Lee, Sasu Tarkoma, Jussi Kangasharju, Pan Hui", "title": "Augmented Informative Cooperative Perception", "comments": "Submitted to ICDCS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Connected vehicles, whether equipped with advanced driver-assistance systems\nor fully autonomous, are currently constrained to visual information in their\nlines-of-sight. A cooperative perception system among vehicles increases their\nsituational awareness by extending their perception ranges. Existing solutions\nimply significant network and computation load, as well as high flow of\nnot-always-relevant data received by vehicles. To address such issues, and thus\naccount for the inherently diverse informativeness of the data, we present\nAugmented Informative Cooperative Perception (AICP) as the first fast-filtering\nsystem which optimizes the informativeness of shared data at vehicles. AICP\ndisplays the filtered data to the drivers in augmented reality head-up display.\nTo this end, an informativeness maximization problem is presented for vehicles\nto select a subset of data to display to their drivers. Specifically, we\npropose (i) a dedicated system design with custom data structure and\nlight-weight routing protocol for convenient data encapsulation, fast\ninterpretation and transmission, and (ii) a comprehensive problem formulation\nand efficient fitness-based sorting algorithm to select the most valuable data\nto display at the application layer. We implement a proof-of-concept prototype\nof AICP with a bandwidth-hungry, latency-constrained real-life augmented\nreality application. The prototype realizes the informative-optimized\ncooperative perception with only 12.6 milliseconds additional latency. Next, we\ntest the networking performance of AICP at scale and show that AICP effectively\nfilter out less relevant packets and decreases the channel busy time.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:04:16 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zhou", "Pengyuan", ""], ["Kortoci", "Pranvera", ""], ["Yau", "Yui-Pan", ""], ["Braud", "Tristan", ""], ["Wang", "Xiujun", ""], ["Finley", "Benjamin", ""], ["Lee", "Lik-Hang", ""], ["Tarkoma", "Sasu", ""], ["Kangasharju", "Jussi", ""], ["Hui", "Pan", ""]]}, {"id": "2101.06053", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Lea Schumann, Bj\\\"orn Schuller", "title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset:\n  Collection, Insights and Improvements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truly real-life data presents a strong, but exciting challenge for sentiment\nand emotion research. The high variety of possible `in-the-wild' properties\nmakes large datasets such as these indispensable with respect to building\nrobust machine learning models. A sufficient quantity of data covering a deep\nvariety in the challenges of each modality to force the exploratory analysis of\nthe interplay of all modalities has not yet been made available in this\ncontext. In this contribution, we present MuSe-CaR, a first of its kind\nmultimodal dataset. The data is publicly available as it recently served as the\ntesting bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on\nthe tasks of emotion, emotion-target engagement, and trustworthiness\nrecognition by means of comprehensively integrating the audio-visual and\nlanguage modalities. Furthermore, we give a thorough overview of the dataset in\nterms of collection and annotation, including annotation tiers not used in this\nyear's MuSe 2020. In addition, for one of the sub-challenges - predicting the\nlevel of trustworthiness - no participant outperformed the baseline model, and\nso we propose a simple, but highly efficient Multi-Head-Attention network that\nexceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 %\nimprovement).\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:40:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Schumann", "Lea", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "2101.06072", "submitter": "Evlampios Apostolidis", "authors": "Evlampios Apostolidis, Eleni Adamantidou, Alexandros I. Metsai,\n  Vasileios Mezaris, Ioannis Patras", "title": "Video Summarization Using Deep Neural Networks: A Survey", "comments": "Journal paper; Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization technologies aim to create a concise and complete\nsynopsis by selecting the most informative parts of the video content. Several\napproaches have been developed over the last couple of decades and the current\nstate of the art is represented by methods that rely on modern deep neural\nnetwork architectures. This work focuses on the recent advances in the area and\nprovides a comprehensive survey of the existing deep-learning-based methods for\ngeneric video summarization. After presenting the motivation behind the\ndevelopment of technologies for video summarization, we formulate the video\nsummarization task and discuss the main characteristics of a typical\ndeep-learning-based analysis pipeline. Then, we suggest a taxonomy of the\nexisting algorithms and provide a systematic review of the relevant literature\nthat shows the evolution of the deep-learning-based video summarization\ntechnologies and leads to suggestions for future developments. We then report\non protocols for the objective evaluation of video summarization algorithms and\nwe compare the performance of several deep-learning-based approaches. Based on\nthe outcomes of these comparisons, as well as some documented considerations\nabout the suitability of evaluation protocols, we indicate potential future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:41:29 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Apostolidis", "Evlampios", ""], ["Adamantidou", "Eleni", ""], ["Metsai", "Alexandros I.", ""], ["Mezaris", "Vasileios", ""], ["Patras", "Ioannis", ""]]}, {"id": "2101.06328", "submitter": "Alan Smeaton", "authors": "Hyowon Lee, Mingming Liu, Hamza Riaz, Navaneethan Rajasekaren, Michael\n  Scriney, Alan F. Smeaton", "title": "Attention Based Video Summaries of Live Online Zoom Classes", "comments": "Presented at AAAI-2021 Workshop on AI Education: \"Imagining\n  Post-COVID Education with AI\" (TIPCE-2021). 9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a system developed to help University students get more\nfrom their online lectures, tutorials, laboratory and other live sessions. We\ndo this by logging their attention levels on their laptops during live Zoom\nsessions and providing them with personalised video summaries of those live\nsessions. Using facial attention analysis software we create personalised video\nsummaries composed of just the parts where a student's attention was below some\nthreshold. We can also factor in other criteria into video summary generation\nsuch as parts where the student was not paying attention while others in the\nclass were, and parts of the video that other students have replayed\nextensively which a given student has not. Attention and usage based video\nsummaries of live classes are a form of personalised content, they are\neducational video segments recommended to highlight important parts of live\nsessions, useful in both topic understanding and in exam preparation. The\nsystem also allows a Professor to review the aggregated attention levels of\nthose in a class who attended a live session and logged their attention levels.\nThis allows her to see which parts of the live activity students were paying\nmost, and least, attention to. The Help-Me-Watch system is deployed and in use\nat our University in a way that protects student's personal data, operating in\na GDPR-compliant way.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:28:52 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Lee", "Hyowon", ""], ["Liu", "Mingming", ""], ["Riaz", "Hamza", ""], ["Rajasekaren", "Navaneethan", ""], ["Scriney", "Michael", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2101.06354", "submitter": "Abhinau Venkataramanan", "authors": "Abhinau K. Venkataramanan and Chengyang Wu and Alan C. Bovik and\n  Ioannis Katsavounidis and Zafar Shahid", "title": "A Hitchhiker's Guide to Structural Similarity", "comments": "Submitted final version to IEEE Access on January 30, 2021", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3056504", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Structural Similarity (SSIM) Index is a very widely used image/video\nquality model that continues to play an important role in the perceptual\nevaluation of compression algorithms, encoding recipes and numerous other\nimage/video processing algorithms. Several public implementations of the SSIM\nand Multiscale-SSIM (MS-SSIM) algorithms have been developed, which differ in\nefficiency and performance. This \"bendable ruler\" makes the process of quality\nassessment of encoding algorithms unreliable. To address this situation, we\nstudied and compared the functions and performances of popular and widely used\nimplementations of SSIM, and we also considered a variety of design choices.\nBased on our studies and experiments, we have arrived at a collection of\nrecommendations on how to use SSIM most effectively, including ways to reduce\nits computational burden.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 02:51:06 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:40:28 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Venkataramanan", "Abhinau K.", ""], ["Wu", "Chengyang", ""], ["Bovik", "Alan C.", ""], ["Katsavounidis", "Ioannis", ""], ["Shahid", "Zafar", ""]]}, {"id": "2101.06383", "submitter": "Soumendu Chakraborty", "authors": "Soumendu Chakraborty, and Anand Singh Jalal", "title": "A Novel Local Binary Pattern Based Blind Feature Image Steganography", "comments": null, "journal-ref": "Multimedia Tools and Applications, vol-79, no-27-28, pp.\n  19561-19574, 2020", "doi": "10.1007/s11042-020-08828-3", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Steganography methods in general terms tend to embed more and more secret\nbits in the cover images. Most of these methods are designed to embed secret\ninformation in such a way that the change in the visual quality of the\nresulting stego image is not detectable. There exists some methods which\npreserve the global structure of the cover after embedding. However, the\nembedding capacity of these methods is very less. In this paper a novel feature\nbased blind image steganography technique is proposed, which preserves the LBP\n(Local binary pattern) feature of the cover with comparable embedding rates.\nLocal binary pattern is a well known image descriptor used for image\nrepresentation. The proposed scheme computes the local binary pattern to hide\nthe bits of the secret image in such a way that the local relationship that\nexists in the cover are preserved in the resulting stego image. The performance\nof the proposed steganography method has been tested on several images of\ndifferent types to show the robustness. State of the art LSB based\nsteganography methods are compared with the proposed method to show the\neffectiveness of feature based image steganography\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 06:37:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chakraborty", "Soumendu", ""], ["Jalal", "Anand Singh", ""]]}, {"id": "2101.06650", "submitter": "Liang Liao", "authors": "Liang Liao, Xuechun Zhang, Xinqiang Wang, Sen Lin, Xin Liu", "title": "Generalized Image Reconstruction over T-Algebra", "comments": "6 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is well known for its capability of\ndimension reduction and data compression. However, when using PCA for\ncompressing/reconstructing images, images need to be recast to vectors. The\nvectorization of images makes some correlation constraints of neighboring\npixels and spatial information lost. To deal with the drawbacks of the\nvectorizations adopted by PCA, we used small neighborhoods of each pixel to\nform compounded pixels and use a tensorial version of PCA, called TPCA\n(Tensorial Principal Component Analysis), to compress and reconstruct a\ncompounded image of compounded pixels. Our experiments on public data show that\nTPCA compares favorably with PCA in compressing and reconstructing images. We\nalso show in our experiments that the performance of TPCA increases when the\norder of compounded pixels increases.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:44:50 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 01:42:02 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 14:51:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liao", "Liang", ""], ["Zhang", "Xuechun", ""], ["Wang", "Xinqiang", ""], ["Lin", "Sen", ""], ["Liu", "Xin", ""]]}, {"id": "2101.07144", "submitter": "William Wallis", "authors": "William Wallis and William Kavanagh and Alice Miller and Tim Storer", "title": "Designing a mobile game to generate player data -- lessons learned", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CY cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  User friendly tools have lowered the requirements of high-quality game design\nto the point where researchers without development experience can release their\nown games. However, there is no established best-practice as few games have\nbeen produced for research purposes. Having developed a mobile game without the\nguidance of similar projects, we realised the need to share our experience so\nfuture researchers have a path to follow. Research into game balancing and\nsystem simulation required an experimental case study, which inspired the\ncreation of \"RPGLite\", a multiplayer mobile game. In creating RPGLitewith no\ndevelopment expertise we learned a series of lessons about effective amateur\ngame development for research purposes. In this paper we reflect on the entire\ndevelopment process and present these lessons.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:16:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wallis", "William", ""], ["Kavanagh", "William", ""], ["Miller", "Alice", ""], ["Storer", "Tim", ""]]}, {"id": "2101.07439", "submitter": "Junghyuk Lee", "authors": "Manri Cheon, Toinon Vigier, Luk\\'a\\v{s} Krasula, Junghyuk Lee, Patrick\n  Le Callet, Jong-Seok Lee", "title": "Ambiguity of Objective Image Quality Metrics: A New Methodology for\n  Performance Evaluation", "comments": null, "journal-ref": "Signal Processing: Image Communication (2021)", "doi": "10.1016/j.image.2021.116150", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objective image quality metrics try to estimate the perceptual quality of the\ngiven image by considering the characteristics of the human visual system.\nHowever, it is possible that the metrics produce different quality scores even\nfor two images that are perceptually indistinguishable by human viewers, which\nhave not been considered in the existing studies related to objective quality\nassessment. In this paper, we address the issue of ambiguity of objective image\nquality assessment. We propose an approach to obtain an ambiguity interval of\nan objective metric, within which the quality score difference is not\nperceptually significant. In particular, we use the visual difference\npredictor, which can consider viewing conditions that are important for visual\nquality perception. In order to demonstrate the usefulness of the proposed\napproach, we conduct experiments with 33 state-of-the-art image quality metrics\nin the viewpoint of their accuracy and ambiguity for three image quality\ndatabases. The results show that the ambiguity intervals can be applied as an\nadditional figure of merit when conventional performance measurement does not\ndetermine superiority between the metrics. The effect of the viewing distance\non the ambiguity interval is also shown.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:13:25 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Cheon", "Manri", ""], ["Vigier", "Toinon", ""], ["Krasula", "Luk\u00e1\u0161", ""], ["Lee", "Junghyuk", ""], ["Callet", "Patrick Le", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2101.07451", "submitter": "Junghyuk Lee", "authors": "Junghyuk Lee, Toinon Vigier, Patrick Le Callet, Jong-Seok Lee", "title": "Wide Color Gamut Image Content Characterization: Method, Evaluation, and\n  Applications", "comments": null, "journal-ref": "IEEE Transactions on Multimedia (2020)", "doi": "10.1109/TMM.2020.3032026", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework to characterize a wide color\ngamut image content based on perceived quality due to the processes that change\ncolor gamut, and demonstrate two practical use cases where the framework can be\napplied. We first introduce the main framework and implementation details.\nThen, we provide analysis for understanding of existing wide color gamut\ndatasets with quantitative characterization criteria on their characteristics,\nwhere four criteria, i.e., coverage, total coverage, uniformity, and total\nuniformity, are proposed. Finally, the framework is applied to content\nselection in a gamut mapping evaluation scenario in order to enhance\nreliability and robustness of the evaluation results. As a result, the\nframework fulfils content characterization for studies where quality of\nexperience of wide color gamut stimuli is involved.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:55:26 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Lee", "Junghyuk", ""], ["Vigier", "Toinon", ""], ["Callet", "Patrick Le", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2101.07618", "submitter": "Qian Huang", "authors": "Chang Li and Qian Huang and Xing Li and Qianhan Wu", "title": "Human Action Recognition Based on Multi-scale Feature Maps from Depth\n  Video Sequences", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition is an active research area in computer vision.\nAlthough great process has been made, previous methods mostly recognize actions\nbased on depth data at only one scale, and thus they often neglect multi-scale\nfeatures that provide additional information action recognition in practical\napplication scenarios. In this paper, we present a novel framework focusing on\nmulti-scale motion information to recognize human actions from depth video\nsequences. We propose a multi-scale feature map called Laplacian pyramid depth\nmotion images(LP-DMI). We employ depth motion images (DMI) as the templates to\ngenerate the multi-scale static representation of actions. Then, we caculate\nLP-DMI to enhance multi-scale dynamic information of motions and reduces\nredundant static information in human bodies. We further extract the\nmulti-granularity descriptor called LP-DMI-HOG to provide more discriminative\nfeatures. Finally, we utilize extreme learning machine (ELM) for action\nclassification. The proposed method yeilds the recognition accuracy of 93.41%,\n85.12%, 91.94% on public MSRAction3D dataset, UTD-MHAD and DHA dataset. Through\nextensive experiments, we prove that our method outperforms state-of-the-art\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:46:42 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Li", "Chang", ""], ["Huang", "Qian", ""], ["Li", "Xing", ""], ["Wu", "Qianhan", ""]]}, {"id": "2101.08123", "submitter": "Panagiotis Kourtesis P.K.", "authors": "Panagiotis Kourtesis, Simona Collina, Leonidas A.A. Doumas, and Sarah\n  E. MacPherson", "title": "Technological Competence is a Precondition for Effective Implementation\n  of Virtual Reality Head Mounted Displays in Human Neuroscience: A\n  Technological Review and Meta-analysis", "comments": "Published in Frontiers in Human Neuroscience, 4 Figures, 4 Tables", "journal-ref": "2019,Frontiers in Human Neuroscience, 13, p.342", "doi": "10.3389/fnhum.2019.00342", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Immersive virtual reality (VR) emerges as a promising research and clinical\ntool. However, several studies suggest that VR induced adverse symptoms and\neffects (VRISE) may undermine the health and safety standards, and the\nreliability of the scientific results. In the current literature review, the\ntechnical reasons for the adverse symptomatology are investigated to provide\nsuggestions and technological knowledge for the implementation of VR\nhead-mounted display (HMD) systems in cognitive neuroscience. The technological\nsystematic literature indicated features pertinent to display, sound, motion\ntracking, navigation, ergonomic interactions, user experience, and computer\nhardware that should be considered by the researchers. Subsequently, a\nmeta-analysis of 44 neuroscientific or neuropsychological studies involving VR\nHMD systems was performed. The meta-analysis of the VR studies demonstrated\nthat new generation HMDs induced significantly less VRISE and marginally fewer\ndropouts.Importantly, the commercial versions of the new generation HMDs with\nergonomic interactions had zero incidents of adverse symptomatology and\ndropouts. HMDs equivalent to or greater than the commercial versions of\ncontemporary HMDs accompanied with ergonomic interactions are suitable for\nimplementation in cognitive neuroscience. In conclusion, researchers\ntechnological competency, along with meticulous methods and reports pertinent\nto software, hardware, and VRISE, are paramount to ensure the health and safety\nstandards and the reliability of neuroscientific results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:48:11 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Collina", "Simona", ""], ["Doumas", "Leonidas A. A.", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2101.08146", "submitter": "Panagiotis Kourtesis P.K.", "authors": "Panagiotis Kourtesis, Simona Collina, Leonidas A.A. Doumas, and Sarah\n  E. MacPherson", "title": "Validation of the Virtual Reality Neuroscience Questionnaire: Maximum\n  Duration of Immersive Virtual Reality Sessions Without the Presence of\n  Pertinent Adverse Symptomatology", "comments": "Published in Frontier in Human Neuroscience", "journal-ref": "2019.Frontiers in human neuroscience, 13, p.417", "doi": "10.3389/fnhum.2019.00417", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research suggests that the duration of a VR session modulates the presence\nand intensity of VRISE, but there are no suggestions regarding the appropriate\nmaximum duration of VR sessions. The implementation of high-end VR HMDs in\nconjunction with ergonomic VR software seems to mitigate the presence of VRISE\nsubstantially. However, a brief tool does not currently exist to appraise and\nreport both the quality of software features and VRISE intensity\nquantitatively. The VRNQ was developed to assess the quality of VR software in\nterms of user experience, game mechanics, in-game assistance, and VRISE. Forty\nparticipants aged between 28 and 43 years were recruited (18 gamers and 22\nnon-gamers) for the study. They participated in 3 different VR sessions until\nthey felt weary or discomfort and subsequently filled in the VRNQ. Our results\ndemonstrated that VRNQ is a valid tool for assessing VR software as it has good\nconvergent, discriminant, and construct validity. The maximum duration of VR\nsessions should be between 55-70 minutes when the VR software meets or exceeds\nthe parsimonious cut-offs of the VRNQ and the users are familiarized with the\nVR system. Also. the gaming experience does not seem to affect how long VR\nsessions should last. Also, while the quality of VR software substantially\nmodulates the maximum duration of VR sessions, age and education do not.\nFinally, deeper immersion, better quality of graphics and sound, and more\nhelpful in-game instructions and prompts were found to reduce VRISE intensity.\nThe VRNQ facilitates the brief assessment and reporting of the quality of VR\nsoftware features and/or the intensity of VRISE, while its minimum and\nparsimonious cut-offs may appraise the suitability of VR software. The findings\nof this study contribute to the establishment of rigorous VR methods that are\ncrucial for the viability of immersive VR as a research and clinical tool.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:10:44 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Collina", "Simona", ""], ["Doumas", "Leonidas A. A.", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2101.08166", "submitter": "Panagiotis Kourtesis P.K.", "authors": "Panagiotis Kourtesis, Danai Korre, Simona Collina, Leonidas A.A.\n  Doumas, and Sarah E. MacPherson", "title": "Guidelines for the Development of Immersive Virtual Reality Software for\n  Cognitive Neuroscience and Neuropsychology: The Development of Virtual\n  Reality Everyday Assessment Lab (VR-EAL)", "comments": "Published in Frontier in Computer Science", "journal-ref": "Frontiers in Computer Science, 1, p.12 (2020)", "doi": "10.3389/fcomp.2019.00012", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual reality (VR) head-mounted displays (HMD) appear to be effective\nresearch tools, which may address the problem of ecological validity in\nneuropsychological testing. However, their widespread implementation is\nhindered by VR induced symptoms and effects (VRISE) and the lack of skills in\nVR software development. This study offers guidelines for the development of VR\nsoftware in cognitive neuroscience and neuropsychology, by describing and\ndiscussing the stages of the development of Virtual Reality Everyday Assessment\nLab (VR-EAL), the first neuropsychological battery in immersive VR. Techniques\nfor evaluating cognitive functions within a realistic storyline are discussed.\nThe utility of various assets in Unity, software development kits, and other\nsoftware are described so that cognitive scientists can overcome challenges\npertinent to VRISE and the quality of the VR software. In addition, this pilot\nstudy attempts to evaluate VR-EAL in accordance with the necessary criteria for\nVR software for research purposes. The VR neuroscience questionnaire (VRNQ;\nKourtesis et al., 2019b) was implemented to appraise the quality of the three\nversions of VR-EAL in terms of user experience, game mechanics, in-game\nassistance, and VRISE. Twenty-five participants aged between 20 and 45 years\nwith 12-16 years of full-time education evaluated various versions of VR-EAL.\nThe final version of VR-EAL achieved high scores in every sub-score of the VRNQ\nand exceeded its parsimonious cut-offs. It also appeared to have better in-game\nassistance and game mechanics, while its improved graphics substantially\nincreased the quality of the user experience and almost eradicated VRISE. The\nresults substantially support the feasibility of the development of effective\nVR research and clinical software without the presence of VRISE during a\n60-minute VR session.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:55:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Korre", "Danai", ""], ["Collina", "Simona", ""], ["Doumas", "Leonidas A. A.", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2101.08502", "submitter": "Shadrokh Samavi", "authors": "Maedeh Jamali, Nader Karimi, Shadrokh Samavi", "title": "Weighted Fuzzy-Based PSNR for Watermarking", "comments": "Five pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the problems of conventional visual quality evaluation criteria such\nas PSNR and MSE is the lack of appropriate standards based on the human visual\nsystem (HVS). They are calculated based on the difference of the corresponding\npixels in the original and manipulated image. Hence, they practically do not\nprovide a correct understanding of the image quality. Watermarking is an image\nprocessing application in which the image's visual quality is an essential\ncriterion for its evaluation. Watermarking requires a criterion based on the\nHVS that provides more accurate values than conventional measures such as PSNR.\nThis paper proposes a weighted fuzzy-based criterion that tries to find\nessential parts of an image based on the HVS. Then these parts will have larger\nweights in computing the final value of PSNR. We compare our results against\nstandard PSNR, and our experiments show considerable consequences.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:41:05 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jamali", "Maedeh", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2101.08779", "submitter": "Ruilong Li", "authors": "Ruilong Li, Shan Yang, David A. Ross, Angjoo Kanazawa", "title": "Learn to Dance with AIST++: Music Conditioned 3D Dance Generation", "comments": "Project page: https://google.github.io/aichoreographer/; Dataset\n  page: https://google.github.io/aistplusplus_dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a transformer-based learning framework for 3D dance\ngeneration conditioned on music. We carefully design our network architecture\nand empirically study the keys for obtaining qualitatively pleasing results.\nThe critical components include a deep cross-modal transformer, which well\nlearns the correlation between the music and dance motion; and the\nfull-attention with future-N supervision mechanism which is essential in\nproducing long-range non-freezing motion. In addition, we propose a new dataset\nof paired 3D motion and music called AIST++, which we reconstruct from the AIST\nmulti-view dance videos. This dataset contains 1.1M frames of 3D dance motion\nin 1408 sequences, covering 10 genres of dance choreographies and accompanied\nwith multi-view camera parameters. To our knowledge it is the largest dataset\nof this kind. Rich experiments on AIST++ demonstrate our method produces much\nbetter results than the state-of-the-art methods both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:59:22 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 05:23:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Ruilong", ""], ["Yang", "Shan", ""], ["Ross", "David A.", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2101.08926", "submitter": "Shuai Li", "authors": "Chuankun Li, Shuai Li, Yanbo Gao, Xiang Zhang, Wanqing Li", "title": "A Two-stream Neural Network for Pose-based Hand Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pose based hand gesture recognition has been widely studied in the recent\nyears. Compared with full body action recognition, hand gesture involves joints\nthat are more spatially closely distributed with stronger collaboration. This\nnature requires a different approach from action recognition to capturing the\ncomplex spatial features. Many gesture categories, such as \"Grab\" and \"Pinch\",\nhave very similar motion or temporal patterns posing a challenge on temporal\nprocessing. To address these challenges, this paper proposes a two-stream\nneural network with one stream being a self-attention based graph convolutional\nnetwork (SAGCN) extracting the short-term temporal information and hierarchical\nspatial information, and the other being a residual-connection enhanced\nbidirectional Independently Recurrent Neural Network (RBi-IndRNN) for\nextracting long-term temporal information. The self-attention based graph\nconvolutional network has a dynamic self-attention mechanism to adaptively\nexploit the relationships of all hand joints in addition to the fixed topology\nand local feature extraction in the GCN. On the other hand, the\nresidual-connection enhanced Bi-IndRNN extends an IndRNN with the capability of\nbidirectional processing for temporal modelling. The two streams are fused\ntogether for recognition. The Dynamic Hand Gesture dataset and First-Person\nHand Action dataset are used to validate its effectiveness, and our method\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:22:26 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Li", "Chuankun", ""], ["Li", "Shuai", ""], ["Gao", "Yanbo", ""], ["Zhang", "Xiang", ""], ["Li", "Wanqing", ""]]}, {"id": "2101.09021", "submitter": "Hoang Trinh Man", "authors": "Trinh Man Hoang, Jinjia Zhou", "title": "B-DRRN: A Block Information Constrained Deep Recursive Residual Network\n  for Video Compression Artifacts Reduction", "comments": null, "journal-ref": "2019 Picture Coding Symposium (PCS), Ningbo, China, 2019, pp. 1-5", "doi": "10.1109/PCS48520.2019.8954521", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the video compression ratio nowadays becomes higher, the video\ncoders such as H.264/AVC, H.265/HEVC, H.266/VVC always suffer from the video\nartifacts. In this paper, we design a neural network to enhance the quality of\nthe compressed frame by leveraging the block information, called B-DRRN (Deep\nRecursive Residual Network with Block information). Firstly, an extra network\nbranch is designed for leveraging the block information of the coding unit\n(CU). Moreover, to avoid a great increase in the network size, Recursive\nResidual structure and sharing weight techniques are applied. We also conduct a\nnew large-scale dataset with 209,152 training samples. Experimental results\nshow that the proposed B-DRRN can reduce 6.16% BD-rate compared to the HEVC\nstandard. After efficiently adding an extra network branch, this work can\nimprove the performance of the main network without increasing any memory for\nstoring.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:35:44 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 05:52:08 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hoang", "Trinh Man", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2101.09412", "submitter": "Yazhou Yao", "authors": "Huafeng Liu, Chuanyi Zhang, Yazhou Yao, Xiushen Wei, Fumin Shen, Jian\n  Zhang, and Zhenmin Tang", "title": "Exploiting Web Images for Fine-Grained Visual Recognition by Eliminating\n  Noisy Samples and Utilizing Hard Ones", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling objects at a subordinate level typically requires expert knowledge,\nwhich is not always available when using random annotators. As such, learning\ndirectly from web images for fine-grained recognition has attracted broad\nattention. However, the presence of label noise and hard examples in web images\nare two obstacles for training robust fine-grained recognition models.\nTherefore, in this paper, we propose a novel approach for removing irrelevant\nsamples from real-world web images during training, while employing useful hard\nexamples to update the network. Thus, our approach can alleviate the harmful\neffects of irrelevant noisy web images and hard examples to achieve better\nperformance. Extensive experiments on three commonly used fine-grained datasets\ndemonstrate that our approach is far superior to current state-of-the-art\nweb-supervised methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 03:58:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Huafeng", ""], ["Zhang", "Chuanyi", ""], ["Yao", "Yazhou", ""], ["Wei", "Xiushen", ""], ["Shen", "Fumin", ""], ["Zhang", "Jian", ""], ["Tang", "Zhenmin", ""]]}, {"id": "2101.09642", "submitter": "Hoang Trinh Man", "authors": "Trinh Man Hoang, Jinjia Zhou, Yibo Fan", "title": "Image Compression with Encoder-Decoder Matched Semantic Segmentation", "comments": null, "journal-ref": "2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW), Seattle, WA, USA, 2020, pp. 619-623", "doi": "10.1109/CVPRW50498.2020.00088", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, layered image compression is demonstrated to be a promising\ndirection, which encodes a compact representation of the input image and apply\nan up-sampling network to reconstruct the image. To further improve the quality\nof the reconstructed image, some works transmit the semantic segment together\nwith the compressed image data. Consequently, the compression ratio is also\ndecreased because extra bits are required for transmitting the semantic\nsegment. To solve this problem, we propose a new layered image compression\nframework with encoder-decoder matched semantic segmentation (EDMS). And then,\nfollowed by the semantic segmentation, a special convolution neural network is\nused to enhance the inaccurate semantic segment. As a result, the accurate\nsemantic segment can be obtained in the decoder without requiring extra bits.\nThe experimental results show that the proposed EDMS framework can get up to\n35.31% BD-rate reduction over the HEVC-based (BPG) codec, 5% bitrate, and 24%\nencoding time saving compare to the state-of-the-art semantic-based image\ncodec.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 04:11:05 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 05:50:57 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hoang", "Trinh Man", ""], ["Zhou", "Jinjia", ""], ["Fan", "Yibo", ""]]}, {"id": "2101.09698", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Hanqing Lu", "title": "Fast Sequence Generation with Multi-Agent Reinforcement Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.04690", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoregressive sequence Generation models have achieved state-of-the-art\nperformance in areas like machine translation and image captioning. These\nmodels are autoregressive in that they generate each word by conditioning on\npreviously generated words, which leads to heavy latency during inference.\nRecently, non-autoregressive decoding has been proposed in machine translation\nto speed up the inference time by generating all words in parallel. Typically,\nthese models use the word-level cross-entropy loss to optimize each word\nindependently. However, such a learning process fails to consider the\nsentence-level consistency, thus resulting in inferior generation quality of\nthese non-autoregressive models. In this paper, we propose a simple and\nefficient model for Non-Autoregressive sequence Generation (NAG) with a novel\ntraining paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL\nformulates NAG as a multi-agent reinforcement learning system where element\npositions in the target sequence are viewed as agents that learn to\ncooperatively maximize a sentence-level reward. On MSCOCO image captioning\nbenchmark, our NAG method achieves a performance comparable to state-of-the-art\nautoregressive models, while brings 13.9x decoding speedup. On WMT14 EN-DE\nmachine translation dataset, our method outperforms cross-entropy trained\nbaseline by 6.0 BLEU points while achieves the greatest decoding speedup of\n17.46x.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:16:45 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["Lu", "Hanqing", ""]]}, {"id": "2101.10039", "submitter": "Mansi Sharma", "authors": "Balasubramanyam Appina, Mansi Sharma, Santosh Kumar", "title": "Latent Factor Modeling of Users Subjective Perception for Stereoscopic\n  3D Video Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous stereoscopic 3D movies are released every year to theaters and\ncreated large revenues. Despite the improvement in stereo capturing and 3D\nvideo post-production technology, stereoscopic artifacts which cause viewer\ndiscomfort continue to appear even in high-budget films. Existing automatic 3D\nvideo quality measurement tools can detect distortions in stereoscopic images\nor videos, but they fail to consider the viewer's subjective perception of\nthose artifacts, and how these distortions affect their choices. In this paper,\nwe introduce a novel recommendation system for stereoscopic 3D movies based on\na latent factor model that meticulously analyse the viewer's subjective ratings\nand influence of 3D video distortions on their preferences. To the best of our\nknowledge, this is a first-of-its-kind model that recommends 3D movies based on\nstereo-film quality ratings accounting correlation between the viewer's visual\ndiscomfort and stereoscopic-artifact perception. The proposed model is trained\nand tested on benchmark Nama3ds1-cospad1 and LFOVIAS3DPh2 S3D video quality\nassessment datasets. The experiments revealed that resulting\nmatrix-factorization based recommendation system is able to generalize\nconsiderably better for the viewer's subjective ratings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:13:32 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Appina", "Balasubramanyam", ""], ["Sharma", "Mansi", ""], ["Kumar", "Santosh", ""]]}, {"id": "2101.10514", "submitter": "Vishal Kaushal", "authors": "Vishal Kaushal, Suraj Kothawade, Anshul Tomar, Rishabh Iyer, Ganesh\n  Ramakrishnan", "title": "How Good is a Video Summary? A New Benchmarking Dataset and Evaluation\n  Framework Towards Realistic Video Summarization", "comments": "19 pages, 6 tables, 4 figures. arXiv admin note: substantial text\n  overlap with arXiv:2007.14560", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video summarization is still an unsolved problem due to several\nchallenges. The currently available datasets either have very short videos or\nhave few long videos of only a particular type. We introduce a new benchmarking\nvideo dataset called VISIOCITY (VIdeo SummarIzatiOn based on Continuity, Intent\nand DiversiTY) which comprises of longer videos across six different categories\nwith dense concept annotations capable of supporting different flavors of video\nsummarization and other vision problems. For long videos, human reference\nsummaries necessary for supervised video summarization techniques are difficult\nto obtain. We explore strategies to automatically generate multiple reference\nsummaries from indirect ground truth present in VISIOCITY. We show that these\nsummaries are at par with human summaries. We also present a study of different\ndesired characteristics of a good summary and demonstrate how it is normal to\nhave two good summaries with different characteristics. Thus we argue that\nevaluating a summary against one or more human summaries and using a single\nmeasure has its shortcomings. We propose an evaluation framework for better\nquantitative assessment of summary quality which is closer to human judgment.\nLastly, we present insights into how a model can be enhanced to yield better\nsummaries. Sepcifically, when multiple diverse ground truth summaries can\nexist, learning from them individually and using a combination of loss\nfunctions measuring different characteristics is better than learning from a\nsingle combined (oracle) ground truth summary using a single loss function. We\ndemonstrate the effectiveness of doing so as compared to some of the\nrepresentative state of the art techniques tested on VISIOCITY. We release\nVISIOCITY as a benchmarking dataset and invite researchers to test the\neffectiveness of their video summarization algorithms on VISIOCITY.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:42:55 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kaushal", "Vishal", ""], ["Kothawade", "Suraj", ""], ["Tomar", "Anshul", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "2101.10562", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Baptist Vandersmissen, Azarakhsh Jalalvand, Ivo\n  Couckuyt, Arnout Van Messem, Wesley De Neve", "title": "Investigating the significance of adversarial attacks and their relation\n  to interpretability for radar-based human activity recognition systems", "comments": "Accepted for publication on Computer Vision and Image Understanding,\n  Special issue on Adversarial Deep Learning in Biometrics & Forensics", "journal-ref": null, "doi": "10.1016/j.cviu.2020.103111", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given their substantial success in addressing a wide range of computer vision\nchallenges, Convolutional Neural Networks (CNNs) are increasingly being used in\nsmart home applications, with many of these applications relying on the\nautomatic recognition of human activities. In this context, low-power radar\ndevices have recently gained in popularity as recording sensors, given that the\nusage of these devices allows mitigating a number of privacy concerns, a key\nissue when making use of conventional video cameras. Another concern that is\noften cited when designing smart home applications is the resilience of these\napplications against cyberattacks. It is, for instance, well-known that the\ncombination of images and CNNs is vulnerable against adversarial examples,\nmischievous data points that force machine learning models to generate wrong\nclassifications during testing time. In this paper, we investigate the\nvulnerability of radar-based CNNs to adversarial attacks, and where these\nradar-based CNNs have been designed to recognize human gestures. Through\nexperiments with four unique threat models, we show that radar-based CNNs are\nsusceptible to both white- and black-box adversarial attacks. We also expose\nthe existence of an extreme adversarial attack case, where it is possible to\nchange the prediction made by the radar-based CNNs by only perturbing the\npadding of the inputs, without touching the frames where the action itself\noccurs. Moreover, we observe that gradient-based attacks exercise perturbation\nnot randomly, but on important features of the input data. We highlight these\nimportant features by making use of Grad-CAM, a popular neural network\ninterpretability method, hereby showing the connection between adversarial\nperturbation and prediction interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:16:16 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ozbulak", "Utku", ""], ["Vandersmissen", "Baptist", ""], ["Jalalvand", "Azarakhsh", ""], ["Couckuyt", "Ivo", ""], ["Van Messem", "Arnout", ""], ["De Neve", "Wesley", ""]]}, {"id": "2101.10795", "submitter": "Dasara Shullani", "authors": "Pengpeng Yang, Daniele Baracchi, Massimo Iuliani, Dasara Shullani,\n  Rongrong Ni, Yao Zhao, Alessandro Piva", "title": "Efficient video integrity analysis through container characterization", "comments": "Accepted by IEEE Journal of Selected Topics in Signal Processing", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 14, no.\n  5, pp. 947-954, Aug. 2020", "doi": "10.1109/JSTSP.2020.3008088", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video forensic techniques look for traces within the data stream that\nare, however, mostly ineffective when dealing with strongly compressed or low\nresolution videos. Recent research highlighted that useful forensic traces are\nalso left in the video container structure, thus offering the opportunity to\nunderstand the life-cycle of a video file without looking at the media stream\nitself.\n  In this paper we introduce a container-based method to identify the software\nused to perform a video manipulation and, in most cases, the operating system\nof the source device. As opposed to the state of the art, the proposed method\nis both efficient and effective and can also provide a simple explanation for\nits decisions. This is achieved by using a decision-tree-based classifier\napplied to a vectorial representation of the video container structure. We\nconducted an extensive validation on a dataset of 7000 video files including\nboth software manipulated contents (ffmpeg, Exiftool, Adobe Premiere, Avidemux,\nand Kdenlive), and videos exchanged through social media platforms (Facebook,\nTikTok, Weibo and YouTube). This dataset has been made available to the\nresearch community. The proposed method achieves an accuracy of 97.6% in\ndistinguishing pristine from tampered videos and classifying the editing\nsoftware, even when the video is cut without re-encoding or when it is\ndownscaled to the size of a thumbnail. Furthermore, it is capable of correctly\nidentifying the operating system of the source device for most of the tampered\nvideos.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:13:39 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yang", "Pengpeng", ""], ["Baracchi", "Daniele", ""], ["Iuliani", "Massimo", ""], ["Shullani", "Dasara", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""], ["Piva", "Alessandro", ""]]}, {"id": "2101.10955", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck, Balu Adsumilli,\n  and Alan C. Bovik", "title": "RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated\n  Content", "comments": "13 pages, 13 figurs, 5 tables. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Blind or no-reference video quality assessment of user-generated content\n(UGC) has become a trending, challenging, unsolved problem. Accurate and\nefficient video quality predictors suitable for this content are thus in great\ndemand to achieve more intelligent analysis and processing of UGC videos.\nPrevious studies have shown that natural scene statistics and deep learning\nfeatures are both sufficient to capture spatial distortions, which contribute\nto a significant aspect of UGC video quality issues. However, these models are\neither incapable or inefficient for predicting the quality of complex and\ndiverse UGC videos in practical applications. Here we introduce an effective\nand efficient video quality model for UGC content, which we dub the Rapid and\nAccurate Video Quality Evaluator (RAPIQUE), which we show performs comparably\nto state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.\nRAPIQUE combines and leverages the advantages of both quality-aware scene\nstatistics features and semantics-aware deep convolutional features, allowing\nus to design the first general and efficient spatial and temporal (space-time)\nbandpass statistics model for video quality modeling. Our experimental results\non recent large-scale UGC video quality databases show that RAPIQUE delivers\ntop performances on all the datasets at a considerably lower computational\nexpense. We hope this work promotes and inspires further efforts towards\npractical modeling of video quality problems for potential real-time and\nlow-latency applications. To promote public usage, an implementation of RAPIQUE\nhas been made freely available online: \\url{https://github.com/vztu/RAPIQUE}.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:23:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Yu", "Xiangxu", ""], ["Wang", "Yilin", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2101.11376", "submitter": "Charles Wilmot", "authors": "Charles Wilmot, Jochen Triesch", "title": "Learning Abstract Representations through Lossy Compression of\n  Multi-Modal Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A key competence for open-ended learning is the formation of increasingly\nabstract representations useful for driving complex behavior. Abstract\nrepresentations ignore specific details and facilitate generalization. Here we\nconsider the learning of abstract representations in a multi-modal setting with\ntwo or more input modalities. We treat the problem as a lossy compression\nproblem and show that generic lossy compression of multimodal sensory input\nnaturally extracts abstract representations that tend to strip away modalitiy\nspecific details and preferentially retain information that is shared across\nthe different modalities. Furthermore, we propose an architecture to learn\nabstract representations by identifying and retaining only the information that\nis shared across multiple modalities while discarding any modality specific\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:19:00 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 08:12:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wilmot", "Charles", ""], ["Triesch", "Jochen", ""]]}, {"id": "2101.11463", "submitter": "Lakmini Abeywardhana", "authors": "D.L.Abeywardhana, C.D.Dangalle, Anupiya Nugaliyadde, Yashas\n  Mallawarachchi", "title": "An Ultra-Specific Image Dataset for Automated Insect Identification", "comments": "This is a pre-print of the manuscript that is currently under review\n  in Multimedia Tools and Applications Journal, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated identification of insects is a tough task where many challenges\nlike data limitation, imbalanced data count, and background noise needs to be\novercome for better performance. This paper describes such an image dataset\nwhich consists of a limited, imbalanced number of images regarding six genera\nof subfamily Cicindelinae (tiger beetles) of order Coleoptera. The diversity of\nimage collection is at a high level as the images were taken from different\nsources, angles and on different scales. Thus, the salient regions of the\nimages have a large variation. Therefore, one of the main intentions in this\nprocess was to get an idea about the image dataset while comparing different\nunique patterns and features in images. The dataset was evaluated on different\nclassification algorithms including deep learning models based on different\napproaches to provide a benchmark. The dynamic nature of the dataset poses a\nchallenge to the image classification algorithms. However transfer learning\nmodels using softmax classifier performed well on current dataset. The tiger\nbeetle classification can be challenging even to a trained human eye,\ntherefore, this dataset opens a new avenue for the classification algorithms to\ndevelop, to identify features which human eyes have not identified.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:48:03 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Abeywardhana", "D. L.", ""], ["Dangalle", "C. D.", ""], ["Nugaliyadde", "Anupiya", ""], ["Mallawarachchi", "Yashas", ""]]}, {"id": "2101.11529", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Anirudh Thatipelli, Neel Trivedi, Ravi Kiran Sarvadevabhatla", "title": "NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions", "comments": "Code repository at https://github.com/skelemoa/ntu-x", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of fine-grained joints such as hand fingers is a fundamental\nperformance bottleneck for state of the art skeleton action recognition models\ntrained on the largest action recognition dataset, NTU-RGBD. To address this\nbottleneck, we introduce a new skeleton based human action dataset - NTU60-X.\nIn addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X\ndataset includes finger and facial joints, enabling a richer skeleton\nrepresentation. We appropriately modify the state of the art approaches to\nenable training using the introduced dataset. Our results demonstrate the\neffectiveness of NTU60-X in overcoming the aforementioned bottleneck and\nimprove state of the art performance, overall and on hitherto worst performing\naction categories.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:33:51 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:19:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Thatipelli", "Anirudh", ""], ["Trivedi", "Neel", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2101.11530", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Pranay Gupta, Divyanshu Sharma, Ravi Kiran Sarvadevabhatla", "title": "Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action\n  Recognition", "comments": "Accepted at ICIP-2021. Code and pretrained models available at\n  https://github.com/skelemoa/synse-zsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce SynSE, a novel syntactically guided generative approach for\nZero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined\ngenerative embedding spaces constrained within and across the involved\nmodalities (visual, language). The inter-modal constraints are defined between\naction sequence embedding and embeddings of Parts of Speech (PoS) tagged words\nin the corresponding action description. We deploy SynSE for the task of\nskeleton-based action sequence recognition. Our design choices enable SynSE to\ngeneralize compositionally, i.e., recognize sequences whose action descriptions\ncontain words not encountered during training. We also extend our approach to\nthe more challenging Generalized Zero-Shot Learning (GZSL) problem via a\nconfidence-based gating mechanism. We are the first to present zero-shot\nskeleton action recognition results on the large-scale NTU-60 and NTU-120\nskeleton action datasets with multiple splits. Our results demonstrate SynSE's\nstate of the art performance in both ZSL and GZSL settings compared to strong\nbaselines on the NTU-60 and NTU-120 datasets. The code and pretrained models\nare available at https://github.com/skelemoa/synse-zsl\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:34:27 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 23:59:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gupta", "Pranay", ""], ["Sharma", "Divyanshu", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2101.11563", "submitter": "Alan Smeaton", "authors": "Rashmiranjan Das and Gaurav Negi and Alan F. Smeaton", "title": "Detecting Deepfake Videos Using Euler Video Magnification", "comments": "Presented at Electronic Imaging: Media Watermarking, Security, and\n  Forensics, 27 January 2021, 6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in artificial intelligence make it progressively hard to\ndistinguish between genuine and counterfeit media, especially images and\nvideos. One recent development is the rise of deepfake videos, based on\nmanipulating videos using advanced machine learning techniques. This involves\nreplacing the face of an individual from a source video with the face of a\nsecond person, in the destination video. This idea is becoming progressively\nrefined as deepfakes are getting progressively seamless and simpler to compute.\nCombined with the outreach and speed of social media, deepfakes could easily\nfool individuals when depicting someone saying things that never happened and\nthus could persuade people in believing fictional scenarios, creating distress,\nand spreading fake news. In this paper, we examine a technique for possible\nidentification of deepfake videos. We use Euler video magnification which\napplies spatial decomposition and temporal filtering on video data to highlight\nand magnify hidden features like skin pulsation and subtle motions. Our\napproach uses features extracted from the Euler technique to train three models\nto classify counterfeit and unaltered videos and compare the results with\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:37:23 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Das", "Rashmiranjan", ""], ["Negi", "Gaurav", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2101.11704", "submitter": "Mahsa Shafaei", "authors": "Mahsa Shafaei, Christos Smailis, Ioannis A. Kakadiaris, Thamar Solorio", "title": "A Case Study of Deep Learning Based Multi-Modal Methods for Predicting\n  the Age-Suitability Rating of Movie Trailers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore different approaches to combine modalities for the\nproblem of automated age-suitability rating of movie trailers. First, we\nintroduce a new dataset containing videos of movie trailers in English\ndownloaded from IMDB and YouTube, along with their corresponding\nage-suitability rating labels. Secondly, we propose a multi-modal deep learning\npipeline addressing the movie trailer age suitability rating problem. This is\nthe first attempt to combine video, audio, and speech information for this\nproblem, and our experimental results show that multi-modal approaches\nsignificantly outperform the best mono and bimodal models in this task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:15:35 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Shafaei", "Mahsa", ""], ["Smailis", "Christos", ""], ["Kakadiaris", "Ioannis A.", ""], ["Solorio", "Thamar", ""]]}, {"id": "2101.11787", "submitter": "Arash Mohammadi", "authors": "Zohreh HajiAkhondi-Meybodi, Arash Mohammadi, Jamshid Abouei, Ming Hou,\n  and Konstantinos N. Plataniotis", "title": "Joint Transmission Scheme and Coded Content Placement in Cluster-centric\n  UAV-aided Cellular Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM eess.SP math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, as a consequence of the COVID-19 pandemic, dependence on\ntelecommunication for remote working and telemedicine has significantly\nincreased. In cellular networks, incorporation of Unmanned Aerial Vehicles\n(UAVs) can result in enhanced connectivity for outdoor users due to the high\nprobability of establishing Line of Sight (LoS) links. The UAV's limited\nbattery life and its signal attenuation in indoor areas, however, make it\ninefficient to manage users' requests in indoor environments. Referred to as\nthe Cluster centric and Coded UAV-aided Femtocaching (CCUF) framework, the\nnetwork's coverage in both indoor and outdoor environments increases via a\ntwo-phase clustering for FAPs' formation and UAVs' deployment. First objective\nis to increase the content diversity. In this context, we propose a coded\ncontent placement in a cluster-centric cellular network, which is integrated\nwith the Coordinated Multi-Point (CoMP) to mitigate the inter-cell interference\nin edge areas. Then, we compute, experimentally, the number of coded contents\nto be stored in each caching node to increase the cache-hit ratio,\nSignal-to-Interference-plus-Noise Ratio (SINR), and cache diversity and\ndecrease the users' access delay and cache redundancy for different content\npopularity profiles. Capitalizing on clustering, our second objective is to\nassign the best caching node to indoor/outdoor users for managing their\nrequests. In this regard, we define the movement speed of ground users as the\ndecision metric of the transmission scheme for serving outdoor users' requests\nto avoid frequent handovers between FAPs and increase the battery life of UAVs.\nSimulation results illustrate that the proposed CCUF implementation increases\nthe cache hit-ratio, SINR, and cache diversity and decrease the users' access\ndelay, cache redundancy and UAVs' energy consumption.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 02:43:31 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 15:33:39 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["HajiAkhondi-Meybodi", "Zohreh", ""], ["Mohammadi", "Arash", ""], ["Abouei", "Jamshid", ""], ["Hou", "Ming", ""], ["Plataniotis", "Konstantinos N.", ""]]}]