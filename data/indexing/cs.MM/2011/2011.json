[{"id": "2011.00176", "submitter": "Guang Hua Dr.", "authors": "Guang Hua, Qingyi Wang, Dengpan Ye, Haijian Zhang", "title": "Reliability of Power System Frequency on Times-Stamping Digital\n  Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power system frequency could be captured by digital recordings and extracted\nto compare with a reference database for forensic time-stamp verification. It\nis known as the electric network frequency (ENF) criterion, enabled by the\nproperties of random fluctuation and intra-grid consistency. In essence, this\nis a task of matching a short random sequence within a long reference, and the\nreliability of this criterion is mainly concerned with whether this match could\nbe unique and correct. In this paper, we comprehensively analyze the factors\naffecting the reliability of ENF matching, including length of test recording,\nlength of reference, temporal resolution, and signal-to-noise ratio (SNR). For\nsynthetic analysis, we incorporate the first-order autoregressive (AR) ENF\nmodel and propose an efficient time-frequency domain (TFD) noisy ENF synthesis\nmethod. Then, the reliability analysis schemes for both synthetic and\nreal-world data are respectively proposed. Through a comprehensive study we\nreveal that while the SNR is an important external factor to determine whether\ntime-stamp verification is viable, the length of test recording is the most\nimportant inherent factor, followed by the length of reference. However, the\ntemporal resolution has little impact on the matching process.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 03:13:16 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hua", "Guang", ""], ["Wang", "Qingyi", ""], ["Ye", "Dengpan", ""], ["Zhang", "Haijian", ""]]}, {"id": "2011.00307", "submitter": "Liang Liao", "authors": "Liang Liao and Stephen John Maybank", "title": "General Data Analytics with Applications to Visual Information Analysis:\n  A Provable Backward-Compatible Semisimple Paradigm over T-Algebra", "comments": "38 page, 12 figures. two typos are removed. Official code repository:\n  https://github.com/liaoliang2020/talgebra", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel backward-compatible paradigm of general data analytics\nover a recently-reported semisimple algebra (called t-algebra). We study the\nabstract algebraic framework over the t-algebra by representing the elements of\nt-algebra by fix-sized multi-way arrays of complex numbers and the algebraic\nstructure over the t-algebra by a collection of direct-product constituents.\nOver the t-algebra, many algorithms are generalized in a straightforward manner\nusing this new semisimple paradigm. To demonstrate the new paradigm's\nperformance and its backward-compatibility, we generalize some canonical\nalgorithms for visual pattern analysis. Experiments on public datasets show\nthat the generalized algorithms compare favorably with their canonical\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 16:41:09 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 16:25:09 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 10:22:55 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 02:03:34 GMT"}, {"version": "v5", "created": "Tue, 5 Jan 2021 12:48:32 GMT"}, {"version": "v6", "created": "Mon, 25 Jan 2021 11:31:49 GMT"}, {"version": "v7", "created": "Thu, 8 Apr 2021 07:47:40 GMT"}, {"version": "v8", "created": "Sun, 2 May 2021 15:45:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liao", "Liang", ""], ["Maybank", "Stephen John", ""]]}, {"id": "2011.00329", "submitter": "Zona Kostic", "authors": "Zona Kostic, Jared Jessup, Jeffrey Baglioni, Nathan Weeks, Johann\n  Philipp Dreessen, Ning Chen, Tianyu Liu", "title": "Visual Companion for Booklovers", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An innumerable number of individual choices go into discovering a new book.\nThere are unmistakably two groups of booklovers: those who like to search\nonline, follow other people's latest readings, or simply react to a system's\nrecommendations; and those who love to wander between library stacks, lose\nthemselves behind bookstore shelves, or simply hide behind piles of\n(un)organized books. Depending on which group a person may fall into, there are\ntwo distinct and corresponding mediums that inform his or her choices: digital,\nthat provides efficient retrieval of information online, and physical, a more\ntactile pursuit that leads to unexpected discoveries and promotes serendipity.\nHow could we possibly bridge the gap between these seemingly disparate mediums\ninto an integrated system that can amplify the benefits they both offer? In\nthis paper, we present the BookVIS application, which uses book-related data\nand generates personalized visualizations to follow users in their quest for a\nnew book. In this new redesigned version, the app brings associative visual\nconnections to support intuitive exploration of easily retrieved digital\ninformation and its relationship with the physical book in hand. BookVIS keeps\ntrack of the user's reading preferences and generates a dataSelfie as an\nindividual snapshot of a personal taste that grows over time. Usability testing\nhas also been conducted and has demonstrated the app's ability to identify\ndistinguishable patterns in readers' tastes that could be further used to\ncommunicate personal preferences in new \"shelf-browsing\" iterations. By\nefficiently supplementing the user's cognitive information needs while still\nsupporting the spontaneity and enjoyment of the book browsing experience,\nBookVIS bridges the gap between real and online realms, and maximizes the\nengagement of personalized mobile visual clues.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 18:16:50 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kostic", "Zona", ""], ["Jessup", "Jared", ""], ["Baglioni", "Jeffrey", ""], ["Weeks", "Nathan", ""], ["Dreessen", "Johann Philipp", ""], ["Chen", "Ning", ""], ["Liu", "Tianyu", ""]]}, {"id": "2011.00512", "submitter": "Hanzhou Wu", "authors": "Xiangyu Zhao, Hanzhou Wu and Xinpeng Zhang", "title": "Watermarking Graph Neural Networks by Random Graphs", "comments": "https://hzwu.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning tasks require us to deal with graph data which contains rich\nrelational information among elements, leading increasing graph neural network\n(GNN) models to be deployed in industrial products for improving the quality of\nservice. However, they also raise challenges to model authentication. It is\nnecessary to protect the ownership of the GNN models, which motivates us to\npresent a watermarking method to GNN models in this paper. In the proposed\nmethod, an Erdos-Renyi (ER) random graph with random node feature vectors and\nlabels is randomly generated as a trigger to train the GNN to be protected\ntogether with the normal samples. During model training, the secret watermark\nis embedded into the label predictions of the ER graph nodes. During model\nverification, by activating a marked GNN with the trigger ER graph, the\nwatermark can be reconstructed from the output to verify the ownership. Since\nthe ER graph was randomly generated, by feeding it to a non-marked GNN, the\nlabel predictions of the graph nodes are random, resulting in a low false alarm\nrate (of the proposed work). Experimental results have also shown that, the\nperformance of a marked GNN on its original task will not be impaired.\nMoreover, it is robust against model compression and fine-tuning, which has\nshown the superiority and applicability.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 14:22:48 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 12:18:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Wu", "Hanzhou", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2011.00569", "submitter": "C.-H. Huck Yang", "authors": "Jia-Hong Huang, Chao-Han Huck Yang, Fangyu Liu, Meng Tian, Yi-Chieh\n  Liu, Ting-Wei Wu, I-Hung Lin, Kang Wang, Hiromasa Morikawa, Hernghua Chang,\n  Jesper Tegner, Marcel Worring", "title": "DeepOpht: Medical Report Generation for Retinal Images via Deep Models\n  and Visual Explanation", "comments": "Accepted to IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose an AI-based method that intends to improve the\nconventional retinal disease treatment procedure and help ophthalmologists\nincrease diagnosis efficiency and accuracy. The proposed method is composed of\na deep neural networks-based (DNN-based) module, including a retinal disease\nidentifier and clinical description generator, and a DNN visual explanation\nmodule. To train and validate the effectiveness of our DNN-based module, we\npropose a large-scale retinal disease image dataset. Also, as ground truth, we\nprovide a retinal image dataset manually labeled by ophthalmologists to\nqualitatively show, the proposed AI-based method is effective. With our\nexperimental results, we show that the proposed method is quantitatively and\nqualitatively effective. Our method is capable of creating meaningful retinal\nimage descriptions and visual explanations that are clinically relevant.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:28:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Yang", "Chao-Han Huck", ""], ["Liu", "Fangyu", ""], ["Tian", "Meng", ""], ["Liu", "Yi-Chieh", ""], ["Wu", "Ting-Wei", ""], ["Lin", "I-Hung", ""], ["Wang", "Kang", ""], ["Morikawa", "Hiromasa", ""], ["Chang", "Hernghua", ""], ["Tegner", "Jesper", ""], ["Worring", "Marcel", ""]]}, {"id": "2011.00773", "submitter": "Varun Behera", "authors": "Ashish Ranjan, Varun Nagesh Jolly Behera, Motahar Reza", "title": "Using a Bi-directional LSTM Model with Attention Mechanism trained on\n  MIDI Data for Generating Unique Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating music is an interesting and challenging problem in the field of\nmachine learning. Mimicking human creativity has been popular in recent years,\nespecially in the field of computer vision and image processing. With the\nadvent of GANs, it is possible to generate new similar images, based on trained\ndata. But this cannot be done for music similarly, as music has an extra\ntemporal dimension. So it is necessary to understand how music is represented\nin digital form. When building models that perform this generative task, the\nlearning and generation part is done in some high-level representation such as\nMIDI (Musical Instrument Digital Interface) or scores. This paper proposes a\nbi-directional LSTM (Long short-term memory) model with attention mechanism\ncapable of generating similar type of music based on MIDI data. The music\ngenerated by the model follows the theme/style of the music the model is\ntrained on. Also, due to the nature of MIDI, the tempo, instrument, and other\nparameters can be defined, and changed, post generation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 06:43:28 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ranjan", "Ashish", ""], ["Behera", "Varun Nagesh Jolly", ""], ["Reza", "Motahar", ""]]}, {"id": "2011.01018", "submitter": "Mathilde Brousmiche", "authors": "Mathilde Brousmiche and St\\'ephane Dupont and Jean Rouat", "title": "AVECL-UMONS database for audio-visual event classification and\n  localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the AVECL-UMons dataset for audio-visual event classification\nand localization in the context of office environments. The audio-visual\ndataset is composed of 11 event classes recorded at several realistic positions\nin two different rooms. Two types of sequences are recorded according to the\nnumber of events in the sequence. The dataset comprises 2662 unilabel sequences\nand 2724 multilabel sequences corresponding to a total of 5.24 hours. The\ndataset is publicly accessible online :\nhttps://zenodo.org/record/3965492#.X09wsobgrCI.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:26:02 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Brousmiche", "Mathilde", ""], ["Dupont", "St\u00e9phane", ""], ["Rouat", "Jean", ""]]}, {"id": "2011.01114", "submitter": "Prateek Manocha", "authors": "Prateek Manocha and Prithwijit Guha", "title": "Facial Keypoint Sequence Generation from Audio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whenever we speak, our voice is accompanied by facial movements and\nexpressions. Several recent works have shown the synthesis of highly\nphoto-realistic videos of talking faces, but they either require a source video\nto drive the target face or only generate videos with a fixed head pose. This\nlack of facial movement is because most of these works focus on the lip\nmovement in sync with the audio while assuming the remaining facial keypoints'\nfixed nature. To address this, a unique audio-keypoint dataset of over 150,000\nvideos at 224p and 25fps is introduced that relates the facial keypoint\nmovement for the given audio. This dataset is then further used to train the\nmodel, Audio2Keypoint, a novel approach for synthesizing facial keypoint\nmovement to go with the audio. Given a single image of the target person and an\naudio sequence (in any language), Audio2Keypoint generates a plausible keypoint\nmovement sequence in sync with the input audio, conditioned on the input image\nto preserve the target person's facial characteristics. To the best of our\nknowledge, this is the first work that proposes an audio-keypoint dataset and\nlearns a model to output the plausible keypoint sequence to go with audio of\nany arbitrary length. Audio2Keypoint generalizes across unseen people with a\ndifferent facial structure allowing us to generate the sequence with the voice\nfrom any source or even synthetic voices. Instead of learning a direct mapping\nfrom audio to video domain, this work aims to learn the audio-keypoint mapping\nthat allows for in-plane and out-of-plane head rotations, while preserving the\nperson's identity using a Pose Invariant (PIV) Encoder.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:47:52 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Manocha", "Prateek", ""], ["Guha", "Prithwijit", ""]]}, {"id": "2011.01414", "submitter": "Songyang Zhang", "authors": "Li Sun, Haoqi Zhang, Songyang Zhang, Jiebo Luo", "title": "Content-based Analysis of the Cultural Differences between TikTok and\n  Douyin", "comments": "Accepted by IEEE Big Data 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-form video social media shifts away from the traditional media paradigm\nby telling the audience a dynamic story to attract their attention. In\nparticular, different combinations of everyday objects can be employed to\nrepresent a unique scene that is both interesting and understandable. Offered\nby the same company, TikTok and Douyin are popular examples of such new media\nthat has become popular in recent years, while being tailored for different\nmarkets (e.g. the United States and China). The hypothesis that they express\ncultural differences together with media fashion and social idiosyncrasy is the\nprimary target of our research. To that end, we first employ the Faster\nRegional Convolutional Neural Network (Faster R-CNN) pre-trained with the\nMicrosoft Common Objects in COntext (MS-COCO) dataset to perform object\ndetection. Based on a suite of objects detected from videos, we perform\nstatistical analysis including label statistics, label similarity, and\nlabel-person distribution. We further use the Two-Stream Inflated 3D ConvNet\n(I3D) pre-trained with the Kinetics dataset to categorize and analyze human\nactions. By comparing the distributional results of TikTok and Douyin, we\nuncover a wealth of similarity and contrast between the two closely related\nvideo social media platforms along the content dimensions of object quantity,\nobject categories, and human action categories.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 01:47:49 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Sun", "Li", ""], ["Zhang", "Haoqi", ""], ["Zhang", "Songyang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2011.01631", "submitter": "Vandana Rajan", "authors": "Vandana Rajan, Alessio Brutti, Andrea Cavallaro", "title": "Robust Latent Representations via Cross-Modal Translation and Alignment", "comments": null, "journal-ref": "ICASSP 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal learning relates information across observation modalities of the\nsame physical phenomenon to leverage complementary information. Most\nmulti-modal machine learning methods require that all the modalities used for\ntraining are also available for testing. This is a limitation when the signals\nfrom some modalities are unavailable or are severely degraded by noise. To\naddress this limitation, we aim to improve the testing performance of uni-modal\nsystems using multiple modalities during training only. The proposed\nmulti-modal training framework uses cross-modal translation and\ncorrelation-based latent space alignment to improve the representations of the\nweaker modalities. The translation from the weaker to the stronger modality\ngenerates a multi-modal intermediate encoding that is representative of both\nmodalities. This encoding is then correlated with the stronger modality\nrepresentations in a shared latent space. We validate the proposed approach on\nthe AVEC 2016 dataset for continuous emotion recognition and show the\neffectiveness of the approach that achieves state-of-the-art (uni-modal)\nperformance for weaker modalities.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:18:04 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 23:16:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Rajan", "Vandana", ""], ["Brutti", "Alessio", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2011.01803", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, N S Raghava", "title": "A novel group based cryptosystem based on electromagnetic rotor machine", "comments": "journal PAPER BASED ON ROTOR MACHINE , PUBLISHED IN Indian Journal of\n  Scientific Research, pp. 131-136 , 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an algorithm is aimed to make a cryptosystem for gray level\nimages based on voice features, secret sharing scheme and electromagnetic rotor\nmachine. Here, Shamir secret sharing (k n) threshold scheme is used to secure a\nkey along with voice features of (n k) users. Keystream is molded by\ncoefficients of a voice sample, using this key stream, rotor machines rotating\ncylinders positions are initialized and internal wiring is decided by pseudo\nrandom number of Henon chaotic map, where initial seed for chaotic system is\nchosen from keystream. And furthermore, shares of key stream are distributed\namong users. Speech processing is fused with electromagnetic machine to provide\nauthentication as well as group based encryption. Perceptual linear predication\n(PLP) coefficients are utilized for formation of secret key. Simulation\nexperiments and statistical analysis demonstrate that the proposed algorithm is\nsensitive to initial secret keystream, entropy, mean value analysis and\nhistogram of the encrypted image is admirable. Hence, the proposed scheme is\nresistible to any vulnerable situation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:50:04 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Kumar", "Ashish", ""], ["Raghava", "N S", ""]]}, {"id": "2011.02620", "submitter": "Wenying Wen", "authors": "Wenying Wen, Rongxin Tu, Yushu Zhang, Yuming Fang, Yong Yang", "title": "A multi-level approach with visual information for encrypted H.265/HEVC\n  videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-efficiency video coding (HEVC) encryption has been proposed to encrypt\nsyntax elements for the purpose of video encryption. To achieve high video\nsecurity, to the best of our knowledge, almost all of the existing HEVC\nencryption algorithms mainly encrypt the whole video, such that the user\nwithout permissions cannot obtain any viewable information. However, these\nencryption algorithms cannot meet the needs of customers who need part of the\ninformation but not the full information in the video. In many cases, such as\nprofessional paid videos or video meetings, users would like to observe some\nvisible information in the encrypted video of the original video to satisfy\ntheir requirements in daily life. Aiming at this demand, this paper proposes a\nmulti-level encryption scheme that is composed of lightweight encryption,\nmedium encryption and heavyweight encryption, where each encryption level can\nobtain a different amount of visual information. It is found that both\nencrypting the luma intraprediction model (IPM) and scrambling the syntax\nelement of the DCT coefficient sign can achieve the performance of a distorted\nvideo in which there is still residual visual information, while encrypting\nboth of them can implement the intensity of encryption and one cannot gain any\nvisual information. The experimental results meet our expectations\nappropriately, indicating that there is a different amount of visual\ninformation in each encryption level. Meanwhile, users can flexibly choose the\nencryption level according to their various requirements.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 02:20:43 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Wen", "Wenying", ""], ["Tu", "Rongxin", ""], ["Zhang", "Yushu", ""], ["Fang", "Yuming", ""], ["Yang", "Yong", ""]]}, {"id": "2011.03322", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Li Liu, Dongyan Zhao and Rui Yan", "title": "Learning to Respond with Your Favorite Stickers: A Framework of Unifying\n  Multi-Modality and User Preference in Multi-Turn Dialog", "comments": "Accepted by TOIS. arXiv admin note: substantial text overlap with\n  arXiv:2003.04679", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stickers with vivid and engaging expressions are becoming increasingly\npopular in online messaging apps, and some works are dedicated to automatically\nselect sticker response by matching the stickers image with previous\nutterances. However, existing methods usually focus on measuring the matching\ndegree between the dialog context and sticker image, which ignores the user\npreference of using stickers. Hence, in this paper, we propose to recommend an\nappropriate sticker to user based on multi-turn dialog context and sticker\nusing history of user. Two main challenges are confronted in this task. One is\nto model the sticker preference of user based on the previous sticker selection\nhistory. Another challenge is to jointly fuse the user preference and the\nmatching between dialog context and candidate sticker into final prediction\nmaking. To tackle these challenges, we propose a \\emph{Preference Enhanced\nSticker Response Selector} (PESRS) model. Specifically, PESRS first employs a\nconvolutional based sticker image encoder and a self-attention based multi-turn\ndialog encoder to obtain the representation of stickers and utterances. Next,\ndeep interaction network is proposed to conduct deep matching between the\nsticker and each utterance. Then, we model the user preference by using the\nrecently selected stickers as input, and use a key-value memory network to\nstore the preference representation. PESRS then learns the short-term and\nlong-term dependency between all interaction results by a fusion network, and\ndynamically fuse the user preference representation into the final sticker\nselection prediction. Extensive experiments conducted on a large-scale\nreal-world dialog dataset show that our model achieves the state-of-the-art\nperformance for all commonly-used metrics. Experiments also verify the\neffectiveness of each component of PESRS.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 03:31:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Liu", "Li", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2011.03451", "submitter": "Rongcheng Tu", "authors": "Rong-Cheng Tu, Xian-Ling Mao, Rongxin Tu, Binbin Bian, Wei Wei, Heyan\n  Huang", "title": "Deep Cross-modal Hashing via Margin-dynamic-softmax Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their high retrieval efficiency and low storage cost for cross-modal\nsearch task, cross-modal hashing methods have attracted considerable attention.\nFor the supervised cross-modal hashing methods, how to make the learned hash\ncodes preserve semantic information sufficiently contained in the label of\ndatapoints is the key to further enhance the retrieval performance. Hence,\nalmost all supervised cross-modal hashing methods usually depends on defining a\nsimilarity between datapoints with the label information to guide the hashing\nmodel learning fully or partly. However, the defined similarity between\ndatapoints can only capture the label information of datapoints partially and\nmisses abundant semantic information, then hinders the further improvement of\nretrieval performance. Thus, in this paper, different from previous works, we\npropose a novel cross-modal hashing method without defining the similarity\nbetween datapoints, called Deep Cross-modal Hashing via\n\\textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains\na proxy hashing network to transform each category information of a dataset\ninto a semantic discriminative hash code, called proxy hash code. Each proxy\nhash code can preserve the semantic information of its corresponding category\nwell. Next, without defining the similarity between datapoints to supervise the\ntraining process of the modality-specific hashing networks , we propose a novel\n\\textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing\ncodes as supervised information. Finally, by minimizing the novel\n\\textit{margin-dynamic-softmax loss}, the modality-specific hashing networks\ncan be trained to generate hash codes which can simultaneously preserve the\ncross-modal similarity and abundant semantic information well.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:02:35 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 15:02:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Tu", "Rong-Cheng", ""], ["Mao", "Xian-Ling", ""], ["Tu", "Rongxin", ""], ["Bian", "Binbin", ""], ["Wei", "Wei", ""], ["Huang", "Heyan", ""]]}, {"id": "2011.04263", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang, Ming Jiang", "title": "Unified Quality Assessment of In-the-Wild Videos with Mixed Datasets\n  Training", "comments": "20 pages, 12 figures, 7 tables, accepted by IJCV. This is the version\n  provided to IJCV office", "journal-ref": null, "doi": "10.1007/s11263-020-01408-w", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality assessment (VQA) is an important problem in computer vision.\nThe videos in computer vision applications are usually captured in the wild. We\nfocus on automatically assessing the quality of in-the-wild videos, which is a\nchallenging problem due to the absence of reference videos, the complexity of\ndistortions, and the diversity of video contents. Moreover, the video contents\nand distortions among existing datasets are quite different, which leads to\npoor performance of data-driven methods in the cross-dataset evaluation\nsetting. To improve the performance of quality assessment models, we borrow\nintuitions from human perception, specifically, content dependency and\ntemporal-memory effects of human visual system. To face the cross-dataset\nevaluation challenge, we explore a mixed datasets training strategy for\ntraining a single VQA model with multiple datasets. The proposed unified\nframework explicitly includes three stages: relative quality assessor,\nnonlinear mapping, and dataset-specific perceptual scale alignment, to jointly\npredict relative quality, perceptual quality, and subjective quality.\nExperiments are conducted on four publicly available datasets for VQA in the\nwild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental\nresults verify the effectiveness of the mixed datasets training strategy and\nprove the superior performance of the unified model in comparison with the\nstate-of-the-art models. For reproducible research, we make the PyTorch\nimplementation of our method available at https://github.com/lidq92/MDTVSFA.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:22:57 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 09:13:58 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "2011.04959", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Hongnian Guo, Yang Du", "title": "Multi-domain Reversible Data Hiding in JPEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a branch of reversible data hiding (RDH), reversible data hiding in JEPG\nis particularly important. Because JPEG images are widely used, it is great\nsignificance to study reversible data hiding algorithm for JEPG images. The\nexisting JEPG reversible data methods can be divided into two categories, one\nis based on Discrete Cosine Transform (DCT) coefficients modification, the\nother is based on Huffman table modification, the methods based on DCT\ncoefficient modification result in large file expansion and visual quality\ndistortion, while the methods based on entropy coding domain modification have\nlow capacity and they may lead to large file expansion. In order to effectively\nsolve the problems in these two kinds of methods, this paper proposes a\nreversible data hiding in JPEG images methods based on multi-domain\nmodification. In this method, the secret data is divided into two parts by\npayload distribution algorithm, part of the secret data is first embedded in\nthe DCT coefficient domain, and then the remaining secret data is embedded in\nthe entropy coding domain. Experimental results demonstrate that most JPEG\nimage files with this scheme have smaller file size increment and higher\npayload than previous RDH schemes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 07:52:35 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Guo", "Hongnian", ""], ["Du", "Yang", ""]]}, {"id": "2011.05049", "submitter": "Zongheng Tang", "authors": "Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu\n  Jiang, Qian Yu, Dong Xu", "title": "Human-centric Spatio-Temporal Video Grounding With Visual Transformers", "comments": "Accept at TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel task - Humancentric Spatio-Temporal Video\nGrounding (HC-STVG). Unlike the existing referring expression tasks in images\nor videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal\ntube of the target person from an untrimmed video based on a given textural\ndescription. This task is useful, especially for healthcare and\nsecurity-related applications, where the surveillance videos can be extremely\nlong but only a specific person during a specific period of time is concerned.\nHC-STVG is a video grounding task that requires both spatial (where) and\ntemporal (when) localization. Unfortunately, the existing grounding methods\ncannot handle this task well. We tackle this task by proposing an effective\nbaseline method named Spatio-Temporal Grounding with Visual Transformers\n(STGVT), which utilizes Visual Transformers to extract cross-modal\nrepresentations for video-sentence matching and temporal localization. To\nfacilitate this task, we also contribute an HC-STVG dataset consisting of 5,660\nvideo-sentence pairs on complex multi-person scenes. Specifically, each video\nlasts for 20 seconds, pairing with a natural query sentence with an average of\n17.25 words. Extensive experiments are conducted on this dataset, demonstrating\nthe newly-proposed method outperforms the existing baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:23:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 06:51:34 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tang", "Zongheng", ""], ["Liao", "Yue", ""], ["Liu", "Si", ""], ["Li", "Guanbin", ""], ["Jin", "Xiaojie", ""], ["Jiang", "Hongxu", ""], ["Yu", "Qian", ""], ["Xu", "Dong", ""]]}, {"id": "2011.05203", "submitter": "Remi Ronfard", "authors": "R\\'emi Ronfard and R\\'emi Colin de Verdi\\`ere", "title": "OpenKinoAI: An Open Source Framework for Intelligent Cinematography and\n  Editing of Live Performances", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenKinoAI is an open source framework for post-production of ultra high\ndefinition video which makes it possible to emulate professional multiclip\nediting techniques for the case of single camera recordings. OpenKinoAI\nincludes tools for uploading raw video footage of live performances on a remote\nweb server, detecting, tracking and recognizing the performers in the original\nmaterial, reframing the raw video into a large choice of cinematographic\nrushes, editing the rushes into movies, and annotating rushes and movies for\ndocumentation purposes. OpenKinoAI is made available to promote research in\nmulticlip video editing of ultra high definition video, and to allow performing\nartists and companies to use this research for archiving, documenting and\nsharing their work online in an innovative fashion.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:21:53 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ronfard", "R\u00e9mi", ""], ["de Verdi\u00e8re", "R\u00e9mi Colin", ""]]}, {"id": "2011.06087", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young and Siobh\\'an Mannion and Sara Wentworth", "title": "Evoking Places from Spaces. The application of multimodal narrative\n  techniques in the creation of \"U Modified\"", "comments": "5 pages", "journal-ref": "15th Sound and Music Computing Conference (SMC2018)", "doi": "10.5281/zenodo.1408596", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal diegetic narrative tools, as applied in multimedia arts practices,\npossess the ability to cross the spaces that exist between the physical world\nand the imaginary. Within this paper we present the findings of a\nmultidiscipline practice based research project that explored the potential of\nan audiovisual art performance to purposefully interact with an audience's\nperception of narrative place. To achieve this goal, research was undertaken to\ninvestigate the function of multimodal diegetic practices as applied in the\ncontext of a sonic art narrative. This project direction was undertaken to\nfacilitate the transformation of previous experiences of place through the\ncreative amalgamation and presentation of collected audio and visual footage\nfrom real world spaces. Through the presentation of multimedia relating to\nfamiliar geographical spatial features, the audience were affected to evoke\nmemories of place and to construct and manipulate their own narrative.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 20:50:53 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Young", "Gareth W.", ""], ["Mannion", "Siobh\u00e1n", ""], ["Wentworth", "Sara", ""]]}, {"id": "2011.06133", "submitter": "Yulia Gryaditskaya Dr", "authors": "Yue Zhong, Yulia Gryaditskaya, Honggang Zhang, Yi-Zhe Song", "title": "Deep Sketch-Based Modeling: Tips and Tricks", "comments": null, "journal-ref": null, "doi": "10.1109/3DV50981.2020.00064", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image-based modeling received lots of attention in recent years, yet the\nparallel problem of sketch-based modeling has only been briefly studied, often\nas a potential application. In this work, for the first time, we identify the\nmain differences between sketch and image inputs: (i) style variance, (ii)\nimprecise perspective, and (iii) sparsity. We discuss why each of these\ndifferences can pose a challenge, and even make a certain class of image-based\nmethods inapplicable. We study alternative solutions to address each of the\ndifference. By doing so, we drive out a few important insights: (i) sparsity\ncommonly results in an incorrect prediction of foreground versus background,\n(ii) diversity of human styles, if not taken into account, can lead to very\npoor generalization properties, and finally (iii) unless a dedicated sketching\ninterface is used, one can not expect sketches to match a perspective of a\nfixed viewpoint. Finally, we compare a set of representative deep single-image\nmodeling solutions and show how their performance can be improved to tackle\nsketch input by taking into consideration the identified critical differences.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 00:34:08 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 13:26:57 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 09:23:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhong", "Yue", ""], ["Gryaditskaya", "Yulia", ""], ["Zhang", "Honggang", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2011.06490", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Content-based Image Retrieval and the Semantic Gap in the Deep Learning\n  Era", "comments": "CBIR workshop at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval has seen astonishing progress over the past\ndecade, especially for the task of retrieving images of the same object that is\ndepicted in the query image. This scenario is called instance or object\nretrieval and requires matching fine-grained visual patterns between images.\nSemantics, however, do not play a crucial role. This brings rise to the\nquestion: Do the recent advances in instance retrieval transfer to more generic\nimage retrieval scenarios? To answer this question, we first provide a brief\noverview of the most relevant milestones of instance retrieval. We then apply\nthem to a semantic image retrieval task and find that they perform inferior to\nmuch less sophisticated and more generic methods in a setting that requires\nimage understanding. Following this, we review existing approaches to closing\nthis so-called semantic gap by integrating prior world knowledge. We conclude\nthat the key problem for the further advancement of semantic image retrieval\nlies in the lack of a standardized task definition and an appropriate benchmark\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:00:08 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "2011.06691", "submitter": "Fabien Racape", "authors": "Franck Galpin, Fabien Racap\\'e, Sunil Jaiswal, Philippe Bordes,\n  Fabrice Le L\\'eannec, Edouard Fran\\c{c}ois", "title": "CNN-based driving of block partitioning for intra slices encoding", "comments": "10 pages", "journal-ref": "2019 Data Compression Conference (DCC)", "doi": "10.1109/DCC.2019.00024", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a technical overview of a deep-learning-based encoder\nmethod aiming at optimizing next generation hybrid video encoders for driving\nthe block partitioning in intra slices. An encoding approach based on\nConvolutional Neural Networks is explored to partly substitute classical\nheuristics-based encoder speed-ups by a systematic and automatic process. The\nsolution allows controlling the trade-off between complexity and coding gains,\nin intra slices, with one single parameter. This algorithm was proposed at the\nCall for Proposals of the Joint Video Exploration Team (JVET) on video\ncompression with capability beyond HEVC. In All Intra configuration, for a\ngiven allowed topology of splits, a speed-up of $\\times 2$ is obtained without\nBD-rate loss, or a speed-up above $\\times 4$ with a loss below 1\\% in BD-rate.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 23:55:12 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Galpin", "Franck", ""], ["Racap\u00e9", "Fabien", ""], ["Jaiswal", "Sunil", ""], ["Bordes", "Philippe", ""], ["L\u00e9annec", "Fabrice Le", ""], ["Fran\u00e7ois", "Edouard", ""]]}, {"id": "2011.07735", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, Gurneet Arora, Navpreet Kaloty", "title": "iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video\n  Captioning and Video Question Answering", "comments": "13 pages, 6 figures, 4 tables, Project Page:\n  https://iperceive.amanchadha.com", "journal-ref": "IEEE Winter Conference on Applications of Computer Vision (WACV)\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prior art in visual understanding relies solely on analyzing the \"what\"\n(e.g., event recognition) and \"where\" (e.g., event localization), which in some\ncases, fails to describe correct contextual relationships between events or\nleads to incorrect underlying visual attention. Part of what defines us as\nhuman and fundamentally different from machines is our instinct to seek\ncausality behind any association, say an event Y that happened as a direct\nresult of event X. To this end, we propose iPerceive, a framework capable of\nunderstanding the \"why\" between events in a video by building a common-sense\nknowledge base using contextual cues to infer causal relationships between\nobjects in the video. We demonstrate the effectiveness of our technique using\nthe dense video captioning (DVC) and video question answering (VideoQA) tasks.\nFurthermore, while most prior work in DVC and VideoQA relies solely on visual\ninformation, other modalities such as audio and speech are vital for a human\nobserver's perception of an environment. We formulate DVC and VideoQA tasks as\nmachine translation problems that utilize multiple modalities. By evaluating\nthe performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet\nCaptions and TVQA datasets respectively, we show that our approach furthers the\nstate-of-the-art. Code and samples are available at: iperceive.amanchadha.com.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:44:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chadha", "Aman", ""], ["Arora", "Gurneet", ""], ["Kaloty", "Navpreet", ""]]}, {"id": "2011.07792", "submitter": "Edoardo Daniele Cannas", "authors": "Luca Bondi, Edoardo Daniele Cannas, Paolo Bestagini, Stefano Tubaro", "title": "Training Strategies and Data Augmentations in CNN-based DeepFake Video\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast and continuous growth in number and quality of deepfake videos calls\nfor the development of reliable detection systems capable of automatically\nwarning users on social media and on the Internet about the potential\nuntruthfulness of such contents. While algorithms, software, and smartphone\napps are getting better every day in generating manipulated videos and swapping\nfaces, the accuracy of automated systems for face forgery detection in videos\nis still quite limited and generally biased toward the dataset used to design\nand train a specific detection system. In this paper we analyze how different\ntraining strategies and data augmentation techniques affect CNN-based deepfake\ndetectors when training and testing on the same dataset or across different\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:50:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bondi", "Luca", ""], ["Cannas", "Edoardo Daniele", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2011.07815", "submitter": "Hao Su", "authors": "Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Ji Wan, Mingliang Xu,\n  Tao Ren", "title": "An End-to-end Method for Producing Scanning-robust Stylized QR Codes", "comments": "11 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quick Response (QR) code is one of the most worldwide used two-dimensional\ncodes.~Traditional QR codes appear as random collections of black-and-white\nmodules that lack visual semantics and aesthetic elements, which inspires the\nrecent works to beautify the appearances of QR codes. However, these works\nadopt fixed generation algorithms and therefore can only generate QR codes with\na pre-defined style. In this paper, combining the Neural Style Transfer\ntechnique, we propose a novel end-to-end method, named ArtCoder, to generate\nthe stylized QR codes that are personalized, diverse, attractive, and\nscanning-robust.~To guarantee that the generated stylized QR codes are still\nscanning-robust, we propose a Sampling-Simulation layer, a module-based code\nloss, and a competition mechanism. The experimental results show that our\nstylized QR codes have high-quality in both the visual effect and the\nscanning-robustness, and they are able to support the real-world application.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 09:38:27 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Su", "Hao", ""], ["Niu", "Jianwei", ""], ["Liu", "Xuefeng", ""], ["Li", "Qingfeng", ""], ["Wan", "Ji", ""], ["Xu", "Mingliang", ""], ["Ren", "Tao", ""]]}, {"id": "2011.08525", "submitter": "Naoki Sugimoto", "authors": "Naoki Sugimoto, Yoshihito Ebine, Kiyoharu Aizawa", "title": "Building Movie Map -- A Tool for Exploring Areas in a City -- and its\n  Evaluation", "comments": null, "journal-ref": "ACM Multimedia 2020", "doi": "10.1145/3394171.3413881", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Movie Map system, with an interface for exploring cities.\nThe system consists of four stages; acquisition, analysis, management, and\ninteraction. In the acquisition stage, omnidirectional videos are taken along\nstreets in target areas. Frames of the video are localized on the map,\nintersections are detected, and videos are segmented. Turning views at\nintersections are subsequently generated. By connecting the video segments\nfollowing the specified movement in an area, we can view the streets better.\nThe interface allows for easy exploration of a target area, and it can show\nvirtual billboards of stores in the view. We conducted user studies to compare\nour system to the GSV in a scenario where users could freely move and explore\nto find a landmark. The experiment showed that our system had a better user\nexperience than GSV.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:24:05 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Sugimoto", "Naoki", ""], ["Ebine", "Yoshihito", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2011.08612", "submitter": "Jing Zhang", "authors": "Jing Zhang and Dacheng Tao", "title": "Empowering Things with Intelligence: A Survey of the Progress,\n  Challenges, and Opportunities in Artificial Intelligence of Things", "comments": "Accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Internet of Things (IoT) era, billions of sensors and devices collect\nand process data from the environment, transmit them to cloud centers, and\nreceive feedback via the internet for connectivity and perception. However,\ntransmitting massive amounts of heterogeneous data, perceiving complex\nenvironments from these data, and then making smart decisions in a timely\nmanner are difficult. Artificial intelligence (AI), especially deep learning,\nis now a proven success in various areas including computer vision, speech\nrecognition, and natural language processing. AI introduced into the IoT\nheralds the era of artificial intelligence of things (AIoT). This paper\npresents a comprehensive survey on AIoT to show how AI can empower the IoT to\nmake it faster, smarter, greener, and safer. Specifically, we briefly present\nthe AIoT architecture in the context of cloud computing, fog computing, and\nedge computing. Then, we present progress in AI research for IoT from four\nperspectives: perceiving, learning, reasoning, and behaving. Next, we summarize\nsome promising applications of AIoT that are likely to profoundly reshape our\nworld. Finally, we highlight the challenges facing AIoT and some potential\nresearch opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:14:28 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.08964", "submitter": "Shoko Imaizumi", "authors": "Shoko Imaizumi and Hitoshi Kiya", "title": "A Block-Permutation-Based Encryption Scheme with Independent Processing\n  of RGB Components", "comments": "8 pages", "journal-ref": "IEICE Trans. Inf. & Syst., vol.E101-D, no.12, pp.3150-3157,\n  December 2018", "doi": "10.1587/transinf.2018EDT0002", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a block-permutation-based encryption (BPBE) scheme for\nthe encryption-then-compression (ETC) system that enhances the color\nscrambling. A BPBE image can be obtained through four processes, positional\nscrambling, block rotation/flip, negative-positive transformation, and color\ncomponent shuffling, after dividing the original image into multiple blocks.\nThe proposed scheme scrambles the R, G, and B components independently in\npositional scrambling, block rotation/flip, and negative-positive\ntransformation, by assigning different keys to each color component. The\nconventional scheme considers the compression efficiency using JPEG and JPEG\n2000, which need a color conversion before the compression process by default.\nTherefore, the conventional scheme scrambles the color components identically\nin each process. In contrast, the proposed scheme takes into account the\nRGB-based compression, such as JPEG-LS, and thus can increase the extent of the\nscrambling. The resilience against jigsaw puzzle solver (JPS) can consequently\nbe increased owing to the wider color distribution of the BPBE image.\nAdditionally, the key space for resilience against brute-force attacks has also\nbeen expanded exponentially. Furthermore, the proposed scheme can maintain the\nJPEG-LS compression efficiency compared to the conventional scheme. We confirm\nthe effectiveness of the proposed scheme by experiments and analyses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 21:57:01 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Imaizumi", "Shoko", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2011.09078", "submitter": "Yizhou Zhao", "authors": "Yizhou Zhao, Liang Qiu, Wensi Ai, Feng Shi, Song-Chun Zhu", "title": "Vertical-Horizontal Structured Attention for Generating Music with\n  Chords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a lightweight music-generating model based on\nvariational autoencoder (VAE) with structured attention. Generating music is\ndifferent from generating text because the melodies with chords give listeners\ndistinguished polyphonic feelings. In a piece of music, a chord consisting of\nmultiple notes comes from either the mixture of multiple instruments or the\ncombination of multiple keys of a single instrument. We focus our study on the\nlatter. Our model captures not only the temporal relations along time but the\nstructure relations between keys. Experimental results show that our model has\na better performance than baseline MusicVAE in capturing notes in a chord.\nBesides, our method accords with music theory since it maintains the\nconfiguration of the circle of fifths, distinguishes major and minor keys from\ninterval vectors, and manifests meaningful structures between music phrases.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 04:08:42 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zhao", "Yizhou", ""], ["Qiu", "Liang", ""], ["Ai", "Wensi", ""], ["Shi", "Feng", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2011.09088", "submitter": "Michael Lyons", "authors": "Michael J. Lyons and Daniel Kluender", "title": "Three Patterns to Support Empathy in Computer-Mediated Human Interaction", "comments": "6 pages, 4 figures", "journal-ref": "ACM CHI'04 Workshop Human-Computer-Human Interaction Patterns:\n  Workshop on the Human Role in HCI Patterns Vienna, Austria, April 25-26, 2004", "doi": "10.5281/zenodo.4278447", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present three patterns for computer-mediated interaction which we\ndiscovered during the design and development of a platform for remote teaching\nand learning of kanji, the Chinese characters used in written Japanese. Our aim\nin developing this system was to provide a basis for embodiment in remote\ninteraction, and in particular to support the experience of empathy by both\nteacher and student. From this study, the essential elements are abstracted and\nsuggested as design patterns for other computer-mediated interaction systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 05:02:20 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Lyons", "Michael J.", ""], ["Kluender", "Daniel", ""]]}, {"id": "2011.09663", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah, Kristen Grauman", "title": "Modeling Fashion Influence from Photos", "comments": "To appear in the IEEE Transactions on Multimedia, 2020. Project page:\n  https://www.cs.utexas.edu/~ziad/influence_from_photos.html. arXiv admin note:\n  substantial text overlap with arXiv:2004.01316", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of clothing styles and their migration across the world is\nintriguing, yet difficult to describe quantitatively. We propose to discover\nand quantify fashion influences from catalog and social media photos. We\nexplore fashion influence along two channels: geolocation and fashion brands.\nWe introduce an approach that detects which of these entities influence which\nother entities in terms of propagating their styles. We then leverage the\ndiscovered influence patterns to inform a novel forecasting model that predicts\nthe future popularity of any given style within any given city or brand. To\ndemonstrate our idea, we leverage public large-scale datasets of 7.7M Instagram\nphotos from 44 major world cities (where styles are worn with variable\nfrequency) as well as 41K Amazon product photos (where styles are purchased\nwith variable frequency). Our model learns directly from the image data how\nstyles move between locations and how certain brands affect each other's\ndesigns in a predictable way. The discovered influence relationships reveal how\nboth cities and brands exert and receive fashion influence for an array of\nvisual styles inferred from the images. Furthermore, the proposed forecasting\nmodel achieves state-of-the-art results for challenging style forecasting\ntasks. Our results indicate the advantage of grounding visual style evolution\nboth spatially and temporally, and for the first time, they quantify the\npropagation of inter-brand and inter-city influences.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 20:24:03 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2011.10432", "submitter": "George Pantazis", "authors": "George Pantazis, George Dimas and Dimitris K. Iakovidis", "title": "SalSum: Saliency-based Video Summarization using Generative Adversarial\n  Networks", "comments": "18 pages, 5 figures. Submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The huge amount of video data produced daily by camera-based systems, such as\nsurveilance, medical and telecommunication systems, emerges the need for\neffective video summarization (VS) methods. These methods should be capable of\ncreating an overview of the video content. In this paper, we propose a novel VS\nmethod based on a Generative Adversarial Network (GAN) model pre-trained with\nhuman eye fixations. The main contribution of the proposed method is that it\ncan provide perceptually compatible video summaries by combining both perceived\ncolor and spatiotemporal visual attention cues in a unsupervised scheme.\nSeveral fusion approaches are considered for robustness under uncertainty, and\npersonalization. The proposed method is evaluated in comparison to\nstate-of-the-art VS approaches on the benchmark dataset VSUMM. The experimental\nresults conclude that SalSum outperforms the state-of-the-art approaches by\nproviding the highest f-measure score on the VSUMM benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:53:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pantazis", "George", ""], ["Dimas", "George", ""], ["Iakovidis", "Dimitris K.", ""]]}, {"id": "2011.10688", "submitter": "Xinwei Yao", "authors": "Xinwei Yao, Ohad Fried, Kayvon Fatahalian, Maneesh Agrawala", "title": "Iterative Text-based Editing of Talking-heads Using Neural Retargeting", "comments": "Project Website is https://davidyao.me/projects/text2vid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a text-based tool for editing talking-head video that enables an\niterative editing workflow. On each iteration users can edit the wording of the\nspeech, further refine mouth motions if necessary to reduce artifacts and\nmanipulate non-verbal aspects of the performance by inserting mouth gestures\n(e.g. a smile) or changing the overall performance style (e.g. energetic,\nmumble). Our tool requires only 2-3 minutes of the target actor video and it\nsynthesizes the video for each iteration in about 40 seconds, allowing users to\nquickly explore many editing possibilities as they iterate. Our approach is\nbased on two key ideas. (1) We develop a fast phoneme search algorithm that can\nquickly identify phoneme-level subsequences of the source repository video that\nbest match a desired edit. This enables our fast iteration loop. (2) We\nleverage a large repository of video of a source actor and develop a new\nself-supervised neural retargeting technique for transferring the mouth motions\nof the source actor to the target actor. This allows us to work with relatively\nshort target actor videos, making our approach applicable in many real-world\nediting scenarios. Finally, our refinement and performance controls give users\nthe ability to further fine-tune the synthesized results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:05:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yao", "Xinwei", ""], ["Fried", "Ohad", ""], ["Fatahalian", "Kayvon", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "2011.10834", "submitter": "Adolfo Almeida", "authors": "A. Almeida, J.P. de Villiers, A. De Freitas, M. Velayudan", "title": "Exploring the multimodal information from video content using deep\n  learning features of appearance, audio and action for video recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the popularisation of media streaming, a number of video streaming\nservices are continuously buying new video content to mine the potential profit\nfrom them. As such, the newly added content has to be handled well to be\nrecommended to suitable users. In this paper, we address the new item\ncold-start problem by exploring the potential of various deep learning features\nto provide video recommendations. The deep learning features investigated\ninclude features that capture the visual-appearance, audio and motion\ninformation from video content. We also explore different fusion methods to\nevaluate how well these feature modalities can be combined to fully exploit the\ncomplementary information captured by them. Experiments on a real-world video\ndataset for movie recommendations show that deep learning features outperform\nhand-crafted features. In particular, recommendations generated with deep\nlearning audio features and action-centric deep learning features are superior\nto MFCC and state-of-the-art iDT features. In addition, the combination of\nvarious deep learning features with hand-crafted features and textual metadata\nyields significant improvement in recommendations compared to combining only\nthe former.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 18:00:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Almeida", "A.", ""], ["de Villiers", "J. P.", ""], ["De Freitas", "A.", ""], ["Velayudan", "M.", ""]]}, {"id": "2011.11286", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Ayush Jaiswal, Wael AbdAlmageed, Prem Natarajan", "title": "MEG: Multi-Evidence GNN for Multimodal Semantic Forensics", "comments": "To be published at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news often involves semantic manipulations across modalities such as\nimage, text, location etc and requires the development of multimodal semantic\nforensics for its detection. Recent research has centered the problem around\nimages, calling it image repurposing -- where a digitally unmanipulated image\nis semantically misrepresented by means of its accompanying multimodal metadata\nsuch as captions, location, etc. The image and metadata together comprise a\nmultimedia package. The problem setup requires algorithms to perform multimodal\nsemantic forensics to authenticate a query multimedia package using a reference\ndataset of potentially related packages as evidences. Existing methods are\nlimited to using a single evidence (retrieved package), which ignores potential\nperformance improvement from the use of multiple evidences. In this work, we\nintroduce a novel graph neural network based model for multimodal semantic\nforensics, which effectively utilizes multiple retrieved packages as evidences\nand is scalable with the number of evidences. We compare the scalability and\nperformance of our model against existing methods. Experimental results show\nthat the proposed model outperforms existing state-of-the-art algorithms with\nan error reduction of up to 25%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 09:01:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sabir", "Ekraam", ""], ["Jaiswal", "Ayush", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Prem", ""]]}, {"id": "2011.11377", "submitter": "Yuzhi Zhao", "authors": "Yuzhi Zhao, Lai-Man Po, Kwok-Wai Cheung, Wing-Yin Yu, Yasar Abbas Ur\n  Rehman", "title": "SCGAN: Saliency Map-guided Colorization with Generative Adversarial\n  Network", "comments": "accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3037688", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a grayscale photograph, the colorization system estimates a visually\nplausible colorful image. Conventional methods often use semantics to colorize\ngrayscale images. However, in these methods, only classification semantic\ninformation is embedded, resulting in semantic confusion and color bleeding in\nthe final colorized image. To address these issues, we propose a fully\nautomatic Saliency Map-guided Colorization with Generative Adversarial Network\n(SCGAN) framework. It jointly predicts the colorization and saliency map to\nminimize semantic confusion and color bleeding in the colorized image. Since\nthe global features from pre-trained VGG-16-Gray network are embedded to the\ncolorization encoder, the proposed SCGAN can be trained with much less data\nthan state-of-the-art methods to achieve perceptually reasonable colorization.\nIn addition, we propose a novel saliency map-based guidance method. Branches of\nthe colorization decoder are used to predict the saliency map as a proxy\ntarget. Moreover, two hierarchical discriminators are utilized for the\ngenerated colorization and saliency map, respectively, in order to strengthen\nvisual perception performance. The proposed system is evaluated on ImageNet\nvalidation set. Experimental results show that SCGAN can generate more\nreasonable colorized images than state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:06:54 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhao", "Yuzhi", ""], ["Po", "Lai-Man", ""], ["Cheung", "Kwok-Wai", ""], ["Yu", "Wing-Yin", ""], ["Rehman", "Yasar Abbas Ur", ""]]}, {"id": "2011.11711", "submitter": "Chavit Denninnart", "authors": "Chavit Denninnart", "title": "Cost- and QoS-Efficient Serverless Cloud Computing", "comments": "PhD thesis, University of Louisiana at Lafayette (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud-based serverless computing systems, either public or privately\nprovisioned, aim to provide the illusion of infinite resources and abstract\nusers from details of the allocation decisions. With the goal of providing a\nlow cost and a high QoS, the serverless computing paradigm offers opportunities\nthat can be harnessed to attain the goals. Specifically, our strategy in this\ndissertation is to avoid redundant computing, in cases where independent task\nrequests are similar to each other and for tasks that are pointless to process.\nWe explore two main approaches to (A) reuse part of computation needed to\nprocess the services and (B) proactively pruning tasks with a low chance of\nsuccess to improve the overall QoS of the system. For the first approach, we\npropose a mechanism to identify various types of \"mergeable\" tasks, which can\nbenefit from computational reuse if they are executed together as a group. To\nevaluate the task merging configurations extensively, we quantify the\nresource-saving magnitude and then leveraging the experimental data to create a\nresource-saving predictor. We investigate multiple tasks merging approaches\nthat suit different workload scenarios to determine when it is appropriate to\naggregate tasks and how to allocate them so that the QoS of other tasks is\nminimally affected. For the second approach, we developed the mechanisms to\nskip tasks whose chance of completing on time is not worth pursuing by drop or\ndefer them. We determined the minimum chance of success thresholds for tasks to\npass to get scheduled and executed. We dynamically adjust such thresholds based\non multiple characteristics of the arriving workload and the system's\nconditions. We employed approximate computing to reduce the pruning mechanism's\ncomputational overheads and ensure that the mechanism can be used practically.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:30:25 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Denninnart", "Chavit", ""]]}, {"id": "2011.11970", "submitter": "Abhilash Nandy", "authors": "Manish Agrawal, Abhilash Nandy", "title": "A Novel Multimodal Music Genre Classifier using Hierarchical Attention\n  and Convolutional Neural Network", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music genre classification is one of the trending topics in regards to the\ncurrent Music Information Retrieval (MIR) Research. Since, the dependency of\ngenre is not only limited to the audio profile, we also make use of textual\ncontent provided as lyrics of the corresponding song. We implemented a CNN\nbased feature extractor for spectrograms in order to incorporate the acoustic\nfeatures and a Hierarchical Attention Network based feature extractor for\nlyrics. We then go on to classify the music track based upon the resulting\nfused feature vector.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 09:02:35 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Agrawal", "Manish", ""], ["Nandy", "Abhilash", ""]]}, {"id": "2011.12091", "submitter": "Xirong Li", "authors": "Xirong Li and Fangming Zhou and Chaoxi Xu and Jiaqi Ji and Gang Yang", "title": "SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries", "comments": "accepted for publication as a REGULAR paper in the IEEE Transactions\n  on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search\n(AVS), is a core theme in multimedia data management and retrieval. The success\nof AVS counts on cross-modal representation learning that encodes both query\nsentences and videos into common spaces for semantic similarity computation.\nInspired by the initial success of previously few works in combining multiple\nsentence encoders, this paper takes a step forward by developing a new and\ngeneral method for effectively exploiting diverse sentence encoders. The\nnovelty of the proposed method, which we term Sentence Encoder Assembly (SEA),\nis two-fold. First, different from prior art that use only a single common\nspace, SEA supports text-video matching in multiple encoder-specific common\nspaces. Such a property prevents the matching from being dominated by a\nspecific encoder that produces an encoding vector much longer than other\nencoders. Second, in order to explore complementarities among the individual\ncommon spaces, we propose multi-space multi-loss learning. As extensive\nexperiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD)\nshow, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to\nimplement. All this makes SEA an appealing solution for AVS and promising for\ncontinuously advancing the task by harvesting new sentence encoders.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:54:28 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Li", "Xirong", ""], ["Zhou", "Fangming", ""], ["Xu", "Chaoxi", ""], ["Ji", "Jiaqi", ""], ["Yang", "Gang", ""]]}, {"id": "2011.12097", "submitter": "Shuyang Sun", "authors": "Shuyang Sun, Liang Chen, Gregory Slabaugh, Philip Torr", "title": "Learning to Sample the Most Useful Training Patches from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some image restoration tasks like demosaicing require difficult training\nsamples to learn effective models. Existing methods attempt to address this\ndata training problem by manually collecting a new training dataset that\ncontains adequate hard samples, however, there are still hard and simple areas\neven within one single image. In this paper, we present a data-driven approach\ncalled PatchNet that learns to select the most useful patches from an image to\nconstruct a new training set instead of manual or random selection. We show\nthat our simple idea automatically selects informative samples out from a\nlarge-scale dataset, leading to a surprising 2.35dB generalisation gain in\nterms of PSNR. In addition to its remarkable effectiveness, PatchNet is also\nresource-friendly as it is applied only during training and therefore does not\nrequire any additional computational cost during inference.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:06:50 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Sun", "Shuyang", ""], ["Chen", "Liang", ""], ["Slabaugh", "Gregory", ""], ["Torr", "Philip", ""]]}, {"id": "2011.12470", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Xuanbai Chen, Xiangyu Yue, Chuang Lin, Pengfei Xu, Ravi\n  Krishna, Jufeng Yang, Guiguang Ding, Alberto L. Sangiovanni-Vincentelli, Kurt\n  Keutzer", "title": "Emotional Semantics-Preserved and Feature-Aligned CycleGAN for Visual\n  Emotion Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to large-scale labeled training data, deep neural networks (DNNs) have\nobtained remarkable success in many vision and multimedia tasks. However,\nbecause of the presence of domain shift, the learned knowledge of the\nwell-trained DNNs cannot be well generalized to new domains or datasets that\nhave few labels. Unsupervised domain adaptation (UDA) studies the problem of\ntransferring models trained on one labeled source domain to another unlabeled\ntarget domain. In this paper, we focus on UDA in visual emotion analysis for\nboth emotion distribution learning and dominant emotion classification.\nSpecifically, we design a novel end-to-end cycle-consistent adversarial model,\ntermed CycleEmotionGAN++. First, we generate an adapted domain to align the\nsource and target domains on the pixel-level by improving CycleGAN with a\nmulti-scale structured cycle-consistency loss. During the image translation, we\npropose a dynamic emotional semantic consistency loss to preserve the emotion\nlabels of the source images. Second, we train a transferable task classifier on\nthe adapted domain with feature-level alignment between the adapted and target\ndomains. We conduct extensive UDA experiments on the Flickr-LDL & Twitter-LDL\ndatasets for distribution learning and ArtPhoto & FI datasets for emotion\nclassification. The results demonstrate the significant improvements yielded by\nthe proposed CycleEmotionGAN++ as compared to state-of-the-art UDA approaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:31:01 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhao", "Sicheng", ""], ["Chen", "Xuanbai", ""], ["Yue", "Xiangyu", ""], ["Lin", "Chuang", ""], ["Xu", "Pengfei", ""], ["Krishna", "Ravi", ""], ["Yang", "Jufeng", ""], ["Ding", "Guiguang", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2011.12836", "submitter": "Yu Zeng", "authors": "Yu Zeng, Zhe Lin, Huchuan Lu, Vishal M. Patel", "title": "CR-Fill: Generative Image Inpainting with Auxiliary Contexutal\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep generative inpainting methods use attention layers to allow the\ngenerator to explicitly borrow feature patches from the known region to\ncomplete a missing region. Due to the lack of supervision signals for the\ncorrespondence between missing regions and known regions, it may fail to find\nproper reference features, which often leads to artifacts in the results. Also,\nit computes pair-wise similarity across the entire feature map during inference\nbringing a significant computational overhead. To address this issue, we\npropose to teach such patch-borrowing behavior to an attention-free generator\nby joint training of an auxiliary contextual reconstruction task, which\nencourages the generated output to be plausible even when reconstructed by\nsurrounding regions. The auxiliary branch can be seen as a learnable loss\nfunction, i.e. named as contextual reconstruction (CR) loss, where\nquery-reference feature similarity and reference-based reconstructor are\njointly optimized with the inpainting generator. The auxiliary branch (i.e. CR\nloss) is required only during training, and only the inpainting generator is\nrequired during the inference. Experimental results demonstrate that the\nproposed inpainting model compares favourably against the state-of-the-art in\nterms of quantitative and visual performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:45:12 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 11:47:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zeng", "Yu", ""], ["Lin", "Zhe", ""], ["Lu", "Huchuan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2011.13075", "submitter": "Haoxin Wang", "authors": "Haoxin Wang, BaekGyu Kim, Jiang Xie and Zhu Han", "title": "Energy Drain of the Object Detection Processing Pipeline for Mobile\n  Devices: Analysis and Implications", "comments": "This is a personal copy of the authors. Not for redistribution. The\n  final version of this paper was accepted by IEEE Transactions on Green\n  Communications and Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applying deep learning to object detection provides the capability to\naccurately detect and classify complex objects in the real world. However,\ncurrently, few mobile applications use deep learning because such technology is\ncomputation-intensive and energy-consuming. This paper, to the best of our\nknowledge, presents the first detailed experimental study of a mobile augmented\nreality (AR) client's energy consumption and the detection latency of executing\nConvolutional Neural Networks (CNN) based object detection, either locally on\nthe smartphone or remotely on an edge server. In order to accurately measure\nthe energy consumption on the smartphone and obtain the breakdown of energy\nconsumed by each phase of the object detection processing pipeline, we propose\na new measurement strategy. Our detailed measurements refine the energy\nanalysis of mobile AR clients and reveal several interesting perspectives\nregarding the energy consumption of executing CNN-based object detection.\nFurthermore, several insights and research opportunities are proposed based on\nour experimental results. These findings from our experimental study will guide\nthe design of energy-efficient processing pipeline of CNN-based object\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:32:07 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Haoxin", ""], ["Kim", "BaekGyu", ""], ["Xie", "Jiang", ""], ["Han", "Zhu", ""]]}, {"id": "2011.13322", "submitter": "Zhen Huang", "authors": "Zhen Huang, Xu Shen, Xinmei Tian, Houqiang Li, Jianqiang Huang and\n  Xian-Sheng Hua", "title": "Spatio-Temporal Inception Graph Convolutional Networks for\n  Skeleton-Based Action Recognition", "comments": "ACMMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Skeleton-based human action recognition has attracted much attention with the\nprevalence of accessible depth sensors. Recently, graph convolutional networks\n(GCNs) have been widely used for this task due to their powerful capability to\nmodel graph data. The topology of the adjacency graph is a key factor for\nmodeling the correlations of the input skeletons. Thus, previous methods mainly\nfocus on the design/learning of the graph topology. But once the topology is\nlearned, only a single-scale feature and one transformation exist in each layer\nof the networks. Many insights, such as multi-scale information and multiple\nsets of transformations, that have been proven to be very effective in\nconvolutional neural networks (CNNs), have not been investigated in GCNs. The\nreason is that, due to the gap between graph-structured skeleton data and\nconventional image/video data, it is very challenging to embed these insights\ninto GCNs. To overcome this gap, we reinvent the split-transform-merge strategy\nin GCNs for skeleton sequence processing. Specifically, we design a simple and\nhighly modularized graph convolutional network architecture for skeleton-based\naction recognition. Our network is constructed by repeating a building block\nthat aggregates multi-granularity information from both the spatial and\ntemporal paths. Extensive experiments demonstrate that our network outperforms\nstate-of-the-art methods by a significant margin with only 1/5 of the\nparameters and 1/10 of the FLOPs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:43:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Huang", "Zhen", ""], ["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["Li", "Houqiang", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2011.13675", "submitter": "Yang Zhao", "authors": "Yang Zhao, Wei Jia, Ronggang Wang, Xiaoping Liu, Xuesong Gao, Weiqiang\n  Chen, and Wen Gao", "title": "Deinterlacing Network for Early Interlaced Videos", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the rapid development of image restoration techniques, high-definition\nreconstruction of early videos has achieved impressive results. However, there\nare few studies about the interlacing artifacts that often appear in early\nvideos and significantly affect visual perception. Traditional deinterlacing\napproaches are mainly focused on early interlacing scanning systems and thus\ncannot handle the complex and complicated artifacts in real-world early\ninterlaced videos. Hence, this paper proposes a specific deinterlacing network\n(DIN), which is motivated by the traditional deinterlacing strategy. The\nproposed DIN consists of two stages, i.e., a cooperative vertical interpolation\nstage for split fields, and a merging stage that is applied to perceive\nmovements and remove ghost artifacts. Experimental results demonstrate that the\nproposed method can effectively remove complex artifacts in early interlaced\nvideos.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:24:36 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhao", "Yang", ""], ["Jia", "Wei", ""], ["Wang", "Ronggang", ""], ["Liu", "Xiaoping", ""], ["Gao", "Xuesong", ""], ["Chen", "Weiqiang", ""], ["Gao", "Wen", ""]]}, {"id": "2011.13740", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul", "title": "Recent Advances in Selective Image Encryption and its Indispensability\n  due to COVID-19", "comments": "6 pages, Published in IEEE RAICS 2020, see https://raics.in", "journal-ref": "2020 IEEE Recent Advances in Intelligent Computational Systems\n  (RAICS), 2020, pp. 201-206", "doi": "10.1109/RAICS51191.2020.9332513", "report-no": null, "categories": "cs.CR cs.CY cs.GR cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic serves as a grim reminder of the unexpected nature of\nthese outbreaks and gives rise to a unique set of research challenges in a\nvariety of fields. As people all over the world adjust to this new 'normal',\nwith most workplaces, from companies to educational institutions shifting\nonline, enormous surges in the transmission of images and videos have been\nobserved, creating record-breaking stresses on the internet backbone. At the\nsame time, maintaining the privacy and security of the users' data is of\nimmense importance, this is where fast and efficient image encryption\nalgorithms play a vital role. This paper discusses the calamitous effects of\nthe pandemic on the world population and how their changes in multimedia\nconsumption have led to an urgent need for the development and deployment of\nsecure and fast image encryption, especially selective image encryption\ntechniques. It carefully surveys the most recent advances in this field,\ndiscusses their real-world effects and finally explores some future research\navenues, to provide swift relief and recover from the disastrous effects of the\npandemic.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:02:54 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 02:11:48 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 10:21:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Paul", "Aditya Jyoti", ""]]}, {"id": "2011.14068", "submitter": "Xiaozhong Xu", "authors": "Xiaozhong Xu, Shan Liu", "title": "Overview of Screen Content Coding in Recently Developed Video Coding\n  Standards", "comments": "11 pages, 10 figures and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, screen content (SC) video including computer generated text,\ngraphics and animations, have drawn more attention than ever, as many related\napplications become very popular. To address the need for efficient coding of\nsuch contents, a number of coding tools have been specifically developed and\nachieved great advances in terms of coding efficiency. The inclusion of screen\ncontent coding (SCC) features in all the recently developed video coding\nstandards (namely, HEVC SCC, VVC, AVS3, AV1 and EVC) demonstrated the\nimportance of supporting such features. This paper provides an overview and\ncomparative study of screen content coding technologies, with discussions on\nthe performance and complexity aspects for the tools developed in these\nstandards.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 04:51:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xu", "Xiaozhong", ""], ["Liu", "Shan", ""]]}, {"id": "2011.14615", "submitter": "Qi Yang", "authors": "Aleksandr Farseev, Qi Yang, Andrey Filchenkov, Kirill Lepikhin, Yu-Yi\n  Chu-Farseeva, Daron-Benjamin Loo", "title": "SoMin.ai: Personality-Driven Content Generation Platform", "comments": "WSDM 2021 - Demonstration", "journal-ref": null, "doi": "10.1145/3437963.3441714", "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical demonstration, we showcase the World's first\npersonality-driven marketing content generation platform, called SoMin.ai. The\nplatform combines deep multi-view personality profiling framework and style\ngenerative adversarial networks facilitating the automatic creation of content\nthat appeals to different human personality types. The platform can be used for\nthe enhancement of the social networking user experience as well as for content\nmarketing routines. Guided by the MBTI personality type, automatically derived\nfrom a user social network content, SoMin.ai generates new social media content\nbased on the preferences of other users with a similar personality type aiming\nat enhancing the user experience on social networking venues as well\ndiversifying the efforts of marketers when crafting new content for digital\nmarketing campaigns. The real-time user feedback to the platform via the\nplatform's GUI fine-tunes the content generation model and the evaluation\nresults demonstrate the promising performance of the proposed multi-view\npersonality profiling framework when being applied in the content generation\nscenario. By leveraging content generation at a large scale, marketers will be\nable to execute more effective digital marketing campaigns at a lower cost.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:33:39 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 11:39:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Farseev", "Aleksandr", ""], ["Yang", "Qi", ""], ["Filchenkov", "Andrey", ""], ["Lepikhin", "Kirill", ""], ["Chu-Farseeva", "Yu-Yi", ""], ["Loo", "Daron-Benjamin", ""]]}, {"id": "2011.14847", "submitter": "Kirill Krinkin", "authors": "Kirill Krinkin, Igor Dronnikov", "title": "Media Content Delivery Protocols Performance and Reliability Evaluation\n  in Cellular Mobile Networks", "comments": "6 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, tens of millions of devices around the world communicate with/\neach other via cellular networks. In this paper, we study the stability of\nnetwork content delivery protocols to the effects of network interference. To\nconduct the research, a tool was developed that allows testing of protocols,\nsuch as TCP, UDP, and QUIC. The analysis and comparison of the obtained test\nresults were carried out. In the conclusion, the best protocols for content\ndelivery were shown\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:39:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Krinkin", "Kirill", ""], ["Dronnikov", "Igor", ""]]}, {"id": "2011.14976", "submitter": "Mahmoud Darwich", "authors": "Xiangbo Li, Mahmoud Darwich, Magdy Bayoumi, Mohsen Amini Salehi", "title": "Cloud-Based Video Streaming Services: A Survey", "comments": "accepted to be published in Elsevier book chapter Advances in\n  Computers Volume 123", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video streaming, in various forms of video on demand (VOD), live, and 360\ndegree streaming, has grown dramatically during the past few years. In\ncomparison to traditional cable broadcasters whose contents can only be watched\non TVs, video streaming is ubiquitous and viewers can flexibly watch the video\ncontents on various devices, ranging from smart-phones to laptops and large TV\nscreens. Such ubiquity and flexibility are enabled by interweaving multiple\ntechnologies, such as video compression, cloud computing, content delivery\nnetworks, and several other technologies. As video streaming gains more\npopularity and dominates the Internet traffic, it is essential to understand\nthe way it operates and the interplay of different technologies involved in it.\nAccordingly, the first goal of this paper is to unveil sophisticated processes\nto deliver a raw captured video to viewers' devices. In particular, we\nelaborate on the video encoding, transcoding, packaging, encryption, and\ndelivery processes. We survey recent efforts in academia and industry to\nenhance these processes. As video streaming industry is increasingly becoming\nreliant on cloud computing, the second goal of this survey is to explore and\nsurvey the ways cloud services are utilized to enable video streaming services.\nThe third goal of the study is to position the undertaken research works in\ncloud-based video streaming and identify challenges that need to be obviated in\nfuture to advance cloud-based video streaming industry to a more flexible and\nuser-centric service.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:48:21 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Xiangbo", ""], ["Darwich", "Mahmoud", ""], ["Bayoumi", "Magdy", ""], ["Salehi", "Mohsen Amini", ""]]}]