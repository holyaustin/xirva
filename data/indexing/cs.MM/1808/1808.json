[{"id": "1808.00039", "submitter": "Rawiphon Charunphankasem", "authors": "Rawiphon Charunphankaseam", "title": "Develop the application for learning place value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The objectives of this research were 1) to develop the application for\nlearning place value, 2) to determine the efficiency of developed application,\n3) to compare academic achievement to compare academic achievement between\npre-lesson and post-lesson of students who learned with the developed\napplication about place value, 4) to compare academic achievement between\nstudents who learned place value by the developed application and through\ntraditional method during post-lesson, 5) to compare retention of academic\nachievement of application for learning place value after 2 weeks, and 6) to\nfind satisfactory of student who learned the application for learning place\nvalue. The sample group selected through purposive sampling was 5 content\nspecialists and 400 pratomsuksa 1 students.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 05:32:29 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Charunphankaseam", "Rawiphon", ""]]}, {"id": "1808.00163", "submitter": "Soumyabrata Dev", "authors": "Atul Nautiyal, Killian McCabe, Murhaf Hossari, Soumyabrata Dev,\n  Matthew Nicholson, Clare Conran, Declan McKibben, Jian Tang, Xu Wei, and\n  Francois Pitie", "title": "An Advert Creation System for Next-Gen Publicity", "comments": "Published in 2018 European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid proliferation of multimedia data in the internet, there has\nbeen a fast rise in the creation of videos for the viewers. This enables the\nviewers to skip the advertisement breaks in the videos, using ad blockers and\n'skip ad' buttons -- bringing online marketing and publicity to a stall. In\nthis paper, we demonstrate a system that can effectively integrate a new\nadvertisement into a video sequence. We use state-of-the-art techniques from\ndeep learning and computational photogrammetry, for effective detection of\nexisting adverts, and seamless integration of new adverts into video sequences.\nThis is helpful for targeted advertisement, paving the path for next-gen\npublicity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 04:43:16 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Nautiyal", "Atul", ""], ["McCabe", "Killian", ""], ["Hossari", "Murhaf", ""], ["Dev", "Soumyabrata", ""], ["Nicholson", "Matthew", ""], ["Conran", "Clare", ""], ["McKibben", "Declan", ""], ["Tang", "Jian", ""], ["Wei", "Xu", ""], ["Pitie", "Francois", ""]]}, {"id": "1808.00184", "submitter": "Hongxiang Gu", "authors": "Hongxiang Gu and Viswanathan Swaminathan", "title": "From Thumbnails to Summaries - A single Deep Neural Network to Rule Them\n  All", "comments": "6 pages, 2 figures, IEEE International Conference on Multimedia and\n  Expo (ICME) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summaries come in many forms, from traditional single-image thumbnails,\nanimated thumbnails, storyboards, to trailer-like video summaries. Content\ncreators use the summaries to display the most attractive portion of their\nvideos; the users use them to quickly evaluate if a video is worth watching.\nAll forms of summaries are essential to video viewers, content creators, and\nadvertisers. Often video content management systems have to generate multiple\nversions of summaries that vary in duration and presentational forms. We\npresent a framework ReconstSum that utilizes LSTM-based autoencoder\narchitecture to extract and select a sparse subset of video frames or keyshots\nthat optimally represent the input video in an unsupervised manner. The encoder\nselects a subset from the input video while the decoder seeks to reconstruct\nthe video from the selection. The goal is to minimize the difference between\nthe original input video and the reconstructed video. Our method is easily\nextendable to generate a variety of applications including static video\nthumbnails, animated thumbnails, storyboards and \"trailer-like\" highlights. We\nspecifically study and evaluate two most popular use cases: thumbnail\ngeneration and storyboard generation. We demonstrate that our methods generate\nbetter results than the state-of-the-art techniques in both use cases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:24:39 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Gu", "Hongxiang", ""], ["Swaminathan", "Viswanathan", ""]]}, {"id": "1808.00337", "submitter": "Miklas S. Kristoffersen", "authors": "Miklas S. Kristoffersen, Sven E. Shepstone, Zheng-Hua Tan", "title": "The Importance of Context When Recommending TV Content: Dataset and\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2019.2944214", "report-no": null, "categories": "cs.IR cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Home entertainment systems feature in a variety of usage scenarios with one\nor more simultaneous users, for whom the complexity of choosing media to\nconsume has increased rapidly over the last decade. Users' decision processes\nare complex and highly influenced by contextual settings, but data supporting\nthe development and evaluation of context-aware recommender systems are scarce.\nIn this paper we present a dataset of self-reported TV consumption enriched\nwith contextual information of viewing situations. We show how choice of genre\nassociates with, among others, the number of present users and users' attention\nlevels. Furthermore, we evaluate the performance of predicting chosen genres\ngiven different configurations of contextual information, and compare the\nresults to contextless predictions. The results suggest that including\ncontextual features in the prediction cause notable improvements, and both\ntemporal and social context show significant contributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 11:17:43 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 10:44:34 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kristoffersen", "Miklas S.", ""], ["Shepstone", "Sven E.", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1808.00630", "submitter": "Bichuan Guo", "authors": "Bichuan Guo, Jiangtao Wen and Yuxing Han", "title": "Two-pass Light Field Image Compression for Spatial Quality and Angular\n  Consistency", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality assessment of light field images presents new challenges to\nconventional compression methods, as the spatial quality is affected by the\noptical distortion of capturing devices, and the angular consistency affects\nthe performance of dynamic rendering applications. In this paper, we propose a\ntwo-pass encoding system for pseudo-temporal sequence based light field image\ncompression with a novel frame level bit allocation framework that optimizes\nspatial quality and angular consistency simultaneously. Frame level\nrate-distortion models are estimated during the first pass, and the second pass\nperforms the actual encoding with optimized bit allocations given by a two-step\nconvex programming. The proposed framework supports various encoder\nconfigurations. Experimental results show that comparing to the anchor HM 16.16\n(HEVC reference software), the proposed two-pass encoding system on average\nachieves 11.2% to 11.9% BD-rate reductions for the all-intra configuration,\n15.8% to 32.7% BD-rate reductions for the random-access configuration, and\n12.1% to 15.7% BD-rate reductions for the low-delay configuration. The\nresulting bit errors are limited, and the total time cost is less than twice of\nthe one-pass anchor. Comparing with our earlier low-delay configuration based\nmethod, the proposed system improves BD-rate reduction by 3.1% to 8.3%, reduces\nthe bit errors by more than 60%, and achieves more than 12x speed up.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 01:57:38 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Guo", "Bichuan", ""], ["Wen", "Jiangtao", ""], ["Han", "Yuxing", ""]]}, {"id": "1808.00876", "submitter": "Che-Wei Huang", "authors": "Che-Wei Huang and Shrikanth S. Narayanan", "title": "Normalization Before Shaking Toward Learning Symmetrically Distributed\n  Representation Without Margin in Speech Emotion Recognition", "comments": "Submission to The IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is crucial to the success of many practical deep learning\nmodels, in particular in a more often than not scenario where there are only a\nfew to a moderate number of accessible training samples. In addition to weight\ndecay, data augmentation and dropout, regularization based on multi-branch\narchitectures, such as Shake-Shake regularization, has been proven successful\nin many applications and attracted more and more attention. However, beyond\nmodel-based representation augmentation, it is unclear how Shake-Shake\nregularization helps to provide further improvement on classification tasks,\nlet alone the baffling interaction between batch normalization and shaking. In\nthis work, we present our investigation on Shake-Shake regularization, drawing\nconnections to the vicinal risk minimization principle and discriminative\nfeature learning in verification tasks. Furthermore, we identify a strong\nresemblance between batch normalized residual blocks and batch normalized\nrecurrent neural networks, where both of them share a similar convergence\nbehavior, which could be mitigated by a proper initialization of batch\nnormalization. Based on the findings, our experiments on speech emotion\nrecognition demonstrate simultaneously an improvement on the classification\naccuracy and a reduction on the generalization gap both with statistical\nsignificance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 15:57:57 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 23:21:54 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Huang", "Che-Wei", ""], ["Narayanan", "Shrikanth S.", ""]]}, {"id": "1808.01992", "submitter": "Zhiding Yu", "authors": "Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, B.\n  V. K. Vijaya Kumar, Jan Kautz", "title": "Simultaneous Edge Alignment and Learning", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is among the most fundamental vision problems for its role in\nperceptual grouping and its wide applications. Recent advances in\nrepresentation learning have led to considerable improvements in this area.\nMany state of the art edge detection models are learned with fully\nconvolutional networks (FCNs). However, FCN-based edge learning tends to be\nvulnerable to misaligned labels due to the delicate structure of edges. While\nsuch problem was considered in evaluation benchmarks, similar issue has not\nbeen explicitly addressed in general edge learning. In this paper, we show that\nlabel misalignment can cause considerably degraded edge learning quality, and\naddress this issue by proposing a simultaneous edge alignment and learning\nframework. To this end, we formulate a probabilistic model where edge alignment\nis treated as latent variable optimization, and is learned end-to-end during\nnetwork training. Experiments show several applications of this work, including\nimproved edge detection with state of the art performance, and automatic\nrefinement of noisy annotations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:58:42 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:42:05 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 05:36:51 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Yu", "Zhiding", ""], ["Liu", "Weiyang", ""], ["Zou", "Yang", ""], ["Feng", "Chen", ""], ["Ramalingam", "Srikumar", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Kautz", "Jan", ""]]}, {"id": "1808.02096", "submitter": "Huiguang He", "authors": "Changde Du, Changying Du, Hao Wang, Jinpeng Li, Wei-Long Zheng,\n  Bao-Liang Lu, Huiguang He", "title": "Semi-supervised Deep Generative Modelling of Incomplete Multi-Modality\n  Emotional Data", "comments": "arXiv admin note: text overlap with arXiv:1704.07548, 2018 ACM\n  Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240528", "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are threefold challenges in emotion recognition. First, it is difficult\nto recognize human's emotional states only considering a single modality.\nSecond, it is expensive to manually annotate the emotional data. Third,\nemotional data often suffers from missing modalities due to unforeseeable\nsensor malfunction or configuration issues. In this paper, we address all these\nproblems under a novel multi-view deep generative framework. Specifically, we\npropose to model the statistical relationships of multi-modality emotional data\nusing multiple modality-specific generative networks with a shared latent\nspace. By imposing a Gaussian mixture assumption on the posterior approximation\nof the shared latent variables, our framework can learn the joint deep\nrepresentation from multiple modalities and evaluate the importance of each\nmodality simultaneously. To solve the labeled-data-scarcity problem, we extend\nour multi-view model to semi-supervised learning scenario by casting the\nsemi-supervised classification problem as a specialized missing data imputation\ntask. To address the missing-modality problem, we further extend our\nsemi-supervised multi-view model to deal with incomplete data, where a missing\nview is treated as a latent variable and integrated out during inference. This\nway, the proposed overall framework can utilize all available (both labeled and\nunlabeled, as well as both complete and incomplete) data to improve its\ngeneralization ability. The experiments conducted on two real multi-modal\nemotion datasets demonstrated the superiority of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:07:36 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Du", "Changde", ""], ["Du", "Changying", ""], ["Wang", "Hao", ""], ["Li", "Jinpeng", ""], ["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""], ["He", "Huiguang", ""]]}, {"id": "1808.02632", "submitter": "Pan Lu", "authors": "Peng Gao, Pan Lu, Hongsheng Li, Shuang Li, Yikang Li, Steven Hoi,\n  Xiaogang Wang", "title": "Question-Guided Hybrid Convolution for Visual Question Answering", "comments": "17 pages, 4 figures, accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC)\nnetwork for Visual Question Answering (VQA). Most state-of-the-art VQA methods\nfuse the high-level textual and visual features from the neural network and\nabandon the visual spatial information when learning multi-modal features.To\naddress these problems, question-guided kernels generated from the input\nquestion are designed to convolute with visual features for capturing the\ntextual and visual relationship in the early stage. The question-guided\nconvolution can tightly couple the textual and visual information but also\nintroduce more parameters when learning kernels. We apply the group\nconvolution, which consists of question-independent kernels and\nquestion-dependent kernels, to reduce the parameter size and alleviate\nover-fitting. The hybrid convolution can generate discriminative multi-modal\nfeatures with fewer parameters. The proposed approach is also complementary to\nexisting bilinear pooling fusion and attention based VQA methods. By\nintegrating with them, our method could further boost the performance.\nExtensive experiments on public VQA datasets validate the effectiveness of\nQGHC.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 05:39:00 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Gao", "Peng", ""], ["Lu", "Pan", ""], ["Li", "Hongsheng", ""], ["Li", "Shuang", ""], ["Li", "Yikang", ""], ["Hoi", "Steven", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1808.02793", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Kesheng Cheng, Lei Zhu, Ruipeng Chen, Zuping Zhang\n  and Fang Huang", "title": "Efficient Continuous Top-$k$ Geo-Image Search on Road Network", "comments": "Multimedia Tools and Applications, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile Internet and cloud computing technology,\nlarge-scale multimedia data, e.g., texts, images, audio and videos have been\ngenerated, collected, stored and shared. In this paper, we propose a novel\nquery problem named continuous top-$k$ geo-image query on road network which\naims to search out a set of geo-visual objects based on road network distance\nproximity and visual content similarity. Existing approaches for spatial\ntextual query and geo-image query cannot address this problem effectively\nbecause they do not consider both of visual content similarity and road network\ndistance proximity on road network. In order to address this challenge\neffectively and efficiently, firstly we propose the definition of geo-visual\nobjects and continuous top-$k$ geo-visual objects query on road network, then\ndevelop a score function for search. To improve the query efficiency in a\nlarge-scale road network, we propose the search algorithm named geo-visual\nsearch on road network based on a novel hybrid indexing framework called\nVIG-Tree, which combines G-Tree and visual inverted index technique. In\naddition, an important notion named safe interval and results updating rule are\nproposed, and based on them we develop an efficient algorithm named moving\nmonitor algorithm to solve continuous query. Experimental evaluation on real\nmultimedia dataset and road network dataset illustrates that our solution\noutperforms state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 14:25:16 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Cheng", "Kesheng", ""], ["Zhu", "Lei", ""], ["Chen", "Ruipeng", ""], ["Zhang", "Zuping", ""], ["Huang", "Fang", ""]]}, {"id": "1808.02950", "submitter": "Renato J Cintra", "authors": "R. S. Oliveira, R. J. Cintra, F. M. Bayer, T. L. T. da Silveira, A.\n  Madanayake, A. Leite", "title": "Low-complexity 8-point DCT Approximation Based on Angle Similarity for\n  Image and Video Coding", "comments": "16 pages, 12 figures, 10 tables", "journal-ref": "Multidimensional Systems and Signal Processing, 1-32, 2018", "doi": "10.1007/s11045-018-0601-5", "report-no": null, "categories": "eess.IV cs.MM eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principal component analysis (PCA) is widely used for data decorrelation\nand dimensionality reduction. However, the use of PCA may be impractical in\nreal-time applications, or in situations were energy and computing constraints\nare severe. In this context, the discrete cosine transform (DCT) becomes a\nlow-cost alternative to data decorrelation. This paper presents a method to\nderive computationally efficient approximations to the DCT. The proposed method\naims at the minimization of the angle between the rows of the exact DCT matrix\nand the rows of the approximated transformation matrix. The resulting\ntransformations matrices are orthogonal and have extremely low arithmetic\ncomplexity. Considering popular performance measures, one of the proposed\ntransformation matrices outperforms the best competitors in both matrix error\nand coding capabilities. Practical applications in image and video coding\ndemonstrate the relevance of the proposed transformation. In fact, we show that\nthe proposed approximate DCT can outperform the exact DCT for image encoding\nunder certain compression ratios. The proposed transform and its direct\ncompetitors are also physically realized as digital prototype circuits using\nFPGA technology.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 21:56:30 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Oliveira", "R. S.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["da Silveira", "T. L. T.", ""], ["Madanayake", "A.", ""], ["Leite", "A.", ""]]}, {"id": "1808.03969", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Ryota Hinami, Shin'ichi Satoh", "title": "Reconfigurable Inverted Index", "comments": "ACMMM 2018 (oral). Code: https://github.com/matsui528/rii", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approximate nearest neighbor search systems suffer from two\nfundamental problems that are of practical importance but have not received\nsufficient attention from the research community. First, although existing\nsystems perform well for the whole database, it is difficult to run a search\nover a subset of the database. Second, there has been no discussion concerning\nthe performance decrement after many items have been newly added to a system.\nWe develop a reconfigurable inverted index (Rii) to resolve these two issues.\nBased on the standard IVFADC system, we design a data layout such that items\nare stored linearly. This enables us to efficiently run a subset search by\nswitching the search method to a linear PQ scan if the size of a subset is\nsmall. Owing to the linear layout, the data structure can be dynamically\nadjusted after new items are added, maintaining the fast speed of the system.\nExtensive comparisons show that Rii achieves a comparable performance with\nstate-of-the art systems such as Faiss.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 16:47:47 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Matsui", "Yusuke", ""], ["Hinami", "Ryota", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1808.04152", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu, and Josef Kittler", "title": "Learning Discriminative Hashing Codes for Cross-Modal Retrieval based on\n  Multi-view Features", "comments": "28 pages, 10 figures, 13 tables. The paper is under consideration at\n  Pattern Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing techniques have been applied broadly in retrieval tasks due to their\nlow storage requirements and high speed of processing. Many hashing methods\nbased on a single view have been extensively studied for information retrieval.\nHowever, the representation capacity of a single view is insufficient and some\ndiscriminative information is not captured, which results in limited\nimprovement. In this paper, we employ multiple views to represent images and\ntexts for enriching the feature information. Our framework exploits the\ncomplementary information among multiple views to better learn the\ndiscriminative compact hash codes. A discrete hashing learning framework that\njointly performs classifier learning and subspace learning is proposed to\ncomplete multiple search tasks simultaneously. Our framework includes two\nstages, namely a kernelization process and a quantization process.\nKernelization aims to find a common subspace where multi-view features can be\nfused. The quantization stage is designed to learn discriminative unified\nhashing codes. Extensive experiments are performed on single-label datasets\n(WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE) and the\nexperimental results indicate the superiority of our method compared with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 11:18:49 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 06:11:35 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 08:36:38 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1808.04432", "submitter": "Sheng Li", "authors": "Longfei Liu, Sheng Li, Yisong Chen and Guoping Wang", "title": "X-GANs: Image Reconstruction Made Easy for Extreme Cases", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction including image restoration and denoising is a\nchallenging problem in the field of image computing. We present a new method,\ncalled X-GANs, for reconstruction of arbitrary corrupted resource based on a\nvariant of conditional generative adversarial networks (conditional GANs). In\nour method, a novel generator and multi-scale discriminators are proposed, as\nwell as the combined adversarial losses, which integrate a VGG perceptual loss,\nan adversarial perceptual loss, and an elaborate corresponding point loss\ntogether based on the analysis of image feature. Our conditional GANs have\nenabled a variety of applications in image reconstruction, including image\ndenoising, image restoration from quite a sparse sampling, image inpainting,\nimage recovery from the severely polluted block or even color-noise dominated\nimages, which are extreme cases and haven't been addressed in the status quo.\nWe have significantly improved the accuracy and quality of image\nreconstruction. Extensive perceptual experiments on datasets ranging from human\nfaces to natural scenes demonstrate that images reconstructed by the presented\napproach are considerably more realistic than alternative work. Our method can\nalso be extended to handle high-ratio image compression.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 10:36:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Liu", "Longfei", ""], ["Li", "Sheng", ""], ["Chen", "Yisong", ""], ["Wang", "Guoping", ""]]}, {"id": "1808.04437", "submitter": "Jun Li", "authors": "Jun Li, Chang Xu, Wankou Yang, Changyin Sun, Dacheng Tao, Hong Zhang", "title": "Discriminative multi-view Privileged Information learning for image\n  re-ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multi-view re-ranking methods usually perform asymmetrical\nmatching between the region of interest (ROI) in the query image and the whole\ntarget image for similarity computation. Due to the inconsistency in the visual\nappearance, this practice tends to degrade the retrieval accuracy particularly\nwhen the image ROI, which is usually interpreted as the image objectness,\naccounts for a smaller region in the image. Since Privileged Information (PI),\nwhich can be viewed as the image prior, enables well characterizing the image\nobjectness, we are aiming at leveraging PI for further improving the\nperformance of the multi-view re-ranking accuracy in this paper. Towards this\nend, we propose a discriminative multi-view re-ranking approach in which both\nthe original global image visual contents and the local auxiliary PI features\nare simultaneously integrated into a unified training framework for generating\nthe latent subspaces with sufficient discriminating power. For the on-the-fly\nre-ranking, since the multi-view PI features are unavailable, we only project\nthe original multi-view image representations onto the latent subspace, and\nthus the re-ranking can be achieved by computing and sorting the distances from\nthe multi-view embeddings to the separating hyperplane. Extensive experimental\nevaluations on the two public benchmarks Oxford5k and Paris6k reveal our\napproach provides further performance boost for accurate image re-ranking,\nwhilst the comparative study demonstrates the advantage of our method against\nother multi-view re-ranking methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 18:57:14 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Jun", ""], ["Xu", "Chang", ""], ["Yang", "Wankou", ""], ["Sun", "Changyin", ""], ["Tao", "Dacheng", ""], ["Zhang", "Hong", ""]]}, {"id": "1808.05636", "submitter": "Yaman Kumar", "authors": "Yaman Kumar, Agniv Sharma, Abhigyan Khaund, Akash Kumar, Ponnurangam\n  Kumaraguru, Rajiv Ratn Shah", "title": "IceBreaker: Solving Cold Start Problem for Video Recommendation Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet has brought about a tremendous increase in content of all forms and,\nin that, video content constitutes the major backbone of the total content\nbeing published as well as watched. Thus it becomes imperative for video\nrecommendation engines such as Hulu to look for novel and innovative ways to\nrecommend the newly added videos to their users. However, the problem with new\nvideos is that they lack any sort of metadata and user interaction so as to be\nable to rate the videos for the consumers. To this effect, this paper\nintroduces the several techniques we develop for the Content Based Video\nRelevance Prediction (CBVRP) Challenge being hosted by Hulu for the ACM\nMultimedia Conference 2018. We employ different architectures on the CBVRP\ndataset to make use of the provided frame and video level features and generate\npredictions of videos that are similar to the other videos. We also implement\nseveral ensemble strategies to explore complementarity between both the types\nof provided features. The obtained results are encouraging and will impel the\nboundaries of research for multimedia based video recommendation systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 18:26:46 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Kumar", "Yaman", ""], ["Sharma", "Agniv", ""], ["Khaund", "Abhigyan", ""], ["Kumar", "Akash", ""], ["Kumaraguru", "Ponnurangam", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "1808.05941", "submitter": "Nitin Khanna Dr.", "authors": "Sharad Joshi, Suraj Saxena, Nitin Khanna", "title": "First Steps Toward CNN based Source Classification of Document Images\n  Shared Over Messaging App", "comments": "10 pages", "journal-ref": null, "doi": "10.1016/j.image.2019.05.020", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of source smartphone corresponding to a document image can be\nhelpful in a variety of applications including copyright infringement,\nownership attribution, leak identification and usage restriction. In this\nletter, we investigate a convolutional neural network-based approach to solve\nsource smartphone identification problem for printed text documents which have\nbeen captured by smartphone cameras and shared over messaging platform. In\nabsence of any publicly available dataset addressing this problem, we introduce\na new image dataset consisting of 315 images of documents printed in three\ndifferent fonts, captured using 21 smartphones and shared over WhatsApp.\nExperiments conducted on this dataset demonstrate that, in all scenarios, the\nproposed system performs as well as or better than the state-of-the-art system\nbased on handcrafted features and classification of letters extracted from\ndocument images. The new dataset and code of the proposed system will be made\npublicly available along with this letter's publication, presently they are\nsubmitted for review.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 17:49:39 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Joshi", "Sharad", ""], ["Saxena", "Suraj", ""], ["Khanna", "Nitin", ""]]}, {"id": "1808.06277", "submitter": "Chengyuan Zhang", "authors": "Lei Zhu, Jun Long, Chengyuan Zhang, Ruipeng Chen, Xinpan Yuan, Zhan\n  Yang", "title": "An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid development of mobile Internet techniques, cloud computation\nand popularity of online social networking and location-based services, massive\namount of multimedia data with geographical information is generated and\nuploaded to the Internet. In this paper, we propose a novel type of cross-modal\nmultimedia retrieval called geo-multimedia cross-modal retrieval which aims to\nsearch out a set of geo-multimedia objects based on geographical distance\nproximity and semantic similarity between different modalities. Previous\nstudies for cross-modal retrieval and spatial keyword search cannot address\nthis problem effectively because they do not consider multimedia data with\ngeo-tags and do not focus on this type of query. In order to address this\nproblem efficiently, we present the definition of $k$NN geo-multimedia\ncross-modal query at the first time and introduce relevant conceptions such as\ncross-modal semantic representation space. To bridge the semantic gap between\ndifferent modalities, we propose a method named cross-modal semantic matching\nwhich contains two important component, i.e., CorrProj and LogsTran, which aims\nto construct a common semantic representation space for cross-modal semantic\nsimilarity measurement. Besides, we designed a framework based on deep learning\ntechniques to implement common semantic representation space construction. In\naddition, a novel hybrid indexing structure named GMR-Tree combining\ngeo-multimedia data and R-Tree is presented and a efficient $k$NN search\nalgorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on\nreal and synthetic dataset clearly demonstrates that our solution outperforms\nthe-state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 00:54:29 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Zhu", "Lei", ""], ["Long", "Jun", ""], ["Zhang", "Chengyuan", ""], ["Chen", "Ruipeng", ""], ["Yuan", "Xinpan", ""], ["Yang", "Zhan", ""]]}, {"id": "1808.06462", "submitter": "Nitish Nag", "authors": "Nitish Nag, Vaibhav Pandey, Preston J. Putzel, Hari Bhimaraju,\n  Srikanth Krishnan, Ramesh C. Jain", "title": "Cross-Modal Health State Estimation", "comments": "Accepted to ACM Multimedia 2018 Conference - Brave New Ideas, Seoul,\n  Korea, ACM ISBN 978-1-4503-5665-7/18/10", "journal-ref": "Nitish Nag, Vaibhav Pandey, Preston J. Putzel, Hari Bhimaraju,\n  Srikanth Krishnan, Ramesh C. Jain, 2018 ACM Multimedia Conference (MM '18),\n  October 22--26, 2018, Seoul, Republic of Korea", "doi": "10.1145/3240508.3241913", "report-no": null, "categories": "cs.CY cs.AI cs.MM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals create and consume more diverse data about themselves today than\nany time in history. Sources of this data include wearable devices, images,\nsocial media, geospatial information and more. A tremendous opportunity rests\nwithin cross-modal data analysis that leverages existing domain knowledge\nmethods to understand and guide human health. Especially in chronic diseases,\ncurrent medical practice uses a combination of sparse hospital based biological\nmetrics (blood tests, expensive imaging, etc.) to understand the evolving\nhealth status of an individual. Future health systems must integrate data\ncreated at the individual level to better understand health status perpetually,\nespecially in a cybernetic framework. In this work we fuse multiple user\ncreated and open source data streams along with established biomedical domain\nknowledge to give two types of quantitative state estimates of cardiovascular\nhealth. First, we use wearable devices to calculate cardiorespiratory fitness\n(CRF), a known quantitative leading predictor of heart disease which is not\nroutinely collected in clinical settings. Second, we estimate inherent genetic\ntraits, living environmental risks, circadian rhythm, and biological metrics\nfrom a diverse dataset. Our experimental results on 24 subjects demonstrate how\nmulti-modal data can provide personalized health insight. Understanding the\ndynamic nature of health status will pave the way for better health based\nrecommendation engines, better clinical decision making and positive lifestyle\nchanges.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 00:38:38 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 21:19:53 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Nag", "Nitish", ""], ["Pandey", "Vaibhav", ""], ["Putzel", "Preston J.", ""], ["Bhimaraju", "Hari", ""], ["Krishnan", "Srikanth", ""], ["Jain", "Ramesh C.", ""]]}, {"id": "1808.06467", "submitter": "Nitish Nag", "authors": "Nitish Nag, Mathias Lux, Ramesh C. Jain", "title": "Intrinsic and Extrinsic Motivation Modeling Essential for Multi-Modal\n  Health Recommender Systems", "comments": "Related to ACM Multimedia HealthMedia Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing health lays the core foundation to enabling quality life\nexperiences. Modern computer science research, and especially the field of\nrecommender systems, has enhanced the quality of experiences in fields such as\nentertainment, shopping, and advertising; yet lags in the health domain. We are\ndeveloping an approach to leverage multimedia for human health based on\nmotivation modeling and recommendation of actions. Health is primarily a\nproduct of our everyday lifestyle actions, yet we have minimal health guidance\non making everyday choices. Recommendations are the key to modern content\nconsumption and decisions. Furthermore, long-term engagement with recommender\nsystems is key for true effectiveness. Distinguishing intrinsic and extrinsic\nmotivations from multi-modal data is key to provide recommendations that\nprimarily fuel the intrinsic intentions, while using extrinsic motivation to\nfurther support intrinsic motivation. This understanding builds the foundation\nof sustainable behavioral adaptation for optimal personalized lifestyle health\nbenefits.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 00:48:16 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Nag", "Nitish", ""], ["Lux", "Mathias", ""], ["Jain", "Ramesh C.", ""]]}, {"id": "1808.06468", "submitter": "Nitish Nag", "authors": "Nitish Nag, Vaibhav Pandey, Ramesh C. Jain", "title": "Endogenous and Exogenous Multi-Modal Layers in Context Aware\n  Recommendation Systems for Health", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People care more about the solutions to their problems rather than data\nalone. Inherently, this means using data to generate a list of recommendations\nfor a given situation. The rapid growth of multi-modal wearables and sensors\nhave not made this jump effectively in the domain of health. Modern user\ncontent consumption and decision making in both cyber (e.g. entertainment,\nnews) and physical (eg. food, shopping) spaces rely heavily on targeted\npersonalized recommender systems. The utility function is the primary ranking\nmethod to predict what a given person would explicitly prefer. In this work we\ndescribe two unique layers of user and context modeling that can be coupled to\ntraditional recommender system approaches. The exogenous layer incorporates\nfactors outside of the person's body (eg. location, weather, social context),\nwhile the endogenous layer integrates data to estimate the physiologic or\ninnate needs of the user. This is accomplished through multi-modal sensor data\nintegration applied to domain-specific utility functions, filters and\nre-ranking weights. We showcase this concept through a nutrition guidance\nsystem focused on controlling sodium intake at a personalized level,\ndramatically improving upon the fixed recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 01:12:39 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Nag", "Nitish", ""], ["Pandey", "Vaibhav", ""], ["Jain", "Ramesh C.", ""]]}, {"id": "1808.06686", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Wael AbdAlmageed, Yue Wu and Prem Natarajan", "title": "Deep Multimodal Image-Repurposing Detection", "comments": "To be published at ACM Multimeda 2018 (orals)", "journal-ref": null, "doi": "10.1145/3240508.3240707", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nefarious actors on social media and other platforms often spread rumors and\nfalsehoods through images whose metadata (e.g., captions) have been modified to\nprovide visual substantiation of the rumor/falsehood. This type of modification\nis referred to as image repurposing, in which often an unmanipulated image is\npublished along with incorrect or manipulated metadata to serve the actor's\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\ndataset, a substantially challenging dataset over that which has been\npreviously available to support research into image repurposing detection. The\nnew dataset includes location, person, and organization manipulations on\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\nmultimodal learning model for assessing the integrity of an image by combining\ninformation extracted from the image with related information from a knowledge\nbase. The proposed method is compared against state-of-the-art techniques on\nexisting datasets as well as MEIR, where it outperforms existing methods across\nthe board, with AUC improvement up to 0.23.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 20:47:56 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Sabir", "Ekraam", ""], ["AbdAlmageed", "Wael", ""], ["Wu", "Yue", ""], ["Natarajan", "Prem", ""]]}, {"id": "1808.07202", "submitter": "Weiqing Min", "authors": "Weiqing Min and Shuqiang Jiang and Linhu Liu and Yong Rui and Ramesh\n  Jain", "title": "A Survey on Food Computing", "comments": "Accepted by ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food is very essential for human life and it is fundamental to the human\nexperience. Food-related study may support multifarious applications and\nservices, such as guiding the human behavior, improving the human health and\nunderstanding the culinary culture. With the rapid development of social\nnetworks, mobile networks, and Internet of Things (IoT), people commonly\nupload, share, and record food images, recipes, cooking videos, and food\ndiaries, leading to large-scale food data. Large-scale food data offers rich\nknowledge about food and can help tackle many central issues of human society.\nTherefore, it is time to group several disparate issues related to food\ncomputing. Food computing acquires and analyzes heterogenous food data from\ndisparate sources for perception, recognition, retrieval, recommendation, and\nmonitoring of food. In food computing, computational approaches are applied to\naddress food related issues in medicine, biology, gastronomy and agronomy. Both\nlarge-scale food data and recent breakthroughs in computer science are\ntransforming the way we analyze food data. Therefore, vast amounts of work has\nbeen conducted in the food area, targeting different food-oriented tasks and\napplications. However, there are very few systematic reviews, which shape this\narea well and provide a comprehensive and in-depth summary of current efforts\nor detail open problems in this area. In this paper, we formalize food\ncomputing and present such a comprehensive overview of various emerging\nconcepts, methods, and tasks. We summarize key challenges and future directions\nahead for food computing. This is the first comprehensive survey that targets\nthe study of computing technology for the food area and also offers a\ncollection of research studies and technologies to benefit researchers and\npractitioners working in different food-related fields.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 03:22:58 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2018 10:04:26 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 06:09:10 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 13:19:09 GMT"}, {"version": "v5", "created": "Tue, 16 Jul 2019 12:25:23 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Min", "Weiqing", ""], ["Jiang", "Shuqiang", ""], ["Liu", "Linhu", ""], ["Rui", "Yong", ""], ["Jain", "Ramesh", ""]]}, {"id": "1808.07272", "submitter": "Sibo Song", "authors": "Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal", "title": "Deep Adaptive Temporal Pooling for Activity Recognition", "comments": "Accepted by ACM Multimedia 2018", "journal-ref": null, "doi": "10.1145/3240508.3240713", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved competitive accuracy for human\nactivity recognition. However, there is room for improvement, especially in\nmodeling long-term temporal importance and determining the activity relevance\nof different temporal segments in a video. To address this problem, we propose\na learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP).\nDATP applies a self-attention mechanism to adaptively pool the classification\nscores of different video segments. Specifically, using frame-level features,\nDATP regresses importance of different temporal segments and generates weights\nfor them. Remarkably, DATP is trained using only the video-level label. There\nis no need of additional supervision except video-level activity class label.\nWe conduct extensive experiments to investigate various input features and\ndifferent weight models. Experimental results show that DATP can learn to\nassign large weights to key video segments. More importantly, DATP can improve\ntraining of frame-level feature extractor. This is because relevant temporal\nsegments are assigned large weights during back-propagation. Overall, we\nachieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:29:38 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Song", "Sibo", ""], ["Cheung", "Ngai-Man", ""], ["Chandrasekhar", "Vijay", ""], ["Mandal", "Bappaditya", ""]]}, {"id": "1808.07275", "submitter": "Valentin Vielzeuf", "authors": "Valentin Vielzeuf, Alexis Lechervy, St\\'ephane Pateux, Fr\\'ed\\'eric\n  Jurie", "title": "CentralNet: a Multilayer Approach for Multimodal Fusion", "comments": null, "journal-ref": "European Conference on Computer Vision Workshops: Multimodal\n  Learning and Applications, Sep 2018, Munich, Germany.\n  https://mula2018.github.io/", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel multimodal fusion approach, aiming to produce\nbest possible decisions by integrating information coming from multiple media.\nWhile most of the past multimodal approaches either work by projecting the\nfeatures of different modalities into the same space, or by coordinating the\nrepresentations of each modality through the use of constraints, our approach\nborrows from both visions. More specifically, assuming each modality can be\nprocessed by a separated deep convolutional network, allowing to take decisions\nindependently from each modality, we introduce a central network linking the\nmodality specific networks. This central network not only provides a common\nfeature embedding but also regularizes the modality specific networks through\nthe use of multi-task learning. The proposed approach is validated on 4\ndifferent computer vision tasks on which it consistently improves the accuracy\nof existing multimodal fusion approaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:37:55 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Lechervy", "Alexis", ""], ["Pateux", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1808.07276", "submitter": "Haodong Li", "authors": "Haodong Li and Bin Li and Shunquan Tan and Jiwu Huang", "title": "Identification of Deep Network Generated Images Using Disparities in\n  Color Components", "comments": "Published in Signal Processing. Please cite: H. Li, B. Li, S. Tan, J.\n  Huang, Identification of Deep Network Generated Images Using Disparities in\n  Color Components, Signal Processing, Signal Processing 174 (2020) 107616", "journal-ref": "Signal Processing 174 (2020) 107616", "doi": "10.1016/j.sigpro.2020.107616", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the powerful deep network architectures, such as generative adversarial\nnetworks, one can easily generate photorealistic images. Although the generated\nimages are not dedicated for fooling human or deceiving biometric\nauthentication systems, research communities and public media have shown great\nconcerns on the security issues caused by these images. This paper addresses\nthe problem of identifying deep network generated (DNG) images. Taking the\ndifferences between camera imaging and DNG image generation into\nconsiderations, we analyze the disparities between DNG images and real images\nin different color components. We observe that the DNG images are more\ndistinguishable from real ones in the chrominance components, especially in the\nresidual domain. Based on these observations, we propose a feature set to\ncapture color image statistics for identifying DNG images. Additionally, we\nevaluate several detection situations, including the training-testing data are\nmatched or mismatched in image sources or generative models and detection with\nonly real images. Extensive experimental results show that the proposed method\ncan accurately identify DNG images and outperforms existing methods when the\ntraining and testing data are mismatched. Moreover, when the GAN model is\nunknown, our methods also achieves good performance with one-class\nclassification by using only real images for training.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:38:11 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 04:53:14 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 02:12:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Li", "Haodong", ""], ["Li", "Bin", ""], ["Tan", "Shunquan", ""], ["Huang", "Jiwu", ""]]}, {"id": "1808.07793", "submitter": "Niluthpol Mithun", "authors": "Niluthpol Chowdhury Mithun, Rameswar Panda, Evangelos E. Papalexakis,\n  Amit K. Roy-Chowdhury", "title": "Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval", "comments": "ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval between visual data and natural language description\nremains a long-standing challenge in multimedia. While recent image-text\nretrieval methods offer great promise by learning deep representations aligned\nacross modalities, most of these methods are plagued by the issue of training\nwith small-scale datasets covering a limited number of images with ground-truth\nsentences. Moreover, it is extremely expensive to create a larger dataset by\nannotating millions of images with sentences and may lead to a biased model.\nInspired by the recent success of webly supervised learning in deep neural\nnetworks, we capitalize on readily-available web images with noisy annotations\nto learn robust image-text joint representation. Specifically, our main idea is\nto leverage web images and corresponding tags, along with fully annotated\ndatasets, in training for learning the visual-semantic joint embedding. We\npropose a two-stage approach for the task that can augment a typical supervised\npair-wise ranking loss based formulation with weakly-annotated web images to\nlearn a more robust visual-semantic embedding. Experiments on two standard\nbenchmark datasets demonstrate that our method achieves a significant\nperformance gain in image-text retrieval compared to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 15:07:52 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Mithun", "Niluthpol Chowdhury", ""], ["Panda", "Rameswar", ""], ["Papalexakis", "Evangelos E.", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1808.08065", "submitter": "Christian Sieber", "authors": "Christian Sieber, Korbinian Hagn, Christian Moldovan, Tobias\n  Ho{\\ss}feld, Wolfgang Kellerer", "title": "Towards Machine Learning-Based Optimal HAS", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile video consumption is increasing and sophisticated video quality\nadaptation strategies are required to deal with mobile throughput fluctuations.\nThese adaptation strategies have to keep the switching frequency low, the\naverage quality high and prevent stalling occurrences to ensure customer\nsatisfaction. This paper proposes a novel methodology for the design of machine\nlearning-based adaptation logics named HASBRAIN. Furthermore, the performance\nof a trained neural network against two algorithms from the literature is\nevaluated. We first use a modified existing optimization formulation to\ncalculate optimal adaptation paths with a minimum number of quality switches\nfor a wide range of videos and for challenging mobile throughput patterns.\nAfterwards we use the resulting optimal adaptation paths to train and compare\ndifferent machine learning models. The evaluation shows that an artificial\nneural network-based model can reach a high average quality with a low number\nof switches in the mobile scenario. The proposed methodology is general enough\nto be extended for further designs of machine learning-based algorithms and the\nprovided model can be deployed in on-demand streaming scenarios or be further\nrefined using reward-based mechanisms such as reinforcement learning. All\ntools, models and datasets created during the work are provided as open-source\nsoftware.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 09:41:26 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Sieber", "Christian", ""], ["Hagn", "Korbinian", ""], ["Moldovan", "Christian", ""], ["Ho\u00dffeld", "Tobias", ""], ["Kellerer", "Wolfgang", ""]]}, {"id": "1808.08402", "submitter": "Brian Kenji Iwana", "authors": "Shailza Jolly, Brian Kenji Iwana, Ryohei Kuroki, Seiichi Uchida", "title": "How do Convolutional Neural Networks Learn Design?", "comments": "Accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to understand the design principles in book cover\nimages which are carefully crafted by experts. Book covers are designed in a\nunique way, specific to genres which convey important information to their\nreaders. By using Convolutional Neural Networks (CNN) to predict book genres\nfrom cover images, visual cues which distinguish genres can be highlighted and\nanalyzed. In order to understand these visual clues contributing towards the\ndecision of a genre, we present the application of Layer-wise Relevance\nPropagation (LRP) on the book cover image classification results. We use LRP to\nexplain the pixel-wise contributions of book cover design and highlight the\ndesign elements contributing towards particular genres. In addition, with the\nuse of state-of-the-art object and text detection methods, insights about\ngenre-specific book cover designs are discovered.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 10:34:05 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Jolly", "Shailza", ""], ["Iwana", "Brian Kenji", ""], ["Kuroki", "Ryohei", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1808.08567", "submitter": "Bo Fu", "authors": "Bo Fu, XiaoYang Zhao, Yi Li, XiangHai Wang", "title": "Patch-based Contour Prior Image Denoising for Salt and Pepper Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The salt and pepper noise brings a significant challenge to image denoising\ntechnology, i.e. how to removal the noise clearly and retain the details\neffectively? In this paper, we propose a patch-based contour prior denoising\napproach for salt and pepper noise. First, noisy image is cut into patches as\nbasic representation unit, a discrete total variation model is designed to\nextract contour structures; Second, a weighted Euclidean distance is designed\nto search the most similar patches, then, corresponding contour stencils are\nextracted from these similar patches; At the last, we build filter from contour\nstencils in the framework of regression. Numerical results illustrate that the\nproposed method is competitive with the state-of-the-art methods in terms of\nthe peak signal-to-noise (PSNR) and visual effects.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 15:02:34 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Fu", "Bo", ""], ["Zhao", "XiaoYang", ""], ["Li", "Yi", ""], ["Wang", "XiangHai", ""]]}, {"id": "1808.09198", "submitter": "Kwei-Herng Lai", "authors": "Chih-Chun Hsia, Kwei-Herng Lai, Yian Chen, Chuan-Ju Wang, Ming-Feng\n  Tsai", "title": "Representation Learning for Image-based Music Recommendation", "comments": "2 pages, LBRS@RecSys'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image perception is one of the most direct ways to provide contextual\ninformation about a user concerning his/her surrounding environment; hence\nimages are a suitable proxy for contextual recommendation. We propose a novel\nrepresentation learning framework for image-based music recommendation that\nbridges the heterogeneity gap between music and image data; the proposed method\nis a key component for various contextual recommendation tasks. Preliminary\nexperiments show that for an image-to-song retrieval task, the proposed method\nretrieves relevant or conceptually similar songs for input images.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 09:47:53 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 04:37:46 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Hsia", "Chih-Chun", ""], ["Lai", "Kwei-Herng", ""], ["Chen", "Yian", ""], ["Wang", "Chuan-Ju", ""], ["Tsai", "Ming-Feng", ""]]}, {"id": "1808.09610", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Yunwu Lin, Lei Zhu, Zuping Zhang, Yan Tang, Fang\n  Huang", "title": "Efficient Region of Visual Interests Search for Geo-multimedia Data", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of online social networking services and mobile smart\ndevices equipped with mobile communications module and position sensor module,\nmassive amount of multimedia data has been collected, stored and shared. This\ntrend has put forward higher request on massive multimedia data retrieval. In\nthis paper, we investigate a novel spatial query named region of visual\ninterests query (RoVIQ), which aims to search users containing geographical\ninformation and visual words. Three baseline methods are presented to introduce\nhow to exploit existing techniques to address this problem. Then we propose the\ndefinition of this query and related notions at the first time. To improve the\nperformance of query, we propose a novel spatial indexing structure called\nquadtree based inverted visual index which is a combination of quadtree,\ninverted index and visual words. Based on it, we design a efficient search\nalgorithm named region of visual interests search to support RoVIQ.\nExperimental evaluations on real geo-image datasets demonstrate that our\nsolution outperforms state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 02:30:54 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Lin", "Yunwu", ""], ["Zhu", "Lei", ""], ["Zhang", "Zuping", ""], ["Tang", "Yan", ""], ["Huang", "Fang", ""]]}, {"id": "1808.09640", "submitter": "Chuan-Ming Song Dr.", "authors": "Chuan-Ming Song and Bo Fu and Xiang-Hai Wang and Ming-Zhe Fu", "title": "Wavelet Video Coding Algorithm Based on Energy Weighted Significance\n  Probability Balancing Tree", "comments": "17 pages, 2 figures, submission to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a 3-D wavelet video coding algorithm. By analyzing the\ncontribution of each biorthogonal wavelet basis to reconstructed signal's\nenergy, we weight each wavelet subband according to its basis energy. Based on\ndistribution of weighted coefficients, we further discuss a 3-D wavelet tree\nstructure named \\textbf{significance probability balancing tree}, which places\nthe coefficients with similar probabilities of being significant on the same\nlayer. It is implemented by using hybrid spatial orientation tree and\ntemporal-domain block tree. Subsequently, a novel 3-D wavelet video coding\nalgorithm is proposed based on the energy-weighted significance probability\nbalancing tree. Experimental results illustrate that our algorithm always\nachieves good reconstruction quality for different classes of video sequences.\nCompared with asymmetric 3-D orientation tree, the average peak signal-to-noise\nratio (PSNR) gain of our algorithm are 1.24dB, 2.54dB and 2.57dB for luminance\n(Y) and chrominance (U,V) components, respectively. Compared with\ntemporal-spatial orientation tree algorithm, our algorithm gains 0.38dB, 2.92dB\nand 2.39dB higher PSNR separately for Y, U, and V components. In addition, the\nproposed algorithm requires lower computation cost than those of the above two\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 05:04:58 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Song", "Chuan-Ming", ""], ["Fu", "Bo", ""], ["Wang", "Xiang-Hai", ""], ["Fu", "Ming-Zhe", ""]]}, {"id": "1808.10351", "submitter": "Albin Correya", "authors": "Albin Andrew Correya, Romain Hennequin, Micka\\\"el Arcos", "title": "Large-Scale Cover Song Detection in Digital Music Libraries Using\n  Metadata, Lyrics and Audio Features", "comments": "Music Information Retrieval, Cover Song Identification, Million Song\n  Dataset, Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cover song detection is a very relevant task in Music Information Retrieval\n(MIR) studies and has been mainly addressed using audio-based systems. Despite\nits potential impact in industrial contexts, low performances and lack of\nscalability have prevented such systems from being adopted in practice for\nlarge applications. In this work, we investigate whether textual music\ninformation (such as metadata and lyrics) can be used along with audio for\nlarge-scale cover identification problem in a wide digital music library. We\nbenchmark this problem using standard text and state of the art audio\nsimilarity measures. Our studies shows that these methods can significantly\nincrease the accuracy and scalability of cover detection systems on Million\nSong Dataset (MSD) and Second Hand Song (SHS) datasets. By only leveraging\nstandard tf-idf based text similarity measures on song titles and lyrics, we\nachieved 35.5% of absolute increase in mean average precision compared to the\ncurrent scalable audio content-based state of the art methods on MSD. These\nexperimental results suggests that new methodologies can be encouraged among\nresearchers to leverage and identify more sophisticated NLP-based techniques to\nimprove current cover song identification systems in digital music libraries\nwith metadata.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:26:49 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Correya", "Albin Andrew", ""], ["Hennequin", "Romain", ""], ["Arcos", "Micka\u00ebl", ""]]}]