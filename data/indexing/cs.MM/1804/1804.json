[{"id": "1804.00229", "submitter": "Alina Striner", "authors": "Alina Striner", "title": "Can Multisensory Cues in VR Help Train Pattern Recognition to Citizen\n  Scientists?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the internet of things (IoT) has integrated physical and digital\ntechnologies, designing for multiple sensory media (mulsemedia) has become more\nattainable. Designing technology for multiple senses has the capacity to\nimprove virtual realism, extend our ability to process information, and more\neasily transfer knowledge between physical and digital environments. HCI\nresearchers are beginning to explore the viability of integrating multimedia\ninto virtual experiences, however research has yet to consider whether\nmulsemedia truly enhances realism, immersion and knowledge transfer. My work\ndeveloping StreamBED, a VR training platform to train citizen science water\nmonitors plans to consider the role of mulsemedia in immersion and learning\ngoals. Future findings about the role of mulsemedia in learning contexts will\npotentially allow learners to experience, connect to, learn from spaces that\nare impossible to experience firsthand.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 00:00:51 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Striner", "Alina", ""]]}, {"id": "1804.00589", "submitter": "Ali H Husseen Al-Nuaimi Mr", "authors": "Ali H. Husseen Al-nuaimi, Shyamaa Shakir Al-juboori, R. J. Mohammed", "title": "Image Compression Using Proposed Enhanced Run Length Encoding Algorithm", "comments": null, "journal-ref": "IBN AL- HAITHAM J. FOR PURE & APPL. SCI. VOL.24 (1) 2011", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we will present p roposed enhance process of image compression\nby using RLE algorithm. This proposed yield to decrease the size of compressing\nimage, but the original method used primarily for compressing a binary images\n[1].Which will yield increasing the size of an original image mostly when used\nfor color images. The test of an enhanced algorithm is performed on sample\nconsists of ten BMP 24-bit true color images, building an application by using\nvisual basic 6.0 to show the size after and before compression process and\ncomputing the compression ratio for RLE and for the enhanced RLE algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 12:07:25 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Al-nuaimi", "Ali H. Husseen", ""], ["Al-juboori", "Shyamaa Shakir", ""], ["Mohammed", "R. J.", ""]]}, {"id": "1804.00935", "submitter": "Hanzhou Wu", "authors": "Yanli Chen, Hongxia Wang, Hanzhou Wu, Yi Chen and Asad Malik", "title": "Intra-Frame Error Concealment Scheme using 3D Reversible Data Hiding in\n  Mobile Cloud Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in mobile cloud environment are mainly transmitted via wireless noisy\nchannels, which may result in transmission errors with a high probability due\nto its unreliable connectivity. For video transmission, unreliable connectivity\nmay cause significant degradation of the content. Improving or keeping video\nquality over lossy channel is therefore a very important research topic. Error\nconcealment with data hiding (ECDH) is an effective way to conceal the errors\nintroduced by channels. It can reduce error propagation between neighbor\nblocks/frames comparing with the methods exploiting temporal/spatial\ncorrelations. The existing video ECDH methods often embed the motion vectors\n(MVs) into the specific locations. Nevertheless, specific embedding locations\ncannot resist against random errors. To compensate the unreliable connectivity\nin mobile cloud environment, in this paper, we present a video ECDH scheme\nusing 3D reversible data hiding (RDH), in which each MV is repeated multiple\ntimes, and the repeated MVs are embedded into different macroblocks (MBs)\nrandomly. Though the multiple embedding requires more embedding space,\nsatisfactory trade-off between the introduced distortion and the reconstructed\nvideo quality can be achieved by tuning the repeating times of the MVs. For\nrandom embedding, the lost probability of the MVs decreases rapidly, resulting\nin better error concealment performance. Experimental results show that the\nPSNR values gain about 5dB at least comparing with the existing ECDH methods.\nMeanwhile, the proposed method improves the video quality significantly.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 12:42:43 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Chen", "Yanli", ""], ["Wang", "Hongxia", ""], ["Wu", "Hanzhou", ""], ["Chen", "Yi", ""], ["Malik", "Asad", ""]]}, {"id": "1804.01035", "submitter": "Eirina Bourtsoulatze", "authors": "Eirina Bourtsoulatze and Deniz G\\\"und\\\"uz", "title": "Cache-Aided Interactive Multiview Video Streaming in Small Cell Wireless\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of novel interactive multimedia applications with high rate and\nlow latency requirements has led to a drastic increase in the video data\ntraffic over wireless cellular networks. Endowing the small base stations of a\nmacro-cell with caches that can store some of the content is a promising\ntechnology to cope with the increasing pressure on the backhaul connections,\nand to reduce the delay for demanding video applications. In this work,\ndelivery of an interactive multiview video to a set of wireless users is\nstudied in an heterogeneous cellular network. Differently from existing works\nthat focus on the optimization of the delivery delay and ignore the video\ncharacteristics, the caching and scheduling policies are jointly optimized,\ntaking into account the quality of the delivered video and the video delivery\ntime constraints. We formulate our joint caching and scheduling problem as the\naverage expected video distortion minimization, and show that this problem is\nNP-hard. We then provide an equivalent formulation based on submodular set\nfunction maximization and propose a greedy solution with\n$\\frac{1}{2}(1-\\mbox{e}^{-1})$ approximation guarantee. The evaluation of the\nproposed joint caching and scheduling policy shows that it significantly\noutperforms benchmark algorithms based on popularity caching and independent\nscheduling. Another important contribution of this paper is a new constant\napproximation ratio for the greedy submodular set function maximization subject\nto a $d$-dimensional knapsack constraint.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 15:23:05 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Bourtsoulatze", "Eirina", ""], ["G\u00fcnd\u00fcz", "Deniz", ""]]}, {"id": "1804.01430", "submitter": "Onur G\\\"unl\\\"u", "authors": "Onur G\\\"unl\\\"u, Kittipong Kittichokechai, Rafael F. Schaefer, Giuseppe\n  Caire", "title": "Controllable Identifier Measurements for Private Authentication with\n  Secret Keys", "comments": "15 pages", "journal-ref": "IEEE Transactions on Information Forensics and Security, vol. 13,\n  no. 8, pp. 1945-1959 (Aug. 2018)", "doi": "10.1109/TIFS.2018.2806937", "report-no": null, "categories": "cs.IT cs.CR cs.MM eess.SP math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of secret-key based authentication under a privacy constraint on\nthe source sequence is considered. The identifier measurements during\nauthentication are assumed to be controllable via a cost-constrained \"action\"\nsequence. Single-letter characterizations of the optimal trade-off among the\nsecret-key rate, storage rate, privacy-leakage rate, and action cost are given\nfor the four problems where noisy or noiseless measurements of the source are\nenrolled to generate or embed secret keys. The results are relevant for several\nuser-authentication scenarios including physical and biometric authentications\nwith multiple measurements. Our results include, as special cases, new results\nfor secret-key generation and embedding with action-dependent side information\nwithout any privacy constraint on the enrolled source sequence.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:25:29 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Kittichokechai", "Kittipong", ""], ["Schaefer", "Rafael F.", ""], ["Caire", "Giuseppe", ""]]}, {"id": "1804.01665", "submitter": "Ruohan Gao", "authors": "Ruohan Gao, Rogerio Feris, Kristen Grauman", "title": "Learning to Separate Object Sounds by Watching Unlabeled Video", "comments": "Published in ECCV 2018; Project Page:\n  http://vision.cs.utexas.edu/projects/separating_object_sounds/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving a scene most fully requires all the senses. Yet modeling how\nobjects look and sound is challenging: most natural scenes and events contain\nmultiple objects, and the audio track mixes all the sound sources together. We\npropose to learn audio-visual object models from unlabeled video, then exploit\nthe visual context to perform audio source separation in novel videos. Our\napproach relies on a deep multi-instance multi-label learning framework to\ndisentangle the audio frequency bases that map to individual visual objects,\neven without observing/hearing those objects in isolation. We show how the\nrecovered disentangled bases can be used to guide audio source separation to\nobtain better-separated, object-level sounds. Our work is the first to learn\naudio source separation from large-scale \"in the wild\" videos containing\nmultiple audio sources per video. We obtain state-of-the-art results on\nvisually-aided audio source separation and audio denoising. Our video results:\nhttp://vision.cs.utexas.edu/projects/separating_object_sounds/\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 04:06:46 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 04:46:24 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Gao", "Ruohan", ""], ["Feris", "Rogerio", ""], ["Grauman", "Kristen", ""]]}, {"id": "1804.01863", "submitter": "Klaus Schoeffmann", "authors": "Klaus Schoeffmann and Bernd M\\\"unzer and J\\\"urgen Primus and Andreas\n  Leibetseder", "title": "The diveXplore System at the Video Browser Showdown 2018 - Final Notes", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This short paper provides further details of the diveXplore system (formerly\nknown as CoViSS), which has been used by team ITEC1 for the Video Browser\nShowdown (VBS) 2018. In particular, it gives a short overview of search\nfeatures and some details of final system changes, not included in the\ncorresponding VBS2018 paper, as well as a basic analysis of how the system has\nbeen used for VBS2018 (from a user perspective).\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 14:08:22 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Schoeffmann", "Klaus", ""], ["M\u00fcnzer", "Bernd", ""], ["Primus", "J\u00fcrgen", ""], ["Leibetseder", "Andreas", ""]]}, {"id": "1804.02680", "submitter": "Seyyed Hossein Soleymani", "authors": "Seyyed Hossein Soleymani", "title": "Semi-fragile Tamper Detection and Recovery based on Region\n  Categorization and Two-Sided Circular Block Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new semi-fragile algorithm for image tamper detection\nand recovery, which is based on region attention and two-sided circular block\ndependency. This method categorizes the image blocks into three categories\naccording to their texture. In this method, less information is extracted from\nareas with the smooth texture, and more information is extracted from areas\nwith the rough texture. Also, the extracted information for each type of blocks\nis embedded in another block with the same type. So, changes in the smooth\nareas are invisible to Human Visual System. To increase the localization power\na two-sided circular block dependency is proposed, which is able to distinguish\npartially destroyed blocks. Pairwise block dependency and circular block\ndependency, which are common methods in the block-based tamper detection, are\nnot able to distinguish the partially destroyed blocks. Cubic interpolation is\nused in order to decrease the blocking effects in the recovery phase. The\nresults of the proposed method for regions with different texture show that the\nproposed method is superior to non-region-attention based methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 12:18:14 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Soleymani", "Seyyed Hossein", ""]]}, {"id": "1804.02691", "submitter": "Sai Ma", "authors": "Sai Ma and Qingxiao Guan and Xianfeng Zhao and Yaqi Liu", "title": "Adaptive Spatial Steganography Based on Probability-Controlled\n  Adversarial Examples", "comments": "This paper contains some problems in theoretical analysis and\n  experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanation from Sai Ma: The experiments in this paper are conducted on Caffe\nframework. In Caffe, there is an API to directly set the gradient in Matlab. I\nwrongly use it to control the 'probability', in fact, I modify the gradient\ndirectly. The misusage of API leads to wrong experiment results, and wrong\ntheoretical analysis.\n  Apologize to readers who have read this paper. We have submitted a correct\nversion of this paper to Multimedia Tools and Applications and it is under\nrevision.\n  Thanks to Dr. Patrick Bas, who is the Associate Editor of TIFS and the\nanonymous reviewers of this paper.\n  Thanks to Tingting Song from Sun Yat-sen University. We discussed some\nproblems of this paper. Her advice helps me to improve the submitted paper to\nMultimedia Tools and Applications.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 13:58:40 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 14:03:06 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 12:31:31 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ma", "Sai", ""], ["Guan", "Qingxiao", ""], ["Zhao", "Xianfeng", ""], ["Liu", "Yaqi", ""]]}, {"id": "1804.02792", "submitter": "Zeyu Chen", "authors": "Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, Guangcong Wang", "title": "Occluded Person Re-identification", "comments": "6 pages, 7 figures, IEEE International Conference of Multimedia and\n  Expo 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) suffers from a serious occlusion problem\nwhen applied to crowded public places. In this paper, we propose to retrieve a\nfull-body person image by using a person image with occlusions. This differs\nsignificantly from the conventional person re-id problem where it is assumed\nthat person images are detected without any occlusion. We thus call this new\nproblem the occluded person re-identitification. To address this new problem,\nwe propose a novel Attention Framework of Person Body (AFPB) based on deep\nlearning, consisting of 1) an Occlusion Simulator (OS) which automatically\ngenerates artificial occlusions for full-body person images, and 2) multi-task\nlosses that force the neural network not only to discriminate a person's\nidentity but also to determine whether a sample is from the occluded data\ndistribution or the full-body data distribution. Experiments on a new occluded\nperson re-id dataset and three existing benchmarks modified to include\nfull-body person images and occluded person images show the superiority of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 01:56:53 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 02:00:47 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 14:22:34 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Zhuo", "Jiaxuan", ""], ["Chen", "Zeyu", ""], ["Lai", "Jianhuang", ""], ["Wang", "Guangcong", ""]]}, {"id": "1804.03481", "submitter": "Huaizheng Zhang", "authors": "Huaizheng Zhang, Han Hu, Guanyu Gao, Yonggang Wen and Kyle Guan", "title": "DeepQoE: A unified Framework for Learning to Predict Video QoE", "comments": "6 pages, 5 figures, ICME2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the prowess of deep learning (DL) based techniques in\nprediction, generalization, and representation learning, we develop a novel\nframework called DeepQoE to predict video quality of experience (QoE). The\nend-to-end framework first uses a combination of DL techniques (e.g., word\nembeddings) to extract generalized features. Next, these features are combined\nand fed into a neural network for representation learning. Such representations\nserve as inputs for classification or regression tasks. Evaluating the\nperformance of DeepQoE with two datasets, we show that for the small dataset,\nthe accuracy of all shallow learning algorithm is improved by using the\nrepresentation derived from DeepQoE. For the large dataset, our DeepQoE\nframework achieves significant performance improvement in comparison to the\nbest baseline method (90.94% vs. 82.84%). Moreover, DeepQoE, also released as\nan open source tool, provides video QoE research much-needed flexibility in\nfitting different datasets, extracting generalized features, and learning\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 12:31:44 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhang", "Huaizheng", ""], ["Hu", "Han", ""], ["Gao", "Guanyu", ""], ["Wen", "Yonggang", ""], ["Guan", "Kyle", ""]]}, {"id": "1804.03507", "submitter": "Li-Kai Chi Mr.", "authors": "Xuefeng Peng, Li-Kai Chi, Jiebo Luo", "title": "The Effect of Pets on Happiness: A Large-scale Multi-Factor Analysis\n  using Social Multimedia", "comments": "Xuefeng Peng, Li-Kai Chi, and Jiebo Luo. 2017. The Effect of Pets on\n  Happiness: A Large-scale Multi-Factor Analysis using Social Multimedia. ACM\n  Trans. Intell. Syst. Technol. 9, 4, Article 39 (June 2017), 15 pages", "journal-ref": "ACM Trans. Intell. Syst. Technol. 9, 4, Article 39 (June 2017), 15\n  pages", "doi": null, "report-no": null, "categories": "cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From reducing stress and loneliness, to boosting productivity and overall\nwell-being, pets are believed to play a significant role in people's daily\nlives. Many traditional studies have identified that frequent interactions with\npets could make individuals become healthier and more optimistic, and\nultimately enjoy a happier life. However, most of those studies are not only\nrestricted in scale, but also may carry biases by using subjective\nself-reports, interviews, and questionnaires as the major approaches. In this\npaper, we leverage large-scale data collected from social media and the\nstate-of-the-art deep learning technologies to study this phenomenon in depth\nand breadth. Our study includes four major steps: 1) collecting timeline posts\nfrom around 20,000 Instagram users, 2) using face detection and recognition on\n2-million photos to infer users' demographics, relationship status, and whether\nhaving children, 3) analyzing a user's degree of happiness based on images and\ncaptions via smiling classification and textual sentiment analysis, 3) applying\ntransfer learning techniques to retrain the final layer of the Inception v3\nmodel for pet classification, and 4) analyzing the effects of pets on happiness\nin terms of multiple factors of user demographics. Our main results have\ndemonstrated the efficacy of our proposed method with many new insights. We\nbelieve this method is also applicable to other domains as a scalable,\nefficient, and effective methodology for modeling and analyzing social\nbehaviors and psychological well-being. In addition, to facilitate the research\ninvolving human faces, we also release our dataset of 700K analyzed faces.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 15:07:16 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Peng", "Xuefeng", ""], ["Chi", "Li-Kai", ""], ["Luo", "Jiebo", ""]]}, {"id": "1804.03809", "submitter": "Bolin Liu", "authors": "Bolin Liu, Xiao Shu, Xiaolin Wu", "title": "Demoir\\'eing of Camera-Captured Screen Images Using Deep Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking photos of optoelectronic displays is a direct and spontaneous way of\ntransferring data and keeping records, which is widely practiced. However, due\nto the analog signal interference between the pixel grids of the display screen\nand camera sensor array, objectionable moir\\'e (alias) patterns appear in\ncaptured screen images. As the moir\\'e patterns are structured and highly\nvariant, they are difficult to be completely removed without affecting the\nunderneath latent image. In this paper, we propose an approach of deep\nconvolutional neural network for demoir\\'eing screen photos. The proposed DCNN\nconsists of a coarse-scale network and a fine-scale network. In the\ncoarse-scale network, the input image is first downsampled and then processed\nby stacked residual blocks to remove the moir\\'e artifacts. After that, the\nfine-scale network upsamples the demoir\\'ed low-resolution image back to the\noriginal resolution. Extensive experimental results have demonstrated that the\nproposed technique can efficiently remove the moir\\'e patterns for camera\nacquired screen images; the new technique outperforms the existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 04:51:25 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Liu", "Bolin", ""], ["Shu", "Xiao", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1804.04164", "submitter": "Boyang Li", "authors": "Hannah Kim, Denys Katerenchuk, Daniel Billet, Jun Huan, Haesun Park,\n  Boyang Li", "title": "Understanding Actors and Evaluating Personae with Gaussian Embeddings", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding narrative content has become an increasingly popular topic.\nNonetheless, research on identifying common types of narrative characters, or\npersonae, is impeded by the lack of automatic and broad-coverage evaluation\nmethods. We argue that computationally modeling actors provides benefits,\nincluding novel evaluation mechanisms for personae. Specifically, we propose\ntwo actor-modeling tasks, cast prediction and versatility ranking, which can\ncapture complementary aspects of the relation between actors and the characters\nthey portray. For an actor model, we present a technique for embedding actors,\nmovies, character roles, genres, and descriptive keywords as Gaussian\ndistributions and translation vectors, where the Gaussian variance corresponds\nto actors' versatility. Empirical results indicate that (1) the technique\nconsiderably outperforms TransE (Bordes et al. 2013) and ablation baselines and\n(2) automatically identified persona topics (Bamman, O'Connor, and Smith 2013)\nyield statistically significant improvements in both tasks, whereas simplistic\npersona descriptors including age and gender perform inconsistently, validating\nprior research.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 17:44:23 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 08:19:59 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kim", "Hannah", ""], ["Katerenchuk", "Denys", ""], ["Billet", "Daniel", ""], ["Huan", "Jun", ""], ["Park", "Haesun", ""], ["Li", "Boyang", ""]]}, {"id": "1804.04318", "submitter": "Yale Song", "authors": "Yale Song and Mohammad Soleymani", "title": "Cross-Modal Retrieval with Implicit Concept Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional cross-modal retrieval assumes explicit association of concepts\nacross modalities, where there is no ambiguity in how the concepts are linked\nto each other, e.g., when we do the image search with a query \"dogs\", we expect\nto see dog images. In this paper, we consider a different setting for\ncross-modal retrieval where data from different modalities are implicitly\nlinked via concepts that must be inferred by high-level reasoning; we call this\nsetting implicit concept association. To foster future research in this\nsetting, we present a new dataset containing 47K pairs of animated GIFs and\nsentences crawled from the web, in which the GIFs depict physical or emotional\nreactions to the scenarios described in the text (called \"reaction GIFs\"). We\nreport on a user study showing that, despite the presence of implicit concept\nassociation, humans are able to identify video-sentence pairs with matching\nconcepts, suggesting the feasibility of our task. Furthermore, we propose a\nnovel visual-semantic embedding network based on multiple instance learning.\nUnlike traditional approaches, we compute multiple embeddings from each\nmodality, each representing different concepts, and measure their similarity by\nconsidering all possible combinations of visual-semantic embeddings in the\nframework of multiple instance learning. We evaluate our approach on two\nvideo-sentence datasets with explicit and implicit concept association and\nreport competitive results compared to existing approaches on cross-modal\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 05:10:33 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 16:30:57 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Song", "Yale", ""], ["Soleymani", "Mohammad", ""]]}, {"id": "1804.04326", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Jiaqing Lin, Akikazu Takeuchi", "title": "STAIR Actions: A Video Dataset of Everyday Home Actions", "comments": "STAIR Actions dataset can be downloaded from\n  http://actions.stair.center", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new large-scale video dataset for human action recognition, called STAIR\nActions is introduced. STAIR Actions contains 100 categories of action labels\nrepresenting fine-grained everyday home actions so that it can be applied to\nresearch in various home tasks such as nursing, caring, and security. In STAIR\nActions, each video has a single action label. Moreover, for each action\ncategory, there are around 1,000 videos that were obtained from YouTube or\nproduced by crowdsource workers. The duration of each video is mostly five to\nsix seconds. The total number of videos is 102,462. We explain how we\nconstructed STAIR Actions and show the characteristics of STAIR Actions\ncompared to existing datasets for human action recognition. Experiments with\nthree major models for action recognition show that STAIR Actions can train\nlarge models and achieve good performance. STAIR Actions can be downloaded from\nhttp://actions.stair.center\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 05:48:06 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 03:26:54 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 05:40:42 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Lin", "Jiaqing", ""], ["Takeuchi", "Akikazu", ""]]}, {"id": "1804.04600", "submitter": "Shota Horiguchi", "authors": "Shota Horiguchi, Sosuke Amano, Makoto Ogawa, Kiyoharu Aizawa", "title": "Personalized Classifier for Food Image Recognition", "comments": "Accepted to IEEE Transaction on Multimedia.\n  http://ieeexplore.ieee.org/document/8316919/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, food image recognition tasks are evaluated against fixed datasets.\nHowever, in real-world conditions, there are cases in which the number of\nsamples in each class continues to increase and samples from novel classes\nappear. In particular, dynamic datasets in which each individual user creates\nsamples and continues the updating process often have content that varies\nconsiderably between different users, and the number of samples per person is\nvery limited. A single classifier common to all users cannot handle such\ndynamic data. Bridging the gap between the laboratory environment and the real\nworld has not yet been accomplished on a large scale. Personalizing a\nclassifier incrementally for each user is a promising way to do this. In this\npaper, we address the personalization problem, which involves adapting to the\nuser's domain incrementally using a very limited number of samples. We propose\na simple yet effective personalization framework which is a combination of the\nnearest class mean classifier and the 1-nearest neighbor classifier based on\ndeep features. To conduct realistic experiments, we made use of a new dataset\nof daily food images collected by a food-logging application. Experimental\nresults show that our proposed method significantly outperforms existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 14:08:13 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Horiguchi", "Shota", ""], ["Amano", "Sosuke", ""], ["Ogawa", "Makoto", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1804.04813", "submitter": "Christos Bampis", "authors": "Christos G. Bampis, Zhi Li, Alan C. Bovik", "title": "SpatioTemporal Feature Integration and Model Fusion for Full Reference\n  Video Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual video quality assessment models are either frame-based or\nvideo-based, i.e., they apply spatiotemporal filtering or motion estimation to\ncapture temporal video distortions. Despite their good performance on video\nquality databases, video-based approaches are time-consuming and harder to\nefficiently deploy. To balance between high performance and computational\nefficiency, Netflix developed the Video Multi-method Assessment Fusion (VMAF)\nframework, which integrates multiple quality-aware features to predict video\nquality. Nevertheless, this fusion framework does not fully exploit temporal\nvideo quality measurements which are relevant to temporal video distortions. To\nthis end, we propose two improvements to the VMAF framework: SpatioTemporal\nVMAF and Ensemble VMAF. Both algorithms exploit efficient temporal video\nfeatures which are fed into a single or multiple regression models. To train\nour models, we designed a large subjective database and evaluated the proposed\nmodels against state-of-the-art approaches. The compared algorithms will be\nmade available as part of the open source package in\nhttps://github.com/Netflix/vmaf.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:42:33 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Bampis", "Christos G.", ""], ["Li", "Zhi", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1804.04866", "submitter": "Luca Rossetto M.Sc.", "authors": "Silvan Heller, Luca Rossetto, Heiko Schuldt", "title": "The PS-Battles Dataset - an Image Collection for Image Manipulation\n  Detection", "comments": "The dataset introduced in this paper can be found on\n  https://github.com/dbisUnibas/PS-Battles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boost of available digital media has led to a significant increase in\nderivative work. With tools for manipulating objects becoming more and more\nmature, it can be very difficult to determine whether one piece of media was\nderived from another one or tampered with. As derivations can be done with\nmalicious intent, there is an urgent need for reliable and easily usable\ntampering detection methods. However, even media considered semantically\nuntampered by humans might have already undergone compression steps or light\npost-processing, making automated detection of tampering susceptible to false\npositives. In this paper, we present the PS-Battles dataset which is gathered\nfrom a large community of image manipulation enthusiasts and provides a basis\nfor media derivation and manipulation detection in the visual domain. The\ndataset consists of 102'028 images grouped into 11'142 subsets, each containing\nthe original image as well as a varying number of manipulated derivatives.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 09:59:54 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Heller", "Silvan", ""], ["Rossetto", "Luca", ""], ["Schuldt", "Heiko", ""]]}, {"id": "1804.05490", "submitter": "Olivier Augereau", "authors": "Olivier Augereau, Motoi Iwata, Koichi Kise", "title": "A survey of comics research in computer science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical novels such as comics and mangas are well known all over the world.\nThe digital transition started to change the way people are reading comics,\nmore and more on smartphones and tablets and less and less on paper. In the\nrecent years, a wide variety of research about comics has been proposed and\nmight change the way comics are created, distributed and read in future years.\nEarly work focuses on low level document image analysis: indeed comic books are\ncomplex, they contains text, drawings, balloon, panels, onomatopoeia, etc.\nDifferent fields of computer science covered research about user interaction\nand content generation such as multimedia, artificial intelligence,\nhuman-computer interaction, etc. with different sets of values. We propose in\nthis paper to review the previous research about comics in computer science, to\nstate what have been done and to give some insights about the main outlooks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 03:17:29 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Augereau", "Olivier", ""], ["Iwata", "Motoi", ""], ["Kise", "Koichi", ""]]}, {"id": "1804.06057", "submitter": "Ryota Hinami", "authors": "Ryota Hinami, Junwei Liang, Shin'ichi Satoh, Alexander Hauptmann", "title": "Multimodal Co-Training for Selecting Good Examples from Webly Labeled\n  Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of learning concept classifiers from videos on the web\nwithout using manually labeled data. Although metadata attached to videos\n(e.g., video titles, descriptions) can be of help collecting training data for\nthe target concept, the collected data is often very noisy. The main challenge\nis therefore how to select good examples from noisy training data. Previous\napproaches firstly learn easy examples that are unlikely to be noise and then\ngradually learn more complex examples. However, hard examples that are much\ndifferent from easy ones are never learned. In this paper, we propose an\napproach called multimodal co-training (MMCo) for selecting good examples from\nnoisy training data. MMCo jointly learns classifiers for multiple modalities\nthat complement each other to select good examples. Since MMCo selects examples\nby consensus of multimodal classifiers, a hard example for one modality can\nstill be used as a training example by exploiting the power of the other\nmodalities. The algorithm is very simple and easily implemented but yields\nconsistent and significant boosts in example selection and classification\nperformance on the FCVID and YouTube8M benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 06:03:14 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Hinami", "Ryota", ""], ["Liang", "Junwei", ""], ["Satoh", "Shin'ichi", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1804.06604", "submitter": "Ana Garc\\'ia del Molino", "authors": "Ana Garc\\'ia del Molino and Michael Gygli", "title": "PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation", "comments": "Accepted for publication at the 2018 ACM Multimedia Conference (MM\n  '18)", "journal-ref": null, "doi": "10.1145/3240508.3240599", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highlight detection models are typically trained to identify cues that make\nvisual content appealing or interesting for the general public, with the\nobjective of reducing a video to such moments. However, the \"interestingness\"\nof a video segment or image is subjective. Thus, such highlight models provide\nresults of limited relevance for the individual user. On the other hand,\ntraining one model per user is inefficient and requires large amounts of\npersonal information which is typically not available. To overcome these\nlimitations, we present a global ranking model which conditions on each\nparticular user's interests. Rather than training one model per user, our model\nis personalized via its inputs, which allows it to effectively adapt its\npredictions, given only a few user-specific examples. To train this model, we\ncreate a large-scale dataset of users and the GIFs they created, giving us an\naccurate indication of their interests. Our experiments show that using the\nuser history substantially improves the prediction accuracy. On our test set of\n850 videos, our model improves the recall by 8% with respect to generic\nhighlight detectors. Furthermore, our method proves more precise than the\nuser-agnostic baselines even with just one person-specific example.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 08:44:11 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 09:10:34 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["del Molino", "Ana Garc\u00eda", ""], ["Gygli", "Michael", ""]]}, {"id": "1804.06628", "submitter": "Yi Chen", "authors": "Yi Chen, Hongxia Wang, Hanzhou Wu, Yong Liu", "title": "Reversible Video Data Hiding Using Zero QDCT Coefficient-Pairs", "comments": "19 pages, 10 figures, accepted by Multimedia Tools and Applications", "journal-ref": "Multimedia Tools and Applications, 2019", "doi": "10.1007/s11042-019-7635-z", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  H.264/Advanced Video Coding (AVC) is one of the most commonly used video\ncompression standard currently. In this paper, we propose a Reversible Data\nHiding (RDH) method based on H.264/AVC videos. In the proposed method, the\nmacroblocks with intra-frame $4\\times 4$ prediction modes in intra frames are\nfirst selected as embeddable blocks. Then, the last zero Quantized Discrete\nCosine Transform (QDCT) coefficients in all $4\\times 4$ blocks of the\nembeddable macroblocks are paired. In the following, a modification mapping\nrule based on making full use of modification directions are given. Finally,\neach zero coefficient-pair is changed by combining the given mapping rule with\nthe to-be-embedded information bits. Since most of last QDCT coefficients in\nall $4\\times 4$ blocks are zero and they are located in high frequency area.\nTherefore, the proposed method can obtain high embedding capacity and low\ndistortion.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 10:04:40 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 13:26:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chen", "Yi", ""], ["Wang", "Hongxia", ""], ["Wu", "Hanzhou", ""], ["Liu", "Yong", ""]]}, {"id": "1804.06645", "submitter": "Yi Chen", "authors": "Yi Chen, Hongxia Wang", "title": "An Improved Reversible Data Hiding Scheme by Changing Modification\n  Direction of Partial Coefficients in JPEG Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper first reviews the reversible data hiding scheme, of Liu et al. in\n2018, for JPEG images. After that, a novel reversible data hiding scheme, in\nwhich modification directions of partial nonzero quantized alternating current\n(AC) coefficients are utilized to decrease distortion and file size increase\ncaused by data hiding, is proposed. Experimental results have shown that the\nproposed scheme has indeed advantages in visual quality and smaller increase in\nfile size of marked JPEG images while compared to the state-of-the-art scheme\nwith the same embedding payload so far.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 10:49:19 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 09:07:36 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 07:16:02 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Chen", "Yi", ""], ["Wang", "Hongxia", ""]]}, {"id": "1804.07006", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Ke Song, Chao Li, Fei Wang, Liyi Xiao", "title": "Large Margin Structured Convolution Operator for Thermal Infrared Object\n  Tracking", "comments": "Accepted as contributed paper in ICPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with visible object tracking, thermal infrared (TIR) object tracking\ncan track an arbitrary target in total darkness since it cannot be influenced\nby illumination variations. However, there are many unwanted attributes that\nconstrain the potentials of TIR tracking, such as the absence of visual color\npatterns and low resolutions. Recently, structured output support vector\nmachine (SOSVM) and discriminative correlation filter (DCF) have been\nsuccessfully applied to visible object tracking, respectively. Motivated by\nthese, in this paper, we propose a large margin structured convolution operator\n(LMSCO) to achieve efficient TIR object tracking. To improve the tracking\nperformance, we employ the spatial regularization and implicit interpolation to\nobtain continuous deep feature maps, including deep appearance features and\ndeep motion features, of the TIR targets. Finally, a collaborative optimization\nstrategy is exploited to significantly update the operators. Our approach not\nonly inherits the advantage of the strong discriminative capability of SOSVM\nbut also achieves accurate and robust tracking with higher-dimensional features\nand more dense samples. To the best of our knowledge, we are the first to\nincorporate the advantages of DCF and SOSVM for TIR object tracking.\nComprehensive evaluations on two thermal infrared tracking benchmarks, i.e.\nVOT-TIR2015 and VOT-TIR2016, clearly demonstrate that our LMSCO tracker\nachieves impressive results and outperforms most state-of-the-art trackers in\nterms of accuracy and robustness with sufficient frame rate.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 06:12:02 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 08:21:20 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Song", "Ke", ""], ["Li", "Chao", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""]]}, {"id": "1804.07019", "submitter": "J\\\"org Bachmann", "authors": "J\\\"org P. Bachmann and Benjamin Hauskeller", "title": "Simple Yet Efficient Content Based Video Copy Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a collection of videos, how to detect content-based copies efficiently\nwith high accuracy? Detecting copies in large video collections still remains\none of the major challenges of multimedia retrieval. While many video copy\ndetection approaches show high computation times and insufficient quality, we\npropose a new efficient content-based video copy detection algorithm improving\nboth aspects. The idea of our approach consists in utilizing self-similarity\nmatrices as video descriptors in order to capture different visual properties.\nWe benchmark our algorithm on the MuscleVCD ST1 benchmark dataset and show that\nour approach is able to achieve a score of 100\\% and a score of at least 93\\%\nin a wide range of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 07:24:55 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Bachmann", "J\u00f6rg P.", ""], ["Hauskeller", "Benjamin", ""]]}, {"id": "1804.07939", "submitter": "Jianhua Yang", "authors": "Jianhua Yang, Kai Liu, Xiangui Kang, Edward K.Wong, Yun-Qing Shi", "title": "Spatial Image Steganography Based on Generative Adversarial Network", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent development of deep learning on steganalysis, embedding\nsecret information into digital images faces great challenges. In this paper, a\nsecure steganography algorithm by using adversarial training is proposed. The\narchitecture contain three component modules: a generator, an embedding\nsimulator and a discriminator. A generator based on U-NET to translate a cover\nimage into an embedding change probability is proposed. To fit the optimal\nembedding simulator and propagate the gradient, a function called\nTanh-simulator is proposed. As for the discriminator, the selection-channel\nawareness (SCA) is incorporated to resist the SCA based steganalytic methods.\nExperimental results have shown that the proposed framework can increase the\nsecurity performance dramatically over the recently reported method ASDL-GAN,\nwhile the training time is only 30% of that used by ASDL-GAN. Furthermore, it\nalso performs better than the hand-crafted steganographic algorithm S-UNIWARD.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 10:35:33 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yang", "Jianhua", ""], ["Liu", "Kai", ""], ["Kang", "Xiangui", ""], ["Wong", "Edward K.", ""], ["Shi", "Yun-Qing", ""]]}, {"id": "1804.08208", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Ke Song, Chao Li, Fei Wang, Liyi Xiao, Yan Zhang", "title": "High Performance Visual Tracking with Circular and Structural Operators", "comments": "Accepted to Knowledge-Based SYSTEMS", "journal-ref": null, "doi": "10.1016/j.knosys.2018.08.008", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel circular and structural operator tracker (CSOT) is\nproposed for high performance visual tracking, it not only possesses the\npowerful discriminative capability of SOSVM but also efficiently inherits the\nsuperior computational efficiency of DCF. Based on the proposed circular and\nstructural operators, a set of primal confidence score maps can be obtained by\ncircular correlating feature maps with their corresponding structural\ncorrelation filters. Furthermore, an implicit interpolation is applied to\nconvert the multi-resolution feature maps to the continuous domain and make all\nprimal confidence score maps have the same spatial resolution. Then, we exploit\nan efficient ensemble post-processor based on relative entropy, which can\ncoalesce primal confidence score maps and create an optimal confidence score\nmap for more accurate localization. The target is localized on the peak of the\noptimal confidence score map. Besides, we introduce a collaborative\noptimization strategy to update circular and structural operators by\niteratively training structural correlation filters, which significantly\nreduces computational complexity and improves robustness. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance in mean AUC\nscores of 71.5% and 69.4% on the OTB-2013 and OTB-2015 benchmarks respectively,\nand obtains a third-best expected average overlap (EAO) score of 29.8% on the\nVOT-2017 benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 01:08:43 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 08:34:03 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 07:06:14 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Song", "Ke", ""], ["Li", "Chao", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""], ["Zhang", "Yan", ""]]}, {"id": "1804.08256", "submitter": "Jiagao Hu", "authors": "Jiagao Hu, Zhengxing Sun, Yunhan Sun, Jinlong Shi", "title": "Progressive refinement: a method of coarse-to-fine image parsing using\n  stacked network", "comments": "Accepted for presentation in an ORAL session at ICME 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To parse images into fine-grained semantic parts, the complex fine-grained\nelements will put it in trouble when using off-the-shelf semantic segmentation\nnetworks. In this paper, for image parsing task, we propose to parse images\nfrom coarse to fine with progressively refined semantic classes. It is achieved\nby stacking the segmentation layers in a segmentation network several times.\nThe former segmentation module parses images at a coarser-grained level, and\nthe result will be feed to the following one to provide effective contextual\nclues for the finer-grained parsing. To recover the details of small\nstructures, we add skip connections from shallow layers of the network to\nfine-grained parsing modules. As for the network training, we merge classes in\ngroundtruth to get coarse-to-fine label maps, and train the stacked network\nwith these hierarchical supervision end-to-end. Our coarse-to-fine stacked\nframework can be injected into many advanced neural networks to improve the\nparsing results. Extensive evaluations on several public datasets including\nface parsing and human parsing well demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 06:33:53 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Hu", "Jiagao", ""], ["Sun", "Zhengxing", ""], ["Sun", "Yunhan", ""], ["Shi", "Jinlong", ""]]}, {"id": "1804.08424", "submitter": "Jens Grubert", "authors": "Fabian G\\\"ottl and Philipp Gagel and Jens Grubert", "title": "Efficient Pose Tracking from Natural Features in Standard Web Browsers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision-based natural feature tracking is at the core of modern\nAugmented Reality applications. Still, Web-based Augmented Reality typically\nrelies on location-based sensing (using GPS and orientation sensors) or\nmarker-based approaches to solve the pose estimation problem.\n  We present an implementation and evaluation of an efficient natural feature\ntracking pipeline for standard Web browsers using HTML5 and WebAssembly. Our\nsystem can track image targets at real-time frame rates tablet PCs (up to 60\nHz) and smartphones (up to 25 Hz).\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 13:46:01 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["G\u00f6ttl", "Fabian", ""], ["Gagel", "Philipp", ""], ["Grubert", "Jens", ""]]}, {"id": "1804.08473", "submitter": "Bei Liu", "authors": "Bei Liu, Jianlong Fu, Makoto P. Kato, Masatoshi Yoshikawa", "title": "Beyond Narrative Description: Generating Poetry from Images by\n  Multi-Adversarial Training", "comments": null, "journal-ref": null, "doi": "10.1145/3240508.3240587", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of natural language from images has attracted extensive\nattention. In this paper, we take one step further to investigate generation of\npoetic language (with multiple lines) to an image for automatic poetry\ncreation. This task involves multiple challenges, including discovering poetic\nclues from the image (e.g., hope from green), and generating poems to satisfy\nboth relevance to the image and poeticness in language level. To solve the\nabove challenges, we formulate the task of poem generation into two correlated\nsub-tasks by multi-adversarial training via policy gradient, through which the\ncross-modal relevance and poetic language style can be ensured. To extract\npoetic clues from images, we propose to learn a deep coupled visual-poetic\nembedding, in which the poetic representation from objects, sentiments and\nscenes in an image can be jointly learned. Two discriminative networks are\nfurther introduced to guide the poem generation, including a multi-modal\ndiscriminator and a poem-style discriminator. To facilitate the research, we\nhave released two poem datasets by human annotators with two distinct\nproperties: 1) the first human annotated image-to-poem pair dataset (with 8,292\npairs in total), and 2) to-date the largest public English poem corpus dataset\n(with 92,265 different poems in total). Extensive experiments are conducted\nwith 8K images, among which 1.5K image are randomly picked for evaluation. Both\nobjective and subjective evaluations show the superior performances against the\nstate-of-the-art methods for poem generation from images. Turing test carried\nout with over 500 human subjects, among which 30 evaluators are poetry experts,\ndemonstrates the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:35:59 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 06:45:53 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 07:35:26 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 03:23:38 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Liu", "Bei", ""], ["Fu", "Jianlong", ""], ["Kato", "Makoto P.", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1804.09066", "submitter": "Mohammadreza Zolfaghari", "authors": "Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox", "title": "ECO: Efficient Convolutional Network for Online Video Understanding", "comments": "Submitted to ECCV 2018. 17 pages, 7 figures, Supplementary Material,\n  https://github.com/mzolfaghari/ECO-efficient-video-understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art in video understanding suffers from two problems: (1)\nThe major part of reasoning is performed locally in the video, therefore, it\nmisses important relationships within actions that span several seconds. (2)\nWhile there are local methods with fast per-frame processing, the processing of\nthe whole video is not efficient and hampers fast video retrieval or online\nclassification of long-term activities. In this paper, we introduce a network\narchitecture that takes long-term content into account and enables fast\nper-video processing at the same time. The architecture is based on merging\nlong-term content already in the network rather than in a post-hoc fusion.\nTogether with a sampling strategy, which exploits that neighboring frames are\nlargely redundant, this yields high-quality action classification and video\ncaptioning at up to 230 videos per second, where each video can consist of a\nfew hundred frames. The approach achieves competitive performance across all\ndatasets while being 10x to 80x faster than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 14:30:56 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 09:46:08 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Zolfaghari", "Mohammadreza", ""], ["Singh", "Kamaljeet", ""], ["Brox", "Thomas", ""]]}, {"id": "1804.09288", "submitter": "Anurag Kumar", "authors": "Ankit Shah, Anurag Kumar, Alexander G. Hauptmann, Bhiksha Raj", "title": "A Closer Look at Weak Label Learning for Audio Events", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio content analysis in terms of sound events is an important research\nproblem for a variety of applications. Recently, the development of weak\nlabeling approaches for audio or sound event detection (AED) and availability\nof large scale weakly labeled dataset have finally opened up the possibility of\nlarge scale AED. However, a deeper understanding of how weak labels affect the\nlearning for sound events is still missing from literature. In this work, we\nfirst describe a CNN based approach for weakly supervised training of audio\nevents. The approach follows some basic design principle desirable in a\nlearning method relying on weakly labeled audio. We then describe important\ncharacteristics, which naturally arise in weakly supervised learning of sound\nevents. We show how these aspects of weak labels affect the generalization of\nmodels. More specifically, we study how characteristics such as label density\nand corruption of labels affects weakly supervised training for audio events.\nWe also study the feasibility of directly obtaining weak labeled data from the\nweb without any manual label and compare it with a dataset which has been\nmanually labeled. The analysis and understanding of these factors should be\ntaken into picture in the development of future weak label learning methods.\nAudioset, a large scale weakly labeled dataset for sound events is used in our\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 23:04:35 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Shah", "Ankit", ""], ["Kumar", "Anurag", ""], ["Hauptmann", "Alexander G.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1804.09539", "submitter": "Yuxin Peng", "authors": "Jinwei Qi, Yuxin Peng and Yuxin Yuan", "title": "Cross-media Multi-level Alignment with Relation Attention Network", "comments": "7 pages, accepted by International Joint Conference on Artificial\n  Intelligence (IJCAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of multimedia data, such as image and text, it is a\nhighly challenging problem to effectively correlate and retrieve the data of\ndifferent media types. Naturally, when correlating an image with textual\ndescription, people focus on not only the alignment between discriminative\nimage regions and key words, but also the relations lying in the visual and\ntextual context. Relation understanding is essential for cross-media\ncorrelation learning, which is ignored by prior cross-media retrieval works. To\naddress the above issue, we propose Cross-media Relation Attention Network\n(CRAN) with multi-level alignment. First, we propose visual-language relation\nattention model to explore both fine-grained patches and their relations of\ndifferent media types. We aim to not only exploit cross-media fine-grained\nlocal information, but also capture the intrinsic relation information, which\ncan provide complementary hints for correlation learning. Second, we propose\ncross-media multi-level alignment to explore global, local and relation\nalignments across different media types, which can mutually boost to learn more\nprecise cross-media correlation. We conduct experiments on 2 cross-media\ndatasets, and compare with 10 state-of-the-art methods to verify the\neffectiveness of proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:22:38 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Qi", "Jinwei", ""], ["Peng", "Yuxin", ""], ["Yuan", "Yuxin", ""]]}, {"id": "1804.09808", "submitter": "Paolo Frasconi", "authors": "Tijn Borghuis, Alessandro Tibo, Simone Conforti, Luca Canciello,\n  Lorenzo Brusci, Paolo Frasconi", "title": "Off the Beaten Track: Using Deep Learning to Interpolate Between Music\n  Genres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system based on deep learning that generates drum patterns in\nthe electronic dance music domain. Experimental results reveal that generated\npatterns can be employed to produce musically sound and creative transitions\nbetween different genres, and that the process of generation is of interest to\npractitioners in the field.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 21:39:39 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 16:56:08 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Borghuis", "Tijn", ""], ["Tibo", "Alessandro", ""], ["Conforti", "Simone", ""], ["Canciello", "Luca", ""], ["Brusci", "Lorenzo", ""], ["Frasconi", "Paolo", ""]]}, {"id": "1804.09864", "submitter": "Jounsup Park", "authors": "Jounsup Park, Philip A. Chou, and Jenq-Neng Hwang", "title": "Rate-Utility Optimized Streaming of Volumetric Media for Augmented\n  Reality", "comments": "18 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric media, popularly known as holograms, need to be delivered to users\nusing both on-demand and live streaming, for new augmented reality (AR) and\nvirtual reality (VR) experiences. As in video streaming, hologram streaming\nmust support network adaptivity and fast startup, but must also moderate large\nbandwidths, multiple simultaneously streaming objects, and frequent user\ninteraction, which requires low delay. In this paper, we introduce the first\nsystem to our knowledge designed specifically for streaming volumetric media.\nThe system reduces bandwidth by introducing 3D tiles, and culling them or\nreducing their level of detail depending on their relation to the user's view\nfrustum and distance to the user. Our system reduces latency by introducing a\nwindow-based buffer, which in contrast to a queue-based buffer allows\ninsertions near the head of the buffer rather than only at the tail of the\nbuffer, to respond quickly to user interaction. To allocate bits between\ndifferent tiles across multiple objects, we introduce a simple greedy yet\nprovably optimal algorithm for rate-utility optimization. We introduce utility\nmeasures based not only on the underlying quality of the representation, but on\nthe level of detail relative to the user's viewpoint and device resolution.\nSimulation results show that the proposed algorithm provides superior quality\ncompared to existing video-streaming approaches adapted to hologram streaming,\nin terms of utility and user experience over variable, throughput-constrained\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 02:49:53 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Park", "Jounsup", ""], ["Chou", "Philip A.", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "1804.09869", "submitter": "Tianyu He", "authors": "Zhibo Chen, Tianyu He, Xin Jin, Feng Wu", "title": "Learning for Video Compression", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2892608", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One key challenge to learning-based video compression is that motion\npredictive coding, a very effective tool for video compression, can hardly be\ntrained into a neural network. In this paper we propose the concept of\nPixelMotionCNN (PMCNN) which includes motion extension and hybrid prediction\nnetworks. PMCNN can model spatiotemporal coherence to effectively perform\npredictive coding inside the learning network. On the basis of PMCNN, we\nfurther explore a learning-based framework for video compression with\nadditional components of iterative analysis/synthesis, binarization, etc.\nExperimental results demonstrate the effectiveness of the proposed scheme.\nAlthough entropy coding and complex configurations are not employed in this\npaper, we still demonstrate superior performance compared with MPEG-2 and\nachieve comparable results with H.264 codec. The proposed learning-based scheme\nprovides a possible new direction to further improve compression efficiency and\nfunctionalities of future video coding.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 03:16:48 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 09:49:58 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Chen", "Zhibo", ""], ["He", "Tianyu", ""], ["Jin", "Xin", ""], ["Wu", "Feng", ""]]}, {"id": "1804.10531", "submitter": "Yan Ke", "authors": "Zhuo Zhang, Jia Liu, Yan Ke, Yu Lei, Jun Li, Minqing Zhang and\n  Xiaoyuan Yang", "title": "Generative Steganography by Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel data-driven information hiding scheme called\ngenerative steganography by sampling (GSS) is proposed. Unlike in traditional\nmodification-based steganography, in our method the stego image is directly\nsampled by a powerful generator: no explicit cover is used. Both parties share\na secret key used for message embedding and extraction. The Jensen-Shannon\ndivergence is introduced as a new criterion for evaluating the security of\ngenerative steganography. Based on these principles, we propose a simple\npractical generative steganography method that uses semantic image inpainting.\nThe message is written in advance to an uncorrupted region that needs to be\nretained in the corrupted image. Then, the corrupted image with the secret\nmessage is fed into a Generator trained by a generative adversarial network\n(GAN) for semantic completion. Message loss and prior loss terms are proposed\nfor penalizing message extraction error and unrealistic stego image. In our\ndesign, we first train a generator whose training target is the generation of\nnew data samples from the same distribution as that of existing training data.\nNext, for the trained generator, backpropagation to the message and prior loss\nare introduced to optimize the coding of the input noise data for the\ngenerator. The presented experiments demonstrate the potential of the proposed\nframework based on both qualitative and quantitative evaluations of the\ngenerated stego images.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 15:34:46 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 14:38:34 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhang", "Zhuo", ""], ["Liu", "Jia", ""], ["Ke", "Yan", ""], ["Lei", "Yu", ""], ["Li", "Jun", ""], ["Zhang", "Minqing", ""], ["Yang", "Xiaoyuan", ""]]}, {"id": "1804.10783", "submitter": "Yiting Shao", "authors": "Yiting Shao, Qi Zhang, Ge Li, Zhu Li", "title": "Hybrid Point Cloud Attribute Compression Using Slice-based Layered\n  Structure and Block-based Intra Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud compression is a key enabler for the emerging applications of\nimmersive visual communication, autonomous driving and smart cities, etc. In\nthis paper, we propose a hybrid point cloud attribute compression scheme built\non an original layered data structure. First, a slice-partition scheme and\ngeometry-adaptive k dimensional-tree (kd-tree) method are devised to generate\nthe four-layer structure. Second, we introduce an efficient block-based intra\nprediction scheme containing a DC prediction mode and several angular modes, in\norder to exploit the spatial correlation between adjacent points. Third, an\nadaptive transform scheme based on Graph Fourier Transform (GFT) is Lagrangian\noptimized to achieve better transform efficiency. The Lagrange multiplier is\noff-line derived based on the statistics of color attribute coding. Last but\nnot least, multiple reordering scan modes are dedicated to improve coding\nefficiency for entropy coding. In intra-frame compression of point cloud color\nattributes, results demonstrate that our method performs better than the\nstate-of-the-art region-adaptive hierarchical transform (RAHT) system, and on\naverage a 29.37$\\%$ BD-rate gain is achieved. Comparing with the test model for\ncategory 1 (TMC1) anchor's coding results, which were recently published by\nMPEG-3DG group on 121st meeting, a 16.37$\\%$ BD-rate gain is obtained.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 10:27:39 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Shao", "Yiting", ""], ["Zhang", "Qi", ""], ["Li", "Ge", ""], ["Li", "Zhu", ""]]}, {"id": "1804.10822", "submitter": "Raphael Abreu", "authors": "Raphael Abreu, Joel dos Santos and Eduardo Bezerra", "title": "A Bimodal Learning Approach to Assist Multi-sensory Effects\n  Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In mulsemedia applications, traditional media content (text, image, audio,\nvideo, etc.) can be related to media objects that target other human senses\n(e.g., smell, haptics, taste). Such applications aim at bridging the virtual\nand real worlds through sensors and actuators. Actuators are responsible for\nthe execution of sensory effects (e.g., wind, heat, light), which produce\nsensory stimulations on the users. In these applications sensory stimulation\nmust happen in a timely manner regarding the other traditional media content\nbeing presented. For example, at the moment in which an explosion is presented\nin the audiovisual content, it may be adequate to activate actuators that\nproduce heat and light. It is common to use some declarative multimedia\nauthoring language to relate the timestamp in which each media object is to be\npresented to the execution of some sensory effect. One problem in this setting\nis that the synchronization of media objects and sensory effects is done\nmanually by the author(s) of the application, a process which is time-consuming\nand error prone. In this paper, we present a bimodal neural network\narchitecture to assist the synchronization task in mulsemedia applications. Our\napproach is based on the idea that audio and video signals can be used\nsimultaneously to identify the timestamps in which some sensory effect should\nbe executed. Our learning architecture combines audio and video signals for the\nprediction of scene components. For evaluation purposes, we construct a dataset\nbased on Google's AudioSet. We provide experiments to validate our bimodal\narchitecture. Our results show that the bimodal approach produces better\nresults when compared to several variants of unimodal architectures.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 15:37:41 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Abreu", "Raphael", ""], ["Santos", "Joel dos", ""], ["Bezerra", "Eduardo", ""]]}, {"id": "1804.10878", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini and Christian Timmerer", "title": "Dynamic Adaptive Point Cloud Streaming", "comments": "6 pages, 23rd ACM Packet Video (PV'18) Workshop, June 12--15, 2018,\n  Amsterdam, Netherlands", "journal-ref": null, "doi": "10.1145/3210424.3210429", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality point clouds have recently gained interest as an emerging form\nof representing immersive 3D graphics. Unfortunately, these 3D media are bulky\nand severely bandwidth intensive, which makes it difficult for streaming to\nresource-limited and mobile devices. This has called researchers to propose\nefficient and adaptive approaches for streaming of high-quality point clouds.\n  In this paper, we run a pilot study towards dynamic adaptive point cloud\nstreaming, and extend the concept of dynamic adaptive streaming over HTTP\n(DASH) towards DASH-PC, a dynamic adaptive bandwidth-efficient and view-aware\npoint cloud streaming system. DASH-PC can tackle the huge bandwidth demands of\ndense point cloud streaming while at the same time can semantically link to\nhuman visual acuity to maintain high visual quality when needed. In order to\ndescribe the various quality representations, we propose multiple thinning\napproaches to spatially sub-sample point clouds in the 3D space, and design a\nDASH Media Presentation Description manifest specific for point cloud\nstreaming. Our initial evaluations show that we can achieve significant\nbandwidth and performance improvement on dense point cloud streaming with minor\nnegative quality impacts compared to the baseline scenario when no adaptations\nis applied.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 05:54:55 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 22:49:22 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Timmerer", "Christian", ""]]}, {"id": "1804.11192", "submitter": "Yongfeng Zhang", "authors": "Yongfeng Zhang and Xu Chen", "title": "Explainable Recommendation: A Survey and New Perspectives", "comments": "101 pages, published in Foundations and Trends in Information\n  Retrieval, 14(1), pp.1-101 (2020)", "journal-ref": "Foundations and Trends in Information Retrieval, 14(1), pp.1-101\n  (2020)", "doi": "10.1561/1500000066", "report-no": null, "categories": "cs.IR cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable recommendation attempts to develop models that generate not only\nhigh-quality recommendations but also intuitive explanations. The explanations\nmay either be post-hoc or directly come from an explainable model (also called\ninterpretable or transparent model in some contexts). Explainable\nrecommendation tries to address the problem of why: by providing explanations\nto users or system designers, it helps humans to understand why certain items\nare recommended by the algorithm, where the human can either be users or system\ndesigners. Explainable recommendation helps to improve the transparency,\npersuasiveness, effectiveness, trustworthiness, and satisfaction of\nrecommendation systems. It also facilitates system designers for better system\ndebugging. In recent years, a large number of explainable recommendation\napproaches -- especially model-based methods -- have been proposed and applied\nin real-world systems.\n  In this survey, we provide a comprehensive review for the explainable\nrecommendation research. We first highlight the position of explainable\nrecommendation in recommender system research by categorizing recommendation\nproblems into the 5W, i.e., what, when, who, where, and why. We then conduct a\ncomprehensive survey of explainable recommendation on three perspectives: 1) We\nprovide a chronological research timeline of explainable recommendation. 2) We\nprovide a two-dimensional taxonomy to classify existing explainable\nrecommendation research. 3) We summarize how explainable recommendation applies\nto different recommendation tasks. We also devote a chapter to discuss the\nexplanation perspectives in broader IR and AI/ML research. We end the survey by\ndiscussing potential future directions to promote the explainable\nrecommendation research area and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 13:49:44 GMT"}, {"version": "v10", "created": "Sun, 13 Sep 2020 03:17:09 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 22:17:18 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 05:45:50 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 02:22:46 GMT"}, {"version": "v5", "created": "Wed, 24 Jul 2019 17:10:28 GMT"}, {"version": "v6", "created": "Mon, 12 Aug 2019 22:27:04 GMT"}, {"version": "v7", "created": "Thu, 15 Aug 2019 15:28:16 GMT"}, {"version": "v8", "created": "Wed, 1 Jan 2020 17:04:33 GMT"}, {"version": "v9", "created": "Fri, 20 Mar 2020 17:06:15 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhang", "Yongfeng", ""], ["Chen", "Xu", ""]]}, {"id": "1804.11240", "submitter": "Seyyed Hossein Soleymani", "authors": "Seyyed Hossein Soleymani, Amir Hossein Taherinia, Amir Hossein\n  Mohajerzadeh", "title": "A blind robust watermarking method based on Arnold Cat map and amplified\n  pseudo-noise strings with weak correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust and blind watermarking method is proposed, which is\nhighly resistant to the common image watermarking attacks, such as noises,\ncompression, and image quality enhancement processing. In this method, Arnold\nCat map is used as a pre-processing on the host image, which increases the\nsecurity and imperceptibility of embedding watermark bits with a strong gain\nfactor. Moreover, two pseudo-noise strings with weak correlation are used as\nthe symbol of each 0 or 1 bit of the watermark, which increases the accuracy in\ndetecting the state of watermark bits at extraction phase in comparison to\nusing two random pseudo-noise strings. In this method, to increase the\nrobustness and further imperceptibility of the embedding, the Arnold Cat mapped\nimage is subjected to non-overlapping blocking, and then the high frequency\ncoefficients of the approximation sub-band of the FDCuT transform are used as\nthe embedding location for each block. Comparison of the proposed method with\nrecent robust methods under the same experimental conditions indicates the\nsuperiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 13:13:04 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Soleymani", "Seyyed Hossein", ""], ["Taherinia", "Amir Hossein", ""], ["Mohajerzadeh", "Amir Hossein", ""]]}]