[{"id": "1505.00519", "submitter": "Cameron Summers", "authors": "Cameron Summers and Phillip Popp", "title": "Large Scale Discovery of Seasonal Music From User Data", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consumption history of online media content such as music and video\noffers a rich source of data from which to mine information. Trends in this\ndata are of particular interest because they reflect user preferences as well\nas associated cultural contexts that can be exploited in systems such as\nrecommendation or search. This paper classifies songs as seasonal using a\nlarge, real-world dataset of user listening data. Results show strong\nperformance of classification of Christmas music with Gaussian Mixture Models.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 03:38:04 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Summers", "Cameron", ""], ["Popp", "Phillip", ""]]}, {"id": "1505.00855", "submitter": "Babak Saleh", "authors": "Babak Saleh and Ahmed Elgammal", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right\n  Metric on The Right Feature", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the number of fine-art collections that are digitized\nand publicly available has been growing rapidly. With the availability of such\nlarge collections of digitized artworks comes the need to develop multimedia\nsystems to archive and retrieve this pool of data. Measuring the visual\nsimilarity between artistic items is an essential step for such multimedia\nsystems, which can benefit more high-level multimedia tasks. In order to model\nthis similarity between paintings, we should extract the appropriate visual\nfeatures for paintings and find out the best approach to learn the similarity\nmetric based on these features. We investigate a comprehensive list of visual\nfeatures and metric learning approaches to learn an optimized similarity\nmeasure between paintings. We develop a machine that is able to make\naesthetic-related semantic-level judgments, such as predicting a painting's\nstyle, genre, and artist, as well as providing similarity measures optimized\nbased on the knowledge available in the domain of art historical\ninterpretation. Our experiments show the value of using this similarity measure\nfor the aforementioned prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 01:25:26 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1505.01214", "submitter": "Babak Saleh", "authors": "Babak Saleh and Mira Dontcheva and Aaron Hertzmann and Zhicheng Liu", "title": "Learning Style Similarity for Searching Infographics", "comments": "6 pages, to appear in the 41st annual conference on Graphics\n  Interface (GI) 2015,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographics are complex graphic designs integrating text, images, charts and\nsketches. Despite the increasing popularity of infographics and the rapid\ngrowth of online design portfolios, little research investigates how we can\ntake advantage of these design resources. In this paper we present a method for\nmeasuring the style similarity between infographics. Based on human perception\ndata collected from crowdsourced experiments, we use computer vision and\nmachine learning algorithms to learn a style similarity metric for infographic\ndesigns. We evaluate different visual features and learning algorithms and find\nthat a combination of color histograms and Histograms-of-Gradients (HoG)\nfeatures is most effective in characterizing the style of infographics. We\ndemonstrate our similarity metric on a preliminary image retrieval test.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 22:59:32 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Saleh", "Babak", ""], ["Dontcheva", "Mira", ""], ["Hertzmann", "Aaron", ""], ["Liu", "Zhicheng", ""]]}, {"id": "1505.01861", "submitter": "Tao Mei", "authors": "Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui", "title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing video content with natural language is a fundamental\nchallenge of multimedia. Recurrent Neural Networks (RNN), which models sequence\ndynamics, has attracted increasing attention on visual interpretation. However,\nmost existing approaches generate a word locally with given previous words and\nthe visual content, while the relationship between sentence semantics and\nvisual content is not holistically exploited. As a result, the generated\nsentences may be contextually correct but the semantics (e.g., subjects, verbs\nor objects) are not true.\n  This paper presents a novel unified framework, named Long Short-Term Memory\nwith visual-semantic Embedding (LSTM-E), which can simultaneously explore the\nlearning of LSTM and visual-semantic embedding. The former aims to locally\nmaximize the probability of generating the next word given previous words and\nvisual content, while the latter is to create a visual-semantic embedding space\nfor enforcing the relationship between the semantics of the entire sentence and\nvisual content. Our proposed LSTM-E consists of three components: a 2-D and/or\n3-D deep convolutional neural networks for learning powerful video\nrepresentation, a deep RNN for generating sentences, and a joint embedding\nmodel for exploring the relationships between visual content and sentence\nsemantics. The experiments on YouTube2Text dataset show that our proposed\nLSTM-E achieves to-date the best reported performance in generating natural\nsentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also\ndemonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)\ntriplets to several state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 20:13:33 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 10:05:50 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2015 07:17:06 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Pan", "Yingwei", ""], ["Mei", "Tao", ""], ["Yao", "Ting", ""], ["Li", "Houqiang", ""], ["Rui", "Yong", ""]]}, {"id": "1505.01933", "submitter": "Hui Wang Mr", "authors": "Hui Wang, Mun Choon Chan and Wei Tsang Ooi", "title": "Wireless Multicast for Zoomable Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zoomable video streaming refers to a new class of interactive video\napplications, where users can zoom into a video stream to view a selected\nregion of interest in higher resolutions and pan around to move the region of\ninterest. The zoom and pan effects are typically achieved by breaking the\nsource video into a grid of independently decodable tiles. Streaming the tiles\nto a set of heterogeneous users using broadcast is challenging, as users have\ndifferent link rates and different regions of interest at different resolution\nlevels. In this paper, we consider the following problem: given the subset of\ntiles that each user requested, the link rate of each user, and the available\ntime slots, at which resolution should each tile be sent, to maximize the\noverall video quality received by all users. We design an efficient algorithm\nto solve the problem above, and evaluate the solution on a testbed using 10\nmobile devices. Our method is able to achieve up to 12dB improvements over\nother heuristic methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 06:03:15 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Wang", "Hui", ""], ["Chan", "Mun Choon", ""], ["Ooi", "Wei Tsang", ""]]}, {"id": "1505.02435", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo and Stefano Ferretti and Moreno Marzolla", "title": "Cloud for Gaming", "comments": "Encyclopedia of Computer Graphics and Games. Newton Lee (Editor).\n  Springer International Publishing, 2015, ISBN 978-3-319-08234-9", "journal-ref": null, "doi": "10.1007/978-3-319-08234-9_39-1", "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud for Gaming refers to the use of cloud computing technologies to build\nlarge-scale gaming infrastructures, with the goal of improving scalability and\nresponsiveness, improve the user's experience and enable new business models.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:04:48 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 13:23:08 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1505.03358", "submitter": "Miriam Redi", "authors": "Rossano Schifanella, Miriam Redi, Luca Aiello", "title": "An Image is Worth More than a Thousand Favorites: Surfacing the Hidden\n  Beauty of Flickr Pictures", "comments": "ICWSM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of attention in social media tend to obey power laws. Attention\nconcentrates on a relatively small number of popular items and neglecting the\nvast majority of content produced by the crowd. Although popularity can be an\nindication of the perceived value of an item within its community, previous\nresearch has hinted to the fact that popularity is distinct from intrinsic\nquality. As a result, content with low visibility but high quality lurks in the\ntail of the popularity distribution. This phenomenon can be particularly\nevident in the case of photo-sharing communities, where valuable photographers\nwho are not highly engaged in online social interactions contribute with\nhigh-quality pictures that remain unseen. We propose to use a computer vision\nmethod to surface beautiful pictures from the immense pool of\nnear-zero-popularity items, and we test it on a large dataset of\ncreative-commons photos on Flickr. By gathering a large crowdsourced ground\ntruth of aesthetics scores for Flickr images, we show that our method retrieves\nphotos whose median perceived beauty score is equal to the most popular ones,\nand whose average is lower by only 1.5%.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 12:40:24 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 10:05:34 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Schifanella", "Rossano", ""], ["Redi", "Miriam", ""], ["Aiello", "Luca", ""]]}, {"id": "1505.05225", "submitter": "Lihua Guo", "authors": "Guo Lihua, Li Fudi", "title": "Image aesthetic evaluation using paralleled deep convolution neural\n  network", "comments": "7 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetic evaluation has attracted much attention in recent years.\nImage aesthetic evaluation methods heavily depend on the effective aesthetic\nfeature. Traditional meth-ods always extract hand-crafted features. However,\nthese hand-crafted features are always designed to adapt particu-lar datasets,\nand extraction of them needs special design. Rather than extracting\nhand-crafted features, an automati-cally learn of aesthetic features based on\ndeep convolutional neural network (DCNN) is first adopt in this paper. As we\nall know, when the training dataset is given, the DCNN architecture with high\ncomplexity may meet the over-fitting problem. On the other side, the DCNN\narchitecture with low complexity would not efficiently extract effective\nfeatures. For these reasons, we further propose a paralleled convolutional\nneural network (PDCNN) with multi-level structures to automatically adapt to\nthe training dataset. Experimental results show that our proposed PDCNN\narchitecture achieves better performance than other traditional methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 02:03:23 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Lihua", "Guo", ""], ["Fudi", "Li", ""]]}, {"id": "1505.05407", "submitter": "Chun-Shien Lu", "authors": "Wei-Jie Liang, Gang-Xuan Lin, and Chun-Shien Lu", "title": "Compressive Sensing of Large-Scale Images: An Assumption-Free Approach", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-efficient compressive sensing of big media data with fast reconstructed\nhigh-quality results is very challenging. In this paper, we propose a new\nlarge-scale image compressive sensing method, composed of operator-based\nstrategy in the context of fixed point continuation method and weighted LASSO\nwith tree structure sparsity pattern. The main characteristic of our method is\nfree from any assumptions and restrictions. The feasibility of our method is\nverified via simulations and comparisons with state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 14:46:51 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Liang", "Wei-Jie", ""], ["Lin", "Gang-Xuan", ""], ["Lu", "Chun-Shien", ""]]}, {"id": "1505.06250", "submitter": "George Toderici", "authors": "Balakrishnan Varadarajan and George Toderici and Sudheendra\n  Vijayanarasimhan and Apostol Natsev", "title": "Efficient Large Scale Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification has advanced tremendously over the recent years. A large\npart of the improvements in video classification had to do with the work done\nby the image classification community and the use of deep convolutional\nnetworks (CNNs) which produce competitive results with hand- crafted motion\nfeatures. These networks were adapted to use video frames in various ways and\nhave yielded state of the art classification results. We present two methods\nthat build on this work, and scale it up to work with millions of videos and\nhundreds of thousands of classes while maintaining a low computational cost. In\nthe context of large scale video processing, training CNNs on video frames is\nextremely time consuming, due to the large number of frames involved. We\npropose to avoid this problem by training CNNs on either YouTube thumbnails or\nFlickr images, and then using these networks' outputs as features for other\nhigher level classifiers. We discuss the challenges of achieving this and\npropose two models for frame-level and video-level classification. The first is\na highly efficient mixture of experts while the latter is based on long short\nterm memory neural networks. We present results on the Sports-1M video dataset\n(1 million videos, 487 classes) and on a new dataset which has 12 million\nvideos and 150,000 labels.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 23:45:32 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Varadarajan", "Balakrishnan", ""], ["Toderici", "George", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Natsev", "Apostol", ""]]}, {"id": "1505.07395", "submitter": "Marko Horvat", "authors": "Marko Horvat, Dujo Duvnjak, Davor Jug", "title": "GWAT: The Geneva Affective Picture Database WordNet Annotation Tool", "comments": "5 pages, 3 figures. In the Proceedings of 38th International\n  Convention on Information and Communication Technology, Electronics and\n  Microelectronics MIPRO 2015 (pp. 1403-1407)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Geneva Affective Picture Database WordNet Annotation Tool (GWAT) is a\nuser-friendly web application for manual annotation of pictures in Geneva\nAffective Picture Database (GAPED) with WordNet. The annotation tool has an\nintuitive interface which can be efficiently used with very little technical\ntraining. A single picture may be labeled with many synsets allowing experts to\ndescribe semantics with different levels of detail. Noun, verb, adjective and\nadverb synsets can be keyword-searched and attached to a specific GAPED picture\nwith their unique identification numbers. Changes are saved automatically in\nthe tool's relational database. The attached synsets can be reviewed, changed\nor deleted later. Additionally, GAPED pictures may be browsed in the tool's\nuser interface using simple commands where previously attached WordNet synsets\nare displayed alongside the pictures. Stored annotations can be exported from\nthe tool's database to different data formats and used in 3rd party\napplications if needed. Since GAPED does not define keywords of individual\npictures but only a general category of picture groups, GWAT represents a\nsignificant improvement towards development of comprehensive picture semantics.\nThe tool was developed with open technologies WordNet API, Apache, PHP5 and\nMySQL. It is freely available for scientific and non-commercial use.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 16:27:23 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:07:05 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Horvat", "Marko", ""], ["Duvnjak", "Dujo", ""], ["Jug", "Davor", ""]]}, {"id": "1505.07398", "submitter": "Marko Horvat", "authors": "Marko Horvat, Davor Kukolja, Dragutin Ivanec", "title": "Comparing affective responses to standardized pictures and videos: A\n  study report", "comments": "5 pages, 4 figures. In the Proceedings of 38th International\n  Convention on Information and Communication Technology, Electronics and\n  Microelectronics MIPRO 2015 (pp. 1394-1398)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia documents such as text, images, sounds or videos elicit emotional\nresponses of different polarity and intensity in exposed human subjects. These\nstimuli are stored in affective multimedia databases. The problem of emotion\nprocessing is an important issue in Human-Computer Interaction and different\ninterdisciplinary studies particularly those related to psychology and\nneuroscience. Accurate prediction of users' attention and emotion has many\npractical applications such as the development of affective computer\ninterfaces, multifaceted search engines, video-on-demand, Internet\ncommunication and video games. To this regard we present results of a study\nwith N=10 participants to investigate the capability of standardized affective\nmultimedia databases in stimulation of emotion. Each participant was exposed to\npicture and video stimuli with previously determined semantics and emotion.\nDuring exposure participants' physiological signals were recorded and estimated\nfor emotion in an off-line analysis. Participants reported their emotion states\nafter each exposure session. The a posteriori and a priori emotion values were\ncompared. The experiment showed, among other reported results, that carefully\ndesigned video sequences induce a stronger and more accurate emotional reaction\nthan pictures. Individual participants' differences greatly influence the\nintensity and polarity of experienced emotion.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 16:39:05 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:11:16 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Horvat", "Marko", ""], ["Kukolja", "Davor", ""], ["Ivanec", "Dragutin", ""]]}, {"id": "1505.07757", "submitter": "Steffen Wendzel", "authors": "Matthias Naumann, Steffen Wendzel, Wojciech Mazurczyk, J\\\"org Keller", "title": "Micro protocol engineering for unstructured carriers: On the embedding\n  of steganographic control protocols into audio transmissions", "comments": "20 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network steganography conceals the transfer of sensitive information within\nunobtrusive data in computer networks. So-called micro protocols are\ncommunication protocols placed within the payload of a network steganographic\ntransfer. They enrich this transfer with features such as reliability, dynamic\noverlay routing, or performance optimization --- just to mention a few. We\npresent different design approaches for the embedding of hidden channels with\nmicro protocols in digitized audio signals under consideration of different\nrequirements. On the basis of experimental results, our design approaches are\ncompared, and introduced into a protocol engineering approach for micro\nprotocols.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 16:54:13 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Naumann", "Matthias", ""], ["Wendzel", "Steffen", ""], ["Mazurczyk", "Wojciech", ""], ["Keller", "J\u00f6rg", ""]]}]