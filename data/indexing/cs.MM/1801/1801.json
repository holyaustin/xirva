[{"id": "1801.01260", "submitter": "Defa Zhu", "authors": "Si Liu, Yao Sun, Defa Zhu, Guanghui Ren, Yu Chen, Jiashi Feng and\n  Jizhong Han", "title": "Cross-domain Human Parsing via Adversarial Feature and Label Adaptation", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human parsing has been extensively studied recently due to its wide\napplications in many important scenarios. Mainstream fashion parsing models\nfocus on parsing the high-resolution and clean images. However, directly\napplying the parsers trained on benchmarks to a particular application scenario\nin the wild, e.g., a canteen, airport or workplace, often gives\nnon-satisfactory performance due to domain shift. In this paper, we explore a\nnew and challenging cross-domain human parsing problem: taking the benchmark\ndataset with extensive pixel-wise labeling as the source domain, how to obtain\na satisfactory parser on a new target domain without requiring any additional\nmanual labeling? To this end, we propose a novel and efficient cross-domain\nhuman parsing model to bridge the cross-domain differences in terms of visual\nappearance and environment conditions and fully exploit commonalities across\ndomains. Our proposed model explicitly learns a feature compensation network,\nwhich is specialized for mitigating the cross-domain differences. A\ndiscriminative feature adversarial network is introduced to supervise the\nfeature compensation to effectively reduce the discrepancy between feature\ndistributions of two domains. Besides, our model also introduces a structured\nlabel adversarial network to guide the parsing results of the target domain to\nfollow the high-order relationships of the structured labels shared across\ndomains. The proposed framework is end-to-end trainable, practical and scalable\nin real applications. Extensive experiments are conducted where LIP dataset is\nthe source domain and 4 different datasets including surveillance videos,\nmovies and runway shows are evaluated as target domains. The results\nconsistently confirm data efficiency and performance advantages of the proposed\nmethod for the cross-domain human parsing problem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:55:59 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 03:25:07 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Liu", "Si", ""], ["Sun", "Yao", ""], ["Zhu", "Defa", ""], ["Ren", "Guanghui", ""], ["Chen", "Yu", ""], ["Feng", "Jiashi", ""], ["Han", "Jizhong", ""]]}, {"id": "1801.01316", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Mu Jung Cho, Anupriya Gagneja, Xiao Yang, Miriam\n  Brinberg, Katie Roehrick, Sagnik Ray Choudhury, Nilam Ram, Byron Reeves and\n  C. Lee Giles", "title": "Text Extraction and Retrieval from Smartphone Screenshots: Building a\n  Repository for Life in Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daily engagement in life experiences is increasingly interwoven with mobile\ndevice use. Screen capture at the scale of seconds is being used in behavioral\nstudies and to implement \"just-in-time\" health interventions. The increasing\npsychological breadth of digital information will continue to make the actual\nscreens that people view a preferred if not required source of data about life\nexperiences. Effective and efficient Information Extraction and Retrieval from\ndigital screenshots is a crucial prerequisite to successful use of screen data.\nIn this paper, we present the experimental workflow we exploited to: (i)\npre-process a unique collection of screen captures, (ii) extract unstructured\ntext embedded in the images, (iii) organize image text and metadata based on a\nstructured schema, (iv) index the resulting document collection, and (v) allow\nfor Image Retrieval through a dedicated vertical search engine application. The\nadopted procedure integrates different open source libraries for traditional\nimage processing, Optical Character Recognition (OCR), and Image Retrieval. Our\naim is to assess whether and how state-of-the-art methodologies can be applied\nto this novel data set. We show how combining OpenCV-based pre-processing\nmodules with a Long short-term memory (LSTM) based release of Tesseract OCR,\nwithout ad hoc training, led to a 74% character-level accuracy of the extracted\ntext. Further, we used the processed repository as baseline for a dedicated\nImage Retrieval system, for the immediate use and application for behavioral\nand prevention scientists. We discuss issues of Text Information Extraction and\nRetrieval that are particular to the screenshot image case and suggest\nimportant future work.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 11:51:26 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chiatti", "Agnese", ""], ["Cho", "Mu Jung", ""], ["Gagneja", "Anupriya", ""], ["Yang", "Xiao", ""], ["Brinberg", "Miriam", ""], ["Roehrick", "Katie", ""], ["Choudhury", "Sagnik Ray", ""], ["Ram", "Nilam", ""], ["Reeves", "Byron", ""], ["Giles", "C. Lee", ""]]}, {"id": "1801.01589", "submitter": "Prateek Verma", "authors": "Prateek Verma, Julius O. Smith", "title": "Neural Style Transfer for Audio Spectograms", "comments": "Appeared in 31st Conference on Neural Information Processing Systems\n  (NIPS 2017), Long Beach, CA, USA at the workshop for Machine Learning for\n  Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been fascinating work on creating artistic transformations of\nimages by Gatys. This was revolutionary in how we can in some sense alter the\n'style' of an image while generally preserving its 'content'. In our work, we\npresent a method for creating new sounds using a similar approach, treating it\nas a style-transfer problem, starting from a random-noise input signal and\niteratively using back-propagation to optimize the sound to conform to\nfilter-outputs from a pre-trained neural architecture of interest.\n  For demonstration, we investigate two different tasks, resulting in bandwidth\nexpansion/compression, and timbral transfer from singing voice to musical\ninstruments. A feature of our method is that a single architecture can generate\nthese different audio-style-transfer types using the same set of parameters\nwhich otherwise require different complex hand-tuned diverse signal processing\npipelines.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 23:59:07 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Verma", "Prateek", ""], ["Smith", "Julius O.", ""]]}, {"id": "1801.01980", "submitter": "Vaneet Aggarwal", "authors": "Anis Elgabli, Ke Liu, and Vaneet Aggarwal", "title": "Optimized Preference-Aware Multi-path Video Streaming with Scalable\n  Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most client hosts are equipped with multiple network interfaces (e.g., WiFi\nand cellular networks). Simultaneous access of multiple interfaces can\nsignificantly improve the users' quality of experience (QoE) in video\nstreaming. An intuitive approach to achieve it is to use Multi-path TCP\n(MPTCP). However, the deployment of MPTCP, especially with link preference,\nrequires OS kernel update at both the client and server side, and a vast amount\nof commercial content providers do not support MPTCP. Thus, in this paper, we\nrealize a multi-path video streaming algorithm in the application layer\ninstead, by considering Scalable Video Coding (SVC), where each layer of every\nchunk can be fetched from only one of the orthogonal paths. We formulate the\nquality decisions of video chunks subject to the available bandwidth of the\ndifferent paths, chunk deadlines, and link preferences as an optimization\nproblem. The objective is to to optimize a QoE metric that maintains a tradeoff\nbetween maximizing the playback rate of every chunk and ensuring fairness among\nchunks. The QoE is a weighted some of the following metrics: skip/stall\nduration, average playback rate, and quality switching rate. However, the\nweights are chosen such that pushing more chunks to the same quality level is\nmore preferable over any other choice. Even though the formulation is a\nnon-convex discrete optimization, we show that the problem can be solved\noptimally with a polynomial complexity in some special cases. We further\npropose an online algorithm where several challenges including bandwidth\nprediction errors, are addressed. Extensive emulated experiments in a real\ntestbed with real traces of public dataset reveal the robustness of our scheme\nand demonstrate its significant performance improvement compared to other\nmulti-path algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 07:18:19 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 18:08:06 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Elgabli", "Anis", ""], ["Liu", "Ke", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1801.02768", "submitter": "Yuanfang Guo", "authors": "Yuanfang Guo, Xiaochun Cao, Wei Zhang, Rui Wang", "title": "Fake Colorized Image Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2018.2806926", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image forensics aims to detect the manipulation of digital images. Currently,\nsplicing detection, copy-move detection and image retouching detection are\ndrawing much attentions from researchers. However, image editing techniques\ndevelop with time goes by. One emerging image editing technique is\ncolorization, which can colorize grayscale images with realistic colors.\nUnfortunately, this technique may also be intentionally applied to certain\nimages to confound object recognition algorithms. To the best of our knowledge,\nno forensic technique has yet been invented to identify whether an image is\ncolorized. We observed that, compared to natural images, colorized images,\nwhich are generated by three state-of-the-art methods, possess statistical\ndifferences for the hue and saturation channels. Besides, we also observe\nstatistical inconsistencies in the dark and bright channels, because the\ncolorization process will inevitably affect the dark and bright channel values.\nBased on our observations, i.e., potential traces in the hue, saturation, dark\nand bright channels, we propose two simple yet effective detection methods for\nfake colorized images: Histogram based Fake Colorized Image Detection\n(FCID-HIST) and Feature Encoding based Fake Colorized Image Detection\n(FCID-FE). Experimental results demonstrate that both proposed methods exhibit\na decent performance against multiple state-of-the-art colorization approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 02:58:45 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 13:33:38 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Guo", "Yuanfang", ""], ["Cao", "Xiaochun", ""], ["Zhang", "Wei", ""], ["Wang", "Rui", ""]]}, {"id": "1801.03773", "submitter": "Tak-Shing Chan", "authors": "Tak-Shing T. Chan and Yi-Hsuan Yang", "title": "Polar $n$-Complex and $n$-Bicomplex Singular Value Decomposition and\n  Principal Component Pursuit", "comments": "12 pages, 2 figures", "journal-ref": "IEEE Trans. Signal Process., vol. 64, no. 24, pp. 6533-6544, Dec.\n  2016", "doi": "10.1109/TSP.2016.2612171", "report-no": null, "categories": "eess.SP cs.MM cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informed by recent work on tensor singular value decomposition and circulant\nalgebra matrices, this paper presents a new theoretical bridge that unifies the\nhypercomplex and tensor-based approaches to singular value decomposition and\nrobust principal component analysis. We begin our work by extending the\nprincipal component pursuit to Olariu's polar $n$-complex numbers as well as\ntheir bicomplex counterparts. In so doing, we have derived the polar\n$n$-complex and $n$-bicomplex proximity operators for both the $\\ell_1$- and\ntrace-norm regularizers, which can be used by proximal optimization methods\nsuch as the alternating direction method of multipliers. Experimental results\non two sets of audio data show that our algebraically-informed formulation\noutperforms tensor robust principal component analysis. We conclude with the\nmessage that an informed definition of the trace norm can bridge the gap\nbetween the hypercomplex and tensor-based approaches. Our approach can be seen\nas a general methodology for generating other principal component pursuit\nalgorithms with proper algebraic structures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 00:45:01 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Chan", "Tak-Shing T.", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1801.03816", "submitter": "Tak-Shing Chan", "authors": "Tak-Shing T. Chan and Yi-Hsuan Yang", "title": "Complex and Quaternionic Principal Component Pursuit and Its Application\n  to Audio Separation", "comments": "5 pages, 1 figure", "journal-ref": "IEEE Signal Process. Lett., vol. 23, no. 2, pp. 287-291, Feb. 2016", "doi": "10.1109/LSP.2016.2514845", "report-no": null, "categories": "eess.SP cs.MM cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the principal component pursuit has received increasing attention\nin signal processing research ranging from source separation to video\nsurveillance. So far, all existing formulations are real-valued and lack the\nconcept of phase, which is inherent in inputs such as complex spectrograms or\ncolor images. Thus, in this letter, we extend principal component pursuit to\nthe complex and quaternionic cases to account for the missing phase\ninformation. Specifically, we present both complex and quaternionic proximity\noperators for the $\\ell_1$- and trace-norm regularizers. These operators can be\nused in conjunction with proximal minimization methods such as the inexact\naugmented Lagrange multiplier algorithm. The new algorithms are then applied to\nthe singing voice separation problem, which aims to separate the singing voice\nfrom the instrumental accompaniment. Results on the iKala and MSD100 datasets\nconfirmed the usefulness of phase information in principal component pursuit.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 00:39:50 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Chan", "Tak-Shing T.", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1801.04076", "submitter": "Marc Chaumont", "authors": "Mehdi Yedroudj, Marc Chaumont, Fr\\'ed\\'eric Comby", "title": "How to augment a small learning set for improving the performances of a\n  CNN-based steganalyzer?", "comments": "EI'2018, in Proceedings of Media Watermarking, Security, and\n  Forensics, Part of IS&T International Symposium on Electronic Imaging, San\n  Francisco, California, USA, 28 Jan. -2 Feb. 2018, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning and convolutional neural networks (CNN) have been intensively\nused in many image processing topics during last years. As far as steganalysis\nis concerned, the use of CNN allows reaching the state-of-the-art results. The\nperformances of such networks often rely on the size of their learning\ndatabase. An obvious preliminary assumption could be considering that \"the\nbigger a database is, the better the results are\". However, it appears that\ncautions have to be taken when increasing the database size if one desire to\nimprove the classification accuracy i.e. enhance the steganalysis efficiency.\nTo our knowledge, no study has been performed on the enrichment impact of a\nlearning database on the steganalysis performance. What kind of images can be\nadded to the initial learning set? What are the sensitive criteria: the camera\nmodels used for acquiring the images, the treatments applied to the images, the\ncameras proportions in the database, etc? This article continues the work\ncarried out in a previous paper, and explores the ways to improve the\nperformances of CNN. It aims at studying the effects of \"base augmentation\" on\nthe performance of steganalysis using a CNN. We present the results of this\nstudy using various experimental protocols and various databases to define the\ngood practices in base augmentation for steganalysis.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 07:30:22 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 14:48:30 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Yedroudj", "Mehdi", ""], ["Chaumont", "Marc", ""], ["Comby", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1801.04081", "submitter": "Jeongsoo Park", "authors": "Jeongsoo Park, Jaeyoung Shin and Kyogu Lee", "title": "Separation of Instrument Sounds using Non-negative Matrix Factorization\n  with Spectral Envelope Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral envelope is one of the most important features that characterize the\ntimbre of an instrument sound. However, it is difficult to use spectral\ninformation in the framework of conventional spectrogram decomposition methods.\nWe overcome this problem by suggesting a simple way to provide a constraint on\nthe spectral envelope calculated by linear prediction. In the first part of\nthis study, we use a pre-trained spectral envelope of known instruments as the\nconstraint. Then we apply the same idea to a blind scenario in which the\ninstruments are unknown. The experimental results reveal that the proposed\nmethod outperforms the conventional methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 08:10:53 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Park", "Jeongsoo", ""], ["Shin", "Jaeyoung", ""], ["Lee", "Kyogu", ""]]}, {"id": "1801.04747", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu, Wei Wang, Jing Dong and Hongxia Wang", "title": "Ensemble Reversible Data Hiding", "comments": "Fig. 1 was updated due to a minor error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional reversible data hiding (RDH) algorithms often consider the\nhost as a whole to embed a secret payload. In order to achieve satisfactory\nrate-distortion performance, the secret bits are embedded into the noise-like\ncomponent of the host such as prediction errors. From the rate-distortion\noptimization view, it may be not optimal since the data embedding units use the\nidentical parameters. This motivates us to present a segmented data embedding\nstrategy for efficient RDH in this paper, in which the raw host could be\npartitioned into multiple subhosts such that each one can freely optimize and\nuse the data embedding parameters. Moreover, it enables us to apply different\nRDH algorithms within different subhosts, which is defined as ensemble. Notice\nthat, the ensemble defined here is different from that in machine learning.\nAccordingly, the conventional operation corresponds to a special case of the\nproposed work. Since it is a general strategy, we combine some state-of-the-art\nalgorithms to construct a new system using the proposed embedding strategy to\nevaluate the rate-distortion performance. Experimental results have shown that,\nthe ensemble RDH system could outperform the original versions in most cases,\nwhich has shown the superiority and applicability.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 11:37:09 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 07:58:28 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 06:12:09 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 11:58:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wu", "Hanzhou", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Wang", "Hongxia", ""]]}, {"id": "1801.04752", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu, Wei Wang, Jing Dong, Yanli Chen and Hongxia Wang", "title": "Reversible Embedding to Covers Full of Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reversible data embedding, to avoid overflow and underflow problem, before\ndata embedding, boundary pixels are recorded as side information, which may be\nlosslessly compressed. The existing algorithms often assume that a natural\nimage has little boundary pixels so that the size of side information is small.\nAccordingly, a relatively high pure payload could be achieved. However, there\nactually may exist a lot of boundary pixels in a natural image, implying that,\nthe size of side information could be very large. Therefore, when to directly\nuse the existing algorithms, the pure embedding capacity may be not sufficient.\nIn order to address this problem, in this paper, we present a new and efficient\nframework to reversible data embedding in images that have lots of boundary\npixels. The core idea is to losslessly preprocess boundary pixels so that it\ncan significantly reduce the side information. Experimental results have shown\nthe superiority and applicability of our work.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 11:48:06 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Wu", "Hanzhou", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Chen", "Yanli", ""], ["Wang", "Hongxia", ""]]}, {"id": "1801.05210", "submitter": "Xiaoda Jiang", "authors": "Xiaoda Jiang, Hancheng Lu, Chang Wen Chen", "title": "Enabling Quality-Driven Scalable Video Transmission over Multi-User NOMA\n  System", "comments": "9 pages, 6 figures. This paper has already been accepted by IEEE\n  INFOCOM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, non-orthogonal multiple access (NOMA) has been proposed to achieve\nhigher spectral efficiency over conventional orthogonal multiple access.\nAlthough it has the potential to meet increasing demands of video services, it\nis still challenging to provide high performance video streaming. In this\nresearch, we investigate, for the first time, a multi-user NOMA system design\nfor video transmission. Various NOMA systems have been proposed for data\ntransmission in terms of throughput or reliability. However, the perceived\nquality, or the quality-of-experience of users, is more critical for video\ntransmission. Based on this observation, we design a quality-driven scalable\nvideo transmission framework with cross-layer support for multi-user NOMA. To\nenable low complexity multi-user NOMA operations, a novel user grouping\nstrategy is proposed. The key features in the proposed framework include the\nintegration of the quality model for encoded video with the physical layer\nmodel for NOMA transmission, and the formulation of multi-user NOMA-based video\ntransmission as a quality-driven power allocation problem. As the problem is\nnon-concave, a global optimal algorithm based on the hidden monotonic property\nand a suboptimal algorithm with polynomial time complexity are developed.\nSimulation results show that the proposed multi-user NOMA system outperforms\nexisting schemes in various video delivery scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 11:12:28 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Jiang", "Xiaoda", ""], ["Lu", "Hancheng", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1801.05264", "submitter": "Shadrokh Samavi", "authors": "Hamidreza Zarrabi, Ali Emami, Nader Karimi, Shadrokh Samavi", "title": "Adaptive Reversible Watermarking Based on Linear Prediction for Medical\n  Videos", "comments": "Algorithms are now presented in a standard format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible video watermarking can guarantee that the watermark logo and the\noriginal frame can be recovered from the watermarked frame without any\ndistortion. Although reversible video watermarking has successfully been\napplied in multimedia, its application has not been extensively explored in\nmedical videos. Reversible watermarking in medical videos is still a\nchallenging problem. The existing reversible video watermarking algorithms,\nwhich are based on error prediction expansion, use motion vectors for\nprediction. In this study, we propose an adaptive reversible watermarking\nmethod for medical videos. We suggest using temporal correlations for improving\nthe prediction accuracy. Hence, two temporal neighbor pixels in upcoming frames\nare used alongside the four spatial rhombus neighboring pixels to minimize the\nprediction error. To the best of our knowledge, this is the first time this\nmethod is applied to medical videos. The method helps to protect patients'\npersonal and medical information by watermarking, i.e., increase the security\nof Health Information Systems (HIS). Experimental results demonstrate the high\nquality of the proposed watermarking method based on PSNR metric and a large\ncapacity for data hiding in medical videos.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 05:17:11 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 17:37:10 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Zarrabi", "Hamidreza", ""], ["Emami", "Ali", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1801.05853", "submitter": "Bo Wu", "authors": "Bo Wu, Wen-Huang Cheng, Yongdong Zhang, Tao Mei", "title": "Time Matters: Multi-scale Temporalization of Social Media Popularity", "comments": "accepted in ACM Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of social media popularity exhibits rich temporality, i.e.,\npopularities change over time at various levels of temporal granularity. This\nis influenced by temporal variations of public attentions or user activities.\nFor example, popularity patterns of street snap on Flickr are observed to\ndepict distinctive fashion styles at specific time scales, such as season-based\nperiodic fluctuations for Trench Coat or one-off peak in days for Evening\nDress. However, this fact is often overlooked by existing research of\npopularity modeling. We present the first study to incorporate multiple\ntime-scale dynamics into predicting online popularity. We propose a novel\ncomputational framework in the paper, named Multi-scale Temporalization, for\nestimating popularity based on multi-scale decomposition and structural\nreconstruction in a tensor space of user, post, and time by joint low-rank\nconstraints. By considering the noise caused by context inconsistency, we\ndesign a data rearrangement step based on context aggregation as preprocessing\nto enhance contextual relevance of neighboring data in the tensor space. As a\nresult, our approach can leverage multiple levels of temporal characteristics\nand reduce the noise of data decomposition to improve modeling effectiveness.\nWe evaluate our approach on two large-scale Flickr image datasets with over 1.8\nmillion photos in total, for the task of popularity prediction. The results\nshow that our approach significantly outperforms state-of-the-art popularity\nprediction techniques, with a relative improvement of 10.9%-47.5% in terms of\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 05:51:47 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Wu", "Bo", ""], ["Cheng", "Wen-Huang", ""], ["Zhang", "Yongdong", ""], ["Mei", "Tao", ""]]}, {"id": "1801.05889", "submitter": "Edip Demirbilek", "authors": "Edip Demirbilek and Jean-Charles Gr\\'egoire", "title": "Perceived Audiovisual Quality Modelling based on Decison Trees, Genetic\n  Programming and Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to build machine learning based models that predict\naudiovisual quality directly from a set of correlated parameters that are\nextracted from a target quality dataset. We have used the bitstream version of\nthe INRS audiovisual quality dataset that reflects contemporary real-time\nconfigurations for video frame rate, video quantization, noise reduction\nparameters and network packet loss rate. We have utilized this dataset to build\nbitstream perceived quality estimation models based on the Random Forests,\nBagging, Deep Learning and Genetic Programming methods.\n  We have taken an empirical approach and have generated models varying from\nvery simple to the most complex depending on the number of features used from\nthe quality dataset. Random Forests and Bagging models have overall generated\nthe most accurate results in terms of RMSE and Pearson correlation coefficient\nvalues. Deep Learning and Genetic Programming based bitstream models have also\nachieved good results but that high performance was observed only with a\nlimited range of features. We have also obtained the epsilon-insensitive RMSE\nvalues for each model and have computed the significance of the difference\nbetween the correlation coefficients.\n  Overall we conclude that computing the bitstream information is worth the\neffort it takes to generate and helps to build more accurate models for\nreal-time communications. However, it is useful only for the deployment of the\nright algorithms with the carefully selected subset of the features. The\ndataset and tools that have been developed during this research are publicly\navailable for research and development purposes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 02:49:27 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Demirbilek", "Edip", ""], ["Gr\u00e9goire", "Jean-Charles", ""]]}, {"id": "1801.06030", "submitter": "Naima Merzougui", "authors": "Naima Merzougui and Naima Merzougui", "title": "Multi-measures fusion based on multi-objective genetic programming for\n  full-reference image quality assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit the flexibility of multi-objective fitness\nfunctions, and the efficiency of the model structure selection ability of a\nstandard genetic programming (GP) with the parameter estimation power of\nclassical regression via multi-gene genetic programming (MGGP), to propose a\nnew fusion technique for image quality assessment (IQA) that is called\nMulti-measures Fusion based on Multi-Objective Genetic Programming (MFMOGP).\nThis technique can automatically select the most significant suitable measures,\nfrom 16 full-reference IQA measures, used in aggregation and finds weights in a\nweighted sum of their outputs while simultaneously optimizing for both accuracy\nand complexity. The obtained well-performing fusion of IQA measures are\nevaluated on four largest publicly available image databases and compared\nagainst state-of-the-art full-reference IQA approaches. Results of comparison\nreveal that the proposed approach outperforms other state-of-the-art recently\ndeveloped fusion approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 09:08:45 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Merzougui", "Naima", ""], ["Merzougui", "Naima", ""]]}, {"id": "1801.06611", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao", "title": "Multiple Description Convolutional Neural Networks for Image Compression", "comments": "13 pages, 3 tables, and 6 figures. Accepted by IEEE Trans. CSVT. Our\n  another MDC paper based on deep learning is \"Deep Multiple Description Coding\n  by Learning Scalar Quantization\", which was accepted by DCC-2019.\n  (arXiv:1811.01504)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple description coding (MDC) is able to stably transmit the signal in\nthe un-reliable and non-prioritized networks, which has been broadly studied\nfor several decades. However, the traditional MDC doesn't well leverage image's\ncontext features to generate multiple descriptions. In this paper, we propose a\nnovel standard-compliant convolutional neural network-based MDC framework in\nterm of image's context features. Firstly, multiple description generator\nnetwork (MDGN) is designed to produce appearance-similar yet feature-different\nmultiple descriptions automatically according to image's content, which are\ncompressed by standard codec. Secondly, we present multiple description\nreconstruction network (MDRN) including side reconstruction network (SRN) and\ncentral reconstruction network (CRN). When any one of two lossy descriptions is\nreceived at the decoder, SRN network is used to improve the quality of this\ndecoded lossy description by removing the compression artifact and up-sampling\nsimultaneously. Meanwhile, we utilize CRN network with two decoded descriptions\nas inputs for better reconstruction, if both of lossy descriptions are\navailable. Thirdly, multiple description virtual codec network (MDVCN) is\nproposed to bridge the gap between MDGN network and MDRN network in order to\ntrain an end-to-end MDC framework. Here, two learning algorithms are provided\nto train our whole framework. In addition to structural similarity loss\nfunction, the produced descriptions are used as opposing labels with multiple\ndescription distance loss function to regularize the training of MDGN network.\nThese losses guarantee that the generated description images are structurally\nsimilar yet finely diverse. Experimental results show a great deal of objective\nand subjective quality measurements to validate the efficiency of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 01:28:20 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 02:47:01 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1801.07239", "submitter": "Luis Herranz", "authors": "Luis Herranz, Weiqing Min, Shuqiang Jiang", "title": "Food recognition and recipe analysis: integrating visual content,\n  context and external knowledge", "comments": "Survey about contextual food recognition and multimodal recipe\n  analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central role of food in our individual and social life, combined with\nrecent technological advances, has motivated a growing interest in applications\nthat help to better monitor dietary habits as well as the exploration and\nretrieval of food-related information. We review how visual content, context\nand external knowledge can be integrated effectively into food-oriented\napplications, with special focus on recipe analysis and retrieval, food\nrecommendation, and the restaurant context as emerging directions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:45:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Herranz", "Luis", ""], ["Min", "Weiqing", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "1801.09741", "submitter": "Muhammad Kamran Dr.", "authors": "Muhammad Kamran, Muddassar Farooq", "title": "An Optimized Information-Preserving Relational Database Watermarking\n  Scheme for Ownership Protection of Medical Data", "comments": "A shortened version of this work was published in IEEE Transactions\n  on Knowledge and Data Engineering in 2012", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, vol. 24, no.\n  11, pp. 1950-1962, 2012", "doi": null, "report-no": "TR-57-Kamran", "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a significant amount of interest has been developed in motivating\nphysicians to use e-health technology (especially Electronic Medical Records\n(EMR) systems). An important utility of such EMR systems is: a next generation\nof Clinical Decision Support Systems (CDSS) will extract knowledge from these\nelectronic medical records to enable physicians to do accurate and effective\ndiagnosis. It is anticipated that in future such medical records will be shared\nthrough cloud among different physicians to improve the quality of health care.\nTherefore, right protection of medical records is important to protect their\nownership once they are shared with third parties. Watermarking is a proven\nwell known technique to achieve this objective. The challenges associated with\nwatermarking of EMR systems are: (1) some fields in EMR are more relevant in\nthe diagnosis process; as a result, small variations in them could change the\ndiagnosis, and (2) a misdiagnosis might not only result in a life threatening\nscenario but also might lead to significant costs of the treatment for the\npatients. The major contribution of this paper is an information-preserving\nwatermarking scheme to address the above-mentioned challenges. We model the\nwatermarking process as a constrained optimization problem. We demonstrate,\nthrough experiments, that our scheme not only preserves the diagnosis accuracy\nbut is also resilient to well known attacks for corrupting the watermark. Last\nbut not least, we also compare our scheme with a well known threshold-based\nscheme to evaluate relative merits of a classifier. Our pilot studies reveal\nthat -- using proposed information-preserving scheme -- the overall\nclassification accuracy is never degraded by more than 1%. In comparison, the\ndiagnosis accuracy, using the threshold-based technique, is degraded by more\nthan 18% in a worst case scenario.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 03:56:14 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Kamran", "Muhammad", ""], ["Farooq", "Muddassar", ""]]}, {"id": "1801.10253", "submitter": "Spencer Cappallo", "authors": "Spencer Cappallo, Stacey Svetlichnaya, Pierre Garrigues, Thomas\n  Mensink, Cees G. M. Snoek", "title": "The New Modality: Emoji Challenges in Prediction, Anticipation, and\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, emoji have emerged as a new and widespread form of\ndigital communication, spanning diverse social networks and spoken languages.\nWe propose to treat these ideograms as a new modality in their own right,\ndistinct in their semantic structure from both the text in which they are often\nembedded as well as the images which they resemble. As a new modality, emoji\npresent rich novel possibilities for representation and interaction. In this\npaper, we explore the challenges that arise naturally from considering the\nemoji modality through the lens of multimedia research. Specifically, the ways\nin which emoji can be related to other common modalities such as text and\nimages. To do so, we first present a large scale dataset of real-world emoji\nusage collected from Twitter. This dataset contains examples of both text-emoji\nand image-emoji relationships. We present baseline results on the challenge of\npredicting emoji from both text and images, using state-of-the-art neural\nnetworks. Further, we offer a first consideration into the problem of how to\naccount for new, unseen emoji - a relevant issue as the emoji vocabulary\ncontinues to expand on a yearly basis. Finally, we present results for\nmultimedia retrieval using emoji as queries.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 23:19:49 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 14:59:47 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Cappallo", "Spencer", ""], ["Svetlichnaya", "Stacey", ""], ["Garrigues", "Pierre", ""], ["Mensink", "Thomas", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1801.10288", "submitter": "Yongfeng Zhang", "authors": "Xu Chen and Yongfeng Zhang and Hongteng Xu and Yixin Cao and Zheng Qin\n  and Hongyuan Zha", "title": "Visually Explainable Recommendation", "comments": "11 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images account for a significant part of user decisions in many application\nscenarios, such as product images in e-commerce, or user image posts in social\nnetworks. It is intuitive that user preferences on the visual patterns of image\n(e.g., hue, texture, color, etc) can be highly personalized, and this provides\nus with highly discriminative features to make personalized recommendations.\n  Previous work that takes advantage of images for recommendation usually\ntransforms the images into latent representation vectors, which are adopted by\na recommendation component to assist personalized user/item profiling and\nrecommendation. However, such vectors are hardly useful in terms of providing\nvisual explanations to users about why a particular item is recommended, and\nthus weakens the explainability of recommendation systems.\n  As a step towards explainable recommendation models, we propose visually\nexplainable recommendation based on attentive neural networks to model the user\nattention on images, under the supervision of both implicit feedback and\ntextual reviews. By this, we can not only provide recommendation results to the\nusers, but also tell the users why an item is recommended by providing\nintuitive visual highlights in a personalized manner. Experimental results show\nthat our models are not only able to improve the recommendation performance,\nbut also can provide persuasive visual explanations for the users to take the\nrecommendations.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 03:16:39 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Chen", "Xu", ""], ["Zhang", "Yongfeng", ""], ["Xu", "Hongteng", ""], ["Cao", "Yixin", ""], ["Qin", "Zheng", ""], ["Zha", "Hongyuan", ""]]}]