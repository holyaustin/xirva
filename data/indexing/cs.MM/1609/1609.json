[{"id": "1609.00045", "submitter": "Cong Zhang", "authors": "Cong Zhang, Jiangchuan Liu and Haiyang Wang", "title": "Towards Hybrid Cloud-assisted Crowdsourced Live Streaming: Measurement\n  and Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/2910642.2910644", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced Live Streaming (CLS), most notably Twitch.tv, has seen explosive\ngrowth in its popularity in the past few years. In such systems, any user can\nlively broadcast video content of interest to others, e.g., from a game player\nto many online viewers. To fulfill the demands from both massive and\nheterogeneous broadcasters and viewers, expensive server clusters have been\ndeployed to provide video ingesting and transcoding services. Despite the\nexistence of highly popular channels, a significant portion of the channels is\nindeed unpopular. Yet as our measurement shows, these broadcasters are\nconsuming considerable system resources; in particular, 25% (resp. 30%) of\nbandwidth (resp. computation) resources are used by the broadcasters who do not\nhave any viewers at all. In this paper, we closely examine the challenge of\nhandling unpopular live-broadcasting channels in CLS systems and present a\ncomprehensive solution for service partitioning on hybrid cloud. The\ntrace-driven evaluation shows that our hybrid cloud-assisted design can smartly\nassign ingesting and transcoding tasks to the elastic cloud virtual machines,\nproviding flexible system deployment cost-effectively.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 21:14:57 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Zhang", "Cong", ""], ["Liu", "Jiangchuan", ""], ["Wang", "Haiyang", ""]]}, {"id": "1609.01388", "submitter": "Yale Song", "authors": "Yale Song, Miriam Redi, Jordi Vallmitjana, Alejandro Jaimes", "title": "To Click or Not To Click: Automatic Selection of Beautiful Thumbnails\n  from Videos", "comments": "To appear in CIKM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thumbnails play such an important role in online videos. As the most\nrepresentative snapshot, they capture the essence of a video and provide the\nfirst impression to the viewers; ultimately, a great thumbnail makes a video\nmore attractive to click and watch. We present an automatic thumbnail selection\nsystem that exploits two important characteristics commonly associated with\nmeaningful and attractive thumbnails: high relevance to video content and\nsuperior visual aesthetic quality. Our system selects attractive thumbnails by\nanalyzing various visual quality and aesthetic metrics of video frames, and\nperforms a clustering analysis to determine the relevance to video content,\nthus making the resulting thumbnails more representative of the video. On the\ntask of predicting thumbnails chosen by professional video editors, we\ndemonstrate the effectiveness of our system against six baseline methods, using\na real-world dataset of 1,118 videos collected from Yahoo Screen. In addition,\nwe study what makes a frame a good thumbnail by analyzing the statistical\nrelationship between thumbnail frames and non-thumbnail frames in terms of\nvarious image quality features. Our study suggests that the selection of a good\nthumbnail is highly correlated with objective visual quality metrics, such as\nthe frame texture and sharpness, implying the possibility of building an\nautomatic thumbnail selection system based on visual aesthetics.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 04:33:34 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Song", "Yale", ""], ["Redi", "Miriam", ""], ["Vallmitjana", "Jordi", ""], ["Jaimes", "Alejandro", ""]]}, {"id": "1609.03058", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Zhou, Hongteng Xu, Junchi Yan, Mingliang Xu, Jianxin\n  Wu, Zicheng Liu", "title": "A Tube-and-Droplet-based Approach for Representing and Analyzing Motion\n  Trajectories", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, DOI:\n  10.1109/TPAMI.2016.2608884, 2016. Code for our work is available at\n  http://min.sjtu.edu.cn/lwydemo/Trajectory%20analysis.htm", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2608884", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory analysis is essential in many applications. In this paper, we\naddress the problem of representing motion trajectories in a highly informative\nway, and consequently utilize it for analyzing trajectories. Our approach first\nleverages the complete information from given trajectories to construct a\nthermal transfer field which provides a context-rich way to describe the global\nmotion pattern in a scene. Then, a 3D tube is derived which depicts an input\ntrajectory by integrating its surrounding motion patterns contained in the\nthermal transfer field. The 3D tube effectively: 1) maintains the movement\ninformation of a trajectory, 2) embeds the complete contextual motion pattern\naround a trajectory, 3) visualizes information about a trajectory in a clear\nand unified way. We further introduce a droplet-based process. It derives a\ndroplet vector from a 3D tube, so as to characterize the high-dimensional 3D\ntube information in a simple but effective way. Finally, we apply our\ntube-and-droplet representation to trajectory analysis applications including\ntrajectory clustering, trajectory classification & abnormality detection, and\n3D action recognition. Experimental comparisons with state-of-the-art\nalgorithms demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 14:33:06 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 12:17:26 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Lin", "Weiyao", ""], ["Zhou", "Yang", ""], ["Xu", "Hongteng", ""], ["Yan", "Junchi", ""], ["Xu", "Mingliang", ""], ["Wu", "Jianxin", ""], ["Liu", "Zicheng", ""]]}, {"id": "1609.03415", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan and S. Saqib Bukhari and Thomas M. Breuel", "title": "Active Canny: Edge Detection and Recovery with Open Active Contour\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an edge detection and recovery framework based on open active\ncontour models (snakelets). This is motivated by the noisy or broken edges\noutput by standard edge detection algorithms, like Canny. The idea is to\nutilize the local continuity and smoothness cues provided by strong edges and\ngrow them to recover the missing edges. This way, the strong edges are used to\nrecover weak or missing edges by considering the local edge structures, instead\nof blindly linking them if gradient magnitudes are above some threshold. We\ninitialize short snakelets on the gradient magnitudes or binary edges\nautomatically and then deform and grow them under the influence of gradient\nvector flow. The output snakelets are able to recover most of the breaks or\nweak edges, and they provide a smooth edge representation of the image; they\ncan also be used for higher level analysis, like contour segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 14:13:26 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Bastan", "Muhammet", ""], ["Bukhari", "S. Saqib", ""], ["Breuel", "Thomas M.", ""]]}, {"id": "1609.04196", "submitter": "Laura Toni", "authors": "Laura Toni, Pascal Frossard", "title": "Optimal Representations for Adaptive Streaming in Interactive Multi-View\n  Video Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive multi-view video streaming (IMVS) services permit to remotely\nimmerse within a 3D scene. This is possible by transmitting a set of reference\ncamera views (anchor views), which are used by the clients to freely navigate\nin the scene and possibly synthesize additional viewpoints of interest. From a\nnetworking perspective, the big challenge in IMVS systems is to deliver to each\nclient the best set of anchor views that maximizes the navigation quality,\nminimizes the view-switching delay and yet satisfies the network constraints.\nIntegrating adaptive streaming solutions in free-viewpoint systems offers a\npromising solution to deploy IMVS in large and heterogeneous scenarios, as long\nas the multi-view video representations on the server are properly selected. We\ntherefore propose to optimize the multi-view data at the server by minimizing\nthe overall resource requirements, yet offering a good navigation quality to\nthe different users. We propose a video representation set optimization for\nmultiview adaptive streaming systems and we show that it is NP-hard. We\ntherefore introduce the concept of multi-view navigation segment that permits\nto cast the video representation set selection as an integer linear programming\nproblem with a bounded computational complexity. We then show that the proposed\nsolution reduces the computational complexity while preserving optimality in\nmost of the 3D scenes. We then provide simulation results for different classes\nof users and show the gain offered by an optimal multi-view video\nrepresentation selection compared to recommended representation sets (e.g.,\nNetflix and Apple ones) or to a baseline representation selection algorithm\nwhere the encoding parameters are decided a priori for all the views.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 10:01:17 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Toni", "Laura", ""], ["Frossard", "Pascal", ""]]}, {"id": "1609.04243", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler, Kyunghyun Cho", "title": "Convolutional Recurrent Neural Networks for Music Classification", "comments": "5 pages, ICASSP 2017 submitted. Revised to fix previous CNN\n  architectures and update experiment results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a convolutional recurrent neural network (CRNN) for music\ntagging. CRNNs take advantage of convolutional neural networks (CNNs) for local\nfeature extraction and recurrent neural networks for temporal summarisation of\nthe extracted features. We compare CRNN with three CNN structures that have\nbeen used for music tagging while controlling the number of parameters with\nrespect to their performance and training time per sample. Overall, we found\nthat CRNNs show a strong performance with respect to the number of parameter\nand training time, indicating the effectiveness of its hybrid structure in\nmusic feature extraction and feature summarisation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:52:08 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 07:50:14 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 06:52:30 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1609.05345", "submitter": "Shicong Liu", "authors": "Shicong Liu, Junru Shao, Hongtao Lu", "title": "Generalized residual vector quantization for large scale data", "comments": "published on International Conference on Multimedia and Expo 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector quantization is an essential tool for tasks involving large scale\ndata, for example, large scale similarity search, which is crucial for\ncontent-based information retrieval and analysis. In this paper, we propose a\nnovel vector quantization framework that iteratively minimizes quantization\nerror. First, we provide a detailed review on a relevant vector quantization\nmethod named \\textit{residual vector quantization} (RVQ). Next, we propose\n\\textit{generalized residual vector quantization} (GRVQ) to further improve\nover RVQ. Many vector quantization methods can be viewed as the special cases\nof our proposed framework. We evaluate GRVQ on several large scale benchmark\ndatasets for large scale search, classification and object retrieval. We\ncompared GRVQ with existing methods in detail. Extensive experiments\ndemonstrate our GRVQ framework substantially outperforms existing methods in\nterm of quantization accuracy and computation efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 14:50:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Liu", "Shicong", ""], ["Shao", "Junru", ""], ["Lu", "Hongtao", ""]]}, {"id": "1609.06018", "submitter": "Junxuan Chen", "authors": "Junxuan Chen, Baigui Sun, Hao Li, Hongtao Lu, Xian-Sheng Hua", "title": "Deep CTR Prediction in Display Advertising", "comments": "This manuscript is the accepted version for ACM Multimedia Conference\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click through rate (CTR) prediction of image ads is the core task of online\ndisplay advertising systems, and logistic regression (LR) has been frequently\napplied as the prediction model. However, LR model lacks the ability of\nextracting complex and intrinsic nonlinear features from handcrafted\nhigh-dimensional image features, which limits its effectiveness. To solve this\nissue, in this paper, we introduce a novel deep neural network (DNN) based\nmodel that directly predicts the CTR of an image ad based on raw image pixels\nand other basic features in one step. The DNN model employs convolution layers\nto automatically extract representative visual features from images, and\nnonlinear CTR features are then learned from visual features and other\ncontextual features by using fully-connected layers. Empirical evaluations on a\nreal world dataset with over 50 million records demonstrate the effectiveness\nand efficiency of this method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 04:50:03 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Chen", "Junxuan", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""], ["Lu", "Hongtao", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1609.06026", "submitter": "Ankit Parag Shah", "authors": "Benjamin Elizalde, Ankit Shah, Siddharth Dalmia, Min Hun Lee, Rohan\n  Badlani, Anurag Kumar, Bhiksha Raj and Ian Lane", "title": "An Approach for Self-Training Audio Event Detectors Using Web Data", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Event Detection (AED) aims to recognize sounds within audio and video\nrecordings. AED employs machine learning algorithms commonly trained and tested\non annotated datasets. However, available datasets are limited in number of\nsamples and hence it is difficult to model acoustic diversity. Therefore, we\npropose combining labeled audio from a dataset and unlabeled audio from the web\nto improve the sound models. The audio event detectors are trained on the\nlabeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever\nthe detectors recognized any of the known sounds with high confidence, the\nunlabeled audio was use to re-train the detectors. The performance of the\nre-trained detectors is compared to the one from the original detectors using\nthe annotated test set. Results showed an improvement of the AED, and uncovered\nchallenges of using web audio from videos.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 05:52:06 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 14:15:13 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 17:09:59 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Elizalde", "Benjamin", ""], ["Shah", "Ankit", ""], ["Dalmia", "Siddharth", ""], ["Lee", "Min Hun", ""], ["Badlani", "Rohan", ""], ["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""], ["Lane", "Ian", ""]]}, {"id": "1609.06109", "submitter": "Maciej Wielgosz", "authors": "Maciej Wielgosz and Micha{\\l} Karwatowski and Marcin Pietro\\'n and\n  Kazimierz Wiatr", "title": "FPGA implementation of the procedures for video quality assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video resolutions used in variety of media are constantly rising. While\nmanufacturers struggle to perfect their screens it is also important to ensure\nhigh quality of displayed image. Overall quality can be measured using Mean\nOpinion Score (MOS). Video quality can be affected by miscellaneous artifacts,\nappearing at every stage of video creation and transmission. In this paper, we\npresent a solution to calculate four distinct video quality metrics that can be\napplied to a real time video quality assessment system. Our assessment module\nis capable of processing 8K resolution in real time set at the level of 30\nframes per second. Throughput of 2.19 GB/s surpasses performance of pure\nsoftware solutions. To concentrate on architectural optimization, the module\nwas created using high level language.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 11:27:16 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 21:23:26 GMT"}, {"version": "v3", "created": "Wed, 3 May 2017 19:52:34 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Wielgosz", "Maciej", ""], ["Karwatowski", "Micha\u0142", ""], ["Pietro\u0144", "Marcin", ""], ["Wiatr", "Kazimierz", ""]]}, {"id": "1609.06302", "submitter": "Lee Prangnell", "authors": "Lee Prangnell and Victor Sanchez", "title": "Color-Based Coding Unit Level Adaptive Quantization for HEVC", "comments": "Some of the textual and mathematical contents in this pre-print\n  working paper have been superseded. Therefore, this pre-print has been\n  removed from arXiv, as requested by the co-author of the paper (at The\n  University of Warwick)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HEVC HM 16 includes a Coding Unit (CU) level perceptual quantization\ntechnique named AdaptiveQP. AdaptiveQP adjusts the Quantization Parameter (QP)\nat the CU level based on the spatial activity of samples in the four\nconstituent NxN sub-blocks of the luma Coding Block (CB), which is contained\nwithin a 2Nx2N CU. In this paper, we propose C-BAQ, which, in contrast to\nAdaptiveQP, adjusts the CU level QP according to the spatial activity of\nsamples in the four constituent NxN sub-blocks of both the luma and chroma CBs.\nBy computing the sum of luma, chroma Cb and chroma Cr spatial activity in a CU,\na richer reflection of spatial activity in the CU is attained. Therefore, a\nmore appropriate CU level QP can be selected, thus leading to important\nimprovements in terms of coding efficiency. We evaluate the proposed technique\nin HEVC HM 16.7 using 4:4:4, 4:2:2 and 4:2:0 YCbCr sequences. Both subjective\nand objective evaluations are undertaken during which we compare C-BAQ with\nAdaptiveQP. The objective evaluation reveals that C-BAQ attains a maximum\nBD-Rate reduction of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a\nmaximum decoding time reduction of 11.0%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 15:28:41 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2016 10:51:56 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Prangnell", "Lee", ""], ["Sanchez", "Victor", ""]]}, {"id": "1609.06374", "submitter": "Dimitrios Adamos Dr", "authors": "Fotis Kalaganis (1), Dimitrios A. Adamos (2 and 3), Nikos Laskaris (1\n  and 3) ((1) AIIA Lab, Department of Informatics, Aristotle University of\n  Thessaloniki, (2) School of Music Studies, Aristotle University of\n  Thessaloniki, (3) Neuroinformatics GRoup, Aristotle University of\n  Thessaloniki)", "title": "A Consumer BCI for Automated Music Evaluation Within a Popular On-Demand\n  Music Streaming Service - Taking Listener's Brainwaves to Extremes", "comments": "12th IFIP WG 12.5 International Conference and Workshops, AIAI 2016,\n  Thessaloniki, Greece, September 16-18, 2016, Proceedings", "journal-ref": "Artificial Intelligence Applications and Innovations, Volume 475\n  of the series IFIP Advances in Information and Communication Technology pp\n  429-440, 2016", "doi": "10.1007/978-3-319-44944-9_37", "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the possibility of using a machine-learning scheme in\nconjunction with commercial wearable EEG-devices for translating listener's\nsubjective experience of music into scores that can be used for the automated\nannotation of music in popular on-demand streaming services. Based on the\nestablished -neuroscientifically sound- concepts of brainwave frequency bands,\nactivation asymmetry index and cross-frequency-coupling (CFC), we introduce a\nBrain Computer Interface (BCI) system that automatically assigns a rating score\nto the listened song. Our research operated in two distinct stages: i) a\ngeneric feature engineering stage, in which features from signal-analytics were\nranked and selected based on their ability to associate music induced\nperturbations in brainwaves with listener's appraisal of music. ii) a\npersonalization stage, during which the efficiency of ex- treme learning\nmachines (ELMs) is exploited so as to translate the derived pat- terns into a\nlistener's score. Encouraging experimental results, from a pragmatic use of the\nsystem, are presented.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 22:29:02 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 11:06:37 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Kalaganis", "Fotis", "", "2 and 3"], ["Adamos", "Dimitrios A.", "", "2 and 3"], ["Laskaris", "Nikos", "", "1\n  and 3"]]}, {"id": "1609.06442", "submitter": "Lee Prangnell", "authors": "Lee Prangnell and Victor Sanchez", "title": "Minimizing Compression Artifacts for High Resolutions with Adaptive\n  Quantization Matrices for HEVC", "comments": "PhD Working Paper, University of Warwick, UK. arXiv admin note:\n  substantial text overlap with arXiv:1606.02042", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Display Units (VDUs), capable of displaying video data at High\nDefinition (HD) and Ultra HD (UHD) resolutions, are frequently employed in a\nvariety of technological domains. Quantization-induced video compression\nartifacts, which are usually unnoticeable in low resolution environments, are\ntypically conspicuous on high resolution VDUs and video data. The default\nquantization matrices (QMs) in HEVC do not take into account specific display\nresolutions of VDUs or video data to determine the appropriate levels of\nquantization required to reduce unwanted compression artifacts. Therefore, we\npropose a novel, adaptive quantization matrix technique for the HEVC standard\nincluding Scalable HEVC (SHVC). Our technique, which is based on a refinement\nof the current QM technique in HEVC, takes into consideration specific display\nresolutions of the target VDUs in order to minimize compression artifacts. We\nundertake a thorough evaluation of the proposed technique by utilizing SHVC SHM\n9.0 (two-layered bit-stream) and the BD-Rate and SSIM metrics. For the BD-Rate\nevaluation, the proposed method achieves maximum BD-Rate reductions of 56.5% in\nthe enhancement layer. For the SSIM evaluation, our technique achieves a\nmaximum structural improvement of 0.8660 vs. 0.8538.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 07:30:18 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Prangnell", "Lee", ""], ["Sanchez", "Victor", ""]]}, {"id": "1609.06612", "submitter": "Edip Demirbilek", "authors": "Edip Demirbilek and Jean-Charles Gr\\'egoire", "title": "Multimedia Communication Quality Assessment Testbeds", "comments": "9 pages, 5 figures. this has not been submitted to any conference\n  yet. however some part of it would be presented in GStreamer Conf 2016. As\n  the GStreamer conf requires only an abstract submission, we though it would\n  be better to share the actual content via arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make an intensive use of multimedia frameworks in our research on modeling\nthe perceived quality estimation in streaming services and real-time\ncommunications. In our preliminary work, we have used the VLC VOD software to\ngenerate reference audiovisual files with various degree of coding and network\ndegradations. We have successfully built machine learning based models on the\nsubjective quality dataset we have generated using these files. However,\nimperfections in the dataset introduced by the multimedia framework we have\nused prevented us from achieving the full potential of these models.\n  In order to develop better models, we have re-created our end-to-end\nmultimedia pipeline using the GStreamer framework for audio and video\nstreaming. A GStreamer based pipeline proved to be significantly more robust to\nnetwork degradations than the VLC VOD framework and allowed us to stream a\nvideo flow at a loss rate up to 5\\% packet very easily. GStreamer has also\nenabled us to collect the relevant RTCP statistics that proved to be more\naccurate than network-deduced information. This dataset is free to the public.\nThe accuracy of the statistics eventually helped us to generate better\nperforming perceived quality estimation models.\n  In this paper, we present the implementation of these VLC and GStreamer-based\nmultimedia communication quality assessment testbeds with the references to\ntheir publicly available code bases.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 15:59:59 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Demirbilek", "Edip", ""], ["Gr\u00e9goire", "Jean-Charles", ""]]}, {"id": "1609.06653", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Land Use Classification using Convolutional Neural Networks Applied to\n  Ground-Level Images", "comments": "ACM SIGSPATIAL 2015, Best Poster Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land use mapping is a fundamental yet challenging task in geographic science.\nIn contrast to land cover mapping, it is generally not possible using overhead\nimagery. The recent, explosive growth of online geo-referenced photo\ncollections suggests an alternate approach to geographic knowledge discovery.\nIn this work, we present a general framework that uses ground-level images from\nFlickr for land use mapping. Our approach benefits from several novel aspects.\nFirst, we address the nosiness of the online photo collections, such as\nimprecise geolocation and uneven spatial distribution, by performing location\nand indoor/outdoor filtering, and semi- supervised dataset augmentation. Our\nindoor/outdoor classifier achieves state-of-the-art performance on several\nbench- mark datasets and approaches human-level accuracy. Second, we utilize\nhigh-level semantic image features extracted using deep learning, specifically\nconvolutional neural net- works, which allow us to achieve upwards of 76%\naccuracy on a challenging eight class land use mapping problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:01:24 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1609.06772", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Spatio-Temporal Sentiment Hotspot Detection Using Geotagged Photos", "comments": "To appear in ACM SIGSPATIAL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform spatio-temporal analysis of public sentiment using geotagged photo\ncollections. We develop a deep learning-based classifier that predicts the\nemotion conveyed by an image. This allows us to associate sentiment with place.\nWe perform spatial hotspot detection and show that different emotions have\ndistinct spatial distributions that match expectations. We also perform\ntemporal analysis using the capture time of the photos. Our spatio-temporal\nhotspot detection correctly identifies emerging concentrations of specific\nemotions and year-by-year analyses of select locations show there are strong\ntemporal correlations between the predicted emotions and known events.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 22:43:54 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1609.06782", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Ting Yao, Yanwei Fu, Yu-Gang Jiang", "title": "Deep Learning for Video Classification and Captioning", "comments": "Book chapter in Frontiers of Multimedia Research", "journal-ref": null, "doi": "10.1145/3122865.3122867", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated by the tremendous increase in Internet bandwidth and storage\nspace, video data has been generated, published and spread explosively,\nbecoming an indispensable part of today's big data. In this paper, we focus on\nreviewing two lines of research aiming to stimulate the comprehension of videos\nwith deep learning: video classification and video captioning. While video\nclassification concentrates on automatically labeling video clips based on\ntheir semantic contents like human actions or complex events, video captioning\nattempts to generate a complete and natural sentence, enriching the single\nlabel as in video classification, to capture the most informative dynamics in\nvideos. In addition, we also provide a review of popular benchmarks and\ncompetitions, which are critical for evaluating the technical progress of this\nvibrant field.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 00:08:59 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 14:59:32 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wu", "Zuxuan", ""], ["Yao", "Ting", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1609.07170", "submitter": "Alexander Wong", "authors": "Prajna Paramita Dash, Akshaya Mishra, and Alexander Wong", "title": "Deep Quality: A Deep No-reference Quality Assessment System", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment (IQA) continues to garner great interest in the\nresearch community, particularly given the tremendous rise in consumer video\ncapture and streaming. Despite significant research effort in IQA in the past\nfew decades, the area of no-reference image quality assessment remains a great\nchallenge and is largely unsolved. In this paper, we propose a novel\nno-reference image quality assessment system called Deep Quality, which\nleverages the power of deep learning to model the complex relationship between\nvisual content and the perceived quality. Deep Quality consists of a novel\nmulti-scale deep convolutional neural network, trained to learn to assess image\nquality based on training samples consisting of different distortions and\ndegradations such as blur, Gaussian noise, and compression artifacts.\nPreliminary results using the CSIQ benchmark image quality dataset showed that\nDeep Quality was able to achieve strong quality prediction performance (89%\npatch-level and 98% image-level prediction accuracy), being able to achieve\nsimilar performance as full-reference IQA methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 21:26:21 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Dash", "Prajna Paramita", ""], ["Mishra", "Akshaya", ""], ["Wong", "Alexander", ""]]}, {"id": "1609.07365", "submitter": "Dimitrios Adamos Dr", "authors": "Dimitrios A. Adamos (1 and 3), Stavros I. Dimitriadis (2), Nikolaos A.\n  Laskaris (2 and 3), ((1) School of Music Studies, Faculty of Fine Arts,\n  Aristotle University of Thessaloniki, (2) AIIA Lab, Department of\n  Informatics, Aristotle University of Thessaloniki, (3) Neuroinformatics\n  GRoup, Aristotle University of Thessaloniki)", "title": "Towards the bio-personalization of music recommendation systems: A\n  single-sensor EEG biomarker of subjective music preference", "comments": null, "journal-ref": "Information Sciences, Volumes 343 - 344, 20 May 2016, Pages 94 -\n  108", "doi": "10.1016/j.ins.2016.01.005", "report-no": null, "categories": "q-bio.NC cs.AI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in biosensors technology and mobile electroencephalographic\n(EEG) interfaces have opened new application fields for cognitive monitoring. A\ncomputable biomarker for the assessment of spontaneous aesthetic brain\nresponses during music listening is introduced here. It derives from\nwell-established measures of cross-frequency coupling (CFC) and quantifies the\nmusic-induced alterations in the dynamic relationships between brain rhythms.\nDuring a stage of exploratory analysis, and using the signals from a suitably\ndesigned experiment, we established the biomarker, which acts on brain\nactivations recorded over the left prefrontal cortex and focuses on the\nfunctional coupling between high-beta and low-gamma oscillations. Based on data\nfrom an additional experimental paradigm, we validated the introduced biomarker\nand showed its relevance for expressing the subjective aesthetic appreciation\nof a piece of music. Our approach resulted in an affordable tool that can\npromote human-machine interaction and, by serving as a personalized music\nannotation strategy, can be potentially integrated into modern flexible music\nrecommendation systems.\n  Keywords: Cross-frequency coupling; Human-computer interaction;\nBrain-computer interface\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 09:24:01 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Adamos", "Dimitrios A.", "", "1 and 3"], ["Dimitriadis", "Stavros I.", "", "2 and 3"], ["Laskaris", "Nikolaos A.", "", "2 and 3"]]}, {"id": "1609.07630", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake, V. A. Coutinho", "title": "Low-complexity Image and Video Coding Based on an Approximate Discrete\n  Tchebichef Transform", "comments": "Fixed diagonal matrix, 11 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2515378", "report-no": null, "categories": "cs.MM cs.CV cs.DS stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of linear transformations has great relevance for data\ndecorrelation applications, like image and video compression. In that sense,\nthe discrete Tchebichef transform (DTT) possesses useful coding and\ndecorrelation properties. The DTT transform kernel does not depend on the input\ndata and fast algorithms can be developed to real time applications. However,\nthe DTT fast algorithm presented in literature possess high computational\ncomplexity. In this work, we introduce a new low-complexity approximation for\nthe DTT. The fast algorithm of the proposed transform is multiplication-free\nand requires a reduced number of additions and bit-shifting operations. Image\nand video compression simulations in popular standards shows good performance\nof the proposed transform. Regarding hardware resource consumption for FPGA\nshows 43.1% reduction of configurable logic blocks and ASIC place and route\nrealization shows 57.7% reduction in the area-time figure when compared with\nthe 2-D version of the exact DTT.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 14:49:31 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 21:18:07 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 17:05:20 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""], ["Coutinho", "V. A.", ""]]}, {"id": "1609.07848", "submitter": "Jared Stein", "authors": "Jeff Burke, Jared J. Stein", "title": "Location-Based and Audience-Aware Storytelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the daily user of digital, Internet-enabled devices has some explicit\ncontrol over what they read and see, the providers fulfilling searches,\noffering options, and presenting material are using increasingly sophisticated\nreal-time algorithms that tune and target content for the particular user. They\nredefine the historical relationships between tellers and users, providing a\nresponsiveness paralleled only by forms of live performance incorporating\nelements of improvisation and audience interaction. The general accessibility\nof algorithmically driven content delivery techniques suggests significant\nuntapped potential for new approaches to narrative beyond advertising and\ncommercially orientated customization.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 05:09:32 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Burke", "Jeff", ""], ["Stein", "Jared J.", ""]]}, {"id": "1609.08042", "submitter": "Xavier Corbillon", "authors": "Xavier Corbillon and Gwendal Simon and Alisa Devlic and Jacob\n  Chakareski", "title": "Viewport-Adaptive Navigable 360-Degree Video Delivery", "comments": "7 pages + 6 figures", "journal-ref": "In proceeding of 2017 IEEE International Conference on\n  Communications (ICC), pages 1-7", "doi": "10.1109/ICC.2017.7996611", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The delivery and display of 360-degree videos on Head-Mounted Displays (HMDs)\npresents many technical challenges. 360-degree videos are ultra high resolution\nspherical videos, which contain an omnidirectional view of the scene. However\nonly a portion of this scene is displayed on the HMD. Moreover, HMD need to\nrespond in 10 ms to head movements, which prevents the server to send only the\ndisplayed video part based on client feedback. To reduce the bandwidth waste,\nwhile still providing an immersive experience, a viewport-adaptive 360-degree\nvideo streaming system is proposed. The server prepares multiple video\nrepresentations, which differ not only by their bit-rate, but also by the\nqualities of different scene regions. The client chooses a representation for\nthe next segment such that its bit-rate fits the available throughput and a\nfull quality region matches its viewing. We investigate the impact of various\nspherical-to-plane projections and quality arrangements on the video quality\ndisplayed to the user, showing that the cube map layout offers the best quality\nfor the given bit-rate budget. An evaluation with a dataset of users navigating\n360-degree videos demonstrates that segments need to be short enough to enable\nfrequent view switches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 16:10:48 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 08:49:35 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Corbillon", "Xavier", ""], ["Simon", "Gwendal", ""], ["Devlic", "Alisa", ""], ["Chakareski", "Jacob", ""]]}, {"id": "1609.08729", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini and Viswanathan Swaminathan", "title": "Adaptive 360 VR Video Streaming: Divide and Conquer!", "comments": "IEEE International Symposium on Multimedia 2016 (ISM '16), December\n  4-7, San Jose, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional multimedia applications such as games and videos are still\npopular, there has been a significant interest in the recent years towards new\n3D media such as 3D immersion and Virtual Reality (VR) applications, especially\n360 VR videos. 360 VR video is an immersive spherical video where the user can\nlook around during playback. Unfortunately, 360 VR videos are extremely\nbandwidth intensive, and therefore are difficult to stream at acceptable\nquality levels. In this paper, we propose an adaptive bandwidth-efficient 360\nVR video streaming system using a divide and conquer approach. In our approach,\nwe propose a dynamic view-aware adaptation technique to tackle the huge\nstreaming bandwidth demands of 360 VR videos. We spatially divide the videos\ninto multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe\nthe spatial relationship of tiles in the 360-degree space, and prioritize the\ntiles in the Field of View (FoV). In order to describe such tiled\nrepresentations, we extend MPEG-DASH SRD to the 3D space of 360 VR videos. We\nspatially partition the underlying 3D mesh, and construct an efficient 3D\ngeometry mesh called hexaface sphere to optimally represent a tiled 360 VR\nvideo in the 3D space. Our initial evaluation results report up to 72%\nbandwidth savings on 360 VR video streaming with minor negative quality impacts\ncompared to the baseline scenario when no adaptations is applied.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 02:07:12 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 17:05:33 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 18:45:41 GMT"}, {"version": "v4", "created": "Mon, 23 Jan 2017 17:18:33 GMT"}, {"version": "v5", "created": "Fri, 17 Nov 2017 17:03:33 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Swaminathan", "Viswanathan", ""]]}]