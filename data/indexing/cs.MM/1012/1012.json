[{"id": "1012.0223", "submitter": "A Kannan", "authors": "A. Kannan, V. Mohan, N. Anbazhagan", "title": "An Effective Method of Image Retrieval using Image Mining Techniques", "comments": null, "journal-ref": null, "doi": "10.5121/ijma.2010.2402", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present research scholars are having keen interest in doing their\nresearch activities in the area of Data mining all over the world. Especially,\n[13]Mining Image data is the one of the essential features in this present\nscenario since image data plays vital role in every aspect of the system such\nas business for marketing, hospital for surgery, engineering for construction,\nWeb for publication and so on. The other area in the Image mining system is the\nContent-Based Image Retrieval (CBIR) which performs retrieval based on the\nsimilarity defined in terms of extracted features with more objectiveness. The\ndrawback in CBIR is the features of the query image alone are considered.\nHence, a new technique called Image retrieval based on optimum clusters is\nproposed for improving user interaction with image retrieval systems by fully\nexploiting the similarity information. The index is created by describing the\nimages according to their color characteristics, with compact feature vectors,\nthat represent typical color distributions [12].\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 15:34:50 GMT"}], "update_date": "2010-12-02", "authors_parsed": [["Kannan", "A.", ""], ["Mohan", "V.", ""], ["Anbazhagan", "N.", ""]]}, {"id": "1012.0397", "submitter": "Ali Ayremlou", "authors": "Ramtin Madani, Ali Ayremlou, Arash Amini and Farrokh Marvasti", "title": "A proposed Optimized Spline Interpolation", "comments": "SampTA 2011, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to design compact support basis spline functions\nthat best approximate a given filter (e.g., an ideal Lowpass filter). The\noptimum function is found by minimizing the least square problem ($\\ell$2 norm\nof the difference between the desired and the approximated filters) by means of\nthe calculus of variation; more precisely, the introduced splines give optimal\nfiltering properties with respect to their time support interval. Both\nmathematical analysis and simulation results confirm the superiority of these\nsplines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 13:02:06 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2011 20:06:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Madani", "Ramtin", ""], ["Ayremlou", "Ali", ""], ["Amini", "Arash", ""], ["Marvasti", "Farrokh", ""]]}, {"id": "1012.1184", "submitter": "Lei Zhang Dr.", "authors": "Weisheng Dong, Lei Zhang, Guangming Shi, Xiaolin Wu", "title": "Image Deblurring and Super-resolution by Adaptive Sparse Domain\n  Selection and Adaptive Regularization", "comments": "35 pages. This paper is under review in IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2011.2108306", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a powerful statistical image modeling technique, sparse representation has\nbeen successfully used in various image restoration applications. The success\nof sparse representation owes to the development of l1-norm optimization\ntechniques, and the fact that natural images are intrinsically sparse in some\ndomain. The image restoration quality largely depends on whether the employed\nsparse domain can represent well the underlying image. Considering that the\ncontents can vary significantly across different images or different patches in\na single image, we propose to learn various sets of bases from a pre-collected\ndataset of example image patches, and then for a given patch to be processed,\none set of bases are adaptively selected to characterize the local sparse\ndomain. We further introduce two adaptive regularization terms into the sparse\nrepresentation framework. First, a set of autoregressive (AR) models are\nlearned from the dataset of example image patches. The best fitted AR models to\na given patch are adaptively selected to regularize the image local structures.\nSecond, the image non-local self-similarity is introduced as another\nregularization term. In addition, the sparsity regularization parameter is\nadaptively estimated for better image restoration performance. Extensive\nexperiments on image deblurring and super-resolution validate that by using\nadaptive sparse domain selection and adaptive regularization, the proposed\nmethod achieves much better results than many state-of-the-art algorithms in\nterms of both PSNR and visual perception.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 14:37:14 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Dong", "Weisheng", ""], ["Zhang", "Lei", ""], ["Shi", "Guangming", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1012.1882", "submitter": "Adrian Paschke", "authors": "Jasmin Opitz, Bijan Parsia, Ulrike Sattler", "title": "Evaluating Modelling Approaches for Medical Image Annotations", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information system designers face many challenges w.r.t. selecting\nappropriate semantic technologies and deciding on a modelling approach for\ntheir system. However, there is no clear methodology yet to evaluate\n\"semantically enriched\" information systems. In this paper we present a case\nstudy on different modelling approaches for annotating medical images and\nintroduce a conceptual framework that can be used to analyse the fitness of\ninformation systems and help designers to spot the strengths and weaknesses of\nvarious modelling approaches as well as managing trade-offs between modelling\neffort and their potential benefits.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 21:45:16 GMT"}], "update_date": "2010-12-10", "authors_parsed": [["Opitz", "Jasmin", ""], ["Parsia", "Bijan", ""], ["Sattler", "Ulrike", ""]]}, {"id": "1012.2518", "submitter": "Jaydip Sen", "authors": "Jaydip Sen and Shomik Bhattacharya", "title": "A Survey on Cross-Layer Design Frameworks for Multimedia Applications\n  over Wireless Networks", "comments": "16 pages, 9 figures", "journal-ref": "International Journal of Computer Science and Information\n  Technology (IJCSIT), Vol: 1, No:1, pp. 29-42, June 2008", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, the Internet throughput, usage and reliability have\nincreased almost exponentially. The introduction of broadband wireless mobile\nad hoc networks (MANETs) and cellular networks together with increased\ncomputational power have opened the door for a new breed of applications to be\ncreated, namely real-time multimedia applications. Delivering real-time\nmultimedia traffic over a complex network like the Internet is a particularly\nchallenging task since these applications have strict quality -of-service (QoS)\nrequirements on bandwidth, delay, and delay jitter. Traditional IP-based best\neffort service will not be able to meet these stringent requirements. The\ntime-varying nature of wireless channels and resource constrained wireless\ndevices make the problem even more difficult. To improve perceived media\nquality by end users over wireless Internet, QoS supports can be addressed in\ndifferent layers, including application layer, transport layer and link layer.\nCross layer design is a well-known approach to achieve this adaptation. In\ncross-layer design, the challenges from the physical wireless medium and the\nQoS-demands from the applications are taken into account so that the rate,\npower, and coding at the physical layer can adapted to meet the requirements of\nthe applications given the current channel and network conditions. A number of\npropositions for cross-layer designs exist in the literature. In this paper, an\nextensive review has been made on these cross-layer architectures that combine\nthe application-layer, transport layer and the link layer controls.\nParticularly the issues like channel estimation techniques, adaptive controls\nat the application and link layers for energy efficiency, priority based\nscheduling, transmission rate control at the transport layer, and adaptive\nautomatic repeat request (ARQ) are discussed in detail.\n", "versions": [{"version": "v1", "created": "Sun, 12 Dec 2010 07:40:07 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Sen", "Jaydip", ""], ["Bhattacharya", "Shomik", ""]]}, {"id": "1012.2965", "submitter": "Rashmi Agarwal", "authors": "Rashmi Agarwal, R. Krishnan, M. S. Santhanam, K. Srinivas, and K.\n  Venugopalan", "title": "Digital watermarking : An approach based on Hilbert transform", "comments": "17 Pages, 52 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the well known algorithms for watermarking of digital images involve\ntransformation of the image data to Fourier or singular vector space. In this\npaper, we introduce watermarking in Hilbert transform domain for digital media.\nGenerally, if the image is a matrix of order $m$ by $n$, then the transformed\nspace is also an image of the same order. However, with Hilbert transforms, the\ntransformed space is of order $2m$ by $2n$. This allows for more latitude in\nstoring the watermark in the host image. Based on this idea, we propose an\nalgorithm for embedding and extracting watermark in a host image and\nanalytically obtain a parameter related to this procedure. Using extensive\nsimulations, we show that the algorithm performs well even if the host image is\ncorrupted by various attacks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 08:50:29 GMT"}], "update_date": "2010-12-15", "authors_parsed": [["Agarwal", "Rashmi", ""], ["Krishnan", "R.", ""], ["Santhanam", "M. S.", ""], ["Srinivas", "K.", ""], ["Venugopalan", "K.", ""]]}, {"id": "1012.5208", "submitter": "Nadia Baaziz", "authors": "Nadia Baaziz, Omar Abahmane and Rokia Missaoui", "title": "Texture feature extraction in the spatial-frequency domain for\n  content-based image retrieval", "comments": "19 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of large scale multimedia databases has led to great challenges in\ncontent-based image retrieval (CBIR). Even though CBIR is considered an\nemerging field of research, however it constitutes a strong background for new\nmethodologies and systems implementations. Therefore, many research\ncontributions are focusing on techniques enabling higher image retrieval\naccuracy while preserving low level of computational complexity. Image\nretrieval based on texture features is receiving special attention because of\nthe omnipresence of this visual feature in most real-world images. This paper\nhighlights the state-of-the-art and current progress relevant to texture-based\nimage retrieval and spatial-frequency image representations. In particular, it\ngives an overview of statistical methodologies and techniques employed for\ntexture feature extraction using most popular spatial-frequency image\ntransforms, namely discrete wavelets, Gabor wavelets, dual-tree complex wavelet\nand contourlets. Indications are also given about used similarity measurement\nfunctions and most important achieved results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Dec 2010 14:10:25 GMT"}], "update_date": "2010-12-24", "authors_parsed": [["Baaziz", "Nadia", ""], ["Abahmane", "Omar", ""], ["Missaoui", "Rokia", ""]]}, {"id": "1012.5573", "submitter": "Goutam Paul", "authors": "Goutam Paul and Imon Mukherjee", "title": "Image Sterilization to Prevent LSB-based Steganographic Transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sterilization is a very popular word used in biomedical testing (like removal\nof all microorganisms on surface of an article or in fluid using appropriate\nchemical products). Motivated by this biological analogy, we, for the first\ntime, introduce the concept of sterilization of an image, i.e., removing any\nsteganographic information embedded in the image. Experimental results show\nthat our technique succeeded in sterilizing around 76% to 91% of stego pixels\nin an image on average, where data is embedded using LSB-based steganography.\n", "versions": [{"version": "v1", "created": "Mon, 27 Dec 2010 07:44:21 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Paul", "Goutam", ""], ["Mukherjee", "Imon", ""]]}]