[{"id": "1605.00305", "submitter": "Abbas Soltanian", "authors": "Ahmad F. B. Alam, Abbas Soltanian, Sami Yangui, Mohammad A.\n  Salahuddin, Roch Glitho, Halima Elbiaze", "title": "A Cloud Platform-as-a-Service for Multimedia Conferencing Service\n  Provisioning", "comments": "6 pages, 6 figures, IEEE ISCC 2016", "journal-ref": null, "doi": "10.1109/ISCC.2016.7543756", "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia conferencing is the real-time exchange of multimedia content\nbetween multiple parties. It is the basis of a wide range of applications\n(e.g., multimedia multiplayer game). Cloud-based provisioning of the\nconferencing services on which these applications rely will bring benefits,\nsuch as easy service provisioning and elastic scalability. However, it remains\na big challenge. This paper proposes a PaaS for conferencing service\nprovisioning. The proposed PaaS is based on a business model from the state of\nthe art. It relies on conferencing IaaSs that, instead of VMs, offer\nconferencing substrates (e.g., dial-in signaling, video mixer and audio mixer).\nThe PaaS enables composition of new conferences from substrates on the fly.\nThis has been prototyped in this paper and, in order to evaluate it, a\nconferencing IaaS is also implemented. Performance measurements are also made.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 20:24:20 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Alam", "Ahmad F. B.", ""], ["Soltanian", "Abbas", ""], ["Yangui", "Sami", ""], ["Salahuddin", "Mohammad A.", ""], ["Glitho", "Roch", ""], ["Elbiaze", "Halima", ""]]}, {"id": "1605.00957", "submitter": "Marco Bertini", "authors": "Andrea Salvi, Simone Ercoli, Marco Bertini and Alberto Del Bimbo", "title": "Bloom Filters and Compact Hash Codes for Efficient and Distributed Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for efficient image retrieval, based on a\nsimple and effective hashing of CNN features and the use of an indexing\nstructure based on Bloom filters. These filters are used as gatekeepers for the\ndatabase of image features, allowing to avoid to perform a query if the query\nfeatures are not stored in the database and speeding up the query process,\nwithout affecting retrieval performance. Thanks to the limited memory\nrequirements the system is suitable for mobile applications and distributed\ndatabases, associating each filter to a distributed portion of the database.\nExperimental validation has been performed on three standard image retrieval\ndatasets, outperforming state-of-the-art hashing methods in terms of precision,\nwhile the proposed indexing method obtains a $2\\times$ speedup.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 15:50:54 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Salvi", "Andrea", ""], ["Ercoli", "Simone", ""], ["Bertini", "Marco", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1605.01115", "submitter": "Mading Li", "authors": "Mading Li, Jiaying Liu, Zhiwei Xiong, Xiaoyan Sun, Zongming Guo", "title": "MARLow: A Joint Multiplanar Autoregressive and Low-Rank Approach for\n  Image Completion", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multiplanar autoregressive (AR) model to\nexploit the correlation in cross-dimensional planes of a similar patch group\ncollected in an image, which has long been neglected by previous AR models. On\nthat basis, we then present a joint multiplanar AR and low-rank based approach\n(MARLow) for image completion from random sampling, which exploits the nonlocal\nself-similarity within natural images more effectively. Specifically, the\nmultiplanar AR model constraints the local stationarity in different\ncross-sections of the patch group, while the low-rank minimization captures the\nintrinsic coherence of nonlocal patches. The proposed approach can be readily\nextended to multichannel images (e.g. color images), by simultaneously\nconsidering the correlation in different channels. Experimental results\ndemonstrate that the proposed approach significantly outperforms\nstate-of-the-art methods, even if the pixel missing rate is as high as 90%.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 23:41:57 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 03:55:13 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Li", "Mading", ""], ["Liu", "Jiaying", ""], ["Xiong", "Zhiwei", ""], ["Sun", "Xiaoyan", ""], ["Guo", "Zongming", ""]]}, {"id": "1605.01600", "submitter": "Fabien Ringeval", "authors": "Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval,\n  Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy\n  Cowie and Maja Pantic", "title": "AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and\n  Challenge", "comments": "Proceedings of the 6th International Workshop on Audio/Visual Emotion\n  Challenge, AVEC'16, co-located with the 24th ACM International Conference on\n  Multimedia, MM 2016, pages 3-10, Amsterdam, The Netherlands, October 2016.\n  ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \"Depression, Mood\nand Emotion\" will be the sixth competition event aimed at comparison of\nmultimedia processing and machine learning methods for automatic audio, visual\nand physiological depression and emotion analysis, with all participants\ncompeting under strictly the same conditions. The goal of the Challenge is to\nprovide a common benchmark test set for multi-modal information processing and\nto bring together the depression and emotion recognition communities, as well\nas the audio, video and physiological processing communities, to compare the\nrelative merits of the various approaches to depression and emotion recognition\nunder well-defined and strictly comparable conditions and establish to what\nextent fusion of the approaches is possible and beneficial. This paper presents\nthe challenge guidelines, the common data used, and the performance of the\nbaseline system on the two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 14:04:50 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 12:34:51 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 08:02:32 GMT"}, {"version": "v4", "created": "Tue, 22 Nov 2016 15:19:24 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Valstar", "Michel", ""], ["Gratch", "Jonathan", ""], ["Schuller", "Bjorn", ""], ["Ringeval", "Fabien", ""], ["Lalanne", "Denis", ""], ["Torres", "Mercedes Torres", ""], ["Scherer", "Stefan", ""], ["Stratou", "Guiota", ""], ["Cowie", "Roddy", ""], ["Pantic", "Maja", ""]]}, {"id": "1605.02401", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Audio Event Detection using Weakly Labeled Data", "comments": "ACM Multimedia 2016", "journal-ref": null, "doi": "10.1145/2964284.2964310", "report-no": null, "categories": "cs.SD cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic event detection is essential for content analysis and description of\nmultimedia recordings. The majority of current literature on the topic learns\nthe detectors through fully-supervised techniques employing strongly labeled\ndata. However, the labels available for majority of multimedia data are\ngenerally weak and do not provide sufficient detail for such methods to be\nemployed. In this paper we propose a framework for learning acoustic event\ndetectors using only weakly labeled data. We first show that audio event\ndetection using weak labels can be formulated as an Multiple Instance Learning\nproblem. We then suggest two frameworks for solving multiple-instance learning,\none based on support vector machines, and the other on neural networks. The\nproposed methods can help in removing the time consuming and expensive process\nof manually annotating data to facilitate fully supervised learning. Moreover,\nit can not only detect events in a recording but can also provide temporal\nlocations of events in the recording. This helps in obtaining a complete\ndescription of the recording and is notable since temporal information was\nnever known in the first place in weakly labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 02:17:12 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 03:33:13 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 05:46:56 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1605.02605", "submitter": "Iyad Jafar", "authors": "Enas N. Jaara and Iyad F. Jafar", "title": "Efficient Reversible Data Hiding Algorithms Based on Dual Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new reversible data hiding (RDH) algorithm that is based on\nthe concept of shifting of prediction error histograms is proposed. The\nalgorithm extends the efficient modification of prediction errors (MPE)\nalgorithm by incorporating two predictors and using one prediction error value\nfor data embedding. The motivation behind using two predictors is driven by the\nfact that predictors have different prediction accuracy which is directly\nrelated to the embedding capacity and quality of the stego image. The key\nfeature of the proposed algorithm lies in using two predictors without the need\nto communicate additional overhead with the stego image. Basically, the\nidentification of the predictor that is used during embedding is done through a\nset of rules. The proposed algorithm is further extended to use two and three\nbins in the prediction errors histogram in order to increase the embedding\ncapacity. Performance evaluation of the proposed algorithm and its extensions\nshowed the advantage of using two predictors in boosting the embedding capacity\nwhile providing competitive quality for the stego image.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 14:39:02 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Jaara", "Enas N.", ""], ["Jafar", "Iyad F.", ""]]}, {"id": "1605.02976", "submitter": "Dajiang Zhou", "authors": "Li Guo, Dajiang Zhou, Shinji Kimura, Satoshi Goto", "title": "Frame-level quality and memory traffic allocation for lossy embedded\n  compression in video codec systems", "comments": "ICME Workshops 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mobile video codecs, the huge energy dissipation for external memory\ntraffic is a critical challenge under the battery power constraint. Lossy\nembedded compression (EC), as a solution to this challenge, is considered in\nthis paper. While previous studies in EC mostly focused on compression\nalgorithms at the block level, this work, to the best of our knowledge, is the\nfirst one that addresses the allocation of video quality and memory traffic at\nthe frame level. For lossy EC, a main difficulty of its application lies in the\nerror propagation from quality degradation of reference frames. Instinctively,\nit is preferred to perform more lossy EC in non-reference frames to minimize\nthe quality loss. The analysis and experiments in this paper, however, will\nshow lossy EC should actually be distributed to more frames. Correspondingly,\nfor hierarchical-B GOPs, we developed an efficient allocation that outperforms\nthe non-reference-only allocation by up to 4.5 dB in PSNR. In comparison, the\nproposed allocation also delivers more consistent quality between frames by\nhaving lower PSNR fluctuation.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 12:29:18 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Guo", "Li", ""], ["Zhou", "Dajiang", ""], ["Kimura", "Shinji", ""], ["Goto", "Satoshi", ""]]}, {"id": "1605.03236", "submitter": "Kairan Sun", "authors": "Kairan Sun, Huazi Zhang, Dapeng Wu", "title": "Delay-aware Fountain Codes for Video Streaming with Optimal Sampling\n  Strategy", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive demand of on-line video from smart mobile devices poses\nunprecedented challenges to delivering high quality of experience (QoE) over\nwireless networks. Streaming high-definition video with low delay is difficult\nmainly due to (i) the stochastic nature of wireless channels and (ii) the\nfluctuating videos bit rate. To address this, we propose a novel delay-aware\nfountain coding (DAF) technique that integrates channel coding and video\ncoding. In this paper, we reveal that the fluctuation of video bit rate can\nalso be exploited to further improve fountain codes for wireless video\nstreaming. Specifically, we develop two coding techniques: the time-based\nsliding window and the optimal window-wise sampling strategy. By adaptively\nselecting the window length and optimally adjusting the sampling pattern\naccording to the ongoing video bit rate, the proposed schemes deliver\nsignificantly higher video quality than existing schemes, with low delay and\nconstant data rate. To validate our design, we implement the protocols of DAF,\nDAF-L (a low-complexity version) and the existing delay-aware video streaming\nschemes by streaming H.264/AVC standard videos over an 802.11b network on CORE\nemulation platform. The results show that the decoding ratio of our scheme is\n15% to 100% higher than the state of the art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 22:55:04 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Sun", "Kairan", ""], ["Zhang", "Huazi", ""], ["Wu", "Dapeng", ""]]}, {"id": "1605.03754", "submitter": "Carlo Noel Ochotorena", "authors": "Carlo Noel Ochotorena and Yukihiko Yamashita", "title": "Regression-based Intra-prediction for Image and Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By utilizing previously known areas in an image, intra-prediction techniques\ncan find a good estimate of the current block. This allows the encoder to store\nonly the error between the original block and the generated estimate, thus\nleading to an improvement in coding efficiency. Standards such as AVC and HEVC\ndescribe expert-designed prediction modes operating in certain angular\norientations alongside separate DC and planar prediction modes. Being designed\npredictors, while these techniques have been demonstrated to perform well in\nimage and video coding applications, they do not necessarily fully utilize\nnatural image structures. In this paper, we describe a novel system for\ndeveloping predictors derived from natural image blocks. The proposed algorithm\nis seeded with designed predictors (e.g. HEVC-style prediction) and allowed to\niteratively refine these predictors through regularized regression. The\nresulting prediction models show significant improvements in estimation quality\nover their designed counterparts across all conditions while maintaining\nreasonable computational complexity. We also demonstrate how the proposed\nalgorithm handles the worst-case scenario of intra-prediction with no error\nreporting.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 10:50:18 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Ochotorena", "Carlo Noel", ""], ["Yamashita", "Yukihiko", ""]]}, {"id": "1605.03815", "submitter": "Zakaria Ye", "authors": "Zakaria Ye, Rachid El-Azouzi, Tania Jimenez, Eitan Altman, Stefan\n  Valentin", "title": "Backward-Shifted Strategies Based on SVC for HTTP Adaptive Video\n  Streaming", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although HTTP-based video streaming can easily penetrate firewalls and profit\nfrom Web caches, the underlying TCP may introduce large delays in case of a\nsudden capacity loss. To avoid an interruption of the video stream in such\ncases we propose the Backward-Shifted Coding (BSC). Based on Scalable Video\nCoding (SVC), BSC adds a time-shifted layer of redundancy to the video stream\nsuch that future frames are downloaded at any instant. This pre-fetched content\nmaintains a fluent video stream even under highly variant network conditions\nand leads to high Quality of Experience (QoE). We characterize this QoE gain by\nanalyzing initial buffering time, re-buffering time and content resolution\nusing the Ballot theorem. The probability generating functions of the playback\ninterruption and of the initial buffering latency are provided in closed form.\nWe further compute the quasi-stationary distribution of the video quality, in\norder to compute the average quality, as well as temporal variability in video\nquality. Employing these analytic results to optimize QoE shows interesting\ntrade-offs and video streaming at outstanding fluency.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 14:01:14 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Ye", "Zakaria", ""], ["El-Azouzi", "Rachid", ""], ["Jimenez", "Tania", ""], ["Altman", "Eitan", ""], ["Valentin", "Stefan", ""]]}, {"id": "1605.04270", "submitter": "Matti Siekkinen", "authors": "Matti Siekkinen and Enrico Masala and Teemu K\\\"am\\\"ar\\\"ainen", "title": "A First Look at Quality of Mobile Live Streaming Experience: the Case of\n  Periscope", "comments": "In Proceedings of the ACM Internet Measurement Conference (IMC) 2016", "journal-ref": null, "doi": "10.1145/2987443.2987472", "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live multimedia streaming from mobile devices is rapidly gaining popularity\nbut little is known about the QoE they provide. In this paper, we examine the\nPeriscope service. We first crawl the service in order to understand its usage\npatterns. Then, we study the protocols used, the typical quality of experience\nindicators, such as playback smoothness and latency, video quality, and the\nenergy consumption of the Android application.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 17:44:52 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 08:34:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Siekkinen", "Matti", ""], ["Masala", "Enrico", ""], ["K\u00e4m\u00e4r\u00e4inen", "Teemu", ""]]}, {"id": "1605.04770", "submitter": "Lamberto Ballan", "authors": "Tiberio Uricchio, Lamberto Ballan, Lorenzo Seidenari, Alberto Del\n  Bimbo", "title": "Automatic Image Annotation via Label Transfer in the Semantic Space", "comments": "To appear in Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2017.05.019", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation is among the fundamental problems in computer\nvision and pattern recognition, and it is becoming increasingly important in\norder to develop algorithms that are able to search and browse large-scale\nimage collections. In this paper, we propose a label propagation framework\nbased on Kernel Canonical Correlation Analysis (KCCA), which builds a latent\nsemantic space where correlation of visual and textual features are well\npreserved into a semantic embedding. The proposed approach is robust and can\nwork either when the training set is well annotated by experts, as well as when\nit is noisy such as in the case of user-generated tags in social media. We\nreport extensive results on four popular datasets. Our results show that our\nKCCA-based framework can be applied to several state-of-the-art label transfer\nmethods to obtain significant improvements. Our approach works even with the\nnoisy tags of social users, provided that appropriate denoising is performed.\nExperiments on a large scale setting show that our method can provide some\nbenefits even when the semantic space is estimated on a subset of training\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 13:45:15 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:24:00 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 13:21:02 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Uricchio", "Tiberio", ""], ["Ballan", "Lamberto", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1605.04850", "submitter": "Michael Gygli", "authors": "Michael Gygli and Yale Song and Liangliang Cao", "title": "Video2GIF: Automatic Generation of Animated GIFs from Video", "comments": "Accepted to CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the novel problem of automatically generating animated GIFs from\nvideo. GIFs are short looping video with no sound, and a perfect combination\nbetween image and video that really capture our attention. GIFs tell a story,\nexpress emotion, turn events into humorous moments, and are the new wave of\nphotojournalism. We pose the question: Can we automate the entirely manual and\nelaborate process of GIF creation by leveraging the plethora of user generated\nGIF content? We propose a Robust Deep RankNet that, given a video, generates a\nranked list of its segments according to their suitability as GIF. We train our\nmodel to learn what visual content is often selected for GIFs by using over\n100K user generated GIFs and their corresponding video sources. We effectively\ndeal with the noisy web data by proposing a novel adaptive Huber loss in the\nranking formulation. We show that our approach is robust to outliers and picks\nup several patterns that are frequently present in popular animated GIFs. On\nour new large-scale benchmark dataset, we show the advantage of our approach\nover several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 17:44:31 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Gygli", "Michael", ""], ["Song", "Yale", ""], ["Cao", "Liangliang", ""]]}, {"id": "1605.04930", "submitter": "Jean-Marc Valin", "authors": "Jean-Marc Valin, Nathan E. Egge, Thomas Daede, Timothy B. Terriberry,\n  Christopher Montgomery", "title": "Daala: A Perceptually-Driven Still Picture Codec", "comments": "Accepted for ICIP 2016, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daala is a new royalty-free video codec based on perceptually-driven coding\ntechniques. We explore using its keyframe format for still picture coding and\nshow how it has improved over the past year. We believe the technology used in\nDaala could be the basis of an excellent, royalty-free image format.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 20:12:02 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Valin", "Jean-Marc", ""], ["Egge", "Nathan E.", ""], ["Daede", "Thomas", ""], ["Terriberry", "Timothy B.", ""], ["Montgomery", "Christopher", ""]]}, {"id": "1605.05118", "submitter": "Fatih Kamisli", "authors": "Fatih Kamisli", "title": "Lossless Compression in HEVC with Integer-to-Integer Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches have been proposed to support lossless coding within video\ncoding standards that are primarily designed for lossy coding. The simplest\napproach is to just skip transform and quantization and directly entropy code\nthe prediction residual, which is used in HEVC version 1. However, this simple\napproach is inefficient for compression. More efficient approaches include\nprocessing the residual with DPCM prior to entropy coding. This paper explores\nan alternative approach based on processing the residual with\ninteger-to-integer (i2i) transforms. I2i transforms map integers to integers,\nhowever, unlike the integer transforms used in HEVC for lossy coding, they do\nnot increase the dynamic range at the output and can be used in lossless\ncoding. Experiments with the HEVC reference software show competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 11:32:13 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Kamisli", "Fatih", ""]]}, {"id": "1605.05319", "submitter": "Fatih Kamisli", "authors": "Fatih Kamisli", "title": "Lossless Intra Coding in HEVC with Integer-to-Integer DST", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.05118", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is desirable to support efficient lossless coding within video coding\nstandards, which are primarily designed for lossy coding, with as little\nmodification as possible. A simple approach is to skip transform and\nquantization, and directly entropy code the prediction residual, but this is\ninefficient for compression. A more efficient and popular approach is to\nprocess the residual block with DPCM prior to entropy coding. This paper\nexplores an alternative approach based on processing the residual block with\ninteger-to-integer (i2i) transforms. I2i transforms map integers to integers,\nhowever, unlike the integer transforms used in HEVC for lossy coding, they do\nnot increase the dynamic range at the output and can be used in lossless\ncoding. We use both an i2i DCT from the literature and a novel i2i\napproximation of the DST. Experiments with the HEVC reference software show\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 11:54:04 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Kamisli", "Fatih", ""]]}, {"id": "1605.05758", "submitter": "Guanyu Gao", "authors": "Guanyu Gao, Yonggang Wen, Cedric Westphal", "title": "Resource Provisioning and Profit Maximization for Transcoding in\n  Information Centric Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive bitrate streaming (ABR) has been widely adopted to support video\nstreaming services over heterogeneous devices and varying network conditions.\nWith ABR, each video content is transcoded into multiple representations in\ndifferent bitrates and resolutions. However, video transcoding is computing\nintensive, which requires the transcoding service providers to deploy a large\nnumber of servers for transcoding the video contents published by the content\nproducers. As such, a natural question for the transcoding service provider is\nhow to provision the computing resource for transcoding the video contents\nwhile maximizing service profit. To address this problem, we design a cloud\nvideo transcoding system by taking the advantage of cloud computing technology\nto elastically allocate computing resource. We propose a method for jointly\nconsidering the task scheduling and resource provisioning problem in two\ntimescales, and formulate the service profit maximization as a two-timescale\nstochastic optimization problem. We derive some approximate policies for the\ntask scheduling and resource provisioning. Based on our proposed methods, we\nimplement our open source cloud video transcoding system Morph and evaluate its\nperformance in a real environment. The experiment results demonstrate that our\nproposed method can reduce the resource consumption and achieve a higher profit\ncompared with the baseline schemes.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 21:10:16 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Gao", "Guanyu", ""], ["Wen", "Yonggang", ""], ["Westphal", "Cedric", ""]]}, {"id": "1605.06921", "submitter": "Luka Crnkovic-Friis", "authors": "Luka Crnkovic-Friis, Louise Crnkovic-Friis", "title": "Generative Choreography using Deep Learning", "comments": "This article will be presented at the 7th International Conference on\n  Computational Creativity, ICCC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have enabled the extraction of high-level\nfeatures from raw sensor data which has opened up new possibilities in many\ndifferent fields, including computer generated choreography. In this paper we\npresent a system chor-rnn for generating novel choreographic material in the\nnuanced choreographic language and style of an individual choreographer. It\nalso shows promising results in producing a higher level compositional\ncohesion, rather than just generating sequences of movement. At the core of\nchor-rnn is a deep recurrent neural network trained on raw motion capture data\nand that can generate new dance sequences for a solo dancer. Chor-rnn can be\nused for collaborative human-machine choreography or as a creative catalyst,\nserving as inspiration for a choreographer.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 07:36:49 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Crnkovic-Friis", "Luka", ""], ["Crnkovic-Friis", "Louise", ""]]}, {"id": "1605.07704", "submitter": "Ming Ma", "authors": "Ming Ma and Zhi Wang and Ke Su and Lifeng Sun", "title": "Understanding the Smartrouter-based Peer CDN for Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a new video delivery paradigm: smartrouter-based\nvideo delivery network, which is enabled by smartrouters deployed at users'\nhomes, together with the conventional video servers deployed in the\ndatacenters. Recently, ChinaCache, a large content delivery network (CDN)\nprovider, and Youku, a video service provider using smartrouters to assist\nvideo delivery, announced their cooperation to create a new paradigm of content\ndelivery based on householders' network resources. This new paradigm is\ndifferent from the conventional peer-to-peer (P2P) approach, because such\ndedicated smartrouters are inherently operated by the centralized video service\nproviders in a coordinative manner. It is intriguing to study the strategies,\nperformance and potential impact on the content delivery ecosystem of such peer\nCDN systems. In this paper, we study the Youku peer CDN, which has deployed\nover 300K smartrouter devices for its video streaming. In our measurement, 78K\nvideos were investigated and 3TB traffic has been analyzed, over controlled\nrouters and players. Our contributions are the following measurement insights.\nFirst, a global replication and caching strategy is essential for the peer CDN\nsystems, and proactively scheduling replication and caching on a daily basis\ncan guarantee their performance. Second, such peer CDN deployment can itself\nform an effective Quality of Service (QoS) monitoring sub-system, which can be\nused for fine-grained user request redirection. We also provide our analysis on\nthe performance issues and potential improvements to the peer CDN systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 01:50:17 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Ma", "Ming", ""], ["Wang", "Zhi", ""], ["Su", "Ke", ""], ["Sun", "Lifeng", ""]]}, {"id": "1605.07705", "submitter": "Ming Ma", "authors": "Ming Ma and Zhi Wang and Ke Su and Lifeng Sun", "title": "Understanding Content Placement Strategies in Smartrouter-based Peer CDN\n  for Video Streaming", "comments": "arXiv admin note: text overlap with arXiv:1605.07704", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a new video delivery paradigm: smartrouter-based\npeer video content delivery network, which is enabled by smartrouters deployed\nat users' homes. ChinaCache (one of the largest CDN providers in China) and\nYouku (a video provider using smartrouters to assist video delivery) announced\ntheir cooperation in 2015, to create a new paradigm of content delivery based\non householders' network resources. This new paradigm is different from the\nconventional peer-to-peer (P2P) approach, because millions of dedicated\nsmartrouters are operated by the centralized video service providers in a\ncoordinative manner. Thus it is intriguing to study the content placement\nstrategies used in a smartrouter-based content delivery system, as well as its\npotential impact on the content delivery ecosystem. In this paper, we carry out\nmeasurement studies of Youku's peer video CDN, who has deployed over 300K\nsmartrouter devices for its video delivery. In our measurement studies, 104K\nvideos were investigated and 4TB traffic has been analyzed, over controlled\nsmartrouter nodes and players. Our measurement insights are as follows. First,\na global content replication strategy is essential for the peer CDN systems.\nSecond, such peer CDN deployment itself can form an effective sub-system for\nend-to-end QoS monitoring, which can be used for fine-grained request\nredirection (e.g., user-level) and content replication. We also show our\nanalysis on the performance limitations and propose potential improvements to\nthe peer CDN systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 01:51:25 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 00:43:25 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Ma", "Ming", ""], ["Wang", "Zhi", ""], ["Su", "Ke", ""], ["Sun", "Lifeng", ""]]}, {"id": "1605.07946", "submitter": "Raphael Couturier", "authors": "Jean-Fran\\c{c}ois Couchot, Rapha\\\"el Couturier, Christophe Guyeux and\n  Michel Salomon", "title": "Steganalysis via a Convolutional Neural Network using Large Convolution\n  Filters for Embedding Process with Same Stego Key", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the past few years, in the race between image steganography and\nsteganalysis, deep learning has emerged as a very promising alternative to\nsteganalyzer approaches based on rich image models combined with ensemble\nclassifiers. A key knowledge of image steganalyzer, which combines relevant\nimage features and innovative classification procedures, can be deduced by a\ndeep learning approach called Convolutional Neural Networks (CNN). These kind\nof deep learning networks is so well-suited for classification tasks based on\nthe detection of variations in 2D shapes that it is the state-of-the-art in\nmany image recognition problems. In this article, we design a CNN-based\nsteganalyzer for images obtained by applying steganography with a unique\nembedding key. This one is quite different from the previous study of {\\em Qian\net al.} and its successor, namely {\\em Pibre et al.} The proposed architecture\nembeds less convolutions, with much larger filters in the final convolutional\nlayer, and is more general: it is able to deal with larger images and lower\npayloads. For the \"same embedding key\" scenario, our proposal outperforms all\nother steganalyzers, in particular the existing CNN-based ones, and defeats\nmany state-of-the-art image steganography schemes.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 15:58:57 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 06:22:36 GMT"}, {"version": "v3", "created": "Sat, 30 Jul 2016 04:57:11 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Couchot", "Jean-Fran\u00e7ois", ""], ["Couturier", "Rapha\u00ebl", ""], ["Guyeux", "Christophe", ""], ["Salomon", "Michel", ""]]}, {"id": "1605.08247", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka and Yudai Miyashita and Tomoaki Yamabe and Soma\n  Shirakabe and Shin'ichi Sato and Hironori Hoshino and Ryo Kato and Kaori Abe\n  and Takaaki Imanari and Naomichi Kobayashi and Shinichiro Morita and Akio\n  Nakamura", "title": "cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey", "comments": "Survey Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"cvpaper.challenge\" is a group composed of members from AIST, Tokyo Denki\nUniv. (TDU), and Univ. of Tsukuba that aims to systematically summarize papers\non computer vision, pattern recognition, and related fields. For this\nparticular review, we focused on reading the ALL 602 conference papers\npresented at the CVPR2015, the premier annual computer vision event held in\nJune 2015, in order to grasp the trends in the field. Further, we are proposing\n\"DeepSurvey\" as a mechanism embodying the entire process from the reading\nthrough all the papers, the generation of ideas, and to the writing of paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 12:08:55 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Miyashita", "Yudai", ""], ["Yamabe", "Tomoaki", ""], ["Shirakabe", "Soma", ""], ["Sato", "Shin'ichi", ""], ["Hoshino", "Hironori", ""], ["Kato", "Ryo", ""], ["Abe", "Kaori", ""], ["Imanari", "Takaaki", ""], ["Kobayashi", "Naomichi", ""], ["Morita", "Shinichiro", ""], ["Nakamura", "Akio", ""]]}, {"id": "1605.08308", "submitter": "Jiahao Li", "authors": "Jiahao Li, Bin Li, Jizheng Xu, and Ruiqin Xiong", "title": "Efficient Multiple Line-Based Intra Prediction for HEVC", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2633377", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional intra prediction usually utilizes the nearest reference line to\ngenerate the predicted block when considering strong spatial correlation.\nHowever, this kind of single line-based method does not always work well due to\nat least two issues. One is the incoherence caused by the signal noise or the\ntexture of other object, where this texture deviates from the inherent texture\nof the current block. The other reason is that the nearest reference line\nusually has worse reconstruction quality in block-based video coding. Due to\nthese two issues, this paper proposes an efficient multiple line-based intra\nprediction scheme to improve coding efficiency. Besides the nearest reference\nline, further reference lines are also utilized. The further reference lines\nwith relatively higher quality can provide potential better prediction. At the\nsame time, the residue compensation is introduced to calibrate the prediction\nof boundary regions in a block when we utilize further reference lines. To\nspeed up the encoding process, this paper designs several fast algorithms.\nExperimental results show that, compared with HM-16.9, the proposed fast search\nmethod achieves 2.0% bit saving on average and up to 3.7%, with increasing the\nencoding time by 112%.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 14:45:35 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 14:44:48 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Li", "Jiahao", ""], ["Li", "Bin", ""], ["Xu", "Jizheng", ""], ["Xiong", "Ruiqin", ""]]}, {"id": "1605.08470", "submitter": "Rajer Sindhu", "authors": "Rajer Sindhu", "title": "A Feature based Approach for Video Compression", "comments": "Conference on Image Recognition, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a high cost problem for panoramic image stitching via image matching\nalgorithm and not practical for real-time performance. In this paper, we take\nfull advantage ofHarris corner invariant characterization method light\nintensity parallel meaning, translation and rotation, and made a realtime\npanoramic image stitching algorithm. According to the basic characteristics and\nperformance FPGA classical algorithm, several modules such as the feature point\nextraction, and matching description is to optimize the feature-based logic.\nReal-time optimization system to achieve high precision match. The new\nalgorithm process the image from pixel domain and obtained from CCD camera\nXilinx Spartan-6 hardware platform. After the image stitching algorithm, will\neventually form a portable interface to output high-definition content on the\ndisplay. The results showed that, the proposed algorithm has higher precision\nwith good real-time performance and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 23:04:24 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Sindhu", "Rajer", ""]]}, {"id": "1605.08928", "submitter": "Yang Cheng", "authors": "Yang Cheng", "title": "Virtual Reality based Learning Systems", "comments": "Computer Communication Technology, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is based on studies of the existing literature, focusing on the\nstates-of-the-arts on virtual reality (VR) and its potential uses in learning.\nDifferent platforms have been used to improve the learning effects of VR that\noffers exciting opportunities in various fields. As more and more students want\nin a distance, part-time, or want to continue their education, VR has attracted\nconsiderable attention in learning, training, and traditional education. VR\nbased learning enables operators to bring together all disciplinary resources\nin a common playground. The VR base multimedia platform has successfully\ndemonstrated great potential of education and training. In this paper, we will\ndiscuss existing systems and their uses and address the technical challenges\nand future directions.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 19:29:13 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Cheng", "Yang", ""]]}, {"id": "1605.08969", "submitter": "Chenglei Wu", "authors": "Chenglei Wu, Zhi Wang, Jiangchuan Liu, Shiqiang Yang", "title": "Improving Crowdsourced Live Streaming with Aggregated Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a dramatic increase of user-generated video\nservices. In such user-generated video services, crowdsourced live streaming\n(e.g., Periscope, Twitch) has significantly challenged today's edge network\ninfrastructure: today's edge networks (e.g., 4G, Wi-Fi) have limited uplink\ncapacity support, making high-bitrate live streaming over such links\nfundamentally impossible. In this paper, we propose to let broadcasters (i.e.,\nusers who generate the video) upload crowdsourced video streams using\naggregated network resources from multiple edge networks. There are several\nchallenges in the proposal: First, how to design a framework that aggregates\nbandwidth from multiple edge networks? Second, how to make this framework\ntransparent to today's crowdsourced live streaming services? Third, how to\nmaximize the streaming quality for the whole system? We design a\nmulti-objective and deployable bandwidth aggregation system BASS to address\nthese challenges: (1) We propose an aggregation framework transparent to\ntoday's crowdsourced live streaming services, using an edge proxy box and\naggregation cloud paradigm; (2) We dynamically allocate geo-distributed cloud\naggregation servers to enable MPTCP (i.e., multi-path TCP), according to\nlocation and network characteristics of both broadcasters and the original\nstreaming servers; (3) We maximize the overall performance gain for the whole\nsystem, by matching streams with the best aggregation paths.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 06:03:26 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Wu", "Chenglei", ""], ["Wang", "Zhi", ""], ["Liu", "Jiangchuan", ""], ["Yang", "Shiqiang", ""]]}, {"id": "1605.09211", "submitter": "Brendan Jou", "authors": "Brendan Jou and Shih-Fu Chang", "title": "Going Deeper for Multilingual Visual Sentiment Detection", "comments": "technical report, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report details several improvements to the visual concept\ndetector banks built on images from the Multilingual Visual Sentiment Ontology\n(MVSO). The detector banks are trained to detect a total of 9,918\nsentiment-biased visual concepts from six major languages: English, Spanish,\nItalian, French, German and Chinese. In the original MVSO release,\nadjective-noun pair (ANP) detectors were trained for the six languages using an\nAlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a\nmore extensive set of experiments, parameter tuning, and training runs, we\ndetail and release higher accuracy models for detecting ANPs across six\nlanguages from the same image pool and setting as in the original release using\na more modern architecture, GoogLeNet, providing comparable or better\nperformance with reduced network parameter cost.\n  In addition, since the image pool in MVSO can be corrupted by user noise from\nsocial interactions, we partitioned out a sub-corpus of MVSO images based on\ntag-restricted queries for higher fidelity labels. We show that as a result of\nthese higher fidelity labels, higher performing AlexNet-styled ANP detectors\ncan be trained using the tag-restricted image subset as compared to the models\nin full corpus. We release all these newly trained models for public research\nuse along with the list of tag-restricted images from the MVSO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 12:57:44 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1605.09425", "submitter": "Jenny Lam", "authors": "David Eppstein, Michael T. Goodrich, Jenny Lam, Nil Mamano, Michael\n  Mitzenmacher, Manuel Torres", "title": "Models and Algorithms for Graph Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce models and algorithmic foundations for graph watermarking. Our\nframeworks include security definitions and proofs, as well as\ncharacterizations when graph watermarking is algorithmically feasible, in spite\nof the fact that the general problem is NP-complete by simple reductions from\nthe subgraph isomorphism or graph edit distance problems. In the digital\nwatermarking of many types of files, an implicit step in the recovery of a\nwatermark is the mapping of individual pieces of data, such as image pixels or\nmovie frames, from one object to another. In graphs, this step corresponds to\napproximately matching vertices of one graph to another based on graph\ninvariants such as vertex degree. Our approach is based on characterizing the\nfeasibility of graph watermarking in terms of keygen, marking, and\nidentification functions defined over graph families with known distributions.\nWe demonstrate the strength of this approach with exemplary watermarking\nschemes for two random graph models, the classic Erd\\H{o}s-R\\'{e}nyi model and\na random power-law graph model, both of which are used to model real-world\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 21:46:31 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Lam", "Jenny", ""], ["Mamano", "Nil", ""], ["Mitzenmacher", "Michael", ""], ["Torres", "Manuel", ""]]}, {"id": "1605.09486", "submitter": "Chenglei Wu", "authors": "Chenglei Wu, Zhi Wang, Shiqiang Yang", "title": "Drone Streaming with Wi-Fi Grid Aggregation for Virtual Tour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide a live, active and high-quality virtual touring streaming\nexperience, we propose an unmanned drone stereoscopic streaming paradigm using\na control and streaming infrastructure of a 2.4GHz Wi-Fi grid. Our system\nallows users to actively control the streaming captured by a drone, receive and\nwatch the streaming using a head mount display (HMD); a Wi-Fi grid is deployed\nacross the remote scene with multi-channel support to enable high-bitrate\nstream- ing broadcast from the drones. The system adopt a joint view adaptation\nand drone control scheme to enable fast viewer movement including both head\nrotation and touring. We implement the prototype on Dji M100 quadcopter and HTC\nVive in a demo scene.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 03:59:03 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Wu", "Chenglei", ""], ["Wang", "Zhi", ""], ["Yang", "Shiqiang", ""]]}]