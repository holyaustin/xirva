[{"id": "1709.00275", "submitter": "Onur G\\\"unl\\\"u", "authors": "Onur G\\\"unl\\\"u, Onurcan \\.I\\c{s}can, Vladimir Sidorenko, and Gerhard\n  Kramer", "title": "Code Constructions for Physical Unclonable Functions and Biometric\n  Secrecy Systems", "comments": "To appear in IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM eess.SP math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-terminal key agreement problem with biometric or physical identifiers\nis considered. Two linear code constructions based on Wyner-Ziv coding are\ndeveloped. The first construction uses random linear codes and achieves all\npoints of the key-leakage-storage regions of the generated-secret and\nchosen-secret models. The second construction uses nested polar codes for\nvector quantization during enrollment and for error correction during\nreconstruction. Simulations show that the nested polar codes achieve\nprivacy-leakage and storage rates that improve on existing code designs. One\nproposed code achieves a rate tuple that cannot be achieved by existing\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 12:44:01 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 09:21:01 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 07:16:14 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["\u0130\u015fcan", "Onurcan", ""], ["Sidorenko", "Vladimir", ""], ["Kramer", "Gerhard", ""]]}, {"id": "1709.00354", "submitter": "Chia-Wei Ao", "authors": "Chia-Wei Ao, Hung-yi Lee", "title": "Query-by-example Spoken Term Detection using Attention-based Multi-hop\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving spoken content with spoken queries, or query-by- example spoken\nterm detection (STD), is attractive because it makes possible the matching of\nsignals directly on the acoustic level without transcribing them into text.\nHere, we propose an end-to-end query-by-example STD model based on an\nattention-based multi-hop network, whose input is a spoken query and an audio\nsegment containing several utterances; the output states whether the audio\nsegment includes the query. The model can be trained in either a supervised\nscenario using labeled data, or in an unsupervised fashion. In the supervised\nscenario, we find that the attention mechanism and multiple hops improve\nperformance, and that the attention weights indicate the time span of the\ndetected terms. In the unsupervised setting, the model mimics the behavior of\nthe existing query-by-example STD system, yielding performance comparable to\nthe existing system but with a lower search time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 14:56:53 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 08:33:39 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Ao", "Chia-Wei", ""], ["Lee", "Hung-yi", ""]]}, {"id": "1709.00649", "submitter": "Sebastian Wagner-Carena", "authors": "Max Hopkins, Michael Mitzenmacher, and Sebastian Wagner-Carena", "title": "Simulated Annealing for JPEG Quantization", "comments": "Appendix not included in arXiv version due to size restrictions. For\n  full paper go to:\n  http://www.eecs.harvard.edu/~michaelm/SimAnneal/PAPER/simulated-annealing-jpeg.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG is one of the most widely used image formats, but in some ways remains\nsurprisingly unoptimized, perhaps because some natural optimizations would go\noutside the standard that defines JPEG. We show how to improve JPEG compression\nin a standard-compliant, backward-compatible manner, by finding improved\ndefault quantization tables. We describe a simulated annealing technique that\nhas allowed us to find several quantization tables that perform better than the\nindustry standard, in terms of both compressed size and image fidelity.\nSpecifically, we derive tables that reduce the FSIM error by over 10% while\nimproving compression by over 20% at quality level 95 in our tests; we also\nprovide similar results for other quality levels. While we acknowledge our\napproach can in some images lead to visible artifacts under large\nmagnification, we believe use of these quantization tables, or additional\ntables that could be found using our methodology, would significantly reduce\nJPEG file sizes with improved overall image quality.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 01:10:18 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hopkins", "Max", ""], ["Mitzenmacher", "Michael", ""], ["Wagner-Carena", "Sebastian", ""]]}, {"id": "1709.00944", "submitter": "Jen-Cheng Hou", "authors": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang,\n  Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional\n  Neural Network", "comments": "This paper is the same as arXiv:1703.10893v2. Apologies for the\n  inconvenience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus on addressing audio information only. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. In the proposed AVDCNN SE model, audio\nand visual data are first processed using individual CNNs, and then, fused into\na joint network to generate enhanced speech at the output layer. The AVDCNN\nmodel is trained in an end-to-end manner, and parameters are jointly learned\nthrough back-propagation. We evaluate enhanced speech using five objective\ncriteria. Results show that the AVDCNN yields notably better performance,\ncompared with an audio-only CNN-based SE model and two conventional SE\napproaches, confirming the effectiveness of integrating visual information into\nthe SE process.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 14:17:53 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 16:06:28 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Hou", "Jen-Cheng", ""], ["Wang", "Syu-Siang", ""], ["Lai", "Ying-Hui", ""], ["Tsao", "Yu", ""], ["Chang", "Hsiu-Wen", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1709.01116", "submitter": "Dimitrios Adamos Dr", "authors": "Fotis Kalaganis (1), Dimitrios A. Adamos (2 and 3), Nikos Laskaris (1\n  and 3) ((1) AIIA Lab, Department of Informatics, Aristotle University of\n  Thessaloniki, (2) School of Music Studies, Aristotle University of\n  Thessaloniki, (3) Neuroinformatics GRoup, Aristotle University of\n  Thessaloniki)", "title": "Musical NeuroPicks: a consumer-grade BCI for on-demand music streaming\n  services", "comments": null, "journal-ref": "Neurocomputing 2017", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the possibility of using a machine-learning scheme in\nconjunction with commercial wearable EEG-devices for translating listener's\nsubjective experience of music into scores that can be used in popular\non-demand music streaming services. Our study resulted into two variants,\ndiffering in terms of performance and execution time, and hence, subserving\ndistinct applications in online streaming music platforms. The first method,\nNeuroPicks, is extremely accurate but slower. It is based on the\nwell-established neuroscientific concepts of brainwave frequency bands,\nactivation asymmetry index and cross frequency coupling (CFC). The second\nmethod, NeuroPicksVQ, offers prompt predictions of lower credibility and relies\non a custom-built version of vector quantization procedure that facilitates a\nnovel parameterization of the music-modulated brainwaves. Beyond the feature\nengineering step, both methods exploit the inherent efficiency of extreme\nlearning machines (ELMs) so as to translate, in a personalized fashion, the\nderived patterns into a listener's score. NeuroPicks method may find\napplications as an integral part of contemporary music recommendation systems,\nwhile NeuroPicksVQ can control the selection of music tracks. Encouraging\nexperimental results, from a pragmatic use of the systems, are presented.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 18:55:35 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kalaganis", "Fotis", "", "2 and 3"], ["Adamos", "Dimitrios A.", "", "2 and 3"], ["Laskaris", "Nikos", "", "1\n  and 3"]]}, {"id": "1709.01295", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Isht Dwivedi, Abhijat Biswas, Sahil\n  Manocha, R. Venkatesh Babu", "title": "SketchParse : Towards Rich Descriptions for Poorly Drawn Sketches using\n  Multi-Task Hierarchical Deep Networks", "comments": "A shorter version of this submission was accepted at ACM Multimedia\n  (ACMMM) 2017. Code, annotated datasets and pre-trained models available at\n  https://github.com/val-iisc/sketch-parse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to semantically interpret hand-drawn line sketches, although very\nchallenging, can pave way for novel applications in multimedia. We propose\nSketchParse, the first deep-network architecture for fully automatic parsing of\nfreehand object sketches. SketchParse is configured as a two-level fully\nconvolutional network. The first level contains shared layers common to all\nobject categories. The second level contains a number of expert sub-networks.\nEach expert specializes in parsing sketches from object categories which\ncontain structurally similar parts. Effectively, the two-level configuration\nenables our architecture to scale up efficiently as additional categories are\nadded. We introduce a router layer which (i) relays sketch features from shared\nlayers to the correct expert (ii) eliminates the need to manually specify\nobject category during inference. To bypass laborious part-level annotation, we\nsketchify photos from semantic object-part image datasets and use them for\ntraining. Our architecture also incorporates object pose prediction as a novel\nauxiliary task which boosts overall performance while providing supplementary\ninformation regarding the sketch. We demonstrate SketchParse's abilities (i) on\ntwo challenging large-scale sketch datasets (ii) in parsing unseen,\nsemantically related object categories (iii) in improving fine-grained\nsketch-based image retrieval. As a novel application, we also outline how\nSketchParse's output can be used to generate caption-style descriptions for\nhand-drawn sketches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:10:59 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Dwivedi", "Isht", ""], ["Biswas", "Abhijat", ""], ["Manocha", "Sahil", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1709.01784", "submitter": "Xin Ji", "authors": "Xin Ji, Wei Wang, Meihui Zhang, Yang Yang", "title": "Cross-Domain Image Retrieval with Attention Modeling", "comments": "8 pages with an extra reference page", "journal-ref": "2017 ACM Multimedia Conference", "doi": "10.1145/3123266.3123429", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the proliferation of e-commerce websites and the ubiquitousness of smart\nphones, cross-domain image retrieval using images taken by smart phones as\nqueries to search products on e-commerce websites is emerging as a popular\napplication. One challenge of this task is to locate the attention of both the\nquery and database images. In particular, database images, e.g. of fashion\nproducts, on e-commerce websites are typically displayed with other\naccessories, and the images taken by users contain noisy background and large\nvariations in orientation and lighting. Consequently, their attention is\ndifficult to locate. In this paper, we exploit the rich tag information\navailable on the e-commerce websites to locate the attention of database\nimages. For query images, we use each candidate image in the database as the\ncontext to locate the query attention. Novel deep convolutional neural network\narchitectures, namely TagYNet and CtxYNet, are proposed to learn the attention\nweights and then extract effective representations of the images. Experimental\nresults on public datasets confirm that our approaches have significant\nimprovement over the existing methods in terms of the retrieval accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:49:46 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ji", "Xin", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Yang", "Yang", ""]]}, {"id": "1709.02251", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Qin Jin", "title": "Multi-modal Conditional Attention Fusion for Dimensional Emotion\n  Prediction", "comments": "Appeared at ACM Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous dimensional emotion prediction is a challenging task where the\nfusion of various modalities usually achieves state-of-the-art performance such\nas early fusion or late fusion. In this paper, we propose a novel multi-modal\nfusion strategy named conditional attention fusion, which can dynamically pay\nattention to different modalities at each time step. Long-short term memory\nrecurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model\nto capture long time dependencies. The weights assigned to different modalities\nare automatically decided by the current input features and recent history\ninformation rather than being fixed at any kinds of situation. Our experimental\nresults on a benchmark dataset AVEC2015 show the effectiveness of our method\nwhich outperforms several common fusion strategies for valence prediction.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 12:05:47 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Chen", "Shizhe", ""], ["Jin", "Qin", ""]]}, {"id": "1709.02908", "submitter": "Haodong Li", "authors": "Bolin Chen, Haodong Li, Weiqi Luo", "title": "Image Processing Operations Identification via Convolutional Neural\n  Network", "comments": null, "journal-ref": "Sci. China Inf. Sci. 63, 139109 (2020)", "doi": "10.1007/s11432-018-9492-6", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, image forensics has attracted more and more attention, and\nmany forensic methods have been proposed for identifying image processing\noperations. Up to now, most existing methods are based on hand crafted\nfeatures, and just one specific operation is considered in their methods. In\nmany forensic scenarios, however, multiple classification for various image\nprocessing operations is more practical. Besides, it is difficult to obtain\neffective features by hand for some image processing operations. In this paper,\ntherefore, we propose a new convolutional neural network (CNN) based method to\nadaptively learn discriminative features for identifying typical image\nprocessing operations. We carefully design the high pass filter bank to get the\nimage residuals of the input image, the channel expansion layer to mix up the\nresulting residuals, the pooling layers, and the activation functions employed\nin our method. The extensive results show that the proposed method can\noutperform the currently best method based on hand crafted features and three\nrelated methods based on CNN for image steganalysis and/or forensics, achieving\nthe state-of-the-art results. Furthermore, we provide more supplementary\nresults to show the rationality and robustness of the proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 04:34:48 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chen", "Bolin", ""], ["Li", "Haodong", ""], ["Luo", "Weiqi", ""]]}, {"id": "1709.03020", "submitter": "Shadrokh Samavi", "authors": "Majid Mohrekesh, Shekoofeh Azizi, Shahram Shirani, Nader Karimi, and\n  Shadrokh Samavi", "title": "Hierarchical Watermarking Framework Based on Analysis of Local\n  Complexity Variations", "comments": "12 pages, 14 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing production and exchange of multimedia content has increased the\nneed for better protection of copyright by means of watermarking. Different\nmethods have been proposed to satisfy the tradeoff between imperceptibility and\nrobustness as two important characteristics in watermarking while maintaining\nproper data-embedding capacity. Many watermarking methods use image independent\nset of parameters. Different images possess different potentials for robust and\ntransparent hosting of watermark data. To overcome this deficiency, in this\npaper we have proposed a new hierarchical adaptive watermarking framework. At\nthe higher level of hierarchy, complexity of an image is ranked in comparison\nwith complexities of images of a dataset. For a typical dataset of images, the\nstatistical distribution of block complexities is found. At the lower level of\nthe hierarchy, for a single cover image that is to be watermarked, complexities\nof blocks can be found. Local complexity variation (LCV) among a block and its\nneighbors is used to adaptively control the watermark strength factor of each\nblock. Such local complexity analysis creates an adaptive embedding scheme,\nwhich results in higher transparency by reducing blockiness effects. This two\nlevel hierarchy has enabled our method to take advantage of all image blocks to\nelevate the embedding capacity while preserving imperceptibility. For testing\nthe effectiveness of the proposed framework, contourlet transform (CT) in\nconjunction with discrete cosine transform (DCT) is used to embed pseudo-random\nbinary sequences as watermark. Experimental results show that the proposed\nframework elevates the performance the watermarking routine in terms of both\nrobustness and transparency.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 00:40:14 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Mohrekesh", "Majid", ""], ["Azizi", "Shekoofeh", ""], ["Shirani", "Shahram", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1709.03708", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Keisuke Ogaki, Toshihiko Yamasaki, Kiyoharu Aizawa", "title": "PQk-means: Billion-scale Clustering for Product-quantized Codes", "comments": "To appear in ACMMM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a fundamental operation in data analysis. For handling\nlarge-scale data, the standard k-means clustering method is not only slow, but\nalso memory-inefficient. We propose an efficient clustering method for\nbillion-scale feature vectors, called PQk-means. By first compressing input\nvectors into short product-quantized (PQ) codes, PQk-means achieves fast and\nmemory-efficient clustering, even for high-dimensional vectors. Similar to\nk-means, PQk-means repeats the assignment and update steps, both of which can\nbe performed in the PQ-code domain. Experimental results show that even\nshort-length (32 bit) PQ-codes can produce competitive results compared with\nk-means. This result is of practical importance for clustering in\nmemory-restricted environments. Using the proposed PQk-means scheme, the\nclustering of one billion 128D SIFT features with K = 10^5 is achieved within\n14 hours, using just 32 GB of memory consumption on a single computer.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 07:00:18 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Matsui", "Yusuke", ""], ["Ogaki", "Keisuke", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1709.03946", "submitter": "Nikhita Vedula", "authors": "Nikhita Vedula, Wei Sun, Hyunhwan Lee, Harsh Gupta, Mitsunori Ogihara,\n  Joseph Johnson, Gang Ren, Srinivasan Parthasarathy", "title": "Multimodal Content Analysis for Effective Advertisements on YouTube", "comments": "11 pages, 5 figures, ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advances in e-commerce and Web 2.0 technologies have greatly\nincreased the impact of commercial advertisements on the general public. As a\nkey enabling technology, a multitude of recommender systems exists which\nanalyzes user features and browsing patterns to recommend appealing\nadvertisements to users. In this work, we seek to study the characteristics or\nattributes that characterize an effective advertisement and recommend a useful\nset of features to aid the designing and production processes of commercial\nadvertisements. We analyze the temporal patterns from multimedia content of\nadvertisement videos including auditory, visual and textual components, and\nstudy their individual roles and synergies in the success of an advertisement.\nThe objective of this work is then to measure the effectiveness of an\nadvertisement, and to recommend a useful set of features to advertisement\ndesigners to make it more successful and approachable to users. Our proposed\nframework employs the signal processing technique of cross modality feature\nlearning where data streams from different components are employed to train\nseparate neural network models and are then fused together to learn a shared\nrepresentation. Subsequently, a neural network model trained on this joint\nfeature embedding representation is utilized as a classifier to predict\nadvertisement effectiveness. We validate our approach using subjective ratings\nfrom a dedicated user study, the sentiment strength of online viewer comments,\nand a viewer opinion metric of the ratio of the Likes and Views received by\neach advertisement from an online platform.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 16:47:21 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Vedula", "Nikhita", ""], ["Sun", "Wei", ""], ["Lee", "Hyunhwan", ""], ["Gupta", "Harsh", ""], ["Ogihara", "Mitsunori", ""], ["Johnson", "Joseph", ""], ["Ren", "Gang", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1709.04427", "submitter": "Gang Cao Dr.", "authors": "Gang Cao, Lihui Huang, Huawei Tian, Xianglin Huang, Yongbin Wang,\n  Ruicong Zhi", "title": "Contrast Enhancement of Brightness-Distorted Images by Improved Adaptive\n  Gamma Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an efficient image contrast enhancement (CE) tool, adaptive gamma\ncorrection (AGC) was previously proposed by relating gamma parameter with\ncumulative distribution function (CDF) of the pixel gray levels within an\nimage. ACG deals well with most dimmed images, but fails for globally bright\nimages and the dimmed images with local bright regions. Such two categories of\nbrightness-distorted images are universal in real scenarios, such as improper\nexposure and white object regions. In order to attenuate such deficiencies,\nhere we propose an improved AGC algorithm. The novel strategy of negative\nimages is used to realize CE of the bright images, and the gamma correction\nmodulated by truncated CDF is employed to enhance the dimmed ones. As such,\nlocal over-enhancement and structure distortion can be alleviated. Both\nqualitative and quantitative experimental results show that our proposed method\nyields consistently good CE results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 17:17:01 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Cao", "Gang", ""], ["Huang", "Lihui", ""], ["Tian", "Huawei", ""], ["Huang", "Xianglin", ""], ["Wang", "Yongbin", ""], ["Zhi", "Ruicong", ""]]}, {"id": "1709.04583", "submitter": "Gang Cao", "authors": "Gang Cao, Huawei Tian, Lifang Yu, Xianglin Huang, Yongbin Wang", "title": "Acceleration of Histogram-Based Contrast Enhancement via Selective\n  Downsampling", "comments": "accepted by IET Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework to accelerate the universal\nhistogram-based image contrast enhancement (CE) algorithms. Both spatial and\ngray-level selective down- sampling of digital images are adopted to decrease\ncomputational cost, while the visual quality of enhanced images is still\npreserved and without apparent degradation. Mapping function calibration is\nnovelly proposed to reconstruct the pixel mapping on the gray levels missed by\ndownsampling. As two case studies, accelerations of histogram equalization (HE)\nand the state-of-the-art global CE algorithm, i.e., spatial mutual information\nand PageRank (SMIRANK), are presented detailedly. Both quantitative and\nqualitative assessment results have verified the effectiveness of our proposed\nCE acceleration framework. In typical tests, computational efficiencies of HE\nand SMIRANK have been speeded up by about 3.9 and 13.5 times, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 01:50:27 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 08:45:10 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Cao", "Gang", ""], ["Tian", "Huawei", ""], ["Yu", "Lifang", ""], ["Huang", "Xianglin", ""], ["Wang", "Yongbin", ""]]}, {"id": "1709.05737", "submitter": "Dong Liu", "authors": "Rui Song, Dong Liu, Houqiang Li, Feng Wu", "title": "Neural network-based arithmetic coding of intra prediction modes in HEVC", "comments": "VCIP 2017", "journal-ref": null, "doi": "10.1109/VCIP.2017.8305104", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both H.264 and HEVC, context-adaptive binary arithmetic coding (CABAC) is\nadopted as the entropy coding method. CABAC relies on manually designed\nbinarization processes as well as handcrafted context models, which may\nrestrict the compression efficiency. In this paper, we propose an arithmetic\ncoding strategy by training neural networks, and make preliminary studies on\ncoding of the intra prediction modes in HEVC. Instead of binarization, we\npropose to directly estimate the probability distribution of the 35 intra\nprediction modes with the adoption of a multi-level arithmetic codec. Instead\nof handcrafted context models, we utilize convolutional neural network (CNN) to\nperform the probability estimation. Simulation results show that our proposed\narithmetic coding leads to as high as 9.9% bits saving compared with CABAC.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 01:32:45 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Song", "Rui", ""], ["Liu", "Dong", ""], ["Li", "Houqiang", ""], ["Wu", "Feng", ""]]}, {"id": "1709.05861", "submitter": "Narotam Singh", "authors": "Narotam Singh (1), Nittin Singh (1), Abhinav Dhall (1) ((1) Indian\n  Institute of Technology Ropar)", "title": "Continuous Multimodal Emotion Recognition Approach for AVEC 2017", "comments": "4 pages, 3 figures, arXiv:1605.06778, arXiv:1512.03385", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the analysis of audio and visual features in predicting\nthe continuous emotion dimensions under the seventh Audio/Visual Emotion\nChallenge (AVEC 2017), which was done as part of a B.Tech. 2nd year internship\nproject. For visual features we used the HOG (Histogram of Gradients) features,\nFisher encodings of SIFT (Scale-Invariant Feature Transform) features based on\nGaussian mixture model (GMM) and some pretrained Convolutional Neural Network\nlayers as features; all these extracted for each video clip. For audio features\nwe used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level\ndescriptors) generated by openXBOW provided by the organisers of the event.\nThen we trained fully connected neural network regression model on the dataset\nfor all these different modalities. We applied multimodal fusion on the output\nmodels to get the Concordance correlation coefficient on Development set as\nwell as Test set.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:01:43 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 12:08:09 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Singh", "Narotam", ""], ["Singh", "Nittin", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1709.05865", "submitter": "Shubham Dham", "authors": "Shubham Dham, Anirudh Sharma, Abhinav Dhall", "title": "Depression Scale Recognition from Audio, Visual and Text Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression is a major mental health disorder that is rapidly affecting lives\nworldwide. Depression not only impacts emotional but also physical and\npsychological state of the person. Its symptoms include lack of interest in\ndaily activities, feeling low, anxiety, frustration, loss of weight and even\nfeeling of self-hatred. This report describes work done by us for Audio Visual\nEmotion Challenge (AVEC) 2017 during our second year BTech summer internship.\nWith the increase in demand to detect depression automatically with the help of\nmachine learning algorithms, we present our multimodal feature extraction and\ndecision level fusion approach for the same. Features are extracted by\nprocessing on the provided Distress Analysis Interview Corpus-Wizard of Oz\n(DAIC-WOZ) database. Gaussian Mixture Model (GMM) clustering and Fisher vector\napproach were applied on the visual data; statistical descriptors on gaze,\npose; low level audio features and head pose and text features were also\nextracted. Classification is done on fused as well as independent features\nusing Support Vector Machine (SVM) and neural networks. The results obtained\nwere able to cross the provided baseline on validation data set by 17% on audio\nfeatures and 24.5% on video features.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:26:01 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Dham", "Shubham", ""], ["Sharma", "Anirudh", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1709.06204", "submitter": "Jungseock Joo", "authors": "Donghyeon Won, Zachary C. Steinert-Threlkeld, Jungseock Joo", "title": "Protest Activity Detection and Perceived Violence Estimation from Social\n  Media Images", "comments": "To appear in Proceedings of the 25th ACM International Conference on\n  Multimedia 2017 (full research paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel visual model which can recognize protesters, describe\ntheir activities by visual attributes and estimate the level of perceived\nviolence in an image. Studies of social media and protests use natural language\nprocessing to track how individuals use hashtags and links, often with a focus\non those items' diffusion. These approaches, however, may not be effective in\nfully characterizing actual real-world protests (e.g., violent or peaceful) or\nestimating the demographics of participants (e.g., age, gender, and race) and\ntheir emotions. Our system characterizes protests along these dimensions. We\nhave collected geotagged tweets and their images from 2013-2017 and analyzed\nmultiple major protest events in that period. A multi-task convolutional neural\nnetwork is employed in order to automatically classify the presence of\nprotesters in an image and predict its visual attributes, perceived violence\nand exhibited emotions. We also release the UCLA Protest Image Dataset, our\nnovel dataset of 40,764 images (11,659 protest images and hard negatives) with\nvarious annotations of visual attributes and sentiments. Using this dataset, we\ntrain our model and demonstrate its effectiveness. We also present experimental\nresults from various analysis on geotagged image data in several prevalent\nprotest events. Our dataset will be made accessible at\nhttps://www.sscnet.ucla.edu/comm/jjoo/mm-protest/.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 23:57:42 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Won", "Donghyeon", ""], ["Steinert-Threlkeld", "Zachary C.", ""], ["Joo", "Jungseock", ""]]}, {"id": "1709.06536", "submitter": "S.M.Reza Soroushmehr", "authors": "Maedeh Jamali, Shima Rafiei, S.M. Reza Soroushmehr, Nader Karimi,\n  Shahram Shirani, Kayvan Najarian, Shadrokh Samavi", "title": "Adaptive Blind Image Watermarking Using Fuzzy Inference System Based on\n  Human Visual Perception", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": "10.3233/JIFS-171805", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of digital content has increased the necessity of copyright\nprotection by means of watermarking. Imperceptibility and robustness are two\nimportant features of watermarking algorithms. The goal of watermarking methods\nis to satisfy the tradeoff between these two contradicting characteristics.\nRecently watermarking methods in transform domains have displayed favorable\nresults. In this paper, we present an adaptive blind watermarking method which\nhas high transparency in areas that are important to human visual system. We\npropose a fuzzy system for adaptive control of the embedding strength factor.\nFeatures such as saliency, intensity, and edge-concentration, are used as fuzzy\nattributes. Redundant embedding in discrete cosine transform (DCT) of wavelet\ndomain has increased the robustness of our method. Experimental results show\nthe efficiency of the proposed method and better results are obtained as\ncompared to comparable methods with same size of watermark logo.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 19:17:46 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 15:08:55 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 18:18:46 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Jamali", "Maedeh", ""], ["Rafiei", "Shima", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Shirani", "Shahram", ""], ["Najarian", "Kayvan", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1709.06729", "submitter": "Kazem Qazanfari", "authors": "Kazem Qazanfari, Reza Safabaksh", "title": "A new adaptive method for hiding data in images", "comments": "6 pages, in Persian, Proceedings of the 6th Iranian Conference on\n  Machine Vision and Image Processing, Tehran, Iran 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSB method is one of the well-known steganography methods which hides the\nmessage bits into the least significant bit of pixel values. This method\nchanges the statistical information of images, which causes to have an\nunsecured channel. To increase the security of this method against the\nsteganalysis methods, in this paper an adaptive method for hiding data into\nimages will be proposed. So, the amount of data and the method which is used\nfor hiding data in each area of image will be different. Experimental results\nshow that the security of the proposed method is higher than general LSB method\nand in some cases the capacity of the carrier signal is increased.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 05:14:54 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Qazanfari", "Kazem", ""], ["Safabaksh", "Reza", ""]]}, {"id": "1709.06734", "submitter": "Ren Yang", "authors": "Ren Yang, Mai Xu, Tie Liu, Zulin Wang and Zhenyu Guan", "title": "Enhancing Quality for HEVC Compressed Videos", "comments": "Submitted to IEEE T-CSVT", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology\n  (2018)", "doi": "10.1109/TCSVT.2018.2867568", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest High Efficiency Video Coding (HEVC) standard has been increasingly\napplied to generate video streams over the Internet. However, HEVC compressed\nvideos may incur severe quality degradation, particularly at low bit-rates.\nThus, it is necessary to enhance the visual quality of HEVC videos at the\ndecoder side. To this end, this paper proposes a Quality Enhancement\nConvolutional Neural Network (QE-CNN) method that does not require any\nmodification of the encoder to achieve quality enhancement for HEVC. In\nparticular, our QE-CNN method learns QE-CNN-I and QE-CNN-P models to reduce the\ndistortion of HEVC I and P frames, respectively. The proposed method differs\nfrom the existing CNN-based quality enhancement approaches, which only handle\nintra-coding distortion and are thus not suitable for P frames. Our\nexperimental results validate that our QE-CNN method is effective in enhancing\nquality for both I and P frames of HEVC videos. To apply our QE-CNN method in\ntime-constrained scenarios, we further propose a Time-constrained Quality\nEnhancement Optimization (TQEO) scheme. Our TQEO scheme controls the\ncomputational time of QE-CNN to meet a target, meanwhile maximizing the quality\nenhancement. Next, the experimental results demonstrate the effectiveness of\nour TQEO scheme from the aspects of time control accuracy and quality\nenhancement under different time constraints. Finally, we design a prototype to\nimplement our TQEO scheme in a real-time scenario.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 06:36:14 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 13:20:51 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Yang", "Ren", ""], ["Xu", "Mai", ""], ["Liu", "Tie", ""], ["Wang", "Zulin", ""], ["Guan", "Zhenyu", ""]]}, {"id": "1709.07200", "submitter": "Valentin Vielzeuf", "authors": "Valentin Vielzeuf, St\\'ephane Pateux, Fr\\'ed\\'eric Jurie", "title": "Temporal Multimodal Fusion for Video Emotion Classification in the Wild", "comments": null, "journal-ref": "ACM - ICMI 2017, Nov 2017, Glasgow, United Kingdom", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the question of emotion classification. The task\nconsists in predicting emotion labels (taken among a set of possible labels)\nbest describing the emotions contained in short video clips. Building on a\nstandard framework -- lying in describing videos by audio and visual features\nused by a supervised classifier to infer the labels -- this paper investigates\nseveral novel directions. First of all, improved face descriptors based on 2D\nand 3D Convo-lutional Neural Networks are proposed. Second, the paper explores\nseveral fusion methods, temporal and multimodal, including a novel hierarchical\nmethod combining features and scores. In addition, we carefully reviewed the\ndifferent stages of the pipeline and designed a CNN architecture adapted to the\ntask; this is important as the size of the training set is small compared to\nthe difficulty of the problem, making generalization difficult. The so-obtained\nmodel ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of\n58.8 %.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 08:14:40 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Pateux", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1709.08084", "submitter": "Hamzeh Ghasemzadeh", "authors": "Hamzeh Ghasemzadeh", "title": "Calibrated steganalysis of mp3stego in multi-encoder scenario", "comments": "8 pages, 4 Tables, 10 Figures, Submitted journal paper", "journal-ref": null, "doi": "10.1016/j.ins.2018.12.035", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing popularity of mp3 and wave with the amount of works published on\neach of them shows mp3 steganalysis has not found adequate attention.\nFurthermore, investigating existing works on mp3 steganalysis shows that a\nmajor factor has been overlooked. Experimenting with different mp3 encoders\nshows there are subtle differences in their outputs. This shows that mp3\nstandard has been implemented in dissimilar fashions, which in turn could\ndegrade performance of steganalysis if it is not addressed properly.\nAdditionally, calibration is a powerful technique which has not found its true\npotential for mp3 steganalysis. This paper tries to fill these gaps. First, we\npresent our analysis on different encoders and show they can be classified\nquite accurately with only four features. Then, we propose a new set of\ncalibrated features based on quantization step. To that end, we show\nquantization step is a band limited signal and steganography noise affects its\nhigh frequency components more prominently. By applying a low pass filter on\nquantization steps, we arrive at an estimation of quantization step, which in\nturn is used for calibrating the features.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 17:38:58 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Ghasemzadeh", "Hamzeh", ""]]}, {"id": "1709.08462", "submitter": "Chuanmin Jia", "authors": "Chuanmin Jia, Shiqi Wang, Xinfeng Zhang, Shanshe Wang, Siwei Ma", "title": "Spatial-Temporal Residue Network Based In-Loop Filter for Video Coding", "comments": "4 pages, 2 figures, accepted by VCIP2017", "journal-ref": null, "doi": "10.1109/VCIP.2017.8305149", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated tremendous break through in the area of\nimage/video processing. In this paper, a spatial-temporal residue network\n(STResNet) based in-loop filter is proposed to suppress visual artifacts such\nas blocking, ringing in video coding. Specifically, the spatial and temporal\ninformation is jointly exploited by taking both current block and co-located\nblock in reference frame into consideration during the processing of in-loop\nfilter. The architecture of STResNet only consists of four convolution layers\nwhich shows hospitality to memory and coding complexity. Moreover, to fully\nadapt the input content and improve the performance of the proposed in-loop\nfilter, coding tree unit (CTU) level control flag is applied in the sense of\nrate-distortion optimization. Extensive experimental results show that our\nscheme provides up to 5.1% bit-rate reduction compared to the state-of-the-art\nvideo coding standard.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 12:58:15 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Jia", "Chuanmin", ""], ["Wang", "Shiqi", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""]]}, {"id": "1709.08763", "submitter": "Chao Chen", "authors": "Chao Chen, Yao-Chung Lin, Anil Kokaram and Steve Benting", "title": "Encoding Bitrate Optimization Using Playback Statistics for HTTP-based\n  Adaptive Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTTP video streaming is in wide use to deliver video over the Internet. With\nHTTP adaptive steaming, a video playback dynamically selects a video stream\nfrom a pre-encoded representation based on available bandwidth and viewport\n(screen) size. The viewer's video quality is therefore influenced by the\nencoded bitrates. We minimize the average delivered bitrate subject to a\nquality lower bound on a per-chunk basis by modeling the probability that a\nplayer selects a particular encoding. Through simulation and real-world\nexperiments, the proposed method saves 9.6% of bandwidth while average\ndelivered video quality comparing with state of the art while keeping average\ndelivered video quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 00:37:58 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 21:50:31 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Chen", "Chao", ""], ["Lin", "Yao-Chung", ""], ["Kokaram", "Anil", ""], ["Benting", "Steve", ""]]}, {"id": "1709.09106", "submitter": "Ryota Hinami", "authors": "Ryota Hinami, Yusuke Matsui, Shin'ichi Satoh", "title": "Region-Based Image Retrieval Revisited", "comments": "To appear in ACM Multimedia 2017 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-based image retrieval (RBIR) technique is revisited. In early attempts\nat RBIR in the late 90s, researchers found many ways to specify region-based\nqueries and spatial relationships; however, the way to characterize the\nregions, such as by using color histograms, were very poor at that time. Here,\nwe revisit RBIR by incorporating semantic specification of objects and\nintuitive specification of spatial relationships. Our contributions are the\nfollowing. First, to support multiple aspects of semantic object specification\n(category, instance, and attribute), we propose a multitask CNN feature that\nallows us to use deep learning technique and to jointly handle multi-aspect\nobject specification. Second, to help users specify spatial relationships among\nobjects in an intuitive way, we propose recommendation techniques of spatial\nrelationships. In particular, by mining the search results, a system can\nrecommend feasible spatial relationships among the objects. The system also can\nrecommend likely spatial relationships by assigned object category names based\non language prior. Moreover, object-level inverted indexing supports very fast\nshortlist generation, and re-ranking based on spatial constraints provides\nusers with instant RBIR experiences.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:09:48 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Hinami", "Ryota", ""], ["Matsui", "Yusuke", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1709.09708", "submitter": "Stefano Ferretti", "authors": "Stefano Ferretti", "title": "On the Complex Network Structure of Musical Pieces: Analysis of Some Use\n  Cases from Different Music Genres", "comments": "accepted to Multimedia Tools and Applications, Springer", "journal-ref": null, "doi": "10.1007/s11042-017-5175-y", "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the modeling of musical melodies as networks. Notes of\na melody can be treated as nodes of a network. Connections are created whenever\nnotes are played in sequence. We analyze some main tracks coming from different\nmusic genres, with melodies played using different musical instruments. We find\nout that the considered networks are, in general, scale free networks and\nexhibit the small world property. We measure the main metrics and assess\nwhether these networks can be considered as formed by sub-communities. Outcomes\nconfirm that peculiar features of the tracks can be extracted from this\nanalysis methodology. This approach can have an impact in several multimedia\napplications such as music didactics, multimedia entertainment, and digital\nmusic generation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 15:04:30 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Ferretti", "Stefano", ""]]}, {"id": "1709.10206", "submitter": "Jun-Ho Choi", "authors": "Jun-Ho Choi, Manri Cheon, Min-Su Choi, Jong-Seok Lee", "title": "Impact of Three-Dimensional Video Scalability on Multi-View Activity\n  Recognition using Deep Learning", "comments": "Accepted as a Thematic Workshops paper at ACM MM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition is one of the important research topics in\ncomputer vision and video understanding. It is often assumed that high quality\nvideo sequences are available for recognition. However, relaxing such a\nrequirement and implementing robust recognition using videos having reduced\ndata rates can achieve efficiency in storing and transmitting video data.\nThree-dimensional video scalability, which refers to the possibility of\nreducing spatial, temporal, and quality resolutions of videos, is an effective\nway for flexible representation and management of video data. In this paper, we\ninvestigate the impact of the video scalability on multi-view activity\nrecognition. We employ both a spatiotemporal feature extraction-based method\nand a deep learning-based method using convolutional and recurrent neural\nnetworks. The recognition performance of the two methods is examined, along\nwith in-depth analysis regarding how their performance vary with respect to\nvarious scalability combinations. In particular, we demonstrate that the deep\nlearning-based method can achieve significantly improved robustness in\ncomparison to the feature-based method. Furthermore, we investigate optimal\nscalability combinations with respect to bitrate in order to provide useful\nguidelines for an optimal operation policy in resource-constrained activity\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 00:39:48 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Choi", "Jun-Ho", ""], ["Cheon", "Manri", ""], ["Choi", "Min-Su", ""], ["Lee", "Jong-Seok", ""]]}]