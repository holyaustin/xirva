[{"id": "1406.0912", "submitter": "Patrick Seeling", "authors": "Patrick Seeling", "title": "Towards Quality of Experience Determination for Video in Augmented\n  Binocular Vision Scenarios", "comments": "Accepted to Signal Processing: Image Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous growth in the consumer markets of mobile smartphones and\nincreasingly in augmented reality wearable devices, several avenues of research\ninvestigate the relationships between the quality perceived by mobile users and\nthe delivery mechanisms at play to support a high quality of experience for\nmobile users. In this paper, we present the first study that evaluates the\nrelationships of mobile movie quality and the viewer-perceived quality thereof\nin an augmented reality setting with see-through devices. We find that\nparticipants tend to overestimate the video quality and exhibit a significant\nvariation of accuracy that leans onto the movie content and its dynamics. Our\nfindings, thus, can broadly impact future media adaptation and delivery\nmechanisms for this new display format of mobile multimedia.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 00:14:06 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 19:56:14 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 14:49:04 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Seeling", "Patrick", ""]]}, {"id": "1406.2018", "submitter": "YenFu Ou", "authors": "Yen-Fu Ou, Wenzhi Lin, Huiqi Zeng, Yao Wang", "title": "Perceptual Quality of Video with Periodic Frame Rate and Quantization\n  Variation-Subjective Studies and Analytical Modeling", "comments": "Keywords: perceptual video quality, frame rate, QS, temporal\n  variation, quality metrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In networked video applications, the frame rate (FR) and quantization\nstepsize (QS) of a compressed video are often adapted in response to the\nchanges of the available bandwidth. It is important to understand how do the\nvariation of FR and QS and their variation pattern affect the video quality. In\nthis paper, we investigate the impact of temporal variation of FR and QS on the\nperceptual video quality. Among all possible variation patterns, we focus on\nvideos in which two FR's (or QS's) alternate over a fixed interval. We explore\nthe human responses to such variation by conducting subjective evaluation of\ntest videos with different variation magnitudes and frequencies. We further\nanalyze statistical significance of the impact of variation magnitude,\nvariation frequency, video content, and their interactions. By analyzing the\nsubjective ratings, we propose two models for predicting the quality of video\nwith alternating FR and QS, respectively, The proposed models have simple\nmathematical forms with a few content-dependent parameters. The models fit the\nmeasured data very well using parameters determined by least square fitting\nwith the measured data. We further propose some guidelines for adaptation of FR\nand QS based on trends observed from subjective test results.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 19:37:06 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Ou", "Yen-Fu", ""], ["Lin", "Wenzhi", ""], ["Zeng", "Huiqi", ""], ["Wang", "Yao", ""]]}, {"id": "1406.2519", "submitter": "Wojciech Mazurczyk", "authors": "Wojciech Mazurczyk, Steffen Wendzel, Ignacio Azagra Villares,\n  Krzysztof Szczypiorski", "title": "On Importance of Steganographic Cost For Network Steganography", "comments": "15 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network steganography encompasses the information hiding techniques that can\nbe applied in communication network environments and that utilize hidden data\ncarriers for this purpose. In this paper we introduce a characteristic called\nsteganographic cost which is an indicator for the degradation or distortion of\nthe carrier caused by the application of the steganographic method. Based on\nexemplary cases for single- and multi-method steganographic cost analyses we\nobserve that it can be an important characteristic that allows to express\nhidden data carrier degradation - similarly as MSE (Mean-Square Error) or PSNR\n(Peak Signal-to-Noise Ratio) are utilized for digital media steganography.\nSteganographic cost can moreover be helpful to analyse the relationships\nbetween two or more steganographic methods applied to the same hidden data\ncarrier.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 12:07:22 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Mazurczyk", "Wojciech", ""], ["Wendzel", "Steffen", ""], ["Villares", "Ignacio Azagra", ""], ["Szczypiorski", "Krzysztof", ""]]}, {"id": "1406.2779", "submitter": "Hadeel Saleh Haj Aliwi Ms.", "authors": "Hadeel Saleh Haj Aliwi and Putra Sumari", "title": "real-time audio translation module between iax and rsw", "comments": "7 pages, 5 figures", "journal-ref": "Hadeel Saleh Haj Aliwi and Putra Sumari (2014),\"REAL-TIME AUDIO\n  TRANSLATION MODULE BETWEEN IAX AND RSW\", International Journal of Computer\n  Networks & Communications (IJCNC) Vol.6, No.3, pp.125-133", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the last few years, multimedia communication has been developed and\nimproved rapidly in order to enable users to communicate between each other\nover the internet. Generally, multimedia communication consists of audio and\nvideo communication. However, this research concentrates on audio conferencing\nonly. The audio translation between protocols is a very critical issue, because\nit solves the communication problems between any two protocols. So, it enables\npeople around the world to talk with each other even they use different\nprotocols. In this research, a real time audio translation module between two\nprotocols has been done. These two protocols are: InterAsterisk eXchange\nProtocol (IAX) and Real-Time Switching Control Protocol (RSW), which they are\nwidely used to provide two ways audio transfer feature. The solution here is to\nprovide inter-working between the two protocols which they have different media\ntransports, audio codecs, header formats and different transport protocols for\nthe audio transmission. This translation will help bridging the gap between the\ntwo protocols by providing inter-working capability between the two audio\nstreams of IAX and RSW. Some related works have been done to provide\ntranslation between IAX and RSW control signalling messages. But, this research\npaper concentrates on the translation that depends on the media transfer. The\nproposed translation module was tested and evaluated in different scenarios in\norder to examine its performance. The obtained results showed that the\nReal-Time Audio Translation Module produces lower rates of packet delay and\njitter than the acceptance values for each of the mentioned performance\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 05:09:59 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Aliwi", "Hadeel Saleh Haj", ""], ["Sumari", "Putra", ""]]}, {"id": "1406.3117", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Amy Banic", "title": "Low-cost Augmented Reality prototype for controlling network devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the evolution of mobile devices, and smart-phones in particular, comes\nthe ability to create new experiences that enhance the way we see, interact,\nand manipulate objects, within the world that surrounds us. It is now possible\nto blend data from our senses and our devices in numerous ways that simply were\nnot possible before using Augmented Reality technology. In a near future, when\nall of the office devices as well as your personal electronic gadgets are on a\ncommon wireless network, operating them using a universal remote controller\nwould be possible. This paper presents an off-the-shelf, low-cost prototype\nthat leverages the Augmented Reality technology to deliver a novel and\ninteractive way of operating office network devices around using a mobile\ndevice. We believe this type of system may provide benefits to controlling\nmultiple integrated devices and visualizing interconnectivity or utilizing\nvisual elements to pass information from one device to another, or may be\nespecially beneficial to control devices when interacting with them physically\nmay be difficult or pose danger or harm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 04:46:32 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Nguyen", "Anh", ""], ["Banic", "Amy", ""]]}, {"id": "1406.3161", "submitter": "Laura Toni", "authors": "Laura Toni, Ramon Aparicio-Pardo, Karine Pires, Gwendal Simon, Alberto\n  Blanc, and Pascal Frossard", "title": "Optimized Adaptive Streaming Representations based on System Dynamics", "comments": null, "journal-ref": null, "doi": "10.1145/2700294", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive streaming addresses the increasing and heterogenous demand of\nmultimedia content over the Internet by offering several encoded versions for\neach video sequence. Each version (or representation) has a different\nresolution and bit rate, aimed at a specific set of users, like TV or mobile\nphone clients. While most existing works on adaptive streaming deal with\neffective playout-control strategies at the client side, we take in this paper\na providers' perspective and propose solutions to improve user satisfaction by\noptimizing the encoding rates of the video sequences. We formulate an integer\nlinear program that maximizes users' average satisfaction, taking into account\nthe network dynamics, the video content information, and the user population\ncharacteristics. The solution of the optimization is a set of encoding\nparameters that permit to create different streams to robustly satisfy users'\nrequests over time. We simulate multiple adaptive streaming sessions\ncharacterized by realistic network connections models, where the proposed\nsolution outperforms commonly used vendor recommendations, in terms of user\nsatisfaction but also in terms of fairness and outage probability. The\nsimulation results further show that video content information as well as\nnetwork constraints and users' statistics play a crucial role in selecting\nproper encoding parameters to provide fairness a mong users and to reduce\nnetwork resource usage. We finally propose a few practical guidelines that can\nbe used to choose the encoding parameters based on the user base\ncharacteristics, the network capacity and the type of video content.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 09:24:32 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 21:36:31 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Toni", "Laura", ""], ["Aparicio-Pardo", "Ramon", ""], ["Pires", "Karine", ""], ["Simon", "Gwendal", ""], ["Blanc", "Alberto", ""], ["Frossard", "Pascal", ""]]}, {"id": "1406.3915", "submitter": "Sankar Mukherjee", "authors": "Sankar Mukherjee, Shyamal Kumar Das Mandal", "title": "A Bengali HMM Based Speech Synthesis System", "comments": null, "journal-ref": "Oriental COCOSDA 2012, pp.225 259", "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The paper presents the capability of an HMM-based TTS system to produce\nBengali speech. In this synthesis method, trajectories of speech parameters are\ngenerated from the trained Hidden Markov Models. A final speech waveform is\nsynthesized from those speech parameters. In our experiments, spectral\nproperties were represented by Mel Cepstrum Coefficients. Both the training and\nsynthesis issues are investigated in this paper using annotated Bengali speech\ndatabase. Experimental evaluation depicts that the developed text-to-speech\nsystem is capable of producing adequately natural speech in terms of\nintelligibility and intonation for Bengali.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 06:41:54 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Mukherjee", "Sankar", ""], ["Mandal", "Shyamal Kumar Das", ""]]}, {"id": "1406.5581", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Amy Banic", "title": "3DTouch: A wearable 3D input device with an optical sensor and a 9-DOF\n  inertial measurement unit", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 3DTouch, a novel 3D wearable input device worn on the fingertip\nfor 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D\ninput device that is self-contained, mobile, and universally working across\nvarious 3D platforms. This paper presents a low-cost solution to designing and\nimplementing such a device. Our approach relies on relative positioning\ntechnique using an optical laser sensor and a 9-DOF inertial measurement unit.\n  3DTouch is self-contained, and designed to universally work on various 3D\nplatforms. The device employs touch input for the benefits of passive haptic\nfeedback, and movement stability. On the other hand, with touch interaction,\n3DTouch is conceptually less fatiguing to use over many hours than 3D spatial\ninput devices. We propose a set of 3D interaction techniques including\nselection, translation, and rotation using 3DTouch. An evaluation also\ndemonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for\nsubtle touch interaction in 3D space. Modular solutions like 3DTouch opens up a\nwhole new design space for interaction techniques to further develop on.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 06:32:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 07:17:44 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Nguyen", "Anh", ""], ["Banic", "Amy", ""]]}, {"id": "1406.6012", "submitter": "Georg Groh", "authors": "Niklas Kl\\\"ugel and Timo Becker and Georg Groh", "title": "Designing Sound Collaboratively - Perceptually Motivated Audio Synthesis", "comments": "Extended version of submission to conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we will discuss a prototype that allows a group of\nusers to design sound collaboratively in real time using a multi-touch\ntabletop. We make use of a machine learning method to generate a mapping from\nperceptual audio features to synthesis parameters. This mapping is then used\nfor visualization and interaction. Finally, we discuss the results of a\ncomparative evaluation study.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 18:29:59 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Kl\u00fcgel", "Niklas", ""], ["Becker", "Timo", ""], ["Groh", "Georg", ""]]}, {"id": "1406.6473", "submitter": "Lani Rachel Mathew", "authors": "Lani Rachel Mathew, Ancy S. Anselam and Sakuntala S. Pillai", "title": "Performance Comparison of Linear Prediction based Vocoders in Linux\n  Platform", "comments": "5 pages, 5 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology\n  (IJETT),V10(11),554-558 April 2014", "doi": "10.14445/22315381/IJETT-V10P310", "report-no": null, "categories": "cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear predictive coders form an important class of speech coders. This paper\ndescribes the software level implementation of linear prediction based\nvocoders, viz. Code Excited Linear Prediction (CELP), Low-Delay CELP (LD-CELP)\nand Mixed Excitation Linear Prediction (MELP) at bit rates of 4.8 kb/s, 16 kb/s\nand 2.4 kb/s respectively. The C programs of the vocoders have been compiled\nand executed in Linux platform. Subjective testing with the help of Mean\nOpinion Score test has been performed. Waveform analysis has been done using\nPraat and Adobe Audition software. The results show that MELP and CELP produce\ncomparable quality while the quality of LD-CELP coder is much higher, at the\nexpense of higher bit rate.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 06:45:02 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Mathew", "Lani Rachel", ""], ["Anselam", "Ancy S.", ""], ["Pillai", "Sakuntala S.", ""]]}, {"id": "1406.6772", "submitter": "Yung-Chih Chen", "authors": "Yung-Chih Chen, Don Towsley, Ramin Khalili", "title": "MSPlayer: Multi-Source and multi-Path LeverAged YoutubER", "comments": "accepted to ACM CoNEXT'14", "journal-ref": null, "doi": "10.1145/2674005.2675007", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online video streaming through mobile devices has become extremely popular\nnowadays. YouTube, for example, reported that the percentage of its traffic\nstreaming to mobile devices has soared from 6% to more than 40% over the past\ntwo years. Moreover, people are constantly seeking to stream high quality video\nfor better experience while often suffering from limited bandwidth. Thanks to\nthe rapid deployment of content delivery networks (CDNs), popular videos are\nnow replicated at different sites, and users can stream videos from close-by\nlocations with low latencies. As mobile devices nowadays are equipped with\nmultiple wireless interfaces (e.g., WiFi and 3G/4G), aggregating bandwidth for\nhigh definition video streaming has become possible.\n  We propose a client-based video streaming solution, MSPlayer, that takes\nadvantage of multiple video sources as well as multiple network paths through\ndifferent interfaces. MSPlayer reduces start-up latency and provides high\nquality video streaming and robust data transport in mobile scenarios. We\nexperimentally demonstrate our solution on a testbed and through the YouTube\nvideo service.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 05:34:19 GMT"}, {"version": "v2", "created": "Sun, 9 Nov 2014 19:26:38 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Chen", "Yung-Chih", ""], ["Towsley", "Don", ""], ["Khalili", "Ramin", ""]]}, {"id": "1406.7226", "submitter": "Nilesh Rathi S", "authors": "Nilesh Rathi and Ganga Holi", "title": "Securing Medical Images by Watermarking Using DWT DCT and SVD", "comments": "9 pages, 14 figures, 4 tables, Published with International Journal\n  of Computer Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V12(2):67-74, June 2014. ISSN:2231-2803. Published by Seventh Sense Research\n  Group", "doi": "10.14445/22312803/IJCTT-V12P113", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telemedicine is well known application where enormous amount of medical data\nneed to be transferred securely over network and manipulate effectively.\nSecurity of digital data, especially medical images, becomes important for many\nreasons such as confidentiality, authentication and integrity. Digital\nwatermarking has emerged as a advanced technology to enhance the security of\ndigital images. The insertion of watermark in medical images can authenticate\nit and guarantee its integrity. The watermark must be generally hidden does not\naffect the quality of the medical image. In this paper, we propose blind\nwatermarking based on Discrete Wavelet Transform (DWT), Discrete Cosine\nTransform (DCT) and Singular Value Decomposition (SVD), we compare the\nperformance of this technique with watermarking based DWT and SVD. The proposed\nmethod DWT, DCT and SVD comparatively better than DWT and SVD method.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 18:27:36 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Rathi", "Nilesh", ""], ["Holi", "Ganga", ""]]}, {"id": "1406.7799", "submitter": "Pedram Mohammadi Mr.", "authors": "Pedram Mohammadi, Abbas Ebrahimi-Moghadam, and Shahram Shirani", "title": "Subjective and Objective Quality Assessment of Image: A Survey", "comments": "50 pages, 12 figures, and 3 Tables. This work has been submitted to\n  Elsevier Journal of Visual Communication and Image Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand for image-based applications, the efficient and\nreliable evaluation of image quality has increased in importance. Measuring the\nimage quality is of fundamental importance for numerous image processing\napplications, where the goal of image quality assessment (IQA) methods is to\nautomatically evaluate the quality of images in agreement with human quality\njudgments. Numerous IQA methods have been proposed over the past years to\nfulfill this goal. In this paper, a survey of the quality assessment methods\nfor conventional image signals, as well as the newly emerged ones, which\nincludes the high dynamic range (HDR) and 3-D images, is presented. A\ncomprehensive explanation of the subjective and objective IQA and their\nclassification is provided. Six widely used subjective quality datasets, and\nperformance measures are reviewed. Emphasis is given to the full-reference\nimage quality assessment (FR-IQA) methods, and 9 often-used quality measures\n(including mean squared error (MSE), structural similarity index (SSIM),\nmulti-scale structural similarity index (MS-SSIM), visual information fidelity\n(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),\nfeature similarity measure for color images (FSIMC), dynamic range independent\nmeasure (DRIM), and tone-mapped images quality index (TMQI)) are carefully\ndescribed, and their performance and computation time on four subjective\nquality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is\nprovided and the issues related to this area of research are reviewed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:25:00 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Mohammadi", "Pedram", ""], ["Ebrahimi-Moghadam", "Abbas", ""], ["Shirani", "Shahram", ""]]}]