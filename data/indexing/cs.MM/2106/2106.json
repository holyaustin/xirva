[{"id": "2106.00828", "submitter": "Emre Can Kaya", "authors": "Emre Can Kaya, Sebastian Schwarz, Ioan Tabus", "title": "Refining the bounding volumes for lossless compression of voxelized\n  point clouds geometry", "comments": "ICIP \\c{opyright} 2021 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses, in any\n  current or future media, including reprinting/republishing this material for\n  advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel lossless compression method for point cloud\ngeometry, building on a recent lossy compression method that aimed at\nreconstructing only the bounding volume of a point cloud. The proposed scheme\nstarts by partially reconstructing the geometry from the two depthmaps\nassociated to a single projection direction. The partial reconstruction\nobtained from the depthmaps is completed to a full reconstruction of the point\ncloud by sweeping section by section along one direction and encoding the\npoints which were not contained in the two depthmaps. The main ingredient is a\nlist-based encoding of the inner points (situated inside the feasible regions)\nby a novel arithmetic three dimensional context coding procedure that\nefficiently utilizes rotational invariances present in the input data.\nState-of-the-art bits-per-voxel results are obtained on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 22:16:06 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kaya", "Emre Can", ""], ["Schwarz", "Sebastian", ""], ["Tabus", "Ioan", ""]]}, {"id": "2106.01111", "submitter": "Wei Sun", "authors": "Wei Sun and Tao Wang and Xiongkuo Min and Fuwang Yi and Guangtao Zhai", "title": "Deep Learning based Full-reference and No-reference Quality Assessment\n  Models for Compressed UGC Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep learning based video quality assessment\n(VQA) framework to evaluate the quality of the compressed user's generated\ncontent (UGC) videos. The proposed VQA framework consists of three modules, the\nfeature extraction module, the quality regression module, and the quality\npooling module. For the feature extraction module, we fuse the features from\nintermediate layers of the convolutional neural network (CNN) network into\nfinal quality-aware feature representation, which enables the model to make\nfull use of visual information from low-level to high-level. Specifically, the\nstructure and texture similarities of feature maps extracted from all\nintermediate layers are calculated as the feature representation for the full\nreference (FR) VQA model, and the global mean and standard deviation of the\nfinal feature maps fused by intermediate feature maps are calculated as the\nfeature representation for the no reference (NR) VQA model. For the quality\nregression module, we use the fully connected (FC) layer to regress the\nquality-aware features into frame-level scores. Finally, a\nsubjectively-inspired temporal pooling strategy is adopted to pool frame-level\nscores into the video-level score. The proposed model achieves the best\nperformance among the state-of-the-art FR and NR VQA models on the Compressed\nUGC VQA database and also achieves pretty good performance on the in-the-wild\nUGC VQA databases.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:23:16 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Sun", "Wei", ""], ["Wang", "Tao", ""], ["Min", "Xiongkuo", ""], ["Yi", "Fuwang", ""], ["Zhai", "Guangtao", ""]]}, {"id": "2106.01266", "submitter": "Leonardo Fanzeres", "authors": "Leonardo A. Fanzeres and Climent Nadeu", "title": "Sound-to-Imagination: Unsupervised Crossmodal Translation Using Deep\n  Dense Network Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR cs.MM eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The motivation of our research is to develop a sound-to-image (S2I)\ntranslation system for enabling a human receiver to visually infer the\noccurrence of sound related events. We expect the computer to 'imagine' the\nscene from the captured sound, generating original images that picture the\nsound emitting source. Previous studies on similar topics opted for simplified\napproaches using data with low content diversity and/or strong supervision.\nDifferently, we propose to perform unsupervised S2I translation using thousands\nof distinct and unknown scenes, with slightly pre-cleaned data, just enough to\nguarantee aural-visual semantic coherence. To that end, we employ conditional\ngenerative adversarial networks (GANs) with a deep densely connected generator.\nBesides, we implemented a moving-average adversarial loss to address GANs\ntraining instability. Though the specified S2I translation problem is quite\nchallenging, we were able to generalize the translator model enough to obtain\nmore than 14%, in average, of interpretable and semantically coherent images\ntranslated from unknown sounds. Additionally, we present a solution using\ninformativity classifiers to perform quantitative evaluation of S2I\ntranslation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:20:43 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fanzeres", "Leonardo A.", ""], ["Nadeu", "Climent", ""]]}, {"id": "2106.01861", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Separated-Spectral-Distribution Estimation Based on Bayesian Inference\n  with Single RGB Camera", "comments": "to appear in IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel method for separately estimating spectral\ndistributions from images captured by a typical RGB camera. The proposed method\nallows us to separately estimate a spectral distribution of illumination,\nreflectance, or camera sensitivity, while recent hyperspectral cameras are\nlimited to capturing a joint spectral distribution from a scene. In addition,\nthe use of Bayesian inference makes it possible to take into account prior\ninformation of both spectral distributions and image noise as probability\ndistributions. As a result, the proposed method can estimate spectral\ndistributions in a unified way, and it can enhance the robustness of the\nestimation against noise, which conventional spectral-distribution estimation\nmethods cannot. The use of Bayesian inference also enables us to obtain the\nconfidence of estimation results. In an experiment, the proposed method is\nshown not only to outperform conventional estimation methods in terms of RMSE\nbut also to be robust against noise.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:31:47 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2106.02036", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar and Kristen Grauman", "title": "Anticipative Video Transformer", "comments": "Ranked #1 on CVPR'21 EPIC-Kitchens Action Anticipation challenge\n  leaderboard. Project page: http://facebookresearch.github.io/AVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Anticipative Video Transformer (AVT), an end-to-end\nattention-based video modeling architecture that attends to the previously\nobserved video in order to anticipate future actions. We train the model\njointly to predict the next action in a video sequence, while also learning\nframe feature encoders that are predictive of successive future frames'\nfeatures. Compared to existing temporal aggregation strategies, AVT has the\nadvantage of both maintaining the sequential progression of observed actions\nwhile still capturing long-range dependencies--both critical for the\nanticipation task. Through extensive experiments, we show that AVT obtains the\nbest reported performance on four popular action anticipation benchmarks:\nEpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including\noutperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:57:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Girdhar", "Rohit", ""], ["Grauman", "Kristen", ""]]}, {"id": "2106.02420", "submitter": "Emna Baccour", "authors": "Emna Baccour, Fatima Haouari, Aiman Erbad, Amr Mohamed, Kashif Bilal,\n  Mohsen Guizani, Mounir Hamdi", "title": "An Intelligent Resource Reservation for Crowdsourced Live Video\n  Streaming Applications in Geo-Distributed Cloud Environment", "comments": "Published in IEEE systems journal", "journal-ref": null, "doi": "10.1109/JSYST.2021.3077707", "report-no": null, "categories": "cs.NI cs.DC cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdsourced live video streaming (livecast) services such as Facebook Live,\nYouNow, Douyu and Twitch are gaining more momentum recently. Allocating the\nlimited resources in a cost-effective manner while maximizing the Quality of\nService (QoS) through real-time delivery and the provision of the appropriate\nrepresentations for all viewers is a challenging problem. In our paper, we\nintroduce a machine-learning based predictive resource allocation framework for\ngeo-distributed cloud sites, considering the delay and quality constraints to\nguarantee the maximum QoS for viewers and the minimum cost for content\nproviders. First, we present an offline optimization that decides the required\ntranscoding resources in distributed regions near the viewers with a trade-off\nbetween the QoS and the overall cost. Second, we use machine learning to build\nforecasting models that proactively predict the approximate transcoding\nresources to be reserved at each cloud site ahead of time. Finally, we develop\na Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource\nallocation of real-time broadcasted videos on the rented resources. Extensive\nsimulations have shown that GNCA outperforms the state-of-the art resource\nallocation approaches for crowdsourced live streaming by achieving more than\n20% gain in terms of system cost while serving the viewers with relatively\nlower latency.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 11:45:09 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Baccour", "Emna", ""], ["Haouari", "Fatima", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Bilal", "Kashif", ""], ["Guizani", "Mohsen", ""], ["Hamdi", "Mounir", ""]]}, {"id": "2106.02634", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B.\n  Tenenbaum, Fredo Durand", "title": "Light Field Networks: Neural Scene Representations with\n  Single-Evaluation Rendering", "comments": "First two authors contributed equally. Project website:\n  https://vsitzmann.github.io/lfns/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring representations of 3D scenes from 2D observations is a fundamental\nproblem of computer graphics, computer vision, and artificial intelligence.\nEmerging 3D-structured neural scene representations are a promising approach to\n3D scene understanding. In this work, we propose a novel neural scene\nrepresentation, Light Field Networks or LFNs, which represent both geometry and\nappearance of the underlying 3D scene in a 360-degree, four-dimensional light\nfield parameterized via a neural implicit representation. Rendering a ray from\nan LFN requires only a *single* network evaluation, as opposed to hundreds of\nevaluations per ray for ray-marching or volumetric based renderers in\n3D-structured neural scene representations. In the setting of simple scenes, we\nleverage meta-learning to learn a prior over LFNs that enables multi-view\nconsistent light field reconstruction from as little as a single image\nobservation. This results in dramatic reductions in time and memory complexity,\nand enables real-time rendering. The cost of storing a 360-degree light field\nvia an LFN is two orders of magnitude lower than conventional methods such as\nthe Lumigraph. Utilizing the analytical differentiability of neural implicit\nrepresentations and a novel parameterization of light space, we further\ndemonstrate the extraction of sparse depth maps from LFNs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:54:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Rezchikov", "Semon", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Durand", "Fredo", ""]]}, {"id": "2106.02738", "submitter": "Tong Mo", "authors": "Tong Mo, Bang Liu", "title": "Encoder-Decoder Neural Architecture Optimization for Keyword Spotting", "comments": "Accepted for Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Keyword spotting aims to identify specific keyword audio utterances. In\nrecent years, deep convolutional neural networks have been widely utilized in\nkeyword spotting systems. However, their model architectures are mainly based\non off-the shelfbackbones such as VGG-Net or ResNet, instead of specially\ndesigned for the task. In this paper, we utilize neural architecture search to\ndesign convolutional neural network models that can boost the performance of\nkeyword spotting while maintaining an acceptable memory footprint.\nSpecifically, we search the model operators and their connections in a specific\nsearch space with Encoder-Decoder neural architecture optimization. Extensive\nevaluations on Google's Speech Commands Dataset show that the model\narchitecture searched by our approach achieves a state-of-the-art accuracy of\nover 97%.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:09:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mo", "Tong", ""], ["Liu", "Bang", ""]]}, {"id": "2106.02864", "submitter": "Suvidha Tripathi Dr", "authors": "Suvidha Tripathi, Satish Kumar Singh, Hwee Kuan Lee", "title": "An End-to-End Breast Tumour Classification Model Using Context-Based\n  Patch Modelling- A BiLSTM Approach for Image Classification", "comments": "36 pages, 5 figures, 9 tables. Published in Computerized Medical\n  Imaging and Graphics", "journal-ref": "Computerized Medical Imaging and Graphics, 87, 101838 (2021)", "doi": "10.1016/j.compmedimag.2020.101838", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers working on computational analysis of Whole Slide Images (WSIs) in\nhistopathology have primarily resorted to patch-based modelling due to large\nresolution of each WSI. The large resolution makes WSIs infeasible to be fed\ndirectly into the machine learning models due to computational constraints.\nHowever, due to patch-based analysis, most of the current methods fail to\nexploit the underlying spatial relationship among the patches. In our work, we\nhave tried to integrate this relationship along with feature-based correlation\namong the extracted patches from the particular tumorous region. For the given\ntask of classification, we have used BiLSTMs to model both forward and backward\ncontextual relationship. RNN based models eliminate the limitation of sequence\nsize by allowing the modelling of variable size images within a deep learning\nmodel. We have also incorporated the effect of spatial continuity by exploring\ndifferent scanning techniques used to sample patches. To establish the\nefficiency of our approach, we trained and tested our model on two datasets,\nmicroscopy images and WSI tumour regions. After comparing with contemporary\nliterature we achieved the better performance with accuracy of 90% for\nmicroscopy image dataset. For WSI tumour region dataset, we compared the\nclassification results with deep learning networks such as ResNet, DenseNet,\nand InceptionV3 using maximum voting technique. We achieved the highest\nperformance accuracy of 84%. We found out that BiLSTMs with CNN features have\nperformed much better in modelling patches into an end-to-end Image\nclassification network. Additionally, the variable dimensions of WSI tumour\nregions were used for classification without the need for resizing. This\nsuggests that our method is independent of tumour image size and can process\nlarge dimensional images without losing the resolution details.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 10:43:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tripathi", "Suvidha", ""], ["Singh", "Satish Kumar", ""], ["Lee", "Hwee Kuan", ""]]}, {"id": "2106.03511", "submitter": "Xin Li", "authors": "Xin Li, Jun Shi and Zhibo Chen", "title": "Task-driven Semantic Coding via Reinforcement Learning", "comments": "13 pages, accepted by IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-driven semantic video/image coding has drawn considerable attention with\nthe development of intelligent media applications, such as license plate\ndetection, face detection, and medical diagnosis, which focuses on maintaining\nthe semantic information of videos/images. Deep neural network (DNN)-based\ncodecs have been studied for this purpose due to their inherent end-to-end\noptimization mechanism. However, the traditional hybrid coding framework cannot\nbe optimized in an end-to-end manner, which makes task-driven semantic fidelity\nmetric unable to be automatically integrated into the rate-distortion\noptimization process. Therefore, it is still attractive and challenging to\nimplement task-driven semantic coding with the traditional hybrid coding\nframework, which should still be widely used in practical industry for a long\ntime. To solve this challenge, we design semantic maps for different tasks to\nextract the pixelwise semantic fidelity for videos/images. Instead of directly\nintegrating the semantic fidelity metric into traditional hybrid coding\nframework, we implement task-driven semantic coding by implementing semantic\nbit allocation based on reinforcement learning (RL). We formulate the semantic\nbit allocation problem as a Markov decision process (MDP) and utilize one RL\nagent to automatically determine the quantization parameters (QPs) for\ndifferent coding units (CUs) according to the task-driven semantic fidelity\nmetric. Extensive experiments on different tasks, such as classification,\ndetection and segmentation, have demonstrated the superior performance of our\napproach by achieving an average bitrate saving of 34.39% to 52.62% over the\nHigh Efficiency Video Coding (H.265/HEVC) anchor under equivalent task-related\nsemantic fidelity.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:02:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Xin", ""], ["Shi", "Jun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2106.04053", "submitter": "Mingjie Sun", "authors": "Mingjie Sun, Jimin Xiao, Eng Gee Lim, Si Liu, John Y. Goulermas", "title": "Discriminative Triad Matching and Reconstruction for Weakly Referring\n  Expression Grounding", "comments": "TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are tackling the weakly-supervised referring expression\ngrounding task, for the localization of a referent object in an image according\nto a query sentence, where the mapping between image regions and queries are\nnot available during the training stage. In traditional methods, an object\nregion that best matches the referring expression is picked out, and then the\nquery sentence is reconstructed from the selected region, where the\nreconstruction difference serves as the loss for back-propagation. The existing\nmethods, however, conduct both the matching and the reconstruction\napproximately as they ignore the fact that the matching correctness is unknown.\nTo overcome this limitation, a discriminative triad is designed here as the\nbasis to the solution, through which a query can be converted into one or\nmultiple discriminative triads in a very scalable way. Based on the\ndiscriminative triad, we further propose the triad-level matching and\nreconstruction modules which are lightweight yet effective for the\nweakly-supervised training, making it three times lighter and faster than the\nprevious state-of-the-art methods. One important merit of our work is its\nsuperior performance despite the simple and neat design. Specifically, the\nproposed method achieves a new state-of-the-art accuracy when evaluated on\nRefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is\n4.17%, 4.08% and 7.8% higher than the previous one, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 02:15:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sun", "Mingjie", ""], ["Xiao", "Jimin", ""], ["Lim", "Eng Gee", ""], ["Liu", "Si", ""], ["Goulermas", "John Y.", ""]]}, {"id": "2106.04151", "submitter": "Jingjing Li", "authors": "Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, Ke Lu", "title": "Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain\n  Adaptation", "comments": "Accepted to CVPR 2021, Codes are avaliable at\n  https://github.com/lijin118/CGDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned\nfrom a well-labeled source domain to an unlabeled target domain. Recently,\nadversarial domain adaptation with two distinct classifiers (bi-classifier) has\nbeen introduced into UDA which is effective to align distributions between\ndifferent domains. Previous bi-classifier adversarial learning methods only\nfocus on the similarity between the outputs of two distinct classifiers.\nHowever, the similarity of the outputs cannot guarantee the accuracy of target\nsamples, i.e., target samples may match to wrong categories even if the\ndiscrepancy between two classifiers is small. To challenge this issue, in this\npaper, we propose a cross-domain gradient discrepancy minimization (CGDM)\nmethod which explicitly minimizes the discrepancy of gradients generated by\nsource samples and target samples. Specifically, the gradient gives a cue for\nthe semantic information of target samples so it can be used as a good\nsupervision to improve the accuracy of target samples. In order to compute the\ngradient signal of target samples, we further obtain target pseudo labels\nthrough a clustering-based self-supervised learning. Extensive experiments on\nthree widely used UDA datasets show that our method surpasses many previous\nstate-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:35:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Du", "Zhekai", ""], ["Li", "Jingjing", ""], ["Su", "Hongzu", ""], ["Zhu", "Lei", ""], ["Lu", "Ke", ""]]}, {"id": "2106.04403", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Ioannis Kazakos, Carles Ventura, Miriam Bellver, Carina Silberer and\n  Xavier Giro-i-Nieto", "title": "SynthRef: Generation of Synthetic Referring Expressions for Object\n  Segmentation", "comments": "Accepted as poster at the NAACL 2021 Visually Grounded Interaction\n  and Language (ViGIL) Workshop. 4 pages. Project website:\n  https://imatge-upc.github.io/synthref/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in deep learning have brought significant progress in visual\ngrounding tasks such as language-guided video object segmentation. However,\ncollecting large datasets for these tasks is expensive in terms of annotation\ntime, which represents a bottleneck. To this end, we propose a novel method,\nnamely SynthRef, for generating synthetic referring expressions for target\nobjects in an image (or video frame), and we also present and disseminate the\nfirst large-scale dataset with synthetic referring expressions for video object\nsegmentation. Our experiments demonstrate that by training with our synthetic\nreferring expressions one can improve the ability of a model to generalize\nacross different datasets, without any additional annotation cost. Moreover,\nour formulation allows its application to any object detection or segmentation\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:28:13 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:39:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kazakos", "Ioannis", ""], ["Ventura", "Carles", ""], ["Bellver", "Miriam", ""], ["Silberer", "Carina", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2106.05481", "submitter": "Hengyu Man", "authors": "Hengyu Man, Xiaopeng Fan, Ruiqin Xiong and Debin Zhao", "title": "Data Clustering-Driven Neural Network for Intra Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a crucial part of video compression, intra prediction utilizes local\ninformation of images to eliminate the redundancy in spatial domain. In both\nH.265/HEVC and H.266/VVC, multiple directional prediction modes are employed to\nfind the texture trend of each small block and then the prediction is made\nbased on reference samples in the selected direction. Recently, the intra\nprediction schemes based on neural networks have achieved great success. In\nthese methods, the networks are trained and applied to intra prediction in\naddition to the directional prediction modes. In this paper, we propose a novel\ndata clustering-driven neural network (dubbed DCDNN) for intra prediction,\nwhich can learn deep features of the clustered data. In DCDNN, each network can\nbe split into two networks by adding or subtracting Gaussian random noise. Then\na data clustering-driven training is applied to train all the derived networks\nrecursively. In each iteration, the entire training dataset is partitioned\naccording to the recovery qualities of the derived networks. For the\nexperiment, DCDNN is implemented into HEVC reference software HM-16.9. The\nexperimental results demonstrate that DCDNN can reach an average of 4.2%\nBjontegaard distortion rate (BDrate) improvement (up to 7.0%) over HEVC with\nall intra configuration. Compared with existing fully connected networkbased\nintra prediction methods, the bitrate saving performance is further improved.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:48:56 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Man", "Hengyu", ""], ["Fan", "Xiaopeng", ""], ["Xiong", "Ruiqin", ""], ["Zhao", "Debin", ""]]}, {"id": "2106.05630", "submitter": "Xu Tan", "authors": "Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, Tie-Yan Liu", "title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training", "comments": "Accepted by ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.IR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic music understanding, which refers to the understanding of music from\nthe symbolic data (e.g., MIDI format, but not audio), covers many music\napplications such as genre classification, emotion classification, and music\npieces matching. While good music representations are beneficial for these\napplications, the lack of training data hinders representation learning.\nInspired by the success of pre-training models in natural language processing,\nin this paper, we develop MusicBERT, a large-scale pre-trained model for music\nunderstanding. To this end, we construct a large-scale symbolic music corpus\nthat contains more than 1 million music songs. Since symbolic music contains\nmore structural (e.g., bar, position) and diverse information (e.g., tempo,\ninstrument, and pitch), simply adopting the pre-training techniques from NLP to\nsymbolic music only brings marginal gains. Therefore, we design several\nmechanisms, including OctupleMIDI encoding and bar-level masking strategy, to\nenhance pre-training with symbolic music data. Experiments demonstrate the\nadvantages of MusicBERT on four music understanding tasks, including melody\ncompletion, accompaniment suggestion, genre classification, and style\nclassification. Ablation studies also verify the effectiveness of our designs\nof OctupleMIDI encoding and bar-level masking strategy in MusicBERT.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:13:05 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zeng", "Mingliang", ""], ["Tan", "Xu", ""], ["Wang", "Rui", ""], ["Ju", "Zeqian", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2106.06489", "submitter": "John See", "authors": "Gen-Bing Liong, John See, Lai-Kuan Wong", "title": "Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression\n  Spotting from Long Videos", "comments": "Accepted for publication in ICIP2021. 9 pages, including 3 pages of\n  supplemental notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Facial expressions vary from the visible to the subtle. In recent years, the\nanalysis of micro-expressions $-$ a natural occurrence resulting from the\nsuppression of one's true emotions, has drawn the attention of researchers with\na broad range of potential applications. However, spotting microexpressions in\nlong videos becomes increasingly challenging when intertwined with normal or\nmacro-expressions. In this paper, we propose a shallow optical flow\nthree-stream CNN (SOFTNet) model to predict a score that captures the\nlikelihood of a frame being in an expression interval. By fashioning the\nspotting task as a regression problem, we introduce pseudo-labeling to\nfacilitate the learning process. We demonstrate the efficacy and efficiency of\nthe proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art\nperformance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM\nLong Videos.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:19:48 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liong", "Gen-Bing", ""], ["See", "John", ""], ["Wong", "Lai-Kuan", ""]]}, {"id": "2106.06736", "submitter": "Mathilde Brousmiche", "authors": "Mathilde Brousmiche and Jean Rouat and St\\'ephane Dupont", "title": "Multi-level Attention Fusion Network for Audio-visual Event Recognition", "comments": "Preprint submitted to the Information Fusion journal in August 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event classification is inherently sequential and multimodal. Therefore, deep\nneural models need to dynamically focus on the most relevant time window and/or\nmodality of a video. In this study, we propose the Multi-level Attention Fusion\nnetwork (MAFnet), an architecture that can dynamically fuse visual and audio\ninformation for event recognition. Inspired by prior studies in neuroscience,\nwe couple both modalities at different levels of visual and audio paths.\nFurthermore, the network dynamically highlights a modality at a given time\nwindow relevant to classify events. Experimental results in AVE (Audio-Visual\nEvent), UCF51, and Kinetics-Sounds datasets show that the approach can\neffectively improve the accuracy in audio-visual event classification. Code is\navailable at: https://github.com/numediart/MAFnet\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:24:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Brousmiche", "Mathilde", ""], ["Rouat", "Jean", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "2106.06924", "submitter": "Ching-Chun Chang", "authors": "Ching-Chun Chang, Xu Wang, Sisheng Chen, Isao Echizen, Victor Sanchez,\n  and Chang-Tsun Li", "title": "Deep Learning for Reversible Steganography: Principles and Insights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning\\textendash{centric} reversible steganography has emerged as a\npromising research paradigm. A direct way of applying deep learning to\nreversible steganography is to construct a pair of encoder and decoder, whose\nparameters are trained jointly, thereby learning the steganographic system as a\nwhole. This end-to-end framework, however, falls short of the reversibility\nrequirement because it is difficult for this kind of monolithic system, as a\nblack box, to create or duplicate intricate reversible mechanisms. In response\nto this issue, a recent approach is to carve up the steganographic system and\nwork on modules independently. In particular, neural networks are deployed in\nan analytics module to learn the data distribution, while an established\nmechanism is called upon to handle the remaining tasks. In this paper, we\ninvestigate the modular framework and deploy deep neural networks in a\nreversible steganographic scheme referred to as prediction-error modulation, in\nwhich an analytics module serves the purpose of pixel intensity prediction. The\nprimary focus of this study is on deep-learning\\textendash{based} context-aware\npixel intensity prediction. We address the unsolved issues reported in related\nliterature, including the impact of pixel initialisation on prediction accuracy\nand the influence of uncertainty propagation in dual-layer embedding.\nFurthermore, we establish a connection between context-aware pixel intensity\nprediction and low-level computer vision and analyse the performance of several\nadvanced neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 05:32:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chang", "Ching-Chun", ""], ["Wang", "Xu", ""], ["Chen", "Sisheng", ""], ["Echizen", "Isao", ""], ["Sanchez", "Victor", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "2106.07029", "submitter": "Riyanka Jena", "authors": "Riyanka Jena, Priyanka Singh, Manoranjan Mohanty", "title": "SSS-PRNU: Privacy-Preserving PRNU Based Camera Attribution using Shamir\n  Secret Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo Response Non-Uniformity(PRNU) noise has proven to be very effective\ntool in camera based forensics. It helps to match a photo to the device that\nclicked it. In today's scenario, where millions and millions of images are\nuploaded every hour, it is very easy to compute this unique PRNU pattern from a\ncouple of shared images on social profiles. This endangers the privacy of the\ncamera owner and becomes a cause of major concern for the privacy-aware\nsociety. We propose SSS-PRNU scheme that facilitates the forensic investigators\nto carry out their crime investigation without breaching the privacy of the\npeople. Thus, maintaining a balance between the two. To preserve privacy,\nextraction of camera fingerprint and PRNU noise for a suspicious image is\ncomputed in a trusted execution environment such as ARM TrustZone. After\nextraction, the sensitive information of camera fingerprint and PRNU noise is\ndistributed into multiple obfuscated shares using Shamir secret sharing(SSS)\nscheme. These shares are information-theoretically secure and leak no\ninformation of underlying content. The encrypted information is distributed to\nmultiple third-part servers where correlation is computed on a share basis\nbetween the camera fingerprint and the PRNU noise. These partial correlation\nvalues are combined together to obtain the final correlation value that becomes\nthe basis for a match decision. Transforming the computation of the correlation\nvalue in the encrypted domain and making it well suited for a distributed\nenvironment is the main contribution of the paper. Experiment results validate\nthe feasibility of the proposed scheme that provides a secure framework for\nPRNU based source camera attribution. The security analysis and evaluation of\ncomputational and storage overheads are performed to analysis the practical\nfeasibility of the scheme.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 15:49:25 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jena", "Riyanka", ""], ["Singh", "Priyanka", ""], ["Mohanty", "Manoranjan", ""]]}, {"id": "2106.07303", "submitter": "Alexander Schl\\\"ogl", "authors": "Alexander Schl\\\"ogl, Tobias Kupek, Rainer B\\\"ohme", "title": "iNNformant: Boundary Samples as Telltale Watermarks", "comments": "Will be presented at IH&MMSEC '21", "journal-ref": null, "doi": "10.1145/3437880.3460411", "report-no": null, "categories": "cs.LG cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary samples are special inputs to artificial neural networks crafted to\nidentify the execution environment used for inference by the resulting output\nlabel. The paper presents and evaluates algorithms to generate transparent\nboundary samples. Transparency refers to a small perceptual distortion of the\nhost signal (i.e., a natural input sample). For two established image\nclassifiers, ResNet on FMNIST and CIFAR10, we show that it is possible to\ngenerate sets of boundary samples which can identify any of four tested\nmicroarchitectures. These sets can be built to not contain any sample with a\nworse peak signal-to-noise ratio than 70dB. We analyze the relationship between\nsearch complexity and resulting transparency.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:18:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Schl\u00f6gl", "Alexander", ""], ["Kupek", "Tobias", ""], ["B\u00f6hme", "Rainer", ""]]}, {"id": "2106.07488", "submitter": "Pei Lv", "authors": "Pei Lv, Jianqi Fan, Xixi Nie, Weiming Dong, Xiaoheng Jiang, Bing Zhou,\n  Mingliang Xu and Changsheng Xu", "title": "User-Guided Personalized Image Aesthetic Assessment based on Deep\n  Reinforcement Learning", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Personalized image aesthetic assessment (PIAA) has recently become a hot\ntopic due to its usefulness in a wide variety of applications such as\nphotography, film and television, e-commerce, fashion design and so on. This\ntask is more seriously affected by subjective factors and samples provided by\nusers. In order to acquire precise personalized aesthetic distribution by small\namount of samples, we propose a novel user-guided personalized image aesthetic\nassessment framework. This framework leverages user interactions to retouch and\nrank images for aesthetic assessment based on deep reinforcement learning\n(DRL), and generates personalized aesthetic distribution that is more in line\nwith the aesthetic preferences of different users. It mainly consists of two\nstages. In the first stage, personalized aesthetic ranking is generated by\ninteractive image enhancement and manual ranking, meanwhile two policy networks\nwill be trained. The images will be pushed to the user for manual retouching\nand simultaneously to the enhancement policy network. The enhancement network\nutilizes the manual retouching results as the optimization goals of DRL. After\nthat, the ranking process performs the similar operations like the retouching\nmentioned before. These two networks will be trained iteratively and\nalternatively to help to complete the final personalized aesthetic assessment\nautomatically. In the second stage, these modified images are labeled with\naesthetic attributes by one style-specific classifier, and then the\npersonalized aesthetic distribution is generated based on the multiple\naesthetic attributes of these images, which conforms to the aesthetic\npreference of users better.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:19:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lv", "Pei", ""], ["Fan", "Jianqi", ""], ["Nie", "Xixi", ""], ["Dong", "Weiming", ""], ["Jiang", "Xiaoheng", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""], ["Xu", "Changsheng", ""]]}, {"id": "2106.08017", "submitter": "Zhao Hengyuan", "authors": "Hengyuan Zhao, Wenhao Wu, Yihao Liu, Dongliang He", "title": "Color2Style: Real-Time Exemplar-Based Image Colorization with\n  Self-Reference Learning and Deep Feature Modulation", "comments": "16 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legacy black-and-white photos are riddled with people's nostalgia and\nglorious memories of the past. To better relive the elapsed frozen moments, in\nthis paper, we present a deep exemplar-based image colorization approach named\nColor2Style to resurrect these grayscale image media by filling them with\nvibrant colors. Generally, for exemplar-based colorization, unsupervised and\nunpaired training are usually adopted, due to the difficulty of obtaining input\nand ground truth image pairs. To train an exemplar-based colorization model,\ncurrent algorithms usually strive to achieve two procedures: i) retrieving a\nlarge number of reference images with high similarity in advance, which is\ninevitably time-consuming and tedious; ii) designing complicated modules to\ntransfer the colors of the reference image to the grayscale image, by\ncalculating and leveraging the deep semantic correspondence between them (e.g.,\nnon-local operation). Contrary to the previous methods, we solve and simplify\nthe above two steps in one end-to-end learning procedure. First, we adopt a\nself-augmented self-reference training scheme, where the reference image is\ngenerated by graphical transformations from the original colorful one whereby\nthe training can be formulated in a paired manner. Second, instead of computing\ncomplex and inexplicable correspondence maps, our method exploits a simple yet\neffective deep feature modulation (DFM) module, which injects the color\nembeddings extracted from the reference image into the deep representations of\nthe input grayscale image. Such design is much more lightweight and\nintelligible, achieving appealing performance with real-time processing speed.\nMoreover, our model does not require multifarious loss functions and\nregularization terms like existing methods, but only two widely used loss\nfunctions. Codes and models will be available at\nhttps://github.com/zhaohengyuan1/Color2Style.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:05:58 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 05:46:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhao", "Hengyuan", ""], ["Wu", "Wenhao", ""], ["Liu", "Yihao", ""], ["He", "Dongliang", ""]]}, {"id": "2106.08104", "submitter": "Mingfu Xue", "authors": "Haoqi Wang, Mingfu Xue, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang\n  Liu", "title": "Detect and remove watermark in deep neural networks via generative\n  adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have achieved remarkable performance in various\nfields. However, training a DNN model from scratch requires a lot of computing\nresources and training data. It is difficult for most individual users to\nobtain such computing resources and training data. Model copyright infringement\nis an emerging problem in recent years. For instance, pre-trained models may be\nstolen or abuse by illegal users without the authorization of the model owner.\nRecently, many works on protecting the intellectual property of DNN models have\nbeen proposed. In these works, embedding watermarks into DNN based on backdoor\nis one of the widely used methods. However, when the DNN model is stolen, the\nbackdoor-based watermark may face the risk of being detected and removed by an\nadversary. In this paper, we propose a scheme to detect and remove watermark in\ndeep neural networks via generative adversarial networks (GAN). We demonstrate\nthat the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based\nwatermark removal attack. The proposed attack method includes two phases. In\nthe first phase, we use the GAN and few clean images to detect and reverse the\nwatermark in the DNN model. In the second phase, we fine-tune the watermarked\nDNN based on the reversed backdoor images. Experimental evaluations on the\nMNIST and CIFAR10 datasets demonstrate that, the proposed method can\neffectively remove about 98% of the watermark in DNN models, as the watermark\nretention rate reduces from 100% to less than 2% after applying the proposed\nattack. In the meantime, the proposed attack hardly affects the model's\nperformance. The test accuracy of the watermarked DNN on the MNIST and the\nCIFAR10 datasets drops by less than 1% and 3%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:45:22 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wang", "Haoqi", ""], ["Xue", "Mingfu", ""], ["Sun", "Shichang", ""], ["Zhang", "Yushu", ""], ["Wang", "Jian", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2106.08867", "submitter": "Tim Murray-Browne", "authors": "Tim Murray-Browne and Panagiotis Tigas", "title": "Latent Mappings: Generating Open-Ended Expressive Mappings Using\n  Variational Autoencoders", "comments": "Published at the International Conference on New Interfaces for\n  Musical Expression, June 2021. 3000 word short paper. 5 figures plus video\n  which may be seen at https://timmb.com/sonified-body-r-and-d-lab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many contexts, creating mappings for gestural interactions can form part\nof an artistic process. Creators seeking a mapping that is expressive, novel,\nand affords them a sense of authorship may not know how to program it up in a\nsignal processing patch. Tools like Wekinator and MIMIC allow creators to use\nsupervised machine learning to learn mappings from example input/output\npairings. However, a creator may know a good mapping when they encounter it yet\nstart with little sense of what the inputs or outputs should be. We call this\nan open-ended mapping process. Addressing this need, we introduce the latent\nmapping, which leverages the latent space of an unsupervised machine learning\nalgorithm such as a Variational Autoencoder trained on a corpus of unlabelled\ngestural data from the creator. We illustrate it with Sonified Body, a system\nmapping full-body movement to sound which we explore in a residency with three\ndancers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:40:53 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Murray-Browne", "Tim", ""], ["Tigas", "Panagiotis", ""]]}, {"id": "2106.08936", "submitter": "Luka Murn", "authors": "Luka Murn, Saverio Blasi, Alan F. Smeaton and Marta Mrak", "title": "Improved CNN-based Learning of Interpolation Filters for Low-Complexity\n  Inter Prediction in Video Coding", "comments": "IEEE Open Journal of Signal Processing Special Issue on Applied AI\n  and Machine Learning for Video Coding and Streaming, June 2021", "journal-ref": null, "doi": "10.1109/OJSP.2021.3089439", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The versatility of recent machine learning approaches makes them ideal for\nimprovement of next generation video compression solutions. Unfortunately,\nthese approaches typically bring significant increases in computational\ncomplexity and are difficult to interpret into explainable models, affecting\ntheir potential for implementation within practical video coding applications.\nThis paper introduces a novel explainable neural network-based inter-prediction\nscheme, to improve the interpolation of reference samples needed for fractional\nprecision motion compensation. The approach requires a single neural network to\nbe trained from which a full quarter-pixel interpolation filter set is derived,\nas the network is easily interpretable due to its linear structure. A novel\ntraining framework enables each network branch to resemble a specific\nfractional shift. This practical solution makes it very efficient to use\nalongside conventional video coding schemes. When implemented in the context of\nthe state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and\n2.25% BD-rate savings can be achieved on average for lower resolution sequences\nunder the random access, low-delay B and low-delay P configurations,\nrespectively, while the complexity of the learned interpolation schemes is\nsignificantly reduced compared to the interpolation with full CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:48:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Murn", "Luka", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["Mrak", "Marta", ""]]}, {"id": "2106.09199", "submitter": "Jicheng Li", "authors": "Jicheng Li, Anjana Bhat, Roghayeh Barmaki", "title": "A Two-stage Multi-modal Affect Analysis Framework for Children with\n  Autism Spectrum Disorder", "comments": "8 pages including reference; 8 figures", "journal-ref": "The AAAI-21 Workshop On Affective Content Analysis; 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autism spectrum disorder (ASD) is a developmental disorder that influences\nthe communication and social behavior of a person in a way that those in the\nspectrum have difficulty in perceiving other people's facial expressions, as\nwell as presenting and communicating emotions and affect via their own faces\nand bodies. Some efforts have been made to predict and improve children with\nASD's affect states in play therapy, a common method to improve children's\nsocial skills via play and games. However, many previous works only used\npre-trained models on benchmark emotion datasets and failed to consider the\ndistinction in emotion between typically developing children and children with\nautism. In this paper, we present an open-source two-stage multi-modal approach\nleveraging acoustic and visual cues to predict three main affect states of\nchildren with ASD's affect states (positive, negative, and neutral) in\nreal-world play therapy scenarios, and achieved an overall accuracy of 72:40%.\nThis work presents a novel way to combine human expertise and machine\nintelligence for ASD affect recognition by proposing a two-stage schema.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:28:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Li", "Jicheng", ""], ["Bhat", "Anjana", ""], ["Barmaki", "Roghayeh", ""]]}, {"id": "2106.09814", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Margarita Geleta, Cristina Punti, Kevin McGuinness, Jordi Pons,\n  Cristian Canton and Xavier Giro-i-Nieto", "title": "PixInWav: Residual Steganography for Hiding Pixels in Audio", "comments": "Extended abstract presented in CVPR 2021 Women in Computer Vision\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Steganography comprises the mechanics of hiding data in a host media that may\nbe publicly available. While previous works focused on unimodal setups (e.g.,\nhiding images in images, or hiding audio in audio), PixInWav targets the\nmultimodal case of hiding images in audio. To this end, we propose a novel\nresidual architecture operating on top of short-time discrete cosine transform\n(STDCT) audio spectrograms. Among our results, we find that the residual audio\nsteganography setup we propose allows independent encoding of the hidden image\nfrom the host audio without compromising quality. Accordingly, while previous\nworks require both host and hidden signals to hide a signal, PixInWav can\nencode images offline -- which can be later hidden, in a residual fashion, into\nany audio signal. Finally, we test our scheme in a lab setting to transmit\nimages over airwaves from a loudspeaker to a microphone verifying our\ntheoretical insights and obtaining promising results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:55:44 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Geleta", "Margarita", ""], ["Punti", "Cristina", ""], ["McGuinness", "Kevin", ""], ["Pons", "Jordi", ""], ["Canton", "Cristian", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2106.09889", "submitter": "Lin Su", "authors": "Lin Su and Nan Duan and Edward Cui and Lei Ji and Chenfei Wu and\n  Huaishao Luo and Yongfei Liu and Ming Zhong and Taroon Bharti and Arun\n  Sacheti", "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:14:13 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Su", "Lin", ""], ["Duan", "Nan", ""], ["Cui", "Edward", ""], ["Ji", "Lei", ""], ["Wu", "Chenfei", ""], ["Luo", "Huaishao", ""], ["Liu", "Yongfei", ""], ["Zhong", "Ming", ""], ["Bharti", "Taroon", ""], ["Sacheti", "Arun", ""]]}, {"id": "2106.10132", "submitter": "Disong Wang", "authors": "Disong Wang, Liqun Deng, Yu Ting Yeung, Xiao Chen, Xunying Liu, Helen\n  Meng", "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised\n  Speech Representation Disentanglement for One-shot Voice Conversion", "comments": "Accepted to Interspeech 2021. Code, pre-trained models and demo are\n  available at https://github.com/Wendison/VQMIVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.MM cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot voice conversion (VC), which performs conversion across arbitrary\nspeakers with only a single target-speaker utterance for reference, can be\neffectively achieved by speech representation disentanglement. Existing work\ngenerally ignores the correlation between different speech representations\nduring training, which causes leakage of content information into the speaker\nrepresentation and thus degrades VC performance. To alleviate this issue, we\nemploy vector quantization (VQ) for content encoding and introduce mutual\ninformation (MI) as the correlation metric during training, to achieve proper\ndisentanglement of content, speaker and pitch representations, by reducing\ntheir inter-dependencies in an unsupervised manner. Experimental results\nreflect the superiority of the proposed method in learning effective\ndisentangled speech representations for retaining source linguistic content and\nintonation variations, while capturing target speaker characteristics. In doing\nso, the proposed approach achieves higher speech naturalness and speaker\nsimilarity than current state-of-the-art one-shot VC systems. Our code,\npre-trained models and demo are available at\nhttps://github.com/Wendison/VQMIVC.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:50:38 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Disong", ""], ["Deng", "Liqun", ""], ["Yeung", "Yu Ting", ""], ["Chen", "Xiao", ""], ["Liu", "Xunying", ""], ["Meng", "Helen", ""]]}, {"id": "2106.10134", "submitter": "Max Graf", "authors": "Max Graf, Harold Chijioke Opara, Mathieu Barthet", "title": "An Audio-Driven System For Real-Time Music Visualisation", "comments": "8 pages, 3 figures", "journal-ref": "AES 150 Convention, Paper 10498, May 2021", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-generated visualisations can accompany recorded or live music to\ncreate novel audiovisual experiences for audiences. We present a system to\nstreamline the creation of audio-driven visualisations based on audio feature\nextraction and mapping interfaces. Its architecture is based on three modular\nsoftware components: backend (audio plugin), frontend (3D game-like\nenvironment), and middleware (visual mapping interface). We conducted a user\nevaluation comprising two stages. Results from the first stage (34\nparticipants) indicate that music visualisations generated with the system were\nsignificantly better at complementing the music than a baseline visualisation.\nNine participants took part in the second stage involving interactive tasks.\nOverall, the system yielded a Creativity Support Index above average (68.1) and\na System Usability Scale index (58.6) suggesting that ease of use can be\nimproved. Thematic analysis revealed that participants enjoyed the system's\nsynchronicity and expressive capabilities, but found technical problems and\ndifficulties understanding the audio feature terminology.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:55:51 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Graf", "Max", ""], ["Opara", "Harold Chijioke", ""], ["Barthet", "Mathieu", ""]]}, {"id": "2106.10430", "submitter": "Brijesh Singh", "authors": "Brijesh Singh, Arijit Sur, and Pinaki Mitra", "title": "Multi-Contextual Design of Convolutional Neural Network for Steganalysis", "comments": "This work has been submitted to the IEEE Transactions on Information\n  Forensics and Security (IEEE-TIFS) for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, deep learning-based steganalysis classifiers became popular\ndue to their state-of-the-art performance. Most deep steganalysis classifiers\nusually extract noise residuals using high-pass filters as preprocessing steps\nand feed them to their deep model for classification. It is observed that\nrecent steganographic embedding does not always restrict their embedding in the\nhigh-frequency zone; instead, they distribute it as per embedding policy.\nTherefore, besides noise residual, learning the embedding zone is another\nchallenging task. In this work, unlike the conventional approaches, the\nproposed model first extracts the noise residual using learned denoising\nkernels to boost the signal-to-noise ratio. After preprocessing, the sparse\nnoise residuals are fed to a novel Multi-Contextual Convolutional Neural\nNetwork (M-CNET) that uses heterogeneous context size to learn the sparse and\nlow-amplitude representation of noise residuals. The model performance is\nfurther improved by incorporating the Self-Attention module to focus on the\nareas prone to steganalytic embedding. A set of comprehensive experiments is\nperformed to show the proposed scheme's efficacy over the prior arts. Besides,\nan ablation study is given to justify the contribution of various modules of\nthe proposed architecture.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 05:38:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Singh", "Brijesh", ""], ["Sur", "Arijit", ""], ["Mitra", "Pinaki", ""]]}, {"id": "2106.10673", "submitter": "Qi Yang", "authors": "Qi Yang, Aleksandr Farseev, Andrey Filchenkov", "title": "Two-Faced Humans on Twitter and Facebook: Harvesting Social Multimedia\n  for Human Personality Profiling", "comments": null, "journal-ref": null, "doi": "10.1145/3463944.3469270", "report-no": null, "categories": "cs.SI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human personality traits are the key drivers behind our decision-making,\ninfluencing our life path on a daily basis. Inference of personality traits,\nsuch as Myers-Briggs Personality Type, as well as an understanding of\ndependencies between personality traits and users' behavior on various social\nmedia platforms is of crucial importance to modern research and industry\napplications. The emergence of diverse and cross-purpose social media avenues\nmakes it possible to perform user personality profiling automatically and\nefficiently based on data represented across multiple data modalities. However,\nthe research efforts on personality profiling from multi-source multi-modal\nsocial media data are relatively sparse, and the level of impact of different\nsocial network data on machine learning performance has yet to be\ncomprehensively evaluated. Furthermore, there is not such dataset in the\nresearch community to benchmark. This study is one of the first attempts\ntowards bridging such an important research gap. Specifically, in this work, we\ninfer the Myers-Briggs Personality Type indicators, by applying a novel\nmulti-view fusion framework, called \"PERS\" and comparing the performance\nresults not just across data modalities but also with respect to different\nsocial network data sources. Our experimental results demonstrate the PERS's\nability to learn from multi-view data for personality profiling by efficiently\nleveraging on the significantly different data arriving from diverse social\nmultimedia sources. We have also found that the selection of a machine learning\napproach is of crucial importance when choosing social network data sources and\nthat people tend to reveal multiple facets of their personality in different\nsocial media avenues. Our released social multimedia dataset facilitates future\nresearch on this direction.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 10:48:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yang", "Qi", ""], ["Farseev", "Aleksandr", ""], ["Filchenkov", "Andrey", ""]]}, {"id": "2106.10681", "submitter": "Jiapeng Wang", "authors": "Jiapeng Wang, Tianwei Wang, Guozhi Tang, Lianwen Jin, Weihong Ma, Kai\n  Ding, Yichao Huang", "title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for\n  Visual Information Extraction using Sequences", "comments": "IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information extraction (VIE) has attracted increasing attention in\nrecent years. The existing methods usually first organized optical character\nrecognition (OCR) results into plain texts and then utilized token-level entity\nannotations as supervision to train a sequence tagging model. However, it\nexpends great annotation costs and may be exposed to label confusion, and the\nOCR errors will also significantly affect the final performance. In this paper,\nwe propose a unified weakly-supervised learning framework called TCPN (Tag,\nCopy or Predict Network), which introduces 1) an efficient encoder to\nsimultaneously model the semantic and layout information in 2D OCR results; 2)\na weakly-supervised training strategy that utilizes only key information\nsequences as supervision; and 3) a flexible and switchable decoder which\ncontains two inference modes: one (Copy or Predict Mode) is to output key\ninformation sequences of different categories by copying a token from the input\nor predicting one in each time step, and the other (Tag Mode) is to directly\ntag the input sequence in a single forward pass. Our method shows new\nstate-of-the-art performance on several public benchmarks, which fully proves\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 11:56:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Jiapeng", ""], ["Wang", "Tianwei", ""], ["Tang", "Guozhi", ""], ["Jin", "Lianwen", ""], ["Ma", "Weihong", ""], ["Ding", "Kai", ""], ["Huang", "Yichao", ""]]}, {"id": "2106.10876", "submitter": "Hao Tang", "authors": "Hao Tang, Nicu Sebe", "title": "Total Generate: Cycle in Cycle Generative Adversarial Networks for\n  Generating Human Faces, Hands, Bodies, and Natural Scenes", "comments": "Accepted to TMM, an extended version of a paper published in ACM MM\n  2019. arXiv admin note: substantial text overlap with arXiv:1908.00999", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and unified Cycle in Cycle Generative Adversarial Network\n(C2GAN) for generating human faces, hands, bodies, and natural scenes. Our\nproposed C2GAN is a cross-modal model exploring the joint exploitation of the\ninput image data and guidance data in an interactive manner. C2GAN contains two\ndifferent generators, i.e., an image-generation generator and a\nguidance-generation generator. Both generators are mutually connected and\ntrained in an end-to-end fashion and explicitly form three cycled subnets,\ni.e., one image generation cycle and two guidance generation cycles. Each cycle\naims at reconstructing the input domain and simultaneously produces a useful\noutput involved in the generation of another cycle. In this way, the cycles\nconstrain each other implicitly providing complementary information from both\nimage and guidance modalities and bringing an extra supervision gradient across\nthe cycles, facilitating a more robust optimization of the whole model.\nExtensive results on four guided image-to-image translation subtasks\ndemonstrate that the proposed C2GAN is effective in generating more realistic\nimages compared with state-of-the-art models. The code is available at\nhttps://github.com/Ha0Tang/C2GAN.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:20:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tang", "Hao", ""], ["Sebe", "Nicu", ""]]}, {"id": "2106.11253", "submitter": "Na Li", "authors": "Na Li and Yao Liu", "title": "Applying VertexShuffle Toward 360-Degree Video Super-Resolution on\n  Focused-Icosahedral-Mesh", "comments": "This paper introduce a new mesh representation and a new upsampling\n  method on a mesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the emerging of 360-degree image/video, augmented reality (AR) and\nvirtual reality (VR), the demand for analysing and processing spherical signals\nget tremendous increase. However, plenty of effort paid on planar signals that\nprojected from spherical signals, which leading to some problems, e.g. waste of\npixels, distortion. Recent advances in spherical CNN have opened up the\npossibility of directly analysing spherical signals. However, they pay\nattention to the full mesh which makes it infeasible to deal with situations in\nreal-world application due to the extremely large bandwidth requirement. To\naddress the bandwidth waste problem associated with 360-degree video streaming\nand save computation, we exploit Focused Icosahedral Mesh to represent a small\narea and construct matrices to rotate spherical content to the focused mesh\narea. We also proposed a novel VertexShuffle operation that can significantly\nimprove both the performance and the efficiency compared to the original\nMeshConv Transpose operation introduced in UGSCNN. We further apply our\nproposed methods on super resolution model, which is the first to propose a\nspherical super-resolution model that directly operates on a mesh\nrepresentation of spherical pixels of 360-degree data. To evaluate our model,\nwe also collect a set of high-resolution 360-degree videos to generate a\nspherical image dataset. Our experiments indicate that our proposed spherical\nsuper-resolution model achieves significant benefits in terms of both\nperformance and inference time compared to the baseline spherical\nsuper-resolution model that uses the simple MeshConv Transpose operation. In\nsummary, our model achieves great super-resolution performance on 360-degree\ninputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices\non the mesh.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:53:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Na", ""], ["Liu", "Yao", ""]]}, {"id": "2106.11963", "submitter": "Yuxin Fang", "authors": "Shusheng Yang, Yuxin Fang, Xinggang Wang, Yu Li, Ying Shan, Bin Feng,\n  Wenyu Liu", "title": "Tracking Instances as Queries", "comments": "Preprint. Work in progress", "journal-ref": "CVPR 2021 Workshop. 2nd Place Solution for YouTube-VOS Challenge\n  2021: Video Instance Segmentation", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, query based deep networks catch lots of attention owing to their\nend-to-end pipeline and competitive results on several fundamental computer\nvision tasks, such as object detection, semantic segmentation, and instance\nsegmentation. However, how to establish a query based video instance\nsegmentation (VIS) framework with elegant architecture and strong performance\nremains to be settled. In this paper, we present \\textbf{QueryTrack} (i.e.,\ntracking instances as queries), a unified query based VIS framework fully\nleveraging the intrinsic one-to-one correspondence between instances and\nqueries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on\nYouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS\nChallenge at CVPR 2021 \\textbf{with a single online end-to-end model, single\nscale testing \\& modest amount of training data}. We also provide\nQueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 val set as references\nfor the VIS community.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:59:12 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:02:24 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yang", "Shusheng", ""], ["Fang", "Yuxin", ""], ["Wang", "Xinggang", ""], ["Li", "Yu", ""], ["Shan", "Ying", ""], ["Feng", "Bin", ""], ["Liu", "Wenyu", ""]]}, {"id": "2106.12174", "submitter": "Dorien Herremans", "authors": "Balamurali B T, Hwan Ing Hee, Saumitra Kapoor, Oon Hoe Teoh, Sung Shin\n  Teng, Khai Pin Lee, Dorien Herremans, Jer Ming Chen", "title": "Deep Neural Network Based Respiratory Pathology Classification Using\n  Cough Sounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent systems are transforming the world, as well as our healthcare\nsystem. We propose a deep learning-based cough sound classification model that\ncan distinguish between children with healthy versus pathological coughs such\nas asthma, upper respiratory tract infection (URTI), and lower respiratory\ntract infection (LRTI). In order to train a deep neural network model, we\ncollected a new dataset of cough sounds, labelled with clinician's diagnosis.\nThe chosen model is a bidirectional long-short term memory network (BiLSTM)\nbased on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting\ntrained model when trained for classifying two classes of coughs -- healthy or\npathology (in general or belonging to a specific respiratory pathology),\nreaches accuracy exceeding 84\\% when classifying cough to the label provided by\nthe physicians' diagnosis. In order to classify subject's respiratory pathology\ncondition, results of multiple cough epochs per subject were combined. The\nresulting prediction accuracy exceeds 91\\% for all three respiratory\npathologies. However, when the model is trained to classify and discriminate\namong the four classes of coughs, overall accuracy dropped: one class of\npathological coughs are often misclassified as other. However, if one consider\nthe healthy cough classified as healthy and pathological cough classified to\nhave some kind of pathologies, then the overall accuracy of four class model is\nabove 84\\%. A longitudinal study of MFCC feature space when comparing\npathological and recovered coughs collected from the same subjects revealed the\nfact that pathological cough irrespective of the underlying conditions occupy\nthe same feature space making it harder to differentiate only using MFCC\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:49:20 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["T", "Balamurali B", ""], ["Hee", "Hwan Ing", ""], ["Kapoor", "Saumitra", ""], ["Teoh", "Oon Hoe", ""], ["Teng", "Sung Shin", ""], ["Lee", "Khai Pin", ""], ["Herremans", "Dorien", ""], ["Chen", "Jer Ming", ""]]}, {"id": "2106.13266", "submitter": "Giorgos Kordopatis-Zilos", "authors": "Giorgos Kordopatis-Zilos, Christos Tzelepis, Symeon Papadopoulos,\n  Ioannis Kompatsiaris, Ioannis Patras", "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:34:24 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Tzelepis", "Christos", ""], ["Papadopoulos", "Symeon", ""], ["Kompatsiaris", "Ioannis", ""], ["Patras", "Ioannis", ""]]}, {"id": "2106.13393", "submitter": "Xiaofeng Liu", "authors": "Wanqing Xie, Lizhong Liang, Yao Lu, Chen Wang, Jihong Shen, Hui Luo,\n  Xiaofeng Liu", "title": "Interpreting Depression From Question-wise Long-term Video Recording of\n  SDS Evaluation", "comments": "Published in IEEE Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:32:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Xie", "Wanqing", ""], ["Liang", "Lizhong", ""], ["Lu", "Yao", ""], ["Wang", "Chen", ""], ["Shen", "Jihong", ""], ["Luo", "Hui", ""], ["Liu", "Xiaofeng", ""]]}, {"id": "2106.13504", "submitter": "Alan Smeaton", "authors": "Hyowon Lee, Mingming Liu, Michael Scriney, Alan F. Smeaton", "title": "Usage-based Summaries of Learning Videos", "comments": "16th European Conference on Technology-Enhanced Learning (EC-TEL),\n  Bozen-Bolzano, Italy (online), September 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the delivery of University education is now by synchronous or\nasynchronous video. For students, one of the challenges is managing the sheer\nvolume of such video material as video presentations of taught material are\ndifficult to abbreviate and summarise because they do not have highlights which\nstand out. Apart from video bookmarks there are no tools available to determine\nwhich parts of video content should be replayed at revision time or just before\nexaminations. We have developed and deployed a digital library for managing\nvideo learning material which has many dozens of hours of short-form video\ncontent from a range of taught courses for hundreds of students at\nundergraduate level. Through a web browser we allow students to access and play\nthese videos and we log their anonymised playback usage. From these logs we\nscore to each segment of each video based on the amount of playback it receives\nfrom across all students, whether the segment has been re-wound and re-played\nin the same student session, whether the on-screen window is the window in\nfocus on the student's desktop/laptop, and speed of playback. We also\nincorporate negative scoring if a video segment is skipped or fast-forward, and\noverarching all this we include a decay function based on recency of playback,\nso the most recent days of playback contribute more to the video segment\nscores. For each video in the library we present a usage-based graph which\nallows students to see which parts of each video attract the most playback from\ntheir peers, which helps them select material at revision time. Usage of the\nsystem is fully anonymised and GDPR-compliant.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:55:11 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Lee", "Hyowon", ""], ["Liu", "Mingming", ""], ["Scriney", "Michael", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2106.13574", "submitter": "Jaros{\\l}aw Samelak", "authors": "Jaros{\\l}aw Samelak, Marek Doma\\'nski", "title": "Multiview Video Compression Using Advanced HEVC Screen Content Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents a new approach to multiview video coding using Screen\nContent Coding. It is assumed that for a time instant the frames corresponding\nto all views are packed into a single frame, i.e. the frame-compatible approach\nto multiview coding is applied. For such coding scenario, the paper\ndemonstrates that Screen Content Coding can be efficiently used for multiview\nvideo coding. Two approaches are considered: the first using standard HEVC\nScreen Content Coding, and the second using Advanced Screen Content Coding. The\nlatter is the original proposal of the authors that exploits quarter-pel motion\nvectors and other nonstandard extensions of HEVC Screen Content Coding. The\nexperimental results demonstrate that multiview video coding even using\nstandard HEVC Screen Content Coding is much more efficient than simulcast HEVC\ncoding. The proposed Advanced Screen Content Coding provides virtually the same\ncoding efficiency as MV-HEVC, which is the state-of-the-art multiview video\ncompression technique. The authors suggest that Advanced Screen Content Coding\ncan be efficiently used within the new Versatile Video Coding (VVC) technology.\nNevertheless a reference multiview extension of VVC does not exist yet,\ntherefore, for VVC-based coding, the experimental comparisons are left for\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:53:48 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Samelak", "Jaros\u0142aw", ""], ["Doma\u0144ski", "Marek", ""]]}, {"id": "2106.13686", "submitter": "Li Liu", "authors": "Jianrong Wang, Ziyue Tang, Xuewei Li, Mei Yu, Qiang Fang, Li Liu", "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cued Speech (CS) is a visual communication system for the deaf or hearing\nimpaired people. It combines lip movements with hand cues to obtain a complete\nphonetic repertoire. Current deep learning based methods on automatic CS\nrecognition suffer from a common problem, which is the data scarcity. Until\nnow, there are only two public single speaker datasets for French (238\nsentences) and British English (97 sentences). In this work, we propose a\ncross-modal knowledge distillation method with teacher-student structure, which\ntransfers audio speech information to CS to overcome the limited data problem.\nFirstly, we pretrain a teacher model for CS recognition with a large amount of\nopen source audio speech data, and simultaneously pretrain the feature\nextractors for lips and hands using CS data. Then, we distill the knowledge\nfrom teacher model to the student model with frame-level and sequence-level\ndistillation strategies. Importantly, for frame-level, we exploit multi-task\nlearning to weigh losses automatically, to obtain the balance coefficient.\nBesides, we establish a five-speaker British English CS dataset for the first\ntime. The proposed method is evaluated on French and British English CS\ndatasets, showing superior CS recognition performance to the state-of-the-art\n(SOTA) by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:12:45 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Jianrong", ""], ["Tang", "Ziyue", ""], ["Li", "Xuewei", ""], ["Yu", "Mei", ""], ["Fang", "Qiang", ""], ["Liu", "Li", ""]]}, {"id": "2106.13921", "submitter": "Douglas Zytko", "authors": "Douglas Zytko, Zexin Ma, Jacob Gleason, Nathaniel Lundquist, Medina\n  Taylor", "title": "Immersive Stories for Health Information: Design Considerations from\n  Binge Drinking in VR", "comments": null, "journal-ref": "In: Toeppe K., Yan H., Chu S.K.W. (eds) Diversity, Divergence,\n  Dialogue. iConference 2021. Lecture Notes in Computer Science, vol 12645.\n  Springer, Cham", "doi": "10.1007/978-3-030-71292-1_25", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive stories for health are 360-degree videos that intend to alter\nviewer perceptions about behaviors detrimental to health. They have potential\nto inform public health at scale, however, immersive story design is still in\nearly stages and largely devoid of best practices. This paper presents a focus\ngroup study with 147 viewers of an immersive story about binge drinking\nexperienced through VR headsets and mobile phones. The objective of the study\nis to identify aspects of immersive story design that influence attitudes\ntowards the health issue exhibited, and to understand how health information is\nconsumed in immersive stories. Findings emphasize the need for an immersive\nstory to provide reasoning behind character engagement in the focal health\nbehavior, to show the main character clearly engaging in the behavior, and to\nenable viewers to experience escalating symptoms of the behavior before the\npenultimate health consequence. Findings also show how the design of supporting\ncharacters can inadvertently distract viewers and lead them to justify the\ndetrimental behavior being exhibited. The paper concludes with design\nconsiderations for enabling immersive stories to better inform public\nperception of health issues.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 01:33:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zytko", "Douglas", ""], ["Ma", "Zexin", ""], ["Gleason", "Jacob", ""], ["Lundquist", "Nathaniel", ""], ["Taylor", "Medina", ""]]}, {"id": "2106.14014", "submitter": "Pulkit Tandon", "authors": "Pulkit Tandon, Shubham Chandak, Pat Pataranutaporn, Yimeng Liu, Anesu\n  M. Mapuranga, Pattie Maes, Tsachy Weissman, Misha Sra", "title": "Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text", "comments": "8 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video represents the majority of internet traffic today leading to a\ncontinuous technological arms race between generating higher quality content,\ntransmitting larger file sizes and supporting network infrastructure. Adding to\nthis is the recent COVID-19 pandemic fueled surge in the use of video\nconferencing tools. Since videos take up substantial bandwidth (~100 Kbps to\nfew Mbps), improved video compression can have a substantial impact on network\nperformance for live and pre-recorded content, providing broader access to\nmultimedia content worldwide. In this work, we present a novel video\ncompression pipeline, called Txt2Vid, which substantially reduces data\ntransmission rates by compressing webcam videos (\"talking-head videos\") to a\ntext transcript. The text is transmitted and decoded into a realistic\nreconstruction of the original video using recent advances in deep learning\nbased voice cloning and lip syncing models. Our generative pipeline achieves\ntwo to three orders of magnitude reduction in the bitrate as compared to the\nstandard audio-video codecs (encoders-decoders), while maintaining equivalent\nQuality-of-Experience based on a subjective evaluation by users (n=242) in an\nonline study. The code for this work is available at\nhttps://github.com/tpulkit/txt2vid.git.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 12:29:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tandon", "Pulkit", ""], ["Chandak", "Shubham", ""], ["Pataranutaporn", "Pat", ""], ["Liu", "Yimeng", ""], ["Mapuranga", "Anesu M.", ""], ["Maes", "Pattie", ""], ["Weissman", "Tsachy", ""], ["Sra", "Misha", ""]]}, {"id": "2106.14016", "submitter": "Li Liu", "authors": "Jianrong Wang, Nan Gu, Mei Yu, Xuewei Li, Qiang Fang, Li Liu", "title": "An Attention Self-supervised Contrastive Learning based Three-stage\n  Model for Hand Shape Feature Representation in Cued Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cued Speech (CS) is a communication system for deaf people or hearing\nimpaired people, in which a speaker uses it to aid a lipreader in phonetic\nlevel by clarifying potentially ambiguous mouth movements with hand shape and\npositions. Feature extraction of multi-modal CS is a key step in CS\nrecognition. Recent supervised deep learning based methods suffer from noisy CS\ndata annotations especially for hand shape modality. In this work, we first\npropose a self-supervised contrastive learning method to learn the feature\nrepresentation of image without using labels. Secondly, a small amount of\nmanually annotated CS data are used to fine-tune the first module. Thirdly, we\npresent a module, which combines Bi-LSTM and self-attention networks to further\nlearn sequential features with temporal and contextual information. Besides, to\nenlarge the volume and the diversity of the current limited CS datasets, we\nbuild a new British English dataset containing 5 native CS speakers. Evaluation\nresults on both French and British English datasets show that our model\nachieves over 90% accuracy in hand shape recognition. Significant improvements\nof 8.75% (for French) and 10.09% (for British English) are achieved in CS\nphoneme recognition correctness compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 13:20:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Jianrong", ""], ["Gu", "Nan", ""], ["Yu", "Mei", ""], ["Li", "Xuewei", ""], ["Fang", "Qiang", ""], ["Liu", "Li", ""]]}, {"id": "2106.14076", "submitter": "Wang Zhihua", "authors": "Zhihua Wang and Zhi-Ri Tang and Jianguo Zhang and Yuming Fang", "title": "Learning from Synthetic Data for Opinion-free Blind Image Quality\n  Assessment in the Wild", "comments": "15 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, most existing blind image quality assessment (BIQA) models 1) are\ndeveloped for synthetically-distorted images and often generalize poorly to\nauthentic ones; 2) heavily rely on human ratings, which are prohibitively\nlabor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method\nthat learns from synthetically-distorted images and multiple agents to assess\nthe perceptual quality of authentically-distorted ones captured in the wild\nwithout relying on human labels. Specifically, we first assemble a large number\nof image pairs from synthetically-distorted images and use a set of\nfull-reference image quality assessment (FR-IQA) models to assign pseudo-binary\nlabels of each pair indicating which image has higher quality as the\nsupervisory signal. We then train a convolutional neural network (CNN)-based\nBIQA model to rank the perceptual quality, optimized for consistency with the\nbinary labels. Since there exists domain shift between the synthetically- and\nauthentically-distorted images, an unsupervised domain adaptation (UDA) module\nis introduced to alleviate this issue. Extensive experiments demonstrate the\neffectiveness of our proposed $opinion$-$free$ BIQA model, yielding\nstate-of-the-art performance in terms of correlation with human opinion scores,\nas well as gMAD competition. Codes will be made publicly available upon\nacceptance.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 18:31:23 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 14:37:08 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 18:38:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wang", "Zhihua", ""], ["Tang", "Zhi-Ri", ""], ["Zhang", "Jianguo", ""], ["Fang", "Yuming", ""]]}, {"id": "2106.14118", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, Ravi Kiran\n  Sarvadevabhatla", "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 00:49:02 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 06:44:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bagchi", "Anurag", ""], ["Mahmood", "Jazib", ""], ["Fernandes", "Dolton", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2106.14136", "submitter": "Haoyu Tang", "authors": "Haoyu Tang, Jihua Zhu, Qinghai Zheng, Zhiyong Cheng", "title": "Query-graph with Cross-gating Attention Model for Text-to-Audio\n  Grounding", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the text-to-audio grounding issue, namely,\ngrounding the segments of the sound event described by a natural language query\nin the untrimmed audio. This is a newly proposed but challenging audio-language\ntask, since it requires to not only precisely localize all the on- and off-sets\nof the desired segments in the audio, but to perform comprehensive acoustic and\nlinguistic understandings and reason the multimodal interactions between the\naudio and query. To tackle those problems, the existing method treats the query\nholistically as a single unit by a global query representation, which fails to\nhighlight the keywords that contain rich semantics. Besides, this method has\nnot fully exploited interactions between the query and audio. Moreover, since\nthe audio and queries are arbitrary and variable in length, many meaningless\nparts of them are not filtered out in this method, which hinders the grounding\nof the desired segments.\n  To this end, we propose a novel Query Graph with Cross-gating Attention\n(QGCA) model, which models the comprehensive relations between the words in\nquery through a novel query graph. Besides, to capture the fine-grained\ninteractions between audio and query, a cross-modal attention module that\nassigns higher weights to the keywords is introduced to generate the\nsnippet-specific query representations. Finally, we also design a cross-gating\nmodule to emphasize the crucial parts as well as weaken the irrelevant ones in\nthe audio and query. We extensively evaluate the proposed QGCA model on the\npublic Audiogrounding dataset with significant improvements over several\nstate-of-the-art methods. Moreover, further ablation study shows the consistent\neffectiveness of different modules in the proposed QGCA model.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 03:54:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tang", "Haoyu", ""], ["Zhu", "Jihua", ""], ["Zheng", "Qinghai", ""], ["Cheng", "Zhiyong", ""]]}, {"id": "2106.14150", "submitter": "Mojtaba Mahdavi", "authors": "Samira Hosseini, Mojtaba Mahdavi", "title": "Image content dependent semi-fragile watermarking with localized tamper\n  detection", "comments": "32 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 05:40:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hosseini", "Samira", ""], ["Mahdavi", "Mojtaba", ""]]}, {"id": "2106.14174", "submitter": "Saeid Hosseini", "authors": "Sana Rahmani, Saeid Hosseini, Raziyeh Zall, Mohammad Reza Kangavari,\n  Sara Kamran, Wen Hua", "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on\n  user latent aspects", "comments": "Under Review on IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 09:10:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rahmani", "Sana", ""], ["Hosseini", "Saeid", ""], ["Zall", "Raziyeh", ""], ["Kangavari", "Mohammad Reza", ""], ["Kamran", "Sara", ""], ["Hua", "Wen", ""]]}, {"id": "2106.14483", "submitter": "Azmi Can \\\"Ozgen", "authors": "Azmi Can \\\"Ozgen, Mahiye Uluya\\u{g}mur \\\"Ozt\\\"urk, Umut Bayraktar", "title": "Cheating Detection Pipeline for Online Interviews and Exams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote examination and job interviews have gained popularity and become\nindispensable because of both pandemics and the advantage of remote working\ncircumstances. Most companies and academic institutions utilize these systems\nfor their recruitment processes and also for online exams. However, one of the\ncritical problems of the remote examination systems is conducting the exams in\na reliable environment. In this work, we present a cheating analysis pipeline\nfor online interviews and exams. The system only requires a video of the\ncandidate, which is recorded during the exam. Then cheating detection pipeline\nis employed to detect another person, electronic device usage, and candidate\nabsence status. The pipeline consists of face detection, face recognition,\nobject detection, and face tracking algorithms. To evaluate the performance of\nthe pipeline we collected a private video dataset. The video dataset includes\nboth cheating activities and clean videos. Ultimately, our pipeline presents an\nefficient and fast guideline to detect and analyze cheating activities in an\nonline interview and exam video.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:52:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["\u00d6zgen", "Azmi Can", ""], ["\u00d6zt\u00fcrk", "Mahiye Uluya\u011fmur", ""], ["Bayraktar", "Umut", ""]]}, {"id": "2106.15130", "submitter": "Ehsan Nowroozi", "authors": "Mauro Conti, Simone Milani, Ehsan Nowroozi, Gabriele Orazi", "title": "Do Not Deceive Your Employer with a Virtual Background: A Video\n  Conferencing Manipulation-Detection System", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 07:31:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Conti", "Mauro", ""], ["Milani", "Simone", ""], ["Nowroozi", "Ehsan", ""], ["Orazi", "Gabriele", ""]]}, {"id": "2106.15262", "submitter": "Hannaneh Barahouei Pasandi", "authors": "Hannaneh Barahouei Pasandi and Tamer Nadeem and Hadi Amirpour", "title": "MU-MIMO Grouping For Real-time Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:40:28 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 19:59:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Pasandi", "Hannaneh Barahouei", ""], ["Nadeem", "Tamer", ""], ["Amirpour", "Hadi", ""]]}, {"id": "2106.15561", "submitter": "Xu Tan", "authors": "Xu Tan, Tao Qin, Frank Soong, Tie-Yan Liu", "title": "A Survey on Neural Speech Synthesis", "comments": "A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:50:51 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 15:58:54 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 12:32:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tan", "Xu", ""], ["Qin", "Tao", ""], ["Soong", "Frank", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2106.15989", "submitter": "Katsufumi Inoue", "authors": "Mizuki Maruyama, Shuvozit Ghose, Katsufumi Inoue, Partha Pratim Roy,\n  Masakazu Iwamura, Michifumi Yoshioka", "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks\n  Focusing on Local Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:30:06 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Maruyama", "Mizuki", ""], ["Ghose", "Shuvozit", ""], ["Inoue", "Katsufumi", ""], ["Roy", "Partha Pratim", ""], ["Iwamura", "Masakazu", ""], ["Yoshioka", "Michifumi", ""]]}, {"id": "2106.16036", "submitter": "Prateek Verma", "authors": "Prateek Verma, Chris Chafe", "title": "A Generative Model for Raw Audio Using Transformer Architectures", "comments": "DAFX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet. This is fully probabilistic,\nauto-regressive, and causal, i.e. each sample generated depends only on the\npreviously observed samples. Our approach outperforms a widely used wavenet\narchitecture by up to 9% on a similar dataset for predicting the next step.\nUsing the attention mechanism, we enable the architecture to learn which audio\nsamples are important for the prediction of the future sample. We show how\ncausal transformer generative models can be used for raw waveform synthesis. We\nalso show that this performance can be improved by another 2% by conditioning\nsamples over a wider context. The flexibility of the current model to\nsynthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:05:31 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 12:41:18 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 15:28:02 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Verma", "Prateek", ""], ["Chafe", "Chris", ""]]}, {"id": "2106.16125", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Xingxu Yao, Jufeng Yang, Guoli Jia, Guiguang Ding,\n  Tat-Seng Chua, Bj\\\"orn W. Schuller, Kurt Keutzer", "title": "Affective Image Content Analysis: Two Decades Review and New\n  Perspectives", "comments": "Accepted by IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:20:56 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhao", "Sicheng", ""], ["Yao", "Xingxu", ""], ["Yang", "Jufeng", ""], ["Jia", "Guoli", ""], ["Ding", "Guiguang", ""], ["Chua", "Tat-Seng", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Keutzer", "Kurt", ""]]}]