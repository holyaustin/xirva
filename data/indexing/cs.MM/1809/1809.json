[{"id": "1809.00190", "submitter": "Xavier Ouvrard", "authors": "Xavier Ouvrard, Jean-Marie Le Goff and Stephane Marchand-Maillet", "title": "Exchange-Based Diffusion in Hb-Graphs: Highlighting Complex\n  Relationships", "comments": "arXiv:1809.00190v1: Accepted version of article submitted at CBMI\n  2018 IEEE This version is an extended version of arXiv:1809.00190v1 currently\n  in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most networks tend to show complex and multiple relationships between\nentities. Networks are usually modeled by graphs or hypergraphs; nonetheless a\ngiven entity can occur many times in a relationship: this brings the need to\ndeal with multisets instead of sets or simple edges. Diffusion processes are\nuseful to highlight interesting parts of a network: they usually start with a\nstroke at one vertex and diffuse throughout the network to reach a uniform\ndistribution. Several iterations of the process are required prior to reaching\na stable solution. We propose an alternative solution to highlighting the main\ncomponents of a network using a diffusion process based on exchanges: it is an\niterative two-phase step exchange process. This process allows to evaluate the\nimportance not only of the vertices but also of the regrouping level. To model\nthe diffusion process, we extend the concept of hypergraphs that are families\nof sets to families of multisets, that we call hb-graphs. This version is an\nextended version of arXiv:1809.00190v1: the overlaps with the v1 are in black,\nthe new content is in blue. The contributions of this extended version are: the\nproofs of conservation and convergence of the extracted sequences of the\ndiffusion process, as well as the illustration of the speed of convergence and\ncomparison to classical and modified random walks; the algorithms of the\nexchange-based diffusion and the modified random walk; the application to a use\ncase based on Arxiv publications. All the figures except one have been either\nmodified or added in this extended version to take into account the new\ndevelopments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 14:15:16 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:34:18 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ouvrard", "Xavier", ""], ["Goff", "Jean-Marie Le", ""], ["Marchand-Maillet", "Stephane", ""]]}, {"id": "1809.00241", "submitter": "Ankit Parag Shah", "authors": "Ankit Shah, Harini Kesavamoorthy, Poorva Rane, Pramati Kalwad,\n  Alexander Hauptmann, Florian Metze", "title": "Activity Recognition on a Large Scale in Short Videos - Moments in Time\n  Dataset", "comments": "Action recognition submission for Moments in Time Dataset - Improved\n  results over challenge submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Moments capture a huge part of our lives. Accurate recognition of these\nmoments is challenging due to the diverse and complex interpretation of the\nmoments. Action recognition refers to the act of classifying the desired\naction/activity present in a given video. In this work, we perform experiments\non Moments in Time dataset to recognize accurately activities occurring in 3\nsecond clips. We use state of the art techniques for visual, auditory and\nspatio temporal localization and develop method to accurately classify the\nactivity in the Moments in Time dataset. Our novel approach of using Visual\nBased Textual features and fusion techniques performs well providing an overall\n89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the\nBaseline TRN model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 19:39:06 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 15:37:34 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Shah", "Ankit", ""], ["Kesavamoorthy", "Harini", ""], ["Rane", "Poorva", ""], ["Kalwad", "Pramati", ""], ["Hauptmann", "Alexander", ""], ["Metze", "Florian", ""]]}, {"id": "1809.00365", "submitter": "Ankit Parag Shah", "authors": "Ankit Shah, Tyler Vuong", "title": "Natural Language Person Search Using Deep Reinforcement Learning", "comments": "Equal Contribution - Work in Progress. Preprint results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent success in deep reinforcement learning is having an agent learn how to\nplay Go and beat the world champion without any prior knowledge of the game. In\nthat task, the agent has to make a decision on what action to take based on the\npositions of the pieces. Person Search is recently explored using natural\nlanguage based text description of images for video surveillance applications\n(S.Li et.al). We see (Fu.et al) provides an end to end approach for\nobject-based retrieval using deep reinforcement learning without constraints\nplaced on which objects are being detected. However, we believe for real-world\napplications such as person search defining specific constraints which identify\na person as opposed to starting with a general object detection will have\nbenefits in terms of performance and computational resources required. In our\ntask, Deep reinforcement learning would localize the person in an image by\nreshaping the sizes of the bounding boxes. Deep Reinforcement learning with\nappropriate constraints would look only for the relevant person in the image as\nopposed to an unconstrained approach where each individual objects in the image\nare ranked. For person search, the agent is trying to form a tight bounding box\naround the person in the image who matches the description. The bounding box is\ninitialized to the full image and at each time step, the agent makes a decision\non how to change the current bounding box so that it has a tighter bound around\nthe person based on the description of the person and the pixel values of the\ncurrent bounding box. After the agent takes an action, it will be given a\nreward based on the Intersection over Union (IoU) of the current bounding box\nand the ground truth box. Once the agent believes that the bounding box is\ncovering the person, it will indicate that the person is found.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 16:19:20 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Shah", "Ankit", ""], ["Vuong", "Tyler", ""]]}, {"id": "1809.00502", "submitter": "Yi Yu", "authors": "Yi Yu, Samuel Beuret, Donghuo Zeng, Keizo Oyama", "title": "Deep Learning of Human Perception in Audio Event Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce our recent studies on human perception in audio\nevent classification by different deep learning models. In particular, the\npre-trained model VGGish is used as feature extractor to process audio data,\nand DenseNet is trained by and used as feature extractor for our\nelectroencephalography (EEG) data. The correlation between audio stimuli and\nEEG is learned in a shared space. In the experiments, we record brain\nactivities (EEG signals) of several subjects while they are listening to music\nevents of 8 audio categories selected from Google AudioSet, using a 16-channel\nEEG headset with active electrodes. Our experimental results demonstrate that\ni) audio event classification can be improved by exploiting the power of human\nperception, and ii) the correlation between audio stimuli and EEG can be\nlearned to complement audio event understanding.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 08:47:32 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Yu", "Yi", ""], ["Beuret", "Samuel", ""], ["Zeng", "Donghuo", ""], ["Oyama", "Keizo", ""]]}, {"id": "1809.00842", "submitter": "Tao Li", "authors": "Tao Li, Minsoo Choi, Kaiming Fu, Lei Lin", "title": "Music Sequence Prediction with Mixture Hidden Markov Models", "comments": "Accepted to the 4th International Conference on Artificial\n  Intelligence and Applications (AI 2018)", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9005695", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems that automatically generate personalized music\nplaylists for users have attracted tremendous attention in recent years.\nNowadays, most music recommendation systems rely on item-based or user-based\ncollaborative filtering or content-based approaches. In this paper, we propose\na novel mixture hidden Markov model (HMM) for music play sequence prediction.\nWe compare the mixture model with state-of-the-art methods and evaluate the\npredictions quantitatively and qualitatively on a large-scale real-world\ndataset in a Kaggle competition. Results show that our model significantly\noutperforms traditional methods as well as other competitors. We conclude by\nenvisioning a next-generation music recommendation system that integrates our\nmodel with recent advances in deep learning, computer vision, and speech\ntechniques, and has promising potential in both academia and industry.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:54:52 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 07:26:37 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 06:55:25 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Li", "Tao", ""], ["Choi", "Minsoo", ""], ["Fu", "Kaiming", ""], ["Lin", "Lei", ""]]}, {"id": "1809.02277", "submitter": "Douglas Turnbull", "authors": "Douglas Turnbull and Luke Waldner", "title": "Local Music Event Recommendation with Long Tail Artists", "comments": "7 pages, ISMIR 2018 LBD Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore the task of local music event recommendation. Many\nlocal artists tend to be obscure long-tail artists with a small digital\nfootprint. That is, it can be hard to find social tag and artist similarity\ninformation for many of the artists who are playing shows in the local music\ncommunity. To address this problem, we explore using Latent Semantic Analysis\n(LSA) to embed artists and tags into a latent feature space and examine how\nwell artists with small digital footprints are represented in this space. We\nfind that only a relatively small digital footprint is needed to effectively\nmodel artist similarity. We also introduce the concept of a Music Event Graph\nas a data structure that makes it easy and efficient to recommend events based\non user-selected genre tags and popular artists. Finally, we conduct a small\nuser study to explore the feasibility of our proposed system for event\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 02:07:05 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Turnbull", "Douglas", ""], ["Waldner", "Luke", ""]]}, {"id": "1809.02587", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Nuno Vasconcelos, Timothy Langlois and Oliver Wang", "title": "Self-Supervised Generation of Spatial Audio for 360 Video", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to convert mono audio recorded by a 360 video camera\ninto spatial audio, a representation of the distribution of sound over the full\nviewing sphere. Spatial audio is an important component of immersive 360 video\nviewing, but spatial audio microphones are still rare in current 360 video\nproduction. Our system consists of end-to-end trainable neural networks that\nseparate individual sound sources and localize them on the viewing sphere,\nconditioned on multi-modal analysis of audio and 360 video frames. We introduce\nseveral datasets, including one filmed ourselves, and one collected in-the-wild\nfrom YouTube, consisting of 360 videos uploaded with spatial audio. During\ntraining, ground-truth spatial audio serves as self-supervision and a mixed\ndown mono track forms the input to our network. Using our approach, we show\nthat it is possible to infer the spatial location of sound sources based only\non 360 video and a mono audio track.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 17:25:59 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Morgado", "Pedro", ""], ["Vasconcelos", "Nuno", ""], ["Langlois", "Timothy", ""], ["Wang", "Oliver", ""]]}, {"id": "1809.03618", "submitter": "Rafael Ballester-Ripoll", "authors": "Rafael Ballester-Ripoll, Renato Pajarola", "title": "Visualization of High-dimensional Scalar Functions Using Principal\n  Parameterizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG cs.MM cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insightful visualization of multidimensional scalar fields, in particular\nparameter spaces, is key to many fields in computational science and\nengineering. We propose a principal component-based approach to visualize such\nfields that accurately reflects their sensitivity to input parameters. The\nmethod performs dimensionality reduction on the vast $L^2$ Hilbert space formed\nby all possible partial functions (i.e., those defined by fixing one or more\ninput parameters to specific values), which are projected to low-dimensional\nparameterized manifolds such as 3D curves, surfaces, and ensembles thereof. Our\nmapping provides a direct geometrical and visual interpretation in terms of\nSobol's celebrated method for variance-based sensitivity analysis. We\nfurthermore contribute a practical realization of the proposed method by means\nof tensor decomposition, which enables accurate yet interactive integration and\nmultilinear principal component analysis of high-dimensional models.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:35:12 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Ballester-Ripoll", "Rafael", ""], ["Pajarola", "Renato", ""]]}, {"id": "1809.03650", "submitter": "Seong-Eun Moon", "authors": "Seong-Eun Moon, Soobeom Jang, Jong-Seok Lee", "title": "Evaluation of Preference of Multimedia Content using Deep Neural\n  Networks for Electroencephalography", "comments": "Accepted for the 10th International Conference on Quality of\n  Multimedia Experience (QoMEX 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of quality of experience (QoE) based on electroencephalography\n(EEG) has received great attention due to its capability of real-time QoE\nmonitoring of users. However, it still suffers from rather low recognition\naccuracy. In this paper, we propose a novel method using deep neural networks\ntoward improved modeling of EEG and thereby improved recognition accuracy. In\nparticular, we aim to model spatio-temporal characteristics relevant for QoE\nanalysis within learning models. The results demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 01:51:24 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 01:14:00 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Jang", "Soobeom", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.03867", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Yunwu Lin, Lei Zhu, Zuping Zhang, Xinpan Yuan, Fang\n  Huang", "title": "Efficient Multimedia Similarity Measurement Using Similar Elements", "comments": "17 pages. arXiv admin note: text overlap with arXiv:1808.09610", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networking techniques and large-scale multimedia systems are\ndeveloping rapidly, which not only has brought great convenience to our daily\nlife, but generated, collected, and stored large-scale multimedia data. This\ntrend has put forward higher requirements and greater challenges on massive\nmultimedia data retrieval. In this paper, we investigate the problem of image\nsimilarity measurement which is used to lots of applications. At first we\npropose the definition of similarity measurement of images and the related\nnotions. Based on it we present a novel basic method of similarity measurement\nnamed SMIN. To improve the performance of calculation, we propose a novel\nindexing structure called SMI Temp Index (SMII for short). Besides, we\nestablish an index of potential similar visual words off-line to solve to\nproblem that the index cannot be reused. Experimental evaluations on two real\nimage datasets demonstrate that our solution outperforms state-of-the-art\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 04:12:14 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Lin", "Yunwu", ""], ["Zhu", "Lei", ""], ["Zhang", "Zuping", ""], ["Yuan", "Xinpan", ""], ["Huang", "Fang", ""]]}, {"id": "1809.04094", "submitter": "Giorgos Kordopatis-Zilos Mr.", "authors": "Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Patras, Ioannis\n  Kompatsiaris", "title": "FIVR: Fine-grained Incident Video Retrieval", "comments": null, "journal-ref": "IEEE Transactions on Multimedia 2019", "doi": "10.1109/TMM.2019.2905741", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the problem of Fine-grained Incident Video Retrieval\n(FIVR). Given a query video, the objective is to retrieve all associated\nvideos, considering several types of associations that range from duplicate\nvideos to videos from the same incident. FIVR offers a single framework that\ncontains several retrieval tasks as special cases. To address the benchmarking\nneeds of all such tasks, we construct and present a large-scale annotated video\ndataset, which we call FIVR-200K, and it comprises 225,960 videos. To create\nthe dataset, we devise a process for the collection of YouTube videos based on\nmajor news events from recent years crawled from Wikipedia and deploy a\nretrieval pipeline for the automatic selection of query videos based on their\nestimated suitability as benchmarks. We also devise a protocol for the\nannotation of the dataset with respect to the four types of video associations\ndefined by FIVR. Finally, we report the results of an experimental study on the\ndataset comparing five state-of-the-art methods developed based on a variety of\nvisual descriptors, highlighting the challenges of the current problem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 18:09:44 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 08:59:05 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Patras", "Ioannis", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1809.04254", "submitter": "Seong-Eun Moon", "authors": "Seong-Eun Moon, Jong-Seok Lee", "title": "Implicit Analysis of Perceptual Multimedia Experience Based on\n  Physiological Response: A Review", "comments": "Published in IEEE Transactions on Multimedia", "journal-ref": "S.-E. Moon and J.-S. Lee, \"Implicit Analysis of Perceptual\n  Multimedia Experience Based on Physiological Response: A Review,\" in IEEE\n  Transactions on Multimedia, vol. 19, no. 2, pp. 340-353, Feb. 2017", "doi": "10.1109/TMM.2016.2614880", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth of popularity of multimedia has led to needs for\nuser-centric adaptive applications that manage multimedia content more\neffectively. Implicit analysis, which examines users' perceptual experience of\nmultimedia by monitoring physiological or behavioral cues, has potential to\nsatisfy such demands. Particularly, physiological signals categorized into\ncerebral physiological signals (electroencephalography, functional magnetic\nresonance imaging, and functional near-infrared spectroscopy) and peripheral\nphysiological signals (heart rate, respiration, skin temperature, etc.) have\nrecently received attention along with notable development of wearable\nphysiological sensors. In this paper, we review existing studies on\nphysiological signal analysis exploring perceptual experience of multimedia.\nFurthermore, we discuss current trends and challenges.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 04:48:56 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.04777", "submitter": "Seong-Eun Moon", "authors": "Seong-Eun Moon, Jong-Seok Lee", "title": "Perceptual Experience Analysis for Tone-mapped HDR Videos based on EEG\n  and Peripheral Physiological Signals", "comments": "Published in IEEE Transactions on Autonomous Mental Development", "journal-ref": "S.-E. Moon and J.-S. Lee, \"Perceptual Experience Analysis for\n  Tone-mapped HDR Videos Based on EEG and Peripheral Physiological Signals,\" in\n  IEEE Transactions on Autonomous Mental Development, vol. 7, no. 3, pp.\n  236-247, Sept. 2015", "doi": "10.1109/TAMD.2015.2449553", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) imaging has been attracting much attention as a\ntechnology that can provide immersive experience. Its ultimate goal is to\nprovide better quality of experience (QoE) via enhanced contrast. In this\npaper, we analyze perceptual experience of tone-mapped HDR videos both\nexplicitly by conducting a subjective questionnaire assessment and implicitly\nby using EEG and peripheral physiological signals. From the results of the\nsubjective assessment, it is revealed that tone-mapped HDR videos are more\ninteresting and more natural, and give better quality than low dynamic range\n(LDR) videos. Physiological signals were recorded during watching tone-mapped\nHDR and LDR videos, and classification systems are constructed to explore\nperceptual difference captured by the physiological signals. Significant\ndifference in the physiological signals is observed between tone-mapped HDR and\nLDR videos in the classification under both a subject-dependent and a\nsubject-independent scenarios. Also, significant difference in the signals\nbetween high versus low perceived contrast and overall quality is detected via\nclassification under the subject-dependent scenario. Moreover, it is shown that\nfeatures extracted from the gamma frequency band are effective for\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 05:08:36 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.05220", "submitter": "Soobeom Jang", "authors": "Soobeom Jang and Jong-Seok Lee", "title": "On Evaluating Perceptual Quality of Online User-Generated Videos", "comments": "Published in IEEE Transactions on Multimedia", "journal-ref": "S. Jang and J. S. Lee, \"On evaluating perceptual quality of online\n  user-generated videos,\"IEEE Transactions on Multimedia, vol. 18, no. 9, pp.\n  1808-1818, Sep. 2016", "doi": "10.1109/TMM.2016.2581582", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the issue of the perceptual quality evaluation of\nuser-generated videos shared online, which is an important step toward\ndesigning video-sharing services that maximize users' satisfaction in terms of\nquality. We first analyze viewers' quality perception patterns by applying\ngraph analysis techniques to subjective rating data. We then examine the\nperformance of existing state-of-the-art objective metrics for the quality\nestimation of user-generated videos. In addition, we investigate the\nfeasibility of metadata accompanied with videos in online video-sharing\nservices for quality estimation. Finally, various issues in the quality\nassessment of online user-generated videos are discussed, including\ndifficulties and opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 02:07:30 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Jang", "Soobeom", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.05239", "submitter": "Xu Chen", "authors": "Tao Ouyang and Zhi Zhou and Xu Chen", "title": "Follow Me at the Edge: Mobility-Aware Dynamic Service Placement for\n  Mobile Edge Computing", "comments": "The paper is accepted by IEEE Journal on Selected Areas in\n  Communications, Aug. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC cs.MM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing is a new computing paradigm, which pushes cloud\ncomputing capabilities away from the centralized cloud to the network edge.\nHowever, with the sinking of computing capabilities, the new challenge incurred\nby user mobility arises: since end-users typically move erratically, the\nservices should be dynamically migrated among multiple edges to maintain the\nservice performance, i.e., user-perceived latency. Tackling this problem is\nnon-trivial since frequent service migration would greatly increase the\noperational cost. To address this challenge in terms of the performance-cost\ntrade-off, in this paper we study the mobile edge service performance\noptimization problem under long-term cost budget constraint. To address user\nmobility which is typically unpredictable, we apply Lyapunov optimization to\ndecompose the long-term optimization problem into a series of real-time\noptimization problems which do not require a priori knowledge such as user\nmobility. As the decomposed problem is NP-hard, we first design an\napproximation algorithm based on Markov approximation to seek a near-optimal\nsolution. To make our solution scalable and amenable to future 5G application\nscenario with large-scale user devices, we further propose a distributed\napproximation scheme with greatly reduced time complexity, based on the\ntechnique of best response update. Rigorous theoretical analysis and extensive\nevaluations demonstrate the efficacy of the proposed centralized and\ndistributed schemes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 03:07:40 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Ouyang", "Tao", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1809.05782", "submitter": "Jean-Baptiste Lamare", "authors": "Ankit Shah, Jean Baptiste Lamare, Tuan Nguyen Anh, Alexander Hauptmann", "title": "CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis", "comments": "Accepted at IEEE International Workshop on Traffic and Street\n  Surveillance for Safety and Security, First three authors contributed\n  equally, 7 pages + 1 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel dataset for traffic accidents analysis. Our goal\nis to resolve the lack of public data for research about automatic\nspatio-temporal annotations for traffic safety in the roads. Through the\nanalysis of the proposed dataset, we observed a significant degradation of\nobject detection in pedestrian category in our dataset, due to the object sizes\nand complexity of the scenes. To this end, we propose to integrate contextual\ninformation into conventional Faster R-CNN using Context Mining (CM) and\nAugmented Context Mining (ACM) to complement the accuracy for small pedestrian\ndetection. Our experiments indicate a considerable improvement in object\ndetection accuracy: +8.51% for CM and +6.20% for ACM. Finally, we demonstrate\nthe performance of accident forecasting in our dataset using Faster R-CNN and\nan Accident LSTM architecture. We achieved an average of 1.684 seconds in terms\nof Time-To-Accident measure with an Average Precision of 47.25%. Our Webpage\nfor the paper is https://goo.gl/cqK2wE\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 00:01:39 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 06:28:36 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Shah", "Ankit", ""], ["Lamare", "Jean Baptiste", ""], ["Anh", "Tuan Nguyen", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1809.05823", "submitter": "Gazi Karam Illahi", "authors": "Gazi Illahi, Thomas Van Gemert, Matti Siekkinen, Enrico Masala, Antti\n  Oulasvirta and Antti Yl\\\"a-J\\\"a\\\"aski", "title": "Cloud Gaming With Foveated Graphics", "comments": "Submitted for publication in ACM TOMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud gaming enables playing high end games, originally designed for PC or\ngame console setups, on low end devices, such as net-books and smartphones, by\noffloading graphics rendering to GPU powered cloud servers. However,\ntransmitting the high end graphics requires a large amount of available network\nbandwidth, even though it is a compressed video stream. Foveated video encoding\n(FVE) reduces the bandwidth requirement by taking advantage of the non-uniform\nacuity of human visual system and by knowing where the user is looking. We have\ndesigned and implemented a system for cloud gaming with foveated graphics using\na consumer grade real-time eye tracker and an open source cloud gaming\nplatform. In this article, we describe the system and its evaluation through\nmeasurements with representative games from different genres to understand the\neffect of parameterization of the FVE scheme on bandwidth requirements and to\nunderstand its feasibility from the latency perspective. We also present\nresults from a user study. The results suggest that it is possible to find a\n\"sweet spot\" for the encoding parameters so that the users hardly notice the\npresence of foveated encoding but at the same time the scheme yields most of\nthe bandwidth savings achievable.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 06:44:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Illahi", "Gazi", ""], ["Van Gemert", "Thomas", ""], ["Siekkinen", "Matti", ""], ["Masala", "Enrico", ""], ["Oulasvirta", "Antti", ""], ["Yl\u00e4-J\u00e4\u00e4ski", "Antti", ""]]}, {"id": "1809.05884", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang and\n  Chunhong Pan", "title": "Multi-Label Image Classification via Knowledge Distillation from\n  Weakly-Supervised Detection", "comments": "accepted by ACM Multimedia 2018, 9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": "10.1145/3240508.3240567", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image classification is a fundamental but challenging task\ntowards general visual understanding. Existing methods found the region-level\ncues (e.g., features from RoIs) can facilitate multi-label classification.\nNevertheless, such methods usually require laborious object-level annotations\n(i.e., object labels and bounding boxes) for effective learning of the\nobject-level visual features. In this paper, we propose a novel and efficient\ndeep framework to boost multi-label classification by distilling knowledge from\nweakly-supervised detection task without bounding box annotations.\nSpecifically, given the image-level annotations, (1) we first develop a\nweakly-supervised detection (WSD) model, and then (2) construct an end-to-end\nmulti-label image classification framework augmented by a knowledge\ndistillation module that guides the classification model by the WSD model\naccording to the class-level predictions for the whole image and the\nobject-level visual features for object RoIs. The WSD model is the teacher\nmodel and the classification model is the student model. After this cross-task\nknowledge distillation, the performance of the classification model is\nsignificantly improved and the efficiency is maintained since the WSD model can\nbe safely discarded in the test phase. Extensive experiments on two large-scale\ndatasets (MS-COCO and NUS-WIDE) show that our framework achieves superior\nperformances over the state-of-the-art methods on both performance and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:35:03 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 12:28:58 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Liu", "Yongcheng", ""], ["Sheng", "Lu", ""], ["Shao", "Jing", ""], ["Yan", "Junjie", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1809.06196", "submitter": "Zhuo Chen", "authors": "Zhuo Chen, Weisi Lin, Shiqi Wang, Lingyu Duan, Alex C. Kot", "title": "Intermediate Deep Feature Compression: the Next Battlefield of\n  Intelligent Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances of hardware technology have made the intelligent analysis\nequipped at the front-end with deep learning more prevailing and practical. To\nbetter enable the intelligent sensing at the front-end, instead of compressing\nand transmitting visual signals or the ultimately utilized top-layer deep\nlearning features, we propose to compactly represent and convey the\nintermediate-layer deep learning features of high generalization capability, to\nfacilitate the collaborating approach between front and cloud ends. This\nstrategy enables a good balance among the computational load, transmission load\nand the generalization ability for cloud servers when deploying the deep neural\nnetworks for large scale cloud based visual analysis. Moreover, the presented\nstrategy also makes the standardization of deep feature coding more feasible\nand promising, as a series of tasks can simultaneously benefit from the\ntransmitted intermediate layers. We also present the results for evaluation of\nlossless deep feature compression with four benchmark data compression methods,\nwhich provides meaningful investigations and baselines for future research and\nstandardization activities.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:38:41 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Chen", "Zhuo", ""], ["Lin", "Weisi", ""], ["Wang", "Shiqi", ""], ["Duan", "Lingyu", ""], ["Kot", "Alex C.", ""]]}, {"id": "1809.06582", "submitter": "Wladyslaw Skarbek", "authors": "Wladyslaw Skarbek", "title": "Symbolic Tensor Neural Networks for Digital Media - from Tensor\n  Processing via BNF Graph Rules to CREAMS Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial material on Convolutional Neural Networks (CNN) and its\napplications in digital media research is based on the concept of Symbolic\nTensor Neural Networks. The set of STNN expressions is specified in Backus-Naur\nForm (BNF) which is annotated by constraints typical for labeled acyclic\ndirected graphs (DAG). The BNF induction begins from a collection of neural\nunit symbols with extra (up to five) decoration fields (including tensor depth\nand sharing fields). The inductive rules provide not only the general graph\nstructure but also the specific shortcuts for residual blocks of units. A\nsyntactic mechanism for network fragments modularization is introduced via user\ndefined units and their instances. Moreover, the dual BNF rules are specified\nin order to generate the Dual Symbolic Tensor Neural Network (DSTNN). The\njoined interpretation of STNN and DSTNN provides the correct flow of gradient\ntensors, back propagated at the training stage. The proposed symbolic\nrepresentation of CNNs is illustrated for six generic digital media\napplications (CREAMS): Compression, Recognition, Embedding, Annotation, 3D\nModeling for human-computer interfacing, and data Security based on digital\nmedia objects. In order to make the CNN description and its gradient flow\ncomplete, for all presented applications, the symbolic representations of\nmathematically defined loss/gain functions and gradient flow equations for all\nused core units, are given. The tutorial is to convince the reader that STNN is\nnot only a convenient symbolic notation for public presentations of CNN based\nsolutions for CREAMS problems but also that it is a design blueprint with a\npotential for automatic generation of application source code.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:25:01 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 09:16:30 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Skarbek", "Wladyslaw", ""]]}, {"id": "1809.07062", "submitter": "Xiaoyu Du", "authors": "Jinhui Tang, Xiaoyu Du, Xiangnan He, Fajie Yuan, Qi Tian, and Tat-Seng\n  Chua", "title": "Adversarial Training Towards Robust Multimedia Recommender System", "comments": "TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the prevalence of multimedia content on the Web, developing recommender\nsolutions that can effectively leverage the rich signal in multimedia data is\nin urgent need. Owing to the success of deep neural networks in representation\nlearning, recent advance on multimedia recommendation has largely focused on\nexploring deep learning methods to improve the recommendation accuracy. To\ndate, however, there has been little effort to investigate the robustness of\nmultimedia representation and its impact on the performance of multimedia\nrecommendation.\n  In this paper, we shed light on the robustness of multimedia recommender\nsystem. Using the state-of-the-art recommendation framework and deep image\nfeatures, we demonstrate that the overall system is not robust, such that a\nsmall (but purposeful) perturbation on the input image will severely decrease\nthe recommendation accuracy. This implies the possible weakness of multimedia\nrecommender system in predicting user preference, and more importantly, the\npotential of improvement by enhancing its robustness. To this end, we propose a\nnovel solution named Adversarial Multimedia Recommendation (AMR), which can\nlead to a more robust multimedia recommender model by using adversarial\nlearning. The idea is to train the model to defend an adversary, which adds\nperturbations to the target image with the purpose of decreasing the model's\naccuracy. We conduct experiments on two representative multimedia\nrecommendation tasks, namely, image recommendation and visually-aware product\nrecommendation. Extensive results verify the positive effect of adversarial\nlearning and demonstrate the effectiveness of our AMR method. Source codes are\navailable in https://github.com/duxy-me/AMR.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:34:16 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 09:25:18 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 07:02:27 GMT"}, {"version": "v4", "created": "Wed, 29 May 2019 11:07:10 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Tang", "Jinhui", ""], ["Du", "Xiaoyu", ""], ["He", "Xiangnan", ""], ["Yuan", "Fajie", ""], ["Tian", "Qi", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1809.07792", "submitter": "Md Mehedi Hasan PhD", "authors": "Md Mehedi Hasan, John F. Arnold, Michael R. Frater", "title": "Binocular Rivalry - Psychovisual Challenge in Stereoscopic Video Error\n  Concealment", "comments": "11 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During Stereoscopic 3D (S3D) video transmission, one or both views can be\naffected by bit errors and packet losses caused by adverse channel conditions,\ndelay or jitter. Typically, the Human Visual System (HVS) is incapable of\naligning and fusing stereoscopic content if one view is affected by artefacts\ncaused by compression, transmission and rendering with distorted patterns being\nperceived as alterations of the original which presents a shimmering effect\nknown as binocular rivalry and is detrimental to a user's Quality of Experience\n(QoE). This study attempts to quantify the effects of binocular rivalry for\nstereoscopic videos. Existing approaches, in which one or more frames are lost\nin one or both views undergo error concealment, are implemented. Then,\nsubjective testing is carried out on the error concealed 3D video sequences.\nThe evaluations provided by these subjects were then combined and analysed\nusing a standard Student t-test thus quantifying the impact of binocular\nrivalry and allowing the impact to be compared with that of monocular viewing.\nThe main focus is implementing error-resilient video communication, avoiding\nthe detrimental effects of binocular rivalry and improving the overall QoE of\nviewers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 03:19:26 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Hasan", "Md Mehedi", ""], ["Arnold", "John F.", ""], ["Frater", "Michael R.", ""]]}, {"id": "1809.07793", "submitter": "Md Mehedi Hasan PhD", "authors": "Md Mehedi Hasan, Michael Frater, John Arnold", "title": "Survey on Error Concealment Strategies and Subjective Testing of 3D\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, different technologies to visualize 3D scenes have been\nintroduced and improved. These technologies include stereoscopic, multi-view,\nintegral imaging and holographic types. Despite increasing consumer interest;\npoor image quality, crosstalk or side effects of 3D displays and also the lack\nof defined broadcast standards has hampered the advancement of 3D displays to\nthe mass consumer market. Also, in real time transmission of 3DTV sequences\nover packet-based networks may results in visual quality degradations due to\npacket loss and others. In the conventional 2D videos different extrapolation\nand directional interpolation strategies have been used for concealing the\nmissing blocks but in 3D, it is still an emerging field of research. Few\nstudies have been carried out to define the assessment methods of stereoscopic\nimages and videos. But through industrial and commercial perspective,\nsubjective quality evaluation is the most direct way to evaluate human\nperception on 3DTV systems. This paper reviews the state-of-the-art error\nconcealment strategies and the subjective evaluation of 3D videos and proposes\na low complexity frame loss concealment method for the video decoder.\nSubjective testing on prominent datasets videos and comparison with existing\nconcealment methods show that the proposed method is very much efficient to\nconceal errors of stereoscopic videos in terms of computation time, comfort and\ndistortion.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 07:18:26 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Hasan", "Md Mehedi", ""], ["Frater", "Michael", ""], ["Arnold", "John", ""]]}, {"id": "1809.07999", "submitter": "Kyungmin Kim", "authors": "Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, Byoung-Tak Zhang", "title": "Multimodal Dual Attention Memory for Video Story Question Answering", "comments": "Accepted for ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a video story question-answering (QA) architecture, Multimodal\nDual Attention Memory (MDAM). The key idea is to use a dual attention mechanism\nwith late fusion. MDAM uses self-attention to learn the latent concepts in\nscene frames and captions. Given a question, MDAM uses the second attention\nover these latent concepts. Multimodal fusion is performed after the dual\nattention processes (late fusion). Using this processing pipeline, MDAM learns\nto infer a high-level vision-language joint representation from an abstraction\nof the full video content. We evaluate MDAM on PororoQA and MovieQA datasets\nwhich have large-scale QA annotations on cartoon videos and movies,\nrespectively. For both datasets, MDAM achieves new state-of-the-art results\nwith significant margins compared to the runner-up models. We confirm the best\nperformance of the dual attention mechanism combined with late fusion by\nablation studies. We also perform qualitative analysis by visualizing the\ninference mechanisms of MDAM.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 09:19:12 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Kim", "Kyung-Min", ""], ["Choi", "Seong-Ho", ""], ["Kim", "Jin-Hwa", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1809.08593", "submitter": "Lydia Weiland", "authors": "Lydia Weiland, Simone Paolo Ponzetto, Wolfgang Effelsberg, and Laura\n  Dietz", "title": "Understanding the Gist of Images - Ranking of Concepts for Multimedia\n  Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, where multimedia data is continuously generated, stored, and\ndistributed, multimedia indexing, with its purpose of group- ing similar data,\nbecomes more important than ever. Understanding the gist (=message) of\nmultimedia instances is framed in related work as a ranking of concepts from a\nknowledge base, i.e., Wikipedia. We cast the task of multimedia indexing as a\ngist understanding problem. Our pipeline benefits from external knowledge and\ntwo subsequent learning- to-rank (l2r) settings. The first l2r produces a\nranking of concepts rep- resenting the respective multimedia instance. The\nsecond l2r produces a mapping between the concept representation of an instance\nand the targeted class topic(s) for the multimedia indexing task. The\nevaluation on an established big size corpus (MIRFlickr25k, with 25,000\nimages), shows that multimedia indexing benefits from understanding the gist.\nFinally, with a MAP of 61.42, it can be shown that the multimedia in- dexing\ntask benefits from understanding the gist. Thus, the presented end-to-end\nsetting outperforms DBM and competes with Hashing-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 13:13:23 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Weiland", "Lydia", ""], ["Ponzetto", "Simone Paolo", ""], ["Effelsberg", "Wolfgang", ""], ["Dietz", "Laura", ""]]}, {"id": "1809.08753", "submitter": "Chih-Chung Hsu", "authors": "Chih-Chung Hsu, Chia-Yen Lee, Ting-Xuan Liao, Jun-Yi Lee, Tsai-Yne\n  Hou, Ying-Chu Kuo, Jing-Wen Lin, Ching-Yi Hsueh, Zhong-Xuan Zhan, Hsiang-Chin\n  Chien", "title": "An Iterative Refinement Approach for Social Media Headline Prediction", "comments": "5 pages, ACM Multimedia Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel iterative refinement approach to predict\nthe popularity score of the social media meta-data effectively. With the rapid\ngrowth of the social media on the Internet, how to adequately forecast the view\ncount or popularity becomes more important. Conventionally, the ensemble\napproach such as random forest regression achieves high and stable performance\non various prediction tasks. However, most of the regression methods may not\nprecisely predict the extreme high or low values. To address this issue, we\nfirst predict the initial popularity score and retrieve their residues. In\norder to correctly compensate those extreme values, we adopt an ensemble\nregressor to compensate the residues to further improve the prediction\nperformance. Comprehensive experiments are conducted to demonstrate the\nproposed iterative refinement approach outperforms the state-of-the-art\nregression approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 04:36:17 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Lee", "Chia-Yen", ""], ["Liao", "Ting-Xuan", ""], ["Lee", "Jun-Yi", ""], ["Hou", "Tsai-Yne", ""], ["Kuo", "Ying-Chu", ""], ["Lin", "Jing-Wen", ""], ["Hsueh", "Ching-Yi", ""], ["Zhan", "Zhong-Xuan", ""], ["Chien", "Hsiang-Chin", ""]]}, {"id": "1809.08754", "submitter": "Chih-Chung Hsu", "authors": "Chih-Chung Hsu, Chia-Yen Lee, Yi-Xiu Zhuang", "title": "Learning to Detect Fake Face Images in the Wild", "comments": "4 pages to appear in IEEE IS3C Conference (IEEE International\n  Symposium on Computer, Consumer and Control Conference), Dec. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Network (GAN) can be used to generate the\nrealistic image, improper use of these technologies brings hidden concerns. For\nexample, GAN can be used to generate a tampered video for specific people and\ninappropriate events, creating images that are detrimental to a particular\nperson, and may even affect that personal safety. In this paper, we will\ndevelop a deep forgery discriminator (DeepFD) to efficiently and effectively\ndetect the computer-generated images. Directly learning a binary classifier is\nrelatively tricky since it is hard to find the common discriminative features\nfor judging the fake images generated from different GANs. To address this\nshortcoming, we adopt contrastive loss in seeking the typical features of the\nsynthesized images generated by different GANs and follow by concatenating a\nclassifier to detect such computer-generated images. Experimental results\ndemonstrate that the proposed DeepFD successfully detected 94.7% fake images\ngenerated by several state-of-the-art GANs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 04:45:24 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 03:07:56 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 14:55:42 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Lee", "Chia-Yen", ""], ["Zhuang", "Yi-Xiu", ""]]}, {"id": "1809.10260", "submitter": "Chi Zhang", "authors": "Chi Zhang, Alexander Loui", "title": "A Coarse-To-Fine Framework For Video Object Segmentation", "comments": "6 pages, 11 figures, to appear in Electronic Imaging, Visual\n  Information Processing and Communication VIII, pp. 32-37(6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we develop an unsupervised coarse-to-fine video analysis\nframework and prototype system to extract a salient object in a video sequence.\nThis framework starts from tracking grid-sampled points along temporal frames,\ntypically using KLT tracking method. The tracking points could be divided into\nseveral groups due to their inconsistent movements. At the same time, the SLIC\nalgorithm is extended into 3D space to generate supervoxels. Coarse\nsegmentation is achieved by combining the categorized tracking points and\nsupervoxels of the corresponding frame in the video sequence. Finally, a\ngraph-based fine segmentation algorithm is used to extract the moving object in\nthe scene. Experimental results reveal that this method outperforms the\nprevious approaches in terms of accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 22:55:10 GMT"}], "update_date": "2018-09-30", "authors_parsed": [["Zhang", "Chi", ""], ["Loui", "Alexander", ""]]}, {"id": "1809.10352", "submitter": "Tahmida Mahmud", "authors": "Tahmida Mahmud, Mohammad Billah, Amit K. Roy-Chowdhury", "title": "Multi-View Frame Reconstruction with Conditional GAN", "comments": "5 pages, 4 figures, 3 tables, Accepted at IEEE Global Conference on\n  Signal and Information Processing, 2018", "journal-ref": null, "doi": "10.1109/GlobalSIP.2018.8646380", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view frame reconstruction is an important problem particularly when\nmultiple frames are missing and past and future frames within the camera are\nfar apart from the missing ones. Realistic coherent frames can still be\nreconstructed using corresponding frames from other overlapping cameras. We\npropose an adversarial approach to learn the spatio-temporal representation of\nthe missing frame using conditional Generative Adversarial Network (cGAN). The\nconditional input to each cGAN is the preceding or following frames within the\ncamera or the corresponding frames in other overlapping cameras, all of which\nare merged together using a weighted average. Representations learned from\nframes within the camera are given more weight compared to the ones learned\nfrom other cameras when they are close to the missing frames and vice versa.\nExperiments on two challenging datasets demonstrate that our framework produces\ncomparable results with the state-of-the-art reconstruction method in a single\ncamera and achieves promising performance in multi-camera scenario.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 05:49:52 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Mahmud", "Tahmida", ""], ["Billah", "Mohammad", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1809.10537", "submitter": "Pablo Gil Pereira", "authors": "Pablo Gil Pereira and Andreas Schmidt and Thorsten Herfet", "title": "Cross-Layer Effects on Training Neural Algorithms for Video Streaming", "comments": null, "journal-ref": null, "doi": "10.1145/3210445.3210453", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays Dynamic Adaptive Streaming over HTTP (DASH) is the most prevalent\nsolution on the Internet for multimedia streaming and responsible for the\nmajority of global traffic. DASH uses adaptive bit rate (ABR) algorithms, which\nselect the video quality considering performance metrics such as throughput and\nplayout buffer level. Pensieve is a system that allows to train ABR algorithms\nusing reinforcement learning within a simulated network environment and is\noutperforming existing approaches in terms of achieved performance. In this\npaper, we demonstrate that the performance of the trained ABR algorithms\ndepends on the implementation of the simulated environment used to train the\nneural network. We also show that the used congestion control algorithm impacts\nthe algorithms' performance due to cross-layer effects.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:25:10 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Pereira", "Pablo Gil", ""], ["Schmidt", "Andreas", ""], ["Herfet", "Thorsten", ""]]}, {"id": "1809.10961", "submitter": "Radu Horaud P", "authors": "Yutong Ban, Xavier Alameda-Pineda, Laurent Girin and Radu Horaud", "title": "Variational Bayesian Inference for Audio-Visual Tracking of Multiple\n  Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of tracking multiple speakers via the\nfusion of visual and auditory information. We propose to exploit the\ncomplementary nature of these two modalities in order to accurately estimate\nsmooth trajectories of the tracked persons, to deal with the partial or total\nabsence of one of the modalities over short periods of time, and to estimate\nthe acoustic status -- either speaking or silent -- of each tracked person\nalong time. We propose to cast the problem at hand into a generative\naudio-visual fusion (or association) model formulated as a latent-variable\ntemporal graphical model. This may well be viewed as the problem of maximizing\nthe posterior joint distribution of a set of continuous and discrete latent\nvariables given the past and current observations, which is intractable. We\npropose a variational inference model which amounts to approximate the joint\ndistribution with a factorized distribution. The solution takes the form of a\nclosed-form expectation maximization procedure. We describe in detail the\ninference algorithm, we evaluate its performance and we compare it with several\nbaseline methods. These experiments show that the proposed audio-visual tracker\nperforms well in informal meetings involving a time-varying number of people.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 11:03:03 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 16:54:55 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ban", "Yutong", ""], ["Alameda-Pineda", "Xavier", ""], ["Girin", "Laurent", ""], ["Horaud", "Radu", ""]]}, {"id": "1809.10998", "submitter": "Vaneet Aggarwal", "authors": "Anis Elgabli and Muhamad Felemban and Vaneet Aggarwal", "title": "GroupCast: Preference-Aware Cooperative Video Streaming with Scalable\n  Video Coding", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a preference-aware cooperative video streaming\nsystem for videos encoded using Scalable Video Coding (SVC) where all the\ncollaborating users are interested in watching a video together on a shared\nscreen. However, each user's willingness to cooperate is subject to her own\nconstraints such as user data plans and/or energy consumption. Using SVC, each\nlayer of every chunk can be fetched through any of the cooperating users. We\nformulate the problem of finding the optimal quality decisions and fetching\npolicy of the SVC layers of video chunks subject to the available bandwidth,\nchunk deadlines, and cooperation willingness of the different users as an\noptimization problem. The objective is to optimize a QoE metric that maintains\na trade-off between maximizing the playback rate of every chunk while ensuring\nfairness among all chunks for the minimum skip/stall duration without violating\nany of the imposed constraints. We propose an offline algorithm to solve the\nnon-convex optimization problem when the bandwidth prediction is non-causally\nknown. This algorithm has a run-time complexity that is polynomial in the video\nlength and the number of cooperating users. Furthermore, we propose an online\nversion of the algorithm for more practical scenarios where erroneous bandwidth\nprediction for a short window is used. Real implementation with android devices\nusing SVC encoded video on public bandwidth traces' dataset reveals the\nrobustness and performance of the proposed algorithm and shows that the\nalgorithm significantly outperforms round robin based mechanisms in terms of\navoiding skips/stalls and fetching video chunks at their highest quality\npossible.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 12:53:30 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Elgabli", "Anis", ""], ["Felemban", "Muhamad", ""], ["Aggarwal", "Vaneet", ""]]}]