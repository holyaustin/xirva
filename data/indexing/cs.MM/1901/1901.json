[{"id": "1901.00295", "submitter": "Xingjian Du", "authors": "Xingjian Du, Mengyao Zhu, Xuan Shi, Xinpeng Zhang, Wen Zhang, Jingdong\n  Chen", "title": "End-to-End Model for Speech Enhancement by Consistent Spectrogram\n  Masking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, phase processing is attracting increasinginterest in speech\nenhancement community. Some researchersintegrate phase estimations module into\nspeech enhancementmodels by using complex-valued short-time Fourier\ntransform(STFT) spectrogram based training targets, e.g. Complex RatioMask\n(cRM) [1]. However, masking on spectrogram would violentits consistency\nconstraints. In this work, we prove that theinconsistent problem enlarges the\nsolution space of the speechenhancement model and causes unintended artifacts.\nConsistencySpectrogram Masking (CSM) is proposed to estimate the\ncomplexspectrogram of a signal with the consistency constraint in asimple but\nnot trivial way. The experiments comparing ourCSM based end-to-end model with\nother methods are conductedto confirm that the CSM accelerate the model\ntraining andhave significant improvements in speech quality. From\nourexperimental results, we assured that our method could enha\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 08:39:05 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Du", "Xingjian", ""], ["Zhu", "Mengyao", ""], ["Shi", "Xuan", ""], ["Zhang", "Xinpeng", ""], ["Zhang", "Wen", ""], ["Chen", "Jingdong", ""]]}, {"id": "1901.00450", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Dmitry Bogdanov, Jisang Yoon, KwangSeob Kim and Xavier\n  Serra", "title": "Automatic playlist continuation using a hybrid recommender system\n  combining features from text and audio", "comments": "5 pages", "journal-ref": "Proceeding RecSys Challenge '18 Proceedings of the ACM Recommender\n  Systems Challenge 2018", "doi": "10.1145/3267471.3267473", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ACM RecSys Challenge 2018 focuses on music recommendation in the context\nof automatic playlist continuation. In this paper, we describe our approach to\nthe problem and the final hybrid system that was submitted to the challenge by\nour team Cocoplaya. This system consists in combining the recommendations\nproduced by two different models using ranking fusion. The first model is based\non Matrix Factorization and it incorporates information from tracks' audio and\nplaylist titles. The second model generates recommendations based on typical\ntrack co-occurrences considering their proximity in the playlists. The proposed\napproach is efficient and achieves a good overall performance, with our model\nranked 4th on the creative track of the challenge leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 16:49:06 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ferraro", "Andres", ""], ["Bogdanov", "Dmitry", ""], ["Yoon", "Jisang", ""], ["Kim", "KwangSeob", ""], ["Serra", "Xavier", ""]]}, {"id": "1901.01085", "submitter": "Md Sahidullah", "authors": "Md Sahidullah, Hector Delgado, Massimiliano Todisco, Tomi Kinnunen,\n  Nicholas Evans, Junichi Yamagishi and Kong-Aik Lee", "title": "Introduction to Voice Presentation Attack Detection and Recent Advances", "comments": "Published as a book-chapter in Handbook of Biometric Anti-Spoofing\n  Presentation Attack Detection (Second Edition)", "journal-ref": "Published in Handbook of Biometric Anti-Spoofing Presentation\n  Attack Detection (Second Edition eBook ISBN 978-3-319-92627-8), 2019", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years significant progress has been made in the field of\npresentation attack detection (PAD) for automatic speaker recognition (ASV).\nThis includes the development of new speech corpora, standard evaluation\nprotocols and advancements in front-end feature extraction and back-end\nclassifiers. The use of standard databases and evaluation protocols has enabled\nfor the first time the meaningful benchmarking of different PAD solutions. This\nchapter summarises the progress, with a focus on studies completed in the last\nthree years. The article presents a summary of findings and lessons learned\nfrom two ASVspoof challenges, the first community-led benchmarking efforts.\nThese show that ASV PAD remains an unsolved problem and that further attention\nis required to develop generalised PAD solutions which have potential to detect\ndiverse and previously unseen spoofing attacks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 13:31:25 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Sahidullah", "Md", ""], ["Delgado", "Hector", ""], ["Todisco", "Massimiliano", ""], ["Kinnunen", "Tomi", ""], ["Evans", "Nicholas", ""], ["Yamagishi", "Junichi", ""], ["Lee", "Kong-Aik", ""]]}, {"id": "1901.01342", "submitter": "Sourish Chaudhuri", "authors": "Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Radhika Marvin, Andrew\n  Gallagher, Liat Kaver, Sharadh Ramaswamy, Arkadiusz Stopczynski, Cordelia\n  Schmid, Zhonghua Xi, Caroline Pantofaru", "title": "AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active speaker detection is an important component in video analysis\nalgorithms for applications such as speaker diarization, video re-targeting for\nmeetings, speech enhancement, and human-robot interaction. The absence of a\nlarge, carefully labeled audio-visual dataset for this task has constrained\nalgorithm evaluations with respect to data diversity, environments, and\naccuracy. This has made comparisons and improvements difficult. In this paper,\nwe present the AVA Active Speaker detection dataset (AVA-ActiveSpeaker) that\nwill be released publicly to facilitate algorithm development and enable\ncomparisons. The dataset contains temporally labeled face tracks in video,\nwhere each face instance is labeled as speaking or not, and whether the speech\nis audible. This dataset contains about 3.65 million human labeled frames or\nabout 38.5 hours of face tracks, and the corresponding audio. We also present a\nnew audio-visual approach for active speaker detection, and analyze its\nperformance, demonstrating both its strength and the contributions of the\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 00:01:06 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 01:28:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Roth", "Joseph", ""], ["Chaudhuri", "Sourish", ""], ["Klejch", "Ondrej", ""], ["Marvin", "Radhika", ""], ["Gallagher", "Andrew", ""], ["Kaver", "Liat", ""], ["Ramaswamy", "Sharadh", ""], ["Stopczynski", "Arkadiusz", ""], ["Schmid", "Cordelia", ""], ["Xi", "Zhonghua", ""], ["Pantofaru", "Caroline", ""]]}, {"id": "1901.01474", "submitter": "Zheng Zhang", "authors": "Yujuan Ding, Wai Kueng Wong, Zhihui Lai, Zheng Zhang", "title": "Bilinear Supervised Hashing Based on 2D Image Features", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been recognized as an efficient representation learning method to\neffectively handle big data due to its low computational complexity and memory\ncost. Most of the existing hashing methods focus on learning the\nlow-dimensional vectorized binary features based on the high-dimensional raw\nvectorized features. However, studies on how to obtain preferable binary codes\nfrom the original 2D image features for retrieval is very limited. This paper\nproposes a bilinear supervised discrete hashing (BSDH) method based on 2D image\nfeatures which utilizes bilinear projections to binarize the image matrix\nfeatures such that the intrinsic characteristics in the 2D image space are\npreserved in the learned binary codes. Meanwhile, the bilinear projection\napproximation and vectorization binary codes regression are seamlessly\nintegrated together to formulate the final robust learning framework.\nFurthermore, a discrete optimization strategy is developed to alternatively\nupdate each variable for obtaining the high-quality binary codes. In addition,\ntwo 2D image features, traditional SURF-based FVLAD feature and CNN-based\nAlexConv5 feature are designed for further improving the performance of the\nproposed BSDH method. Results of extensive experiments conducted on four\nbenchmark datasets show that the proposed BSDH method almost outperforms all\ncompeting hashing methods with different input features by different evaluation\nprotocols.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 23:39:27 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ding", "Yujuan", ""], ["Wong", "Wai Kueng", ""], ["Lai", "Zhihui", ""], ["Zhang", "Zheng", ""]]}, {"id": "1901.01848", "submitter": "Roberto Azevedo", "authors": "Roberto G. de A. Azevedo, Neil Birkbeck, Francesca De Simone, Ivan\n  Janatra, Balu Adsumilli, Pascal Frossard", "title": "Visual Distortions in 360-degree Videos", "comments": "First appeared online at: December 30, 2018.\n  https://www.researchgate.net/publication/330008773_Visual_Distortions_in_360-degree_Videos", "journal-ref": "in IEEE Transactions on Circuits and Systems for Video Technology,\n  vol. 30, no. 8, pp. 2524-2537, Aug. 2020", "doi": "10.1109/TCSVT.2019.2927344", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional (or 360-degree) images and videos are emergent signals in\nmany areas such as robotics and virtual/augmented reality. In particular, for\nvirtual reality, they allow an immersive experience in which the user is\nprovided with a 360-degree field of view and can navigate throughout a scene,\ne.g., through the use of Head Mounted Displays. Since it represents the full\n360-degree field of view from one point of the scene, omnidirectional content\nis naturally represented as spherical visual signals. Current approaches for\ncapturing, processing, delivering, and displaying 360-degree content, however,\npresent many open technical challenges and introduce several types of\ndistortions in these visual signals. Some of the distortions are specific to\nthe nature of 360-degree images, and often different from those encountered in\nthe classical image communication framework. This paper provides a first\ncomprehensive review of the most common visual distortions that alter\n360-degree signals undergoing state of the art processing in common\napplications. While their impact on viewers' visual perception and on the\nimmersive experience at large is still unknown ---thus, it stays an open\nresearch topic--- this review serves the purpose of identifying the main causes\nof visual distortions in the end-to-end 360-degree content distribution\npipeline. It is essential as a basis for benchmarking different processing\ntechniques, allowing the effective design of new algorithms and applications.\nIt is also necessary to the deployment of proper psychovisual studies to\ncharacterise the human perception of these new images in interactive and\nimmersive applications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 14:52:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Azevedo", "Roberto G. de A.", ""], ["Birkbeck", "Neil", ""], ["De Simone", "Francesca", ""], ["Janatra", "Ivan", ""], ["Adsumilli", "Balu", ""], ["Frossard", "Pascal", ""]]}, {"id": "1901.02529", "submitter": "Xuan Thanh Nguyen", "authors": "X. T. Nguyen, T. D. Ngo and T. H. Le", "title": "A Spatial-temporal 3D Human Pose Reconstruction Framework", "comments": "10 pages. JIPS Journal 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose reconstruction from single-view camera is a difficult and\nchallenging topic. Many approaches have been proposed, but almost focusing on\nframe-by-frame independently while inter-frames are highly correlated in a pose\nsequence. In contrast, we introduce a novel spatial-temporal 3D reconstruction\nframework that leverages both intra and inter frame relationships in\nconsecutive 2D pose sequences. Orthogonal Matching Pursuit (OMP) algorithm,\npre-trained Pose-angle Limits and Temporal Models have been implemented. We\nquantitatively compare our framework versus recent works on CMU motion capture\ndataset and Vietnamese traditional dance sequences. Our method outperforms\nothers with 10 percent lower of Euclidean reconstruction error and robustness\nagainst Gaussian noise. Additionally, it is also important to mention that our\nreconstructed 3D pose sequences are smoother and more natural than others.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 21:46:52 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 09:20:23 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Nguyen", "X. T.", ""], ["Ngo", "T. D.", ""], ["Le", "T. H.", ""]]}, {"id": "1901.02701", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Dolzodmaa Davaasuren, Nilam Ram, Prasenjit Mitra,\n  Byron Reeves, Thomas Robinson", "title": "Guess What's on my Screen? Clustering Smartphone Screenshots with Active\n  Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant proportion of individuals' daily activities is experienced\nthrough digital devices. Smartphones in particular have become one of the\npreferred interfaces for content consumption and social interaction.\nIdentifying the content embedded in frequently-captured smartphone screenshots\nis thus a crucial prerequisite to studies of media behavior and health\nintervention planning that analyze activity interplay and content switching\nover time. Screenshot images can depict heterogeneous contents and\napplications, making the a priori definition of adequate taxonomies a\ncumbersome task, even for humans. Privacy protection of the sensitive data\ncaptured on screens means the costs associated with manual annotation are\nlarge, as the effort cannot be crowd-sourced. Thus, there is need to examine\nutility of unsupervised and semi-supervised methods for digital screenshot\nclassification. This work introduces the implications of applying clustering on\nlarge screenshot sets when only a limited amount of labels is available. In\nthis paper we develop a framework for combining K-Means clustering with Active\nLearning for efficient leveraging of labeled and unlabeled samples, with the\ngoal of discovering latent classes and describing a large collection of\nscreenshot data. We tested whether SVM-embedded or XGBoost-embedded solutions\nfor class probability propagation provide for more well-formed cluster\nconfigurations. Visual and textual vector representations of the screenshot\nimages are derived and combined to assess the relative contribution of\nmulti-modal features to the overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 12:51:36 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 11:22:33 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Chiatti", "Agnese", ""], ["Davaasuren", "Dolzodmaa", ""], ["Ram", "Nilam", ""], ["Mitra", "Prasenjit", ""], ["Reeves", "Byron", ""], ["Robinson", "Thomas", ""]]}, {"id": "1901.03404", "submitter": "Mallesham Dasari", "authors": "Dasari Mallesham, Christina Vlachou, Shruti Sanadhya, Pranjal Sahu,\n  Yang Qiu, Kyu-Han Kim, Samir R. Das", "title": "Handcrafted vs Deep Learning Classification for Scalable Video QoE\n  Modeling", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile video traffic is dominant in cellular and enterprise wireless\nnetworks. With the advent of diverse applications, network administrators face\nthe challenge to provide high QoE in the face of diverse wireless conditions\nand application contents. Yet, state-of-the-art networks lack analytics for\nQoE, as this requires support from the application or user feedback. While\nthere are existing techniques to map QoS to QoE by training machine learning\nmodels without requiring user feedback, these techniques are limited to only\nfew applications, due to insufficient QoE ground-truth annotation for ML. To\naddress these limitations, we focus on video telephony applications and model\nkey artefacts of spatial and temporal video QoE. Our key contribution is\ndesigning content- and device-independent metrics and training across diverse\nWiFi conditions. We show that our metrics achieve a median 90% accuracy by\ncomparing with mean-opinion-score from more than 200 users and 800 video\nsamples over three popular video telephony applications -- Skype, FaceTime and\nGoogle Hangouts. We further extend our metrics by using deep neural networks,\nmore specifically we use a combined CNN and LSTM model. We achieve a median\naccuracy of 95% by combining our QoE metrics with the deep learning model,\nwhich is a 38% improvement over the state-of-the-art well known techniques.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 21:33:19 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Mallesham", "Dasari", ""], ["Vlachou", "Christina", ""], ["Sanadhya", "Shruti", ""], ["Sahu", "Pranjal", ""], ["Qiu", "Yang", ""], ["Kim", "Kyu-Han", ""], ["Das", "Samir R.", ""]]}, {"id": "1901.03536", "submitter": "David Glowacki", "authors": "Lisa May Thomas, Helen M. Deeks, Alex J. Jones, Oussama Metatla, David\n  R. Glowacki", "title": "Somatic Practices for Understanding Real, Imagined, and Virtual\n  Realities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In most VR experiences, the visual sense dominates other modes of sensory\ninput, encouraging non-visual senses to respond as if the visual were real. The\nsimulated visual world thus becomes a sort of felt actuality, where the\n'actual' physical body and environment can 'drop away', opening up\npossibilities for designing entirely new kinds of experience. Most VR\nexperiences place visual sensory input (of the simulated environment) in the\nperceptual foreground, and the physical body in the background. In what\nfollows, we discuss methods for resolving the apparent tension which arises\nfrom VR's prioritization of visual perception. We specifically aim to\nunderstand how somatic techniques encouraging participants to 'attend to their\nattention' enable them to access more subtle aspects of sensory phenomena in a\nVR experience, bound neither by rigid definitions of vision-based virtuality\nnor body-based corporeality. During a series of workshops, we implemented\nexperimental somatic-dance practices to better understand perceptual and\nimaginative subtleties that arise for participants whilst they are embedded in\na multi-person VR framework. Our preliminary observations suggest that somatic\nmethods can be used to design VR experiences which enable (i) a tactile quality\nor felt sense of phenomena in the virtual environment (VE), (ii) lingering\nimpacts on participant imagination even after the VR headset is taken off, and\n(iii) an expansion of imaginative potential.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:06:46 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Thomas", "Lisa May", ""], ["Deeks", "Helen M.", ""], ["Jones", "Alex J.", ""], ["Metatla", "Oussama", ""], ["Glowacki", "David R.", ""]]}, {"id": "1901.03756", "submitter": "Esube Bekele", "authors": "Esube Bekele and Wallace Lawson", "title": "The Deeper, the Better: Analysis of Person Attributes Recognition", "comments": "8 pages, 34 png figures and 1 pdf figure, uses FG2019.sty, submitted\n  to FG2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In person attributes recognition, we describe a person in terms of their\nappearance. Typically, this includes a wide range of traits including age,\ngender, clothing, and footwear. Although this could be used in a wide variety\nof scenarios, it generally is applied to video surveillance, where attribute\nrecognition is impacted by low resolution, and other issues such as variable\npose, occlusion and shadow. Recent approaches have used deep convolutional\nneural networks (CNNs) to improve the accuracy in person attribute recognition.\nHowever, many of these networks are relatively shallow and it is unclear to\nwhat extent they use contextual cues to improve classification accuracy. In\nthis paper, we propose deeper methods for person attribute recognition.\nInterpreting the reasons behind the classification is highly important, as it\ncan provide insight into how the classifier is making decisions. Interpretation\nsuggests that deeper networks generally take more contextual information into\nconsideration, which helps improve classification accuracy and\ngeneralizability. We present experimental analysis and results for whole body\nattributes using the PA-100K and PETA datasets and facial attributes using the\nCelebA dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 21:52:57 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bekele", "Esube", ""], ["Lawson", "Wallace", ""]]}, {"id": "1901.03892", "submitter": "Kevin Zhang", "authors": "Kevin Alex Zhang, Alfredo Cuesta-Infante, Lei Xu, Kalyan\n  Veeramachaneni", "title": "SteganoGAN: High Capacity Image Steganography with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image steganography is a procedure for hiding messages inside pictures. While\nother techniques such as cryptography aim to prevent adversaries from reading\nthe secret message, steganography aims to hide the presence of the message\nitself. In this paper, we propose a novel technique for hiding arbitrary binary\ndata in images using generative adversarial networks which allow us to optimize\nthe perceptual quality of the images produced by our model. We show that our\napproach achieves state-of-the-art payloads of 4.4 bits per pixel, evades\ndetection by steganalysis tools, and is effective on images from multiple\ndatasets. To enable fair comparisons, we have released an open source library\nthat is available online at https://github.com/DAI-Lab/SteganoGAN.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 18:47:38 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:39:54 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Zhang", "Kevin Alex", ""], ["Cuesta-Infante", "Alfredo", ""], ["Xu", "Lei", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "1901.04268", "submitter": "Zehang Lin", "authors": "Zhenguo Yang, Zehang Lin, Peipei Kang, Jianming Lv, Qing Li and Wenyin\n  Liu", "title": "Learning Shared Semantic Space with Correlation Alignment for\n  Cross-modal Event Retrieval", "comments": "22 pages, submitted to ACM Transactions on Multimedia Computing\n  Communications and Applications(ACM TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to learn shared semantic space with correlation\nalignment (${S}^{3}CA$) for multimodal data representations, which aligns\nnonlinear correlations of multimodal data distributions in deep neural networks\ndesigned for heterogeneous data. In the context of cross-modal (event)\nretrieval, we design a neural network with convolutional layers and\nfully-connected layers to extract features for images, including images on\nFlickr-like social media. Simultaneously, we exploit a fully-connected neural\nnetwork to extract semantic features for texts, including news articles from\nnews media. In particular, nonlinear correlations of layer activations in the\ntwo neural networks are aligned with correlation alignment during the joint\ntraining of the networks. Furthermore, we project the multimodal data into a\nshared semantic space for cross-modal (event) retrieval, where the distances\nbetween heterogeneous data samples can be measured directly. In addition, we\ncontribute a Wiki-Flickr Event dataset, where the multimodal data samples are\nnot describing each other in pairs like the existing paired datasets, but all\nof them are describing semantic events. Extensive experiments conducted on both\npaired and unpaired datasets manifest the effectiveness of ${S}^{3}CA$,\noutperforming the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 12:48:53 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 13:19:55 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 00:31:51 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Yang", "Zhenguo", ""], ["Lin", "Zehang", ""], ["Kang", "Peipei", ""], ["Lv", "Jianming", ""], ["Li", "Qing", ""], ["Liu", "Wenyin", ""]]}, {"id": "1901.04555", "submitter": "Zain Nasrullah", "authors": "Zain Nasrullah, Yue Zhao", "title": "Music Artist Classification with Convolutional Recurrent Neural Networks", "comments": "Proceedings of the 2019 International Joint Conference on Neural\n  Networks (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous attempts at music artist classification use frame level audio\nfeatures which summarize frequency content within short intervals of time.\nComparatively, more recent music information retrieval tasks take advantage of\ntemporal structure in audio spectrograms using deep convolutional and recurrent\nmodels. This paper revisits artist classification with this new framework and\nempirically explores the impacts of incorporating temporal structure in the\nfeature representation. To this end, an established classification\narchitecture, a Convolutional Recurrent Neural Network (CRNN), is applied to\nthe artist20 music artist identification dataset under a comprehensive set of\nconditions. These include audio clip length, which is a novel contribution in\nthis work, and previously identified considerations such as dataset split and\nfeature level. Our results improve upon baseline works, verify the influence of\nthe producer effect on classification performance and demonstrate the\ntrade-offs between audio length and training set size. The best performing\nmodel achieves an average F1 score of 0.937 across three independent trials\nwhich is a substantial improvement over the corresponding baseline under\nsimilar conditions. Additionally, to showcase the effectiveness of the CRNN's\nfeature extraction capabilities, we visualize audio samples at the model's\nbottleneck layer demonstrating that learned representations segment into\nclusters belonging to their respective artists.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 20:33:44 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 20:29:11 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Nasrullah", "Zain", ""], ["Zhao", "Yue", ""]]}, {"id": "1901.05712", "submitter": "Amr Rizk", "authors": "Bastian Alt, Trevor Ballard, Ralf Steinmetz, Heinz Koeppl, Amr Rizk", "title": "CBA: Contextual Quality Adaptation for Adaptive Bitrate Video Streaming\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in quality adaptation algorithms leave adaptive bitrate (ABR)\nstreaming architectures at a crossroads: When determining the sustainable video\nquality one may either rely on the information gathered at the client vantage\npoint or on server and network assistance. The fundamental problem here is to\ndetermine how valuable either information is for the adaptation decision. This\nproblem becomes particularly hard in future Internet settings such as Named\nData Networking (NDN) where the notion of a network connection does not exist.\n  In this paper, we provide a fresh view on ABR quality adaptation for QoE\nmaximization, which we formalize as a decision problem under uncertainty, and\nfor which we contribute a sparse Bayesian contextual bandit algorithm denoted\nCBA. This allows taking high-dimensional streaming context information,\nincluding client-measured variables and network assistance, to find online the\nmost valuable information for the quality adaptation. Since sparse Bayesian\nestimation is computationally expensive, we develop a fast new inference scheme\nto support online video adaptation. We perform an extensive evaluation of our\nadaptation algorithm in the particularly challenging setting of NDN, where we\nuse an emulation testbed to demonstrate the efficacy of CBA compared to\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 10:10:30 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Alt", "Bastian", ""], ["Ballard", "Trevor", ""], ["Steinmetz", "Ralf", ""], ["Koeppl", "Heinz", ""], ["Rizk", "Amr", ""]]}, {"id": "1901.06279", "submitter": "Marta Orduna", "authors": "Marta Orduna, C\\'esar D\\'iaz, Lara Mu\\~noz, Pablo P\\'erez, Ignacio\n  Benito, and Narciso Garc\\'ia", "title": "Video Multimethod Assessment Fusion (VMAF) on 360VR contents", "comments": null, "journal-ref": "IEEE Trans. Consumer Electronics, vol. 66, no. 1, pp. 22-31, Feb.\n  2020", "doi": "10.1109/TCE.2019.2957987", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the subjective experiments and subsequent analysis\ncarried out to validate the application of one of the most robust and\ninfluential video quality metrics, Video Multimethod Assessment Fusion (VMAF),\nto 360VR contents. VMAF is a full reference metric initially designed to work\nwith traditional 2D contents. Hence, at first, it cannot be assumed to be\ncompatible with the particularities of the scenario where omnidirectional\ncontent is visualized using a Head-Mounted Display (HMD). Therefore, through a\ncomplete set of tests, we prove that this metric can be successfully used\nwithout any specific training or adjustments to obtain the quality of 360VR\nsequences actually perceived by users.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 14:49:37 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Orduna", "Marta", ""], ["D\u00edaz", "C\u00e9sar", ""], ["Mu\u00f1oz", "Lara", ""], ["P\u00e9rez", "Pablo", ""], ["Benito", "Ignacio", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "1901.06838", "submitter": "Yanzhen Ren", "authors": "Yanzhen Ren, Dengkai Liu, Qiaochu Xiong, Jianming Fu, and Lina Wang", "title": "Spec-ResNet: A General Audio Steganalysis scheme based on Deep Residual\n  Network of Spectrogram", "comments": "12 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread application of audio and video communication technology make\nthe compressed audio data flowing over the Internet, and make it become an\nimportant carrier for covert communication. There are many steganographic\nschemes emerged in the mainstream audio compression data, such as AAC and MP3,\nfollowed by many steganalysis schemes. However, these steganalysis schemes are\nonly effective in the specific embedded domain. In this paper, a general\nsteganalysis scheme Spec-ResNet (Deep Residual Network of Spectrogram) is\nproposed to detect the steganography schemes of different embedding domain for\nAAC and MP3. The basic idea is that the steganographic modification of\ndifferent embedding domain will all introduce the change of the decoded audio\nsignal. In this paper, the spectrogram, which is the visual representation of\nthe spectrum of frequencies of audio signal, is adopted as the input of the\nfeature network to extract the universal features introduced by steganography\nschemes; Deep Neural Network Spec-ResNet is well-designed to represent the\nsteganalysis feature; and the features extracted from different spectrogram\nwindows are combined to fully capture the steganalysis features. The experiment\nresults show that the proposed scheme has good detection accuracy and\ngenerality. The proposed scheme has better detection accuracy for three\ndifferent AAC steganographic schemes and MP3Stego than the state-of-arts\nsteganalysis schemes which are based on traditional hand-crafted or CNN-based\nfeature. To the best of our knowledge, the audio steganalysis scheme based on\nthe spectrogram and deep residual network is first proposed in this paper. The\nmethod proposed in this paper can be extended to the audio steganalysis of\nother codec or audio forensics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 09:32:21 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 10:43:19 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Ren", "Yanzhen", ""], ["Liu", "Dengkai", ""], ["Xiong", "Qiaochu", ""], ["Fu", "Jianming", ""], ["Wang", "Lina", ""]]}, {"id": "1901.06919", "submitter": "Mikael Le Pendu", "authors": "Mikael Le Pendu and Christine Guillemot and Aljosa Smolic", "title": "A Fourier Disparity Layer representation for Light Fields", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2922099", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a new Light Field representation for efficient\nLight Field processing and rendering called Fourier Disparity Layers (FDL). The\nproposed FDL representation samples the Light Field in the depth (or\nequivalently the disparity) dimension by decomposing the scene as a discrete\nsum of layers. The layers can be constructed from various types of Light Field\ninputs including a set of sub-aperture images, a focal stack, or even a\ncombination of both. From our derivations in the Fourier domain, the layers are\nsimply obtained by a regularized least square regression performed\nindependently at each spatial frequency, which is efficiently parallelized in a\nGPU implementation. Our model is also used to derive a gradient descent based\ncalibration step that estimates the input view positions and an optimal set of\ndisparity values required for the layer construction. Once the layers are\nknown, they can be simply shifted and filtered to produce different viewpoints\nof the scene while controlling the focus and simulating a camera aperture of\narbitrary shape and size. Our implementation in the Fourier domain allows real\ntime Light Field rendering. Finally, direct applications such as view\ninterpolation or extrapolation and denoising are presented and evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:11:11 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Pendu", "Mikael Le", ""], ["Guillemot", "Christine", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1901.06991", "submitter": "Ovidiu Banias", "authors": "Ovidiu Banias, Camil Octavian Milincu", "title": "Hybrid Design Tools - Image Quality Assessment of a Digitally Augmented\n  Blackboard Integrated System", "comments": null, "journal-ref": "Informatics 2019, 6(1), 5;\n  https://doi.org/10.3390/informatics6010006", "doi": "10.3390/informatics6010006", "report-no": null, "categories": "cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, Interactive White Boards (IWBs) have been widely\navailable as a pedagogic tool. The usability of these boards for teaching\ndisciplines where complex drawings are needed, we consider debatable in\nmultiple regards. In a previous study, we proposed an alternative to the IWBs\nas a blackboard augmented with a minimum of necessary digital elements. The\ncurrent study continues our previous research on hybrid design tools, analyzing\nthe limitations of the developed hybrid system regarding the perceived quality\nof the images being repeatedly captured, annotated, and reprojected onto the\nboard. We validated the hybrid system by evaluating the quality of the\nprojected and reprojected images over a blackboard, using both objective\nmeasurements and subjective human perception in extensive and realistic case\nstudies. Based on the results achieved in the current research, we conclude\nthat the proposed hybrid system provides good quality support for teaching\ndisciplines that require complex drawings and board interaction.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 16:28:41 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Banias", "Ovidiu", ""], ["Milincu", "Camil Octavian", ""]]}, {"id": "1901.07172", "submitter": "Ronald Salloum", "authors": "Ronald Salloum and C.-C. Jay Kuo", "title": "Efficient Image Splicing Localization via Contrastive Feature Extraction", "comments": "This manuscript was submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new data visualization and clustering technique\nfor discovering discriminative structures in high-dimensional data. This\ntechnique, referred to as cPCA++, utilizes the fact that the interesting\nfeatures of a \"target\" dataset may be obscured by high variance components\nduring traditional PCA. By analyzing what is referred to as a \"background\"\ndataset (i.e., one that exhibits the high variance principal components but not\nthe interesting structures), our technique is capable of efficiently\nhighlighting the structure that is unique to the \"target\" dataset. Similar to\nanother recently proposed algorithm called \"contrastive PCA\" (cPCA), the\nproposed cPCA++ method identifies important dataset specific patterns that are\nnot detected by traditional PCA in a wide variety of settings. However, the\nproposed cPCA++ method is significantly more efficient than cPCA, because it\ndoes not require the parameter sweep in the latter approach. We applied the\ncPCA++ method to the problem of image splicing localization. In this\napplication, we utilize authentic edges as the background dataset and the\nspliced edges as the target dataset. The proposed method is significantly more\nefficient than state-of-the-art methods, as the former does not require\niterative updates of filter weights via stochastic gradient descent and\nbackpropagation, nor the training of a classifier. Furthermore, the cPCA++\nmethod is shown to provide performance scores comparable to the\nstate-of-the-art Multi-task Fully Convolutional Network (MFCN).\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 04:37:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Salloum", "Ronald", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1901.07840", "submitter": "Valery Gorbachev", "authors": "V.N. Gorbachev, L.A. Denisov, E.M. Kaynarova, I.K. Metelev, E.S.\n  Yakovleva", "title": "On basis images for the digital image representation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital array orthogonal transformations that can be presented as a\ndecomposition over basis items or basis images are considered. The orthogonal\ntransform provides digital data scattering, a process of pixel energy\nredistributing, that is illustrated with the help of basis images. Data\nscattering plays important role for applications as image coding and\nwatermarking. We established a simple quantum analogues of basis images. They\nare representations of quantum operators that describe transition of single\nparticle between its states.\n  Considering basis images as items of a matrix, we introduced a block matrix\nthat is suitable for orthogonal transforms of multi-dimensional arrays such as\nblock vector, components of which are matrices. We present an orthogonal\ntransform that produces correlation between arrays. Due to correlation new\nfeature of data scattering was found. A presented detection algorithm is an\nexample of how it can be used in frequency domain watermarking.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 12:10:39 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Gorbachev", "V. N.", ""], ["Denisov", "L. A.", ""], ["Kaynarova", "E. M.", ""], ["Metelev", "I. K.", ""], ["Yakovleva", "E. S.", ""]]}, {"id": "1901.08025", "submitter": "Dipjyoti Paul", "authors": "Dipjyoti Paul, Md Sahidullah, Goutam Saha", "title": "Generalization of Spoofing Countermeasures: a Case Study with ASVspoof\n  2015 and BTAS 2016 Corpora", "comments": null, "journal-ref": "Published in 2017 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP 2017), New Orleans, LA, USA", "doi": null, "report-no": null, "categories": "cs.MM eess.AS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Voice-based biometric systems are highly prone to spoofing attacks. Recently,\nvarious countermeasures have been developed for detecting different kinds of\nattacks such as replay, speech synthesis (SS) and voice conversion (VC). Most\nof the existing studies are conducted with a specific training set defined by\nthe evaluation protocol. However, for realistic scenarios, selecting\nappropriate training data is an open challenge for the system administrator.\nMotivated by this practical concern, this work investigates the generalization\ncapability of spoofing countermeasures in restricted training conditions where\nspeech from a broad attack types are left out in the training database. We\ndemonstrate that different spoofing types have considerably different\ngeneralization capabilities. For this study, we analyze the performance using\ntwo kinds of features, mel-frequency cepstral coefficients (MFCCs) which are\nconsidered as baseline and recently proposed constant Q cepstral coefficients\n(CQCCs). The experiments are conducted with standard Gaussian mixture model -\nmaximum likelihood (GMM-ML) classifier on two recently released spoofing\ncorpora: ASVspoof 2015 and BTAS 2016 that includes cross-corpora performance\nanalysis. Feature-level analysis suggests that static and dynamic coefficients\nof spectral features, both are important for detecting spoofing attacks in the\nreal-life condition.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 17:55:39 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Paul", "Dipjyoti", ""], ["Sahidullah", "Md", ""], ["Saha", "Goutam", ""]]}, {"id": "1901.08608", "submitter": "Xinyu Li", "authors": "Xinyu Li, Venkata Chebiyyam, Katrin Kirchhoff", "title": "Multi-stream Network With Temporal Attention For Environmental Sound\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental sound classification systems often do not perform robustly\nacross different sound classification tasks and audio signals of varying\ntemporal structures. We introduce a multi-stream convolutional neural network\nwith temporal attention that addresses these problems. The network relies on\nthree input streams consisting of raw audio and spectral features and utilizes\na temporal attention function computed from energy changes over time. Training\nand classification utilizes decision fusion and data augmentation techniques\nthat incorporate uncertainty. We evaluate this network on three commonly used\ndata sets for environmental sound and audio scene classification and achieve\nnew state-of-the-art performance without any changes in network architecture or\nfront-end preprocessing, thus demonstrating better generalizability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 19:02:17 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Li", "Xinyu", ""], ["Chebiyyam", "Venkata", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "1901.09498", "submitter": "Adele Lu Jia", "authors": "Adele Lu Jia, Xiaoxue Shen, Siqi Shen, Jun Xu", "title": "User Donations in a Crowdsourced Video System", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced video systems like YouTube and Twitch.tv have been a major\ninternet phenomenon and are nowadays entertaining over a billion users. In\naddition to video sharing and viewing, over the years they have developed new\nfeatures to boost the community engagement and some managed to attract users to\ndonate, to the community as well as to other users. User donation directly\nreflects and influences user engagement in the community, and has a great\nimpact on the success of such systems. Nevertheless, user donations in\ncrowdsourced video systems remain trade secrets for most companies and to date\nare still unexplored. In this work, we attempt to fill this gap, and we obtain\nand provide a publicly available dataset on user donations in one crowdsourced\nvideo system named BiliBili. Based on information on nearly 40 thousand\ndonators, we examine the dynamics of user donations and their social\nrelationships, we quantitively reveal the factors that potentially impact user\ndonation, and we adopt machine-learned classifiers and network representation\nlearning models to timely and accurately predict the destinations of the\nmajority and the individual donations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 03:14:07 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Jia", "Adele Lu", ""], ["Shen", "Xiaoxue", ""], ["Shen", "Siqi", ""], ["Xu", "Jun", ""]]}, {"id": "1901.09774", "submitter": "Hao Tang", "authors": "Hao Tang, Xinya Chen, Wei Wang, Dan Xu, Jason J. Corso, Nicu Sebe, Yan\n  Yan", "title": "Attribute-Guided Sketch Generation", "comments": "7 pages, 6 figures, accepted to FG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes are important since they provide a detailed description and\ndetermine the visual appearance of human faces. In this paper, we aim at\nconverting a face image to a sketch while simultaneously generating facial\nattributes. To this end, we propose a novel Attribute-Guided Sketch Generative\nAdversarial Network (ASGAN) which is an end-to-end framework and contains two\npairs of generators and discriminators, one of which is used to generate faces\nwith attributes while the other one is employed for image-to-sketch\ntranslation. The two generators form a W-shaped network (W-net) and they are\ntrained jointly with a weight-sharing constraint. Additionally, we also propose\ntwo novel discriminators, the residual one focusing on attribute generation and\nthe triplex one helping to generate realistic looking sketches. To validate our\nmodel, we have created a new large dataset with 8,804 images, named the\nAttribute Face Photo & Sketch (AFPS) dataset which is the first dataset\ncontaining attributes associated to face sketch images. The experimental\nresults demonstrate that the proposed network (i) generates more\nphoto-realistic faces with sharper facial attributes than baselines and (ii)\nhas good generalization capability on different generative tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:20:20 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 15:27:38 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Tang", "Hao", ""], ["Chen", "Xinya", ""], ["Wang", "Wei", ""], ["Xu", "Dan", ""], ["Corso", "Jason J.", ""], ["Sebe", "Nicu", ""], ["Yan", "Yan", ""]]}, {"id": "1901.10332", "submitter": "Zhuoran Liu", "authors": "Zhuoran Liu, Zhengyu Zhao, Martha Larson", "title": "Who's Afraid of Adversarial Queries? The Impact of Image Modifications\n  on Content-based Image Retrieval", "comments": "To appear at the ACM International Conference on Multimedia Retrieval\n  (ICMR 2019). Our code is available at https://github.com/liuzrcc/PIRE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An adversarial query is an image that has been modified to disrupt\ncontent-based image retrieval (CBIR) while appearing nearly untouched to the\nhuman eye. This paper presents an analysis of adversarial queries for CBIR\nbased on neural, local, and global features. We introduce an innovative neural\nimage perturbation approach, called Perturbations for Image Retrieval Error\n(PIRE), that is capable of blocking neural-feature-based CBIR. PIRE differs\nsignificantly from existing approaches that create images adversarial with\nrespect to CNN classifiers because it is unsupervised, i.e., it needs no\nlabelled data from the data set to which it is applied. Our experimental\nanalysis demonstrates the surprising effectiveness of PIRE in blocking CBIR,\nand also covers aspects of PIRE that must be taken into account in practical\nsettings, including saving images, image quality and leaking adversarial\nqueries into the background collection. Our experiments also compare PIRE (a\nneural approach) with existing keypoint removal and injection approaches (which\nmodify local features). Finally, we discuss the challenges that face multimedia\nresearchers in the future study of adversarial queries.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 15:09:14 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 11:28:35 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 11:57:18 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Liu", "Zhuoran", ""], ["Zhao", "Zhengyu", ""], ["Larson", "Martha", ""]]}, {"id": "1901.10721", "submitter": "Shanyun Liu", "authors": "Shanyun Liu, Yunquan Dong, Pingyi Fan, Rui She, Shuo Wan", "title": "Matching Users' Preference Under Target Revenue Constraints in Optimal\n  Data Recommendation Systems", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": "10.3390/e21020205", "report-no": null, "categories": "cs.IT cs.MM cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of finding a particular data recommendation\nstrategy based on the user preferences and a system expected revenue. To this\nend, we formulate this problem as an optimization by designing the\nrecommendation mechanism as close to the user behavior as possible with a\ncertain revenue constraint. In fact, the optimal recommendation distribution is\nthe one that is the closest to the utility distribution in the sense of\nrelative entropy and satisfies expected revenue. We show that the optimal\nrecommendation distribution follows the same form as the message importance\nmeasure (MIM) if the target revenue is reasonable, i.e., neither too small nor\ntoo large. Therefore, the optimal recommendation distribution can be regarded\nas the normalized MIM, where the parameter, called importance coefficient,\npresents the concern of the system and switches the attention of the system\nover data sets with different occurring probability. By adjusting the\nimportance coefficient, our MIM based framework of data recommendation can then\nbe applied to system with various system requirements and data\ndistributions.Therefore,the obtained results illustrate the physical meaning of\nMIM from the data recommendation perspective and validate the rationality of\nMIM in one aspect.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 09:31:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Liu", "Shanyun", ""], ["Dong", "Yunquan", ""], ["Fan", "Pingyi", ""], ["She", "Rui", ""], ["Wan", "Shuo", ""]]}, {"id": "1901.10744", "submitter": "Pasquale De Luca", "authors": "Pasquale De Luca, Vincenzo Maria Russiello, Raffaele Ciro Sannino and\n  Lorenzo Valente", "title": "A study for Image compression using Re-Pair algorithm", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The compression is an important topic in computer science which allows we to\nstorage more amount of data on our data storage. There are several techniques\nto compress any file. In this manuscript will be described the most important\nalgorithm to compress images such as JPEG and it will be compared with another\nmethod to retrieve good reason to not use this method on images. So to compress\nthe text the most encoding technique known is the Huffman Encoding which it\nwill be explained in exhaustive way. In this manuscript will showed how to\ncompute a text compression method on images in particular the method and the\nreason to choice a determinate image format against the other. The method\nstudied and analyzed in this manuscript is the Re-Pair algorithm which is\npurely for grammatical context to be compress. At the and it will be showed the\ngood result of this application.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 10:17:52 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 13:56:41 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 11:36:43 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["De Luca", "Pasquale", ""], ["Russiello", "Vincenzo Maria", ""], ["Sannino", "Raffaele Ciro", ""], ["Valente", "Lorenzo", ""]]}, {"id": "1901.10812", "submitter": "Yehuda Dar", "authors": "Yehuda Dar and Alfred M. Bruckstein", "title": "Benefiting from Duplicates of Compressed Data: Shift-Based Holographic\n  Compression of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage systems often rely on multiple copies of the same compressed data,\nenabling recovery in case of binary data errors, of course, at the expense of a\nhigher storage cost. In this paper we show that a wiser method of duplication\nentails great potential benefits for data types tolerating approximate\nrepresentations, like images and videos. We propose a method to produce a set\nof distinct compressed representations for a given signal, such that any subset\nof them allows reconstruction of the signal at a quality depending only on the\nnumber of compressed representations utilized. Essentially, we implement the\nholographic representation idea, where all the representations are equally\nimportant in refining the reconstruction. Here we propose to exploit the shift\nsensitivity of common compression processes and generate holographic\nrepresentations via compression of various shifts of the signal. Two\nimplementations for the idea, based on standard compression methods, are\npresented: the first is a simple, optimization-free design. The second approach\noriginates in a challenging rate-distortion optimization, mitigated by the\nalternating direction method of multipliers (ADMM), leading to a process of\nrepeatedly applying standard compression techniques. Evaluation of the\napproach, in conjunction with the JPEG2000 image compression standard, shows\nthe effectiveness of the optimization in providing compressed holographic\nrepresentations that, by means of an elementary reconstruction process, enable\nimpressive gains of several dBs in PSNR over exact duplications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 13:23:36 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 18:09:20 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Dar", "Yehuda", ""], ["Bruckstein", "Alfred M.", ""]]}]