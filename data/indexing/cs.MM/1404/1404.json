[{"id": "1404.1313", "submitter": "Valery Gorbachev", "authors": "V.N. Gorbachev, E.M. Kaynarova, I.K. Metelev, E.S. Yakovleva", "title": "Color to Gray and Back transformation for distributing color digital\n  images", "comments": "5 pages, 3 figures, submitted to The International Conference on\n  Computer Graphics and Vision, GraphiCon'2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Color to Gray and Back transformation watermarking with a secrete key is\nconsidered. Color is embedded into the bit planes of the luminosity component\nof the YUV color space with the help of a block algorithm that allows using not\nonly the least significant bits. An application of the problem of distributing\ncolor digital images from a data base among legitimate users is discussed. The\nproposed protocol can protect original images from unauthorized copying.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 14:19:04 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Gorbachev", "V. N.", ""], ["Kaynarova", "E. M.", ""], ["Metelev", "I. K.", ""], ["Yakovleva", "E. S.", ""]]}, {"id": "1404.1314", "submitter": "Meenakshi Kollati", "authors": "K.Meenakshi, Ch.Srinivasa Rao, and K. Satya Prasad", "title": "Robust Video Watermarking Schemes in Phase domain Using Binary Phase\n  Shift Keying", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust video watermarking scheme in Discrete Fourier\nTransform (DFT) and Sequencyordered Complex Hadamard Transform (SCHT). The DFT\nand SCHT coefficients are complex and consist of both magnitude and phase and\nare well suited to adopt phase shift keying techniques to embed the watermark.\nIn the proposed schemes, the phases of DFT and SCHT coefficients are modified\nto convey watermark information using binary phase shift keying in cover video.\nLow amplitude block selection (LABS) is used to improve transparency, amplitude\nboost to improve the resistance of watermark from signal processing and\ncompression attacks and spread spectrum technique is used for encrypting\nwatermark in order to protect it from third party. It is observed that both\nalgorithms showing more or less same robustness but SCHT offers high\ntransparency, simple implementation and less computational cost than DFT.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 06:10:27 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Meenakshi", "K.", ""], ["Rao", "Ch. Srinivasa", ""], ["Prasad", "K. Satya", ""]]}, {"id": "1404.1514", "submitter": "Avinash Bhute", "authors": "Avinash N Bhute, B.B. Meshram", "title": "Text Based Approach For Indexing And Retrieval Of Image And Video: A\n  Review", "comments": "12 pages", "journal-ref": "Advances in Vision: An International Journal, Vol 1, no. 1, March\n  2014", "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DL cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Text data present in multimedia contain useful information for automatic\nannotation, indexing. Extracted information used for recognition of the overlay\nor scene text from a given video or image. The Extracted text can be used for\nretrieving the videos and images. In this paper, firstly, we are discussed the\ndifferent techniques for text extraction from images and videos. Secondly, we\nare reviewed the techniques for indexing and retrieval of image and videos by\nusing extracted text.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 19:58:38 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Bhute", "Avinash N", ""], ["Meshram", "B. B.", ""]]}, {"id": "1404.2237", "submitter": "Lukasz Swierczewski", "authors": "Monika Kwiatkowska, Lukasz Swierczewski", "title": "Steganography - coding and intercepting the information from encoded\n  pictures in the absence of any initial information", "comments": "10 pages, 5 figures, 5 tables, LVEE 2014 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The work includes implementation and extraction algorithms capabilities test,\nwithout any additional data (starting position, the number of bits used, gap\nbetween the amount of data encoded) information from encoded files (mostly\nimages). The software is written using OpenMP standard [1], which allowed them\nto run on parallel computers. Performance tests were carried out on computers,\nBlue Gene/P [2], Blue Gene/Q [3] and the system consisting of four AMD Opteron\n6272 [4]. Source code is available under GNU GPL v3 license and are available\nin a repository OLib [5].\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 17:09:56 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Kwiatkowska", "Monika", ""], ["Swierczewski", "Lukasz", ""]]}, {"id": "1404.2343", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev", "title": "Wireless Transmission of Video for Biomechanical Analysis", "comments": null, "journal-ref": "Information systems. Problems, perspectives, innovation approaches\n  Volume 2. 2007. Saint Petersburg State University of Aerospace\n  Instrumentation. ISBN 978-5-80888-0244-5", "doi": null, "report-no": null, "categories": "cs.CE cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is a possibility to wirelessly stream video over a network, a\nsophisticated computer analysis of the transmitted video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it using specific software such as Arial Performance Analysis Systems\nor Dartfish. This manuscript presents some approaches and challenges in\nstreaming video as well as some applications of Information Technology in\nbiomechanics. An example of how scientists from Indiana State University\napproached the wireless transmission of video is also introduced.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 01:19:29 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Mirzoev", "Dr. Timur", ""]]}, {"id": "1404.2344", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev", "title": "Analysis of Computer Hardware Affecting Video Transmission via IEEE\n  1394a connection", "comments": null, "journal-ref": "Information systems. Problems, perspectives, innovation\n  approaches. Volume 2. 2007. Saint Petersburg State University of Aerospace\n  Instrumentation. ISBN 978-5-80888-0244-5", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When 60 de-interlaced fields per second digital uncompressed video is\nstreamed to a computer, some video fields are lost and not able to be stored on\na computer s hard drive successfully. Additionally, this problem amplifies once\nmultiple video sources are deployed. If it is possible to stream digital\nuncompressed video without dropped video fields, then a sophisticated computer\nanalysis of the transmitted via IEEE 1394a connection video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it. If a loss of video fields occurs, then a quality analysis of\nvideo is not possible.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 01:23:31 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Mirzoev", "Dr. Timur", ""]]}, {"id": "1404.2592", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev", "title": "Reduction of Field Loss by a Video Processing System", "comments": "arXiv admin note: substantial text overlap with arXiv:1404.2344", "journal-ref": "Southeast Decision Sciences Institute. Conference Proceedings.\n  SEDSI 2008", "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming of 60 de-interlaced fields per second digital uncompressed video\nwith 720x480 resolution without a loss of video fields is one of the desired\ntechnologies by scientists in biomechanics. If it is possible to stream digital\nuncompressed video without dropped video fields, then a sophisticated computer\nanalysis of the transmitted via IEEE 1394a connection video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it using specific software such as Arial Performance Analysis\nSystems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 01:35:26 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Mirzoev", "Dr. Timur", ""]]}, {"id": "1404.2952", "submitter": "Oussama Noui", "authors": "Noui Oussama and Noui Lemnouar", "title": "A blind robust watermarking scheme based on svd and circulant matrices", "comments": "12 pages, 6 figures, 5 tables, Second International Conference on\n  Computational Science & Engineering (CSE - 2014)", "journal-ref": "Second International Conference on Computational Science and\n  Engineering (CSE-2014) Dubai, UAE, April 04 ~ 05 - 2014 Volume Editors:\n  Dhinaharan Nagamalai, Sundarapandian Vaidyanathan ISBN : 978-1-921987-30-4 pp\n  65 - 77", "doi": "10.5121/csit.2014.4406", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Multimedia security has been the aim point of considerable research activity\nbecause of its wide application area. The major technology to achieve copyright\nprotection, content authentication, access control and multimedia security is\nwatermarking which is the process of embedding data into a multimedia element\nsuch as image or audio, this embedded data can later be extracted from, or\ndetected in the embedded element for different purposes. In this work, a blind\nwatermarking algorithm based on SVD and circulant matrices has been presented.\nEvery circulant matrix is associated with a matrix for which the SVD\ndecomposition coincides with the spectral decomposition. This leads to improve\nthe Chandra algorithm [1], our presentation will include a discussion on the\ndata hiding capacity, watermark transparency and robustness against a wide\nrange of common image processing attacks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 20:56:21 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Oussama", "Noui", ""], ["Lemnouar", "Noui", ""]]}, {"id": "1404.3018", "submitter": "Huazi Zhang", "authors": "Huazi Zhang, Yichao Jin, Weiwen Zhang, Yonggang Wen", "title": "Enhancing User Experience for Multi-Screen Social TV Streaming over\n  Wireless Networks", "comments": "submitted to IEEE GLOBECOM 2014", "journal-ref": null, "doi": "10.1109/GLOCOMW.2014.7063414", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multi-screen cloud social TV is invented to transform TV into\nsocial experience. People watching the same content on social TV may come from\ndifferent locations, while freely interact with each other through text, image,\naudio and video. This crucial virtual living-room experience adds social\naspects into existing performance metrics. In this paper, we parse social TV\nuser experience into three elements (i.e., inter-user delay, video quality of\nexperience (QoE), and resource efficiency), and provide a joint analytical\nframework to enhance user experience. Specifically, we propose a cloud-based\noptimal playback rate allocation scheme to maximize the overall QoE while upper\nbounding inter-user delay. Experiment results show that our algorithm achieves\nnear-optimal tradeoff between inter-user delay and video quality, and\ndemonstrates resilient performance even under very fast wireless channel\nfading.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 07:13:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhang", "Huazi", ""], ["Jin", "Yichao", ""], ["Zhang", "Weiwen", ""], ["Wen", "Yonggang", ""]]}, {"id": "1404.3290", "submitter": "Yehuda Dar", "authors": "Yehuda Dar, Alfred M. Bruckstein", "title": "Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block-based motion estimation (ME) and compensation (MC) techniques are\nwidely used in modern video processing algorithms and compression systems. The\ngreat variety of video applications and devices results in numerous compression\nspecifications. Specifically, there is a diversity of frame-rates and\nbit-rates. In this paper, we study the effect of frame-rate and compression\nbit-rate on block-based ME and MC as commonly utilized in inter-frame coding\nand frame-rate up conversion (FRUC). This joint examination yields a\ncomprehensive foundation for comparing MC procedures in coding and FRUC. First,\nthe video signal is modeled as a noisy translational motion of an image. Then,\nwe theoretically model the motion-compensated prediction of an available and\nabsent frames as in coding and FRUC applications, respectively. The theoretic\nMC-prediction error is further analyzed and its autocorrelation function is\ncalculated for coding and FRUC applications. We show a linear relation between\nthe variance of the MC-prediction error and temporal-distance. While the\naffecting distance in MC-coding is between the predicted and reference frames,\nMC-FRUC is affected by the distance between the available frames used for the\ninterpolation. Moreover, the dependency in temporal-distance implies an inverse\neffect of the frame-rate. FRUC performance analysis considers the prediction\nerror variance, since it equals to the mean-squared-error of the interpolation.\nHowever, MC-coding analysis requires the entire autocorrelation function of the\nerror; hence, analytic simplicity is beneficial. Therefore, we propose two\nconstructions of a separable autocorrelation function for prediction error in\nMC-coding. We conclude by comparing our estimations with experimental results.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 14:21:21 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Dar", "Yehuda", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1404.4026", "submitter": "Yehuda Dar", "authors": "Yehuda Dar, Alfred M. Bruckstein", "title": "Improving Low Bit-Rate Video Coding using Spatio-Temporal Down-Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good quality video coding for low bit-rate applications is important for\ntransmission over narrow-bandwidth channels and for storage with limited memory\ncapacity. In this work, we develop a previous analysis for image compression at\nlow bit-rates to adapt it to video signals. Improving compression using\ndown-scaling in the spatial and temporal dimensions is examined. We show, both\ntheoretically and experimentally, that at low bit-rates, we benefit from\napplying spatio-temporal scaling. The proposed method includes down-scaling\nbefore the compression and a corresponding up-scaling afterwards, while the\ncodec itself is left unmodified. We propose analytic models for low bit-rate\ncompression and spatio-temporal scaling operations. Specifically, we use\ntheoretic models of motion-compensated prediction of available and absent\nframes as in coding and frame-rate up-conversion (FRUC) applications,\nrespectively. The proposed models are designed for multi-resolution analysis.\nIn addition, we formulate a bit-allocation procedure and propose a method for\nestimating good down-scaling factors of a given video based on its second-order\nstatistics and the given bit-budget. We validate our model with experimental\nresults of H.264 compression.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 19:26:13 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 10:24:57 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Dar", "Yehuda", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1404.4181", "submitter": "Patrice Brault", "authors": "Matthieu Moinard, Isabelle Amonou, Pierre Duhamel and Patrice Brault", "title": "Prediction of Transformed (DCT) Video Coding Residual for Video\n  Compression", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video compression has been investigated by means of analysis-synthesis, and\nmore particularly by means of inpainting. The first part of our approach has\nbeen to develop the inpainting of DCT coefficients in an image. This has shown\ngood results for image compression without overpassing todays compression\nstandards like JPEG. We then looked at integrating the same approach in a video\ncoder, and in particular in the widely used H264 AVC standard coder, but the\nsame approach can be used in the framework of HEVC. The originality of this\nwork consists in cancelling at the coder, then automatically restoring, at the\ndecoder, some well chosen DCT residual coefficients. For this purpose, we have\ndeveloped a restoration model of transformed coefficients. By using a total\nvariation based model, we derive conditions for the reconstruction of\ntransformed coefficients that have been suppressed or altered. The main purpose\nhere, in a video coding context, is to improve the ratedistortion performance\nof existing coders. To this end DCT restoration is used as an additional\nprediction step to the spatial prediction of the transformed coefficients,\nbased on an image regularization process. The method has been successfully\ntested with the H.264 AVC video codec standard.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 10:03:12 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Moinard", "Matthieu", ""], ["Amonou", "Isabelle", ""], ["Duhamel", "Pierre", ""], ["Brault", "Patrice", ""]]}, {"id": "1404.4543", "submitter": "Behrang  Qasemizadeh", "authors": "Hadi Restgou Haghi and Mohammadreza Kangavari and Behrang QasemiZadeh", "title": "A Novel Approach for Video Temporal Annotation", "comments": "Published in a Local Confrence, 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computing, communication, and data storage have led to an\nincreasing number of large digital libraries publicly available on the\nInternet. Main problem of content-based video retrieval is inferring semantics\nfrom raw video data. Video data play an important role in these libraries.\nInstead of words, a video retrieval system deals with collections of video\nrecords. Therefore, the system is confronted with the problem of video\nunderstanding. Because machine understanding of the video data is still an\nunsolved research problem, text annotations are usually used to describe the\ncontent of video data according to the annotator's understanding and the\npurpose of that video data. Most of proposed systems for video annotation are\ndomain dependent. In addition, in many of these systems, an important feature\nof video data, temporality, is disregarded. In this paper, we proposed a\nframework for video temporal annotation. The proposed system uses domain\nknowledge and a time ontology to perform temporal annotation of input video.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 14:42:03 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Haghi", "Hadi Restgou", ""], ["Kangavari", "Mohammadreza", ""], ["QasemiZadeh", "Behrang", ""]]}, {"id": "1404.4607", "submitter": "Olivier Aubert", "authors": "Olivier Aubert, Yannick Pri\\'e, Camila Canellas", "title": "Leveraging video annotations in video-based e-learning", "comments": "7th International Conference on Computer Supported Education (CSEDU),\n  Barcelone : Spain (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The e-learning community has been producing and using video content for a\nlong time, and in the last years, the advent of MOOCs greatly relied on video\nrecordings of teacher courses. Video annotations are information pieces that\ncan be anchored in the temporality of the video so as to sustain various\nprocesses ranging from active reading to rich media editing. In this position\npaper we study how video annotations can be used in an e-learning context -\nespecially MOOCs - from the triple point of view of pedagogical processes,\ncurrent technical platforms functionalities, and current challenges. Our\nanalysis is that there is still plenty of room for leveraging video annotations\nin MOOCs beyond simple active reading, namely live annotation, performance\nannotation and annotation for assignment; and that new developments are needed\nto accompany this evolution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 15:13:21 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Aubert", "Olivier", ""], ["Pri\u00e9", "Yannick", ""], ["Canellas", "Camila", ""]]}, {"id": "1404.7237", "submitter": "Srinivasa Rao tumma", "authors": "T. Srinivasa Rao (1), Rajasekhar R. Kurra (2) ((1)\n  InformationTechnology, UshaRama College of Engineering & Technology, India\n  (2) Principal, Sri Prakash College of Engineering & Technology, India)", "title": "A Smart Intelligent Way of Video Authentication Using Classification and\n  Decomposition of Watermarking Methods", "comments": "7 Pages, 4 Figures", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V10(3):136-142, Apr 2014. ISSN:2231-2803. www.ijcttjournal.org. Published by\n  Seventh Sense Research Group", "doi": "10.14445/22312803/IJCTT-V10P123", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Video Watermarking serves as a new technology mainly used to provide security\nto the illegal distribution of digital video over the web. The purpose of any\nvideo watermarking scheme is to embed extra information into video in such a\nway that must be perceptually undetectable while still holding enough\ninformation in order to extract the watermark beginning with the resultant\nvideo. Information which is embedded within the original image is a Digital\nWatermark, which could be visible or invisible. To improved more security,\nembedding and extraction Watermark process should be complex against attackers.\nRecent research indicates SVD (Singular Value Decomposition) algorithms are\nemployed owing to their simple scheme with mathematical function. In this\nproposed work an advanced SVD transformation algorithm is used for embedding\nand extraction process. Experimental results show proposed watermarking process\nis more secured than existing SVD approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 05:20:25 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Rao", "T. Srinivasa", ""], ["Kurra", "Rajasekhar R.", ""]]}, {"id": "1404.7335", "submitter": "Florent Jacquemard", "authors": "Florent Jacquemard (Inria Paris-Rocquencourt, STMS), Cl\\'ement\n  Poncelet Sanchez (Inria Paris-Rocquencourt, STMS)", "title": "Antescofo Intermediate Representation", "comments": "RR-8520 (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an intermediate language designed as a medium-level internal\nrepresentation of programs of the interactive music system Antescofo. This\nrepresentation is independent both of the Antescofo source language and of the\narchitecture of the execution platform. It is used in tasks such as\nverification of timings, model-based conformance testing, static control-flow\nanalysis or simulation. This language is essentially a flat representation of\nAntescofo's code, as a finite state machine extended with local and global\nvariables, with delays and with concurrent threads creation. It features a\nsmall number of simple instructions which are either blocking (wait for\nexternal event, signal or duration) or not (variable assignment, message\nemission and control).\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 12:30:36 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Jacquemard", "Florent", "", "Inria Paris-Rocquencourt, STMS"], ["Sanchez", "Cl\u00e9ment Poncelet", "", "Inria Paris-Rocquencourt, STMS"]]}, {"id": "1404.7796", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (IST Austria), Amaury Habrard (LHC), St\\'ephane Ayache\n  (LIF)", "title": "Majority Vote of Diverse Classifiers for Late Fusion", "comments": "IAPR Joint International Workshops on Statistical Techniques in\n  Pattern Recognition and Structural and Syntactic Pattern Recignition, Joensuu\n  : Finland (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, a lot of attention has been devoted to multimedia\nindexing by fusing multimodal informations. Two kinds of fusion schemes are\ngenerally considered: The early fusion and the late fusion. We focus on late\nclassifier fusion, where one combines the scores of each modality at the\ndecision level. To tackle this problem, we investigate a recent and elegant\nwell-founded quadratic program named MinCq coming from the machine learning\nPAC-Bayesian theory. MinCq looks for the weighted combination, over a set of\nreal-valued functions seen as voters, leading to the lowest misclassification\nrate, while maximizing the voters' diversity. We propose an extension of MinCq\ntailored to multimedia indexing. Our method is based on an order-preserving\npairwise loss adapted to ranking that allows us to improve Mean Averaged\nPrecision measure while taking into account the diversity of the voters that we\nwant to fuse. We provide evidence that this method is naturally adapted to late\nfusion procedures and confirm the good behavior of our approach on the\nchallenging PASCAL VOC'07 benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 16:55:00 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 08:06:24 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Morvant", "Emilie", "", "IST Austria"], ["Habrard", "Amaury", "", "LHC"], ["Ayache", "St\u00e9phane", "", "LIF"]]}]