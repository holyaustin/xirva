[{"id": "1701.00474", "submitter": "Meysam Ghaffari", "authors": "Ghassem Alikhajeh, Abdolreza Mirzaei, Mehran Safayani and Meysam\n  Ghaffari", "title": "Duplicate matching and estimating features for detection of copy-move\n  images forgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy-move forgery is the most popular and simplest image manipulation method.\nIn this type of forgery, an area from the image copied, then after post\nprocessing such as rotation and scaling, placed on the destination. The goal of\nCopy-move forgery is to hide or duplicate one or more objects in the image.\nKey-point based Copy-move forgery detection methods have five main steps:\npreprocessing, feature extraction, matching, transform estimation and post\nprocessing that matching and transform estimation have important effect on the\ndetection. More over the error could happens in some steps due to the noise.\nThe existing methods process these steps separately and in case of having an\nerror in a step, this error could be propagated to the following steps and\naffects the detection. To solve the above mentioned problem, in this paper the\nsteps of the detection system interact with each other and if an error happens\nin a step, following steps are trying to detect and solve it. We formulate this\ninteraction by defining and optimizing a cost function. This function includes\nmatching and transform estimation steps. Then in an iterative procedure the\nsteps are executed and in case of detecting error, the error will be corrected.\nThe efficiency of the proposed method analyzed in diverse cases such as pixel\nimage precision level on the simple forgery images, robustness to the rotation\nand scaling, detecting professional forgery images and the precision of the\ntransformation matrix. The results indicate the better efficiency of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 18:25:45 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Alikhajeh", "Ghassem", ""], ["Mirzaei", "Abdolreza", ""], ["Safayani", "Mehran", ""], ["Ghaffari", "Meysam", ""]]}, {"id": "1701.00599", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Michael Gygli, Luc Van Gool", "title": "AENet: Learning Deep Audio Features for Video Analysis", "comments": "12 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1604.07160", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep network for audio event recognition, called AENet. In\ncontrast to speech, sounds coming from audio events may be produced by a wide\nvariety of sources. Furthermore, distinguishing them often requires analyzing\nan extended time period due to the lack of clear sub-word units that are\npresent in speech. In order to incorporate this long-time frequency structure\nof audio events, we introduce a convolutional neural network (CNN) operating on\na large temporal input. In contrast to previous works this allows us to train\nan audio event detection system end-to-end. The combination of our network\narchitecture and a novel data augmentation outperforms previous methods for\naudio event detection by 16%. Furthermore, we perform transfer learning and\nshow that our model learnt generic audio features, similar to the way CNNs\nlearn generic features on vision tasks. In video analysis, combining visual\nfeatures and traditional audio features such as MFCC typically only leads to\nmarginal improvements. Instead, combining visual features with our AENet\nfeatures, which can be computed efficiently on a GPU, leads to significant\nperformance improvements on action recognition and video highlight detection.\nIn video highlight detection, our audio features improve the performance by\nmore than 8% over visual features alone.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 07:35:54 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 04:07:11 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Takahashi", "Naoya", ""], ["Gygli", "Michael", ""], ["Van Gool", "Luc", ""]]}, {"id": "1701.01392", "submitter": "Stefano D'Aronco", "authors": "Stefano D'Aronco, Laura Toni, Pascal Frossard", "title": "Price-based Controller for Quality-Fair HTTP Adaptive Streaming\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTTP adaptive streaming (HAS) has become the universal technology for video\nstreaming over the Internet. Many HAS system designs aim at sharing the network\nbandwidth in a rate-fair manner. However, rate fairness is in general not\nequivalent to quality fairness as different video sequences might have\ndifferent characteristics and resource requirements. In this work, we focus on\nthis limitation and propose a novel controller for HAS clients that is able to\nreach quality fairness while preserving the main characteristics of HAS systems\nand with a limited support from the network devices. In particular, we adopt a\nprice-based mechanism in order to build a controller that maximizes the\naggregate video quality for a set of HAS clients that share a common\nbottleneck. When network resources are scarce, the clients with simple video\nsequences reduce the requested bitrate in favor of users that subscribe to more\ncomplex video sequences, leading to a more efficient network usage. The\nproposed controller has been implemented in a network simulator, and the\nsimulation results demonstrate its ability to share the available bandwidth\namong the HAS users in a quality-fair manner.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 17:16:58 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["D'Aronco", "Stefano", ""], ["Toni", "Laura", ""], ["Frossard", "Pascal", ""]]}, {"id": "1701.01500", "submitter": "Haiqiang Wang", "authors": "Haiqiang Wang, Ioannis Katsavounidis, Jiantong Zhou, Jeonghoon Park,\n  Shawmin Lei, Xin Zhou, Man-On Pun, Xin Jin, Ronggang Wang, Xu Wang, Yun\n  Zhang, Jiwu Huang, Sam Kwong and C.-C. Jay Kuo", "title": "VideoSet: A Large-Scale Compressed Video Quality Dataset Based on JND\n  Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new methodology to measure coded image/video quality using the\njust-noticeable-difference (JND) idea was proposed. Several small JND-based\nimage/video quality datasets were released by the Media Communications Lab at\nthe University of Southern California. In this work, we present an effort to\nbuild a large-scale JND-based coded video quality dataset. The dataset consists\nof 220 5-second sequences in four resolutions (i.e., $1920 \\times 1080$, $1280\n\\times 720$, $960 \\times 540$ and $640 \\times 360$). For each of the 880 video\nclips, we encode it using the H.264 codec with $QP=1, \\cdots, 51$ and measure\nthe first three JND points with 30+ subjects. The dataset is called the\n\"VideoSet\", which is an acronym for \"Video Subject Evaluation Test (SET)\". This\nwork describes the subjective test procedure, detection and removal of outlying\nmeasured data, and the properties of collected JND data. Finally, the\nsignificance and implications of the VideoSet to future video coding research\nand standardization efforts are pointed out. All source/coded video clips as\nwell as measured JND data included in the VideoSet are available to the public\nin the IEEE DataPort.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 23:14:01 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 04:30:59 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Wang", "Haiqiang", ""], ["Katsavounidis", "Ioannis", ""], ["Zhou", "Jiantong", ""], ["Park", "Jeonghoon", ""], ["Lei", "Shawmin", ""], ["Zhou", "Xin", ""], ["Pun", "Man-On", ""], ["Jin", "Xin", ""], ["Wang", "Ronggang", ""], ["Wang", "Xu", ""], ["Zhang", "Yun", ""], ["Huang", "Jiwu", ""], ["Kwong", "Sam", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1701.02141", "submitter": "Mattia Rossi", "authors": "Mattia Rossi and Pascal Frossard", "title": "Light Field Super-Resolution Via Graph-Based Regularization", "comments": "This new version includes more material. In particular, we added: a\n  new section on the computational complexity of the proposed algorithm,\n  experimental comparisons with a CNN-based super-resolution algorithm, and new\n  experiments on a third dataset", "journal-ref": null, "doi": "10.1109/TIP.2018.2828983", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras capture the 3D information in a scene with a single\nexposure. This special feature makes light field cameras very appealing for a\nvariety of applications: from post-capture refocus, to depth estimation and\nimage-based rendering. However, light field cameras suffer by design from\nstrong limitations in their spatial resolution, which should therefore be\naugmented by computational methods. On the one hand, off-the-shelf single-frame\nand multi-frame super-resolution algorithms are not ideal for light field data,\nas they do not consider its particular structure. On the other hand, the few\nsuper-resolution algorithms explicitly tailored for light field data exhibit\nsignificant limitations, such as the need to estimate an explicit disparity map\nat each view. In this work we propose a new light field super-resolution\nalgorithm meant to address these limitations. We adopt a multi-frame alike\nsuper-resolution approach, where the complementary information in the different\nlight field views is used to augment the spatial resolution of the whole light\nfield. We show that coupling the multi-frame approach with a graph regularizer,\nthat enforces the light field structure via nonlocal self similarities, permits\nto avoid the costly and challenging disparity estimation step for all the\nviews. Extensive experiments show that the new algorithm compares favorably to\nthe other state-of-the-art methods for light field super-resolution, both in\nterms of PSNR and visual quality.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 11:32:30 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 17:17:15 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Rossi", "Mattia", ""], ["Frossard", "Pascal", ""]]}, {"id": "1701.02669", "submitter": "Rajeev Kumar", "authors": "Rajeev Kumar, Robert S Margolies, Rittwik Jana, Yong Liu and Shivendra\n  Panwar", "title": "WiLiTV: A Low-Cost Wireless Framework for Live TV Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the evolution of HDTV and Ultra HDTV, the bandwidth requirement for\nIP-based TV content is rapidly increasing. Consumers demand uninterrupted\nservice with a high Quality of Experience (QoE). Service providers are\nconstantly trying to differentiate themselves by innovating new ways of\ndistributing content more efficiently with lower cost and higher penetration.\nIn this work, we propose a cost-efficient wireless framework (WiLiTV) for\ndelivering live TV services, consisting of a mix of wireless access\ntechnologies (e.g. Satellite, WiFi and LTE overlay links). In the proposed\narchitecture, live TV content is injected into the network at a few residential\nlocations using satellite dishes. The content is then further distributed to\nother homes using a house-to-house WiFi network or via an overlay LTE network.\nOur problem is to construct an optimal TV distribution network with the minimum\nnumber of satellite injection points, while preserving the highest QoE, for\ndifferent neighborhood densities. We evaluate the framework using realistic\ntime-varying demand patterns and a diverse set of home location data. Our study\ndemonstrates that the architecture requires 75 - 90% fewer satellite injection\npoints, compared to traditional architectures. Furthermore, we show that most\ncost savings can be obtained using simple and practical relay routing\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:31:37 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Kumar", "Rajeev", ""], ["Margolies", "Robert S", ""], ["Jana", "Rittwik", ""], ["Liu", "Yong", ""], ["Panwar", "Shivendra", ""]]}, {"id": "1701.03126", "submitter": "Chiori Hori Dr.", "authors": "Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R.\n  Hershey, Tim K. Marks", "title": "Attention-Based Multimodal Fusion for Video Description", "comments": "Resubmitted to the rebuttal for CVPR 2017 for review, 8 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently successful methods for video description are based on\nencoder-decoder sentence generation using recur-rent neural networks (RNNs).\nRecent work has shown the advantage of integrating temporal and/or spatial\nattention mechanisms into these models, in which the decoder net-work predicts\neach word in the description by selectively giving more weight to encoded\nfeatures from specific time frames (temporal attention) or to features from\nspecific spatial regions (spatial attention). In this paper, we propose to\nexpand the attention model to selectively attend not just to specific times or\nspatial regions, but to specific modalities of input such as image features,\nmotion features, and audio features. Our new modality-dependent attention\nmechanism, which we call multimodal attention, provides a natural way to fuse\nmultimodal information for video description. We evaluate our method on the\nYoutube2Text dataset, achieving results that are competitive with current state\nof the art. More importantly, we demonstrate that our model incorporating\nmultimodal attention as well as temporal attention significantly outperforms\nthe model that uses temporal attention alone.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 19:16:42 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 22:57:10 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Hori", "Chiori", ""], ["Hori", "Takaaki", ""], ["Lee", "Teng-Yok", ""], ["Sumi", "Kazuhiro", ""], ["Hershey", "John R.", ""], ["Marks", "Tim K.", ""]]}, {"id": "1701.03274", "submitter": "Jun Chen", "authors": "Jun Chen, Chaokun Wang", "title": "Investigating the role of musical genre in human perception of music\n  stretching resistance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To stretch a music piece to a given length is a common demand in people's\ndaily lives, e.g., in audio-video synchronization and animation production.\nHowever, it is not always guaranteed that the stretched music piece is\nacceptable for general audience since music stretching suffers from people's\nperceptual artefacts. Over-stretching a music piece will make it uncomfortable\nfor human psychoacoustic hearing. The research on music stretching resistance\nattempts to estimate the maximum stretchability of music pieces to further\navoid over-stretch. It has been observed that musical genres can significantly\nimprove the accuracy of automatic estimation of music stretching resistance,\nbut how musical genres are related to music stretching resistance has never\nbeen explained or studied in detail in the literature. In this paper, the\ncharacteristics of music stretching resistance are compared across different\nmusical genres. It is found that music stretching resistance has strong\nintra-genre cohesiveness and inter-genre discrepancies in the experiments.\nMoreover, the ambiguity and the symmetry of music stretching resistance are\nalso observed in the experimental analysis. These findings lead to a new\nmeasurement on the similarity between different musical genres based on their\nmusic stretching resistance. In addition, the analysis of variance (ANOVA) also\nsupports the findings in this paper by verifying the significance of musical\ngenre in shaping music stretching resistance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 09:26:22 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Chen", "Jun", ""], ["Wang", "Chaokun", ""]]}, {"id": "1701.03968", "submitter": "Arturo Deza", "authors": "Arturo Deza, Jeffrey R. Peters, Grant S. Taylor, Amit Surana and\n  Miguel P. Eckstein", "title": "Attention Allocation Aid for Visual Search", "comments": "To be presented at the ACM CHI conference in Denver, Colorado in May\n  2017", "journal-ref": null, "doi": "10.1145/3025453.3025834", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines the development and testing of a novel, feedback-enabled\nattention allocation aid (AAAD), which uses real-time physiological data to\nimprove human performance in a realistic sequential visual search task. Indeed,\nby optimizing over search duration, the aid improves efficiency, while\npreserving decision accuracy, as the operator identifies and classifies targets\nwithin simulated aerial imagery. Specifically, using experimental eye-tracking\ndata and measurements about target detectability across the human visual field,\nwe develop functional models of detection accuracy as a function of search\ntime, number of eye movements, scan path, and image clutter. These models are\nthen used by the AAAD in conjunction with real time eye position data to make\nprobabilistic estimations of attained search accuracy and to recommend that the\nobserver either move on to the next image or continue exploring the present\nimage. An experimental evaluation in a scenario motivated from human\nsupervisory control in surveillance missions confirms the benefits of the AAAD.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 21:58:28 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Deza", "Arturo", ""], ["Peters", "Jeffrey R.", ""], ["Taylor", "Grant S.", ""], ["Surana", "Amit", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1701.04185", "submitter": "Rohit M Thanki", "authors": "Rohit M. Thanki, Ved Vyas Dwivedi and Komal R. Borisagar", "title": "A Watermarking Technique Using Discrete Curvelet Transform for Security\n  of Multiple Biometric Features", "comments": null, "journal-ref": "International Journal of Information Processing,volume 10, issue\n  1, pp. 103 - 114 (2016)", "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The robustness and security of the biometric watermarking approach can be\nimproved by using a multiple watermarking. This multiple watermarking proposed\nfor improving security of biometric features and data. When the imposter tries\nto create the spoofed biometric feature, the invisible biometric watermark\nfeatures can provide appropriate protection to multimedia data. In this paper,\na biometric watermarking technique with multiple biometric watermarks are\nproposed in which biometric features of fingerprint, face, iris and signature\nis embedded in the image. Before embedding, fingerprint, iris, face and\nsignature features are extracted using Shen-Castan edge detection and Principal\nComponent Analysis. These all biometric watermark features are embedded into\nvarious mid band frequency curvelet coefficients of host image. All four\nfingerprint features, iris features, facial features and signature features are\nthe biometric characteristics of the individual and they are used for cross\nverification and copyright protection if any manipulation occurs. The proposed\ntechnique is fragile enough; features cannot be extracted from the watermarked\nimage when an imposter tries to remove watermark features illegally. It can use\nfor multiple copyright authentication and verification.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 06:41:21 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Thanki", "Rohit M.", ""], ["Dwivedi", "Ved Vyas", ""], ["Borisagar", "Komal R.", ""]]}, {"id": "1701.05611", "submitter": "Hamzeh Ghasemzadeh", "authors": "Hamzeh Ghasemzadeh, Mohammad H. Kayvanrad", "title": "Comprehensive Review of Audio Steganalysis Methods", "comments": "Submitted journal paper", "journal-ref": "IET Signal Processing, 2018", "doi": "10.1049/iet-spr.2016.0651", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, merging signal processing techniques with information security\nservices has found a lot of attention. Steganography and steganalysis are among\nthose trends. Like their counterparts in cryptology, steganography and\nsteganalysis are in a constant battle. Steganography methods try to hide the\npresence of covert messages in innocuous-looking data, whereas steganalysis\nmethods try to reveal existence of such messages and to break steganography\nmethods. The stream nature of audio signals, their popularity, and their wide\nspread usage make them very suitable media for steganography. This has led to a\nvery rich literature on both steganography and steganalysis of audio signals.\nThis paper intends to conduct a comprehensive review of audio steganalysis\nmethods aggregated over near fifteen years. Furthermore, we implement some of\nthe most recent audio steganalysis methods and conduct a comparative analysis\non their performances. Finally, the paper provides some possible directions for\nfuture researches on audio steganalysis.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 21:43:07 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 05:42:31 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 18:58:29 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Ghasemzadeh", "Hamzeh", ""], ["Kayvanrad", "Mohammad H.", ""]]}, {"id": "1701.05614", "submitter": "Hamzeh Ghasemzadeh", "authors": "Hamzeh Ghasemzadeh, Meisam Khalil Arjmandi", "title": "Universal Audio Steganalysis Based on Calibration and Reversed Frequency\n  Resolution of Human Auditory System", "comments": "Submitted journal paper", "journal-ref": null, "doi": "10.1049/iet-spr.2016.0690", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration and higher order statistics (HOS) are standard components of many\nimage steganalysis systems. These techniques have not yet found adequate\nattention in audio steganalysis context. Specifically, most of current works\nare either non-calibrated or only based on noise removal approach. This paper\naims to fill these gaps by proposing a new set of calibrated features based on\nre-embedding technique. Additionally, we show that least significant bit (LSB)\nis the most sensitive bit-plane to data hiding algorithms and therefore it can\nbe employed as a universal embedding method. Furthermore, the proposed features\nare based on a model that has the maximum deviation from human auditory system\n(HAS), and therefore are more suitable for the purpose of steganalysis.\nPerformance of the proposed method is evaluated on a wide range of data hiding\nalgorithms in both targeted and universal paradigms. Simulation results show\nthat the proposed method can detect the finest traces of data hiding algorithms\nand in very low embedding rates. The system detects steghide at capacity of\n0.06 bit per symbol (BPS) with sensitivity of 98.6% (music) and 78.5% (speech).\nThese figures are respectively 7.1% and 27.5% higher than state-of-the-art\nresults based on RMFCC.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 21:49:06 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 05:42:44 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Ghasemzadeh", "Hamzeh", ""], ["Arjmandi", "Meisam Khalil", ""]]}, {"id": "1701.06509", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini and Viswanathan Swaminathan", "title": "Adaptive 360 VR Video Streaming based on MPEG-DASH SRD", "comments": "IEEE International Symposium on Multimedia 2016 (ISM '16), December\n  4-7, San Jose, California, USA. arXiv admin note: substantial text overlap\n  with arXiv:1609.08729", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an adaptive bandwidth-efficient 360 VR video streaming system\nbased on MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR\nvideos, and showcase a dynamic view-aware adaptation technique to tackle the\nhigh bandwidth demands of streaming 360 VR videos to wireless VR headsets. We\nspatially partition the underlying 3D mesh into multiple 3D sub-meshes, and\nconstruct an efficient 3D geometry mesh called hexaface sphere to optimally\nrepresent tiled 360 VR videos in the 3D space. We then spatially divide the 360\nvideos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to\ndescribe the spatial relationship of tiles in the 3D space, and prioritize the\ntiles in the Field of View (FoV) for view-aware adaptation. Our initial\nevaluation results show that we can save up to 72% of the required bandwidth on\n360 VR video streaming with minor negative quality impacts compared to the\nbaseline scenario when no adaptations is applied.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 17:11:32 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Swaminathan", "Viswanathan", ""]]}, {"id": "1701.09182", "submitter": "Furrakh Shahzad", "authors": "Maruf Pasha and Furrakh Shahzad and Arslan Ahmad", "title": "Analysis of challenges faced by WebRTC videoconferencing and a remedial\n  architecture", "comments": null, "journal-ref": "Vol. 14 No. 10 OCTOBER 2016 International Journal of Computer\n  Science and Information Security", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lately, World Wide Web came up with an evolution in the niche of\nvideoconference applications. Latest technologies give browsers a capacity to\ninitiate real-time communications. WebRTC is one of the free and open source\nprojects that aim at providing the users freedom to enjoy real-time\ncommunications, and it does so by following and redefining the standards.\nHowever, WebRTC is still a new project and it lacks some high-end\nvideoconferencing features such as media mixing, recording of a session and\ndifferent network conditions adaptation. This paper is an attempt at analyzing\nthe shortcomings and challenges faced by WebRTC and proposing a Multipoint\nControl Unit or traditional communications entity based architecture as a\nsolution.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 18:09:08 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 04:23:50 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Pasha", "Maruf", ""], ["Shahzad", "Furrakh", ""], ["Ahmad", "Arslan", ""]]}]