[{"id": "1602.00238", "submitter": "Mar Gonzalez-Franco", "authors": "Jacob Thorn, Rodrigo Pizarro, Bernhard Spanlang, Pablo Bermell-Garcia\n  and Mar Gonzalez-Franco", "title": "Assessing 3D scan quality in Virtual Reality through paired-comparisons\n  psychophysics test", "comments": "9 pages, 10 figures, video: https://youtu.be/vC3Tx07szXU", "journal-ref": "Proceedings of the 2016 ACM on Multimedia Conference (MM '16).\n  ACM, New York, NY, USA, 147-151", "doi": "10.1145/2964284.2967200", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer 3D scanners and depth cameras are increasingly being used to\ngenerate content and avatars for Virtual Reality (VR) environments and avoid\nthe inconveniences of hand modeling; however, it is sometimes difficult to\nevaluate quantitatively the mesh quality at which 3D scans should be exported,\nand whether the object perception might be affected by its shading. We propose\nusing a paired-comparisons test based on psychophysics of perception to do that\nevaluation. As psychophysics is not subject to opinion, skill level, mental\nstate, or economic situation it can be considered a quantitative way to measure\nhow people perceive the mesh quality. In particular, we propose using the\npsychophysical measure for the comparison of four different levels of mesh\nquality (1K, 5K, 10K and 20K triangles). We present two studies within\nsubjects: in one we investigate the quality perception variations of seeing an\nobject in a regular screen monitor against an stereoscopic Head Mounted Display\n(HMD); while in the second experiment we aim at detecting the effects of\nshading into quality perception. At each iteration of the pair-test comparisons\nparticipants pick the mesh that they think had higher quality; by the end of\nthe experiment we compile a preference matrix. The matrix evidences the\ncorrelation between real quality and assessed quality. Regarding the shading\nmode, we find an interaction with quality and shading when the model has high\ndefinition. Furthermore, we assess the subjective realism of the most/least\npreferred scans using an Immersive Augmented Reality (IAR) video-see-through\nsetup. Results show higher levels of realism were perceived through the HMD\nthan when using a monitor, although the quality was similarly perceived in both\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 12:43:34 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 22:09:27 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Thorn", "Jacob", ""], ["Pizarro", "Rodrigo", ""], ["Spanlang", "Bernhard", ""], ["Bermell-Garcia", "Pablo", ""], ["Gonzalez-Franco", "Mar", ""]]}, {"id": "1602.00489", "submitter": "Amit Dvir Dr.", "authors": "Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar, Itay Richman and Ofir\n  Trabelsi", "title": "Real Time Video Quality Representation Classification of Encrypted HTTP\n  Adaptive Video Streaming - the Case of Safari", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of HTTP adaptive video streaming services has\ndramatically increased bandwidth requirements on operator networks, which\nattempt to shape their traffic through Deep Packet Inspection (DPI). However,\nGoogle and certain content providers have started to encrypt their video\nservices. As a result, operators often encounter difficulties in shaping their\nencrypted video traffic via DPI. This highlights the need for new traffic\nclassification methods for encrypted HTTP adaptive video streaming to enable\nsmart traffic shaping. These new methods will have to effectively estimate the\nquality representation layer and playout buffer. We present a new method and\nshow for the first time that video quality representation classification for\n(YouTube) encrypted HTTP adaptive streaming is possible. We analyze the\nperformance of this classification method with Safari over HTTPS. Based on a\nlarge number of offline and online traffic classification experiments, we\ndemonstrate that it can independently classify, in real time, every video\nsegment into one of the quality representation layers with 97.18% average\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 12:12:17 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 15:23:02 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Dubin", "Ran", ""], ["Dvir", "Amit", ""], ["Pele", "Ofir", ""], ["Hadar", "Ofer", ""], ["Richman", "Itay", ""], ["Trabelsi", "Ofir", ""]]}, {"id": "1602.00490", "submitter": "Amit Dvir Dr.", "authors": "Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar", "title": "I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video\n  Streaming Title Classification", "comments": "9 pages. arXiv admin note: text overlap with arXiv:1602.00489", "journal-ref": "IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 12,\n  NO. 12, DECEMBER 2017", "doi": "10.1109/TIFS.2017.2730819", "report-no": null, "categories": "cs.MM cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Desktops and laptops can be maliciously exploited to violate privacy. There\nare two main types of attack scenarios: active and passive. In this paper, we\nconsider the passive scenario where the adversary does not interact actively\nwith the device, but he is able to eavesdrop on the network traffic of the\ndevice from the network side. Most of the Internet traffic is encrypted and\nthus passive attacks are challenging. Previous research has shown that\ninformation can be extracted from encrypted multimedia streams. This includes\nvideo title classification of non HTTP adaptive streams (non-HAS). This paper\npresents an algorithm for encrypted HTTP adaptive video streaming title\nclassification. We show that an external attacker can identify the video title\nfrom video HTTP adaptive streams (HAS) sites such as YouTube. To the best of\nour knowledge, this is the first work that shows this. We provide a large data\nset of 10000 YouTube video streams of 100 popular video titles (each title\ndownloaded 100 times) as examples for this task. The dataset was collected\nunder real-world network conditions. We present several machine algorithms for\nthe task and run a through set of experiments, which shows that our\nclassification accuracy is more than 95%. We also show that our algorithms are\nable to classify video titles that are not in the training set as unknown and\nsome of the algorithms are also able to eliminate false prediction of video\ntitles and instead report unknown. Finally, we evaluate our algorithms\nrobustness to delays and packet losses at test time and show that a solution\nthat uses SVM is the most robust against these changes given enough training\ndata. We provide the dataset and the crawler for future research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 12:15:27 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 06:42:01 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Dubin", "Ran", ""], ["Dvir", "Amit", ""], ["Pele", "Ofir", ""], ["Hadar", "Ofer", ""]]}, {"id": "1602.01168", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Yaming Wang, Larry Davis, Walt Andrews, Viktor Rozgic", "title": "Learning Discriminative Features via Label Consistent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) enforces supervised information only\nat the output layer, and hidden layers are trained by back propagating the\nprediction error from the output layer without explicit supervision. We propose\na supervised feature learning approach, Label Consistent Neural Network, which\nenforces direct supervision in late hidden layers. We associate each neuron in\na hidden layer with a particular class label and encourage it to be activated\nfor input signals from the same class. More specifically, we introduce a label\nconsistency regularization called \"discriminative representation error\" loss\nfor late hidden layers and combine it with classification error loss to build\nour overall objective function. This label consistency constraint alleviates\nthe common problem of gradient vanishing and tends to faster convergence; it\nalso makes the features derived from late hidden layers discriminative enough\nfor classification even using a simple $k$-NN classifier, since input signals\nfrom the same class will have very similar representations. Experimental\nresults demonstrate that our approach achieves state-of-the-art performances on\nseveral public benchmarks for action and object category recognition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 02:41:33 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 02:45:35 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Wang", "Yaming", ""], ["Davis", "Larry", ""], ["Andrews", "Walt", ""], ["Rozgic", "Viktor", ""]]}, {"id": "1602.01178", "submitter": "Tam Nguyen", "authors": "Erik Cambria, Tam V. Nguyen, Brian Cheng, Kenneth Kwok, Jose Sepulveda", "title": "GECKA3D: A 3D Game Engine for Commonsense Knowledge Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge representation and reasoning is key for tasks such as\nartificial intelligence and natural language understanding. Since commonsense\nconsists of information that humans take for granted, gathering it is an\nextremely difficult task. In this paper, we introduce a novel 3D game engine\nfor commonsense knowledge acquisition (GECKA3D) which aims to collect\ncommonsense from game designers through the development of serious games.\nGECKA3D integrates the potential of serious games and games with a purpose.\nThis provides a platform for the acquisition of re-usable and multi-purpose\nknowledge, and also enables the development of games that can provide\nentertainment value and teach players something meaningful about the actual\nworld they live in.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 03:32:31 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Cambria", "Erik", ""], ["Nguyen", "Tam V.", ""], ["Cheng", "Brian", ""], ["Kwok", "Kenneth", ""], ["Sepulveda", "Jose", ""]]}, {"id": "1602.01890", "submitter": "Archith Bency", "authors": "Archith J. Bency, S. Karthikeyan, Carter De Leo, Santhoshkumar\n  Sunderrajan and B. S. Manjunath", "title": "Search Tracker: Human-derived object tracking in-the-wild through\n  large-scale search and retrieval", "comments": "Under review with the IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2555718", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans use context and scene knowledge to easily localize moving objects in\nconditions of complex illumination changes, scene clutter and occlusions. In\nthis paper, we present a method to leverage human knowledge in the form of\nannotated video libraries in a novel search and retrieval based setting to\ntrack objects in unseen video sequences. For every video sequence, a document\nthat represents motion information is generated. Documents of the unseen video\nare queried against the library at multiple scales to find videos with similar\nmotion characteristics. This provides us with coarse localization of objects in\nthe unseen video. We further adapt these retrieved object locations to the new\nvideo using an efficient warping scheme. The proposed method is validated on\nin-the-wild video surveillance datasets where we outperform state-of-the-art\nappearance-based trackers. We also introduce a new challenging dataset with\ncomplex object appearance changes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 00:01:13 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Bency", "Archith J.", ""], ["Karthikeyan", "S.", ""], ["De Leo", "Carter", ""], ["Sunderrajan", "Santhoshkumar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1602.02030", "submitter": "Amit Dvir Dr.", "authors": "Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar, Itay Katz, Ori Mashiach", "title": "Adaptation Logic for HTTP Dynamic Adaptive Streaming using\n  Geo-Predictive Crowdsourcing", "comments": "10 pages", "journal-ref": null, "doi": "10.1007/s00530-016-0525-6", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing demand for video streaming services with high Quality of\nExperience (QoE) has prompted a lot of research on client-side adaptation logic\napproaches. However, most algorithms use the client's previous download\nexperience and do not use a crowd knowledge database generated by users of a\nprofessional service. We propose a new crowd algorithm that maximizes the QoE.\nAdditionally, we show how crowd information can be integrated into existing\nalgorithms and illustrate this with two state-of-the-art algorithms. We\nevaluate our algorithm and state-of-the-art algorithms (including our modified\nalgorithms) on a large, real-life crowdsourcing dataset that contains 336,551\nsamples on network performance. The dataset was provided by WeFi LTD. Our new\nalgorithm outperforms all other methods in terms of QoS (eMOS).\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 14:17:52 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Dubin", "Ran", ""], ["Dvir", "Amit", ""], ["Pele", "Ofir", ""], ["Hadar", "Ofer", ""], ["Katz", "Itay", ""], ["Mashiach", "Ori", ""]]}, {"id": "1602.02692", "submitter": "Joachim Allgaier", "authors": "Joachim Allgaier", "title": "Science on YouTube: What users find when they search for climate science\n  and climate manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online video-sharing sites such as YouTube are very popular and also used by\na lot of people to obtain knowledge and information, also on science, health\nand technology. Technically they could be valuable tools for the public\ncommunication of science and technology, but the users of YouTube are also\nconfronted with conspiracy theories and erroneous and misleading information\nthat deviates from scientific consensus views. This contribution details the\nresults of a study that investigates what kind of information users find when\nthey are searching for climate science and climate manipulation topics on\nYouTube and whether this information corresponds with or challenges scientific\nconsensus views. An innovative methodological approach using the anonymization\nnetwork Tor is introduced for drawing randomized samples of YouTube videos.\nThis approach was used to select and examine a sample of 140 YouTube videos on\nclimate topics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 18:43:30 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 12:42:27 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Allgaier", "Joachim", ""]]}, {"id": "1602.02834", "submitter": "Omar Salim Hazim", "authors": "Omar H. Salim, Wei Xiang, Ali A. Nasi, Gengkun Wang, and Hani\n  Mehrpouyan", "title": "Joint Data Detection and Phase Noise Mitigation for Light Field Video\n  Transmission in MIMO-OFDM Systems", "comments": "Submitted to IEEE Transactions on Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies in the literature for video transmission over wireless\ncommunication systems focused on combating the effects of additive channel\nnoise and fading channels without taking the impairments in the physical layer\nsuch as phase noise (PHN) into account. Oscillator phase noise impairs the\nperformance of multi-input multi-output- orthogonal frequency division\nmultiplexing (MIMO-OFDM) systems in providing high data rates for video\napplications and may lead to decoding failure. In this paper, we propose a\nlight field (LF) video transmission system in wireless channels, and analyze\njoint data detection and phase mitigation in MIMO-OFDM systems for LF video\ntransmission. The signal model and rate-distortion (RD) model for LF video\ntransmission in the presence of multiple PHNs are discussed. Moreover, we\npropose an iterative algorithm based on the extended Kalman filter for joint\ndata detection and PHN tracking. Numerical results show that the proposed\ndetector can significantly improve the average bit-error rate (BER) and\npeak-to-noise ratio (PSNR) performance for LF video transmission compared to\nexisting algorithms. Moreover, the BER and PSNR performance of the proposed\nsystem is closer to that of the ideal case of perfect PHN estimation. Finally,\nit is demonstrated that the proposed system model and algorithm are well suited\nfor LF video transmission in wireless channels.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 01:30:32 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Salim", "Omar H.", ""], ["Xiang", "Wei", ""], ["Nasi", "Ali A.", ""], ["Wang", "Gengkun", ""], ["Mehrpouyan", "Hani", ""]]}, {"id": "1602.03308", "submitter": "David Barina", "authors": "David Barina", "title": "Gabor Wavelets in Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work shows the use of a two-dimensional Gabor wavelets in image\nprocessing. Convolution with such a two-dimensional wavelet can be separated\ninto two series of one-dimensional ones. The key idea of this work is to\nutilize a Gabor wavelet as a multiscale partial differential operator of a\ngiven order. Gabor wavelets are used here to detect edges, corners and blobs. A\nperformance of such an interest point detector is compared to detectors\nutilizing a Haar wavelet and a derivative of a Gaussian function. The proposed\napproach may be useful when a fast implementation of the Gabor transform is\navailable or when the transform is already precomputed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 09:45:38 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Barina", "David", ""]]}, {"id": "1602.03716", "submitter": "Mohammed Al-Maqri", "authors": "Mohammed A. Al-Maqri, Mohamed Othman, Borhanuddin Mohd Ali, Zurina\n  Mohd Hanapi", "title": "Feasible HCCA Polling Mechanism for Video Transmission in IEEE 802.11e\n  WLANs", "comments": null, "journal-ref": null, "doi": "10.1007/s11277-015-2816-1", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IEEE 802.11e standard defines two Medium Access Control (MAC) functions to\nsupport Quality of Service (QoS) for wireless local area networks: Enhanced\nDistributed Channel Access (EDCA) and HCF Controlled Channel Access (HCCA).\nEDCA provides fair prioritized QoS support while HCCA guarantees parameterized\nQoS for the traffics with rigid QoS requirements. The latter shows higher QoS\nprovisioning with Constant Bit Rate (CBR) traffics. However, it does not\nefficiently cope with the fluctuation of the Variable Bit Rate (VBR) video\nstreams since its reference scheduler generates a schedule based on the mean\ncharacteristics of the traffic. Scheduling based on theses characteristics is\nnot always accurate as these tra_cs show high irregularity over the time. In\nthis paper, we propose an enhancement on the HCCA polling mechanism to address\nthe problem of scheduling pre-recorded VBR video streams. Our approach enhances\nthe polling mechanism by feed-backing the arrival time of the subsequent video\nframe of the uplink traffic obtained through cross-layering approach.\nSimulation experiments have been conducted on several publicly available video\ntraces in order to show the efficiency of our mechanism. The simulation results\nreveal the efficiency of the proposed mechanism in providing less delay and\nhigh throughput with conserving medium channel through minimizing the number of\nNull-Frames caused by wasted polls\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 13:09:05 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Al-Maqri", "Mohammed A.", ""], ["Othman", "Mohamed", ""], ["Ali", "Borhanuddin Mohd", ""], ["Hanapi", "Zurina Mohd", ""]]}, {"id": "1602.03886", "submitter": "Mohammed Al-Maqri", "authors": "Mohammed A. Al-Maqri, Mohamed Othman, Borhanuddin Mohd Ali, Zurina\n  Mohd Hanapi", "title": "Providing Dynamic TXOP for QoS Support of Video Transmission in IEEE\n  802.11e WLANs", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.03699", "journal-ref": null, "doi": "10.4304/jnw.10.9.501-511", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IEEE 802.11e standard introduced by IEEE 802.11 Task Group E (TGe)\nenhances the Quality of Service (QoS) by means of HCF Controlled Channel Access\n(HCCA). The scheduler of HCCA allocates Transmission Opportunities (TXOPs) to\nQoS-enabled Station (QSTA) based on their TS Specifications (TSPECs) negotiated\nat the traffic setup time so that it is only efficient for Constant Bit Rate\n(CBR) applications. However, Variable Bit Rate (VBR) traffics are not\nefficiently supported as they exhibit nondeterministic profile during the time.\nIn this paper, we present a dynamic TXOP assignment Scheduling Algorithm for\nsupporting the video traffics transmission over IEEE 802.11e wireless networks.\nThis algorithm uses a piggybacked information about the size of the subsequent\nvideo frames of the uplink traffic to assist the Hybrid Coordinator accurately\nassign the TXOP according to the fast changes in the VBR profile. The proposed\nscheduling algorithm has been evaluated using simulation with different\nvariability level video streams. The simulation results show that the proposed\nalgorithm reduces the delay experienced by VBR traffic streams comparable to\nHCCA scheduler due to the accurate assignment of the TXOP which preserve the\nchannel time for transmission.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 13:34:14 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Al-Maqri", "Mohammed A.", ""], ["Othman", "Mohamed", ""], ["Ali", "Borhanuddin Mohd", ""], ["Hanapi", "Zurina Mohd", ""]]}, {"id": "1602.04845", "submitter": "Jean-Marc Valin", "authors": "Jean-Marc Valin, Gregory Maxwell, Timothy B. Terriberry, Koen Vos", "title": "High-Quality, Low-Delay Music Coding in the Opus Codec", "comments": "10 pages, 135th AES Convention. Proceedings of the 135th AES\n  Convention, October 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The IETF recently standardized the Opus codec as RFC6716. Opus targets a wide\nrange of real-time Internet applications by combining a linear prediction coder\nwith a transform coder. We describe the transform coder, with particular\nattention to the psychoacoustic knowledge built into the format. The result\nout-performs existing audio codecs that do not operate under real-time\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 21:30:54 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Valin", "Jean-Marc", ""], ["Maxwell", "Gregory", ""], ["Terriberry", "Timothy B.", ""], ["Vos", "Koen", ""]]}, {"id": "1602.04921", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Mi, Weiyue Wang, Jianxin Wu, Jingdong Wang, Tao Mei", "title": "A diffusion and clustering-based approach for finding coherent motions\n  and understanding crowd scenes", "comments": "This manuscript is the accepted version for TIP (IEEE Transactions on\n  Image Processing), 2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2531281", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of detecting coherent motions in crowd\nscenes and presents its two applications in crowd scene understanding: semantic\nregion detection and recurrent activity mining. It processes input motion\nfields (e.g., optical flow fields) and produces a coherent motion filed, named\nas thermal energy field. The thermal energy field is able to capture both\nmotion correlation among particles and the motion trends of individual\nparticles which are helpful to discover coherency among them. We further\nintroduce a two-step clustering process to construct stable semantic regions\nfrom the extracted time-varying coherent motions. These semantic regions can be\nused to recognize pre-defined activities in crowd scenes. Finally, we introduce\na cluster-and-merge process which automatically discovers recurrent activities\nin crowd scenes by clustering and merging the extracted coherent motions.\nExperiments on various videos demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 06:25:30 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Lin", "Weiyao", ""], ["Mi", "Yang", ""], ["Wang", "Weiyue", ""], ["Wu", "Jianxin", ""], ["Wang", "Jingdong", ""], ["Mei", "Tao", ""]]}, {"id": "1602.05209", "submitter": "Jean-Marc Valin", "authors": "Jean-Marc Valin, Timothy B. Terriberry", "title": "Perceptual Vector Quantization For Video Coding", "comments": "11 pages, Proceedings of SPIE Visual Information Processing and\n  Communication, 2015", "journal-ref": "Proc. SPIE 9410, Visual Information Processing and Communication\n  VI, 941009 (March 4, 2015)", "doi": "10.1117/12.2080529", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper applies energy conservation principles to the Daala video codec\nusing gain-shape vector quantization to encode a vector of AC coefficients as a\nlength (gain) and direction (shape). The technique originates from the CELT\nmode of the Opus audio codec, where it is used to conserve the spectral\nenvelope of an audio signal. Conserving energy in video has the potential to\npreserve textures rather than low-passing them. Explicitly quantizing a gain\nallows a simple contrast masking model with no signaling cost. Vector\nquantizing the shape keeps the number of degrees of freedom the same as scalar\nquantization, avoiding redundancy in the representation. We demonstrate how to\npredict the vector by transforming the space it is encoded in, rather than\nsubtracting off the predictor, which would make energy conservation impossible.\nWe also derive an encoding of the vector-quantized codewords that takes\nadvantage of their non-uniform distribution. We show that the resulting\ntechnique outperforms scalar quantization by an average of 0.90 dB on still\nimages, equivalent to a 24.8% reduction in bitrate at equal quality, while for\nvideos, the improvement averages 0.83 dB, equivalent to a 13.7% reduction in\nbitrate.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 21:27:58 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Valin", "Jean-Marc", ""], ["Terriberry", "Timothy B.", ""]]}, {"id": "1602.05311", "submitter": "Jean-Marc Valin", "authors": "Jean-Marc Valin, Timothy B. Terriberry, Gregory Maxwell", "title": "A Full-Bandwidth Audio Codec With Low Complexity And Very Low Delay", "comments": "5 pages, Proceedings of EUSIPCO 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an audio codec that addresses the low-delay requirements of some\napplications such as network music performance. The codec is based on the\nmodified discrete cosine transform (MDCT) with very short frames and uses\ngain-shape quantization to preserve the spectral envelope. The short frame\nsizes required for low delay typically hinder the performance of transform\ncodecs. However, at 96 kbit/s and with only 4 ms algorithmic delay, the\nproposed codec out-performs the ULD codec operating at the same rate. The total\ncomplexity of the codec is small, at only 17 WMOPS for real-time operation at\n48 kHz.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 05:50:50 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Valin", "Jean-Marc", ""], ["Terriberry", "Timothy B.", ""], ["Maxwell", "Gregory", ""]]}, {"id": "1602.05526", "submitter": "Jean-Marc Valin", "authors": "Jean-Marc Valin, Timothy B. Terriberry, Christopher Montgomery,\n  Gregory Maxwell", "title": "A High-Quality Speech and Audio Codec With Less Than 10 ms Delay", "comments": "10 pages", "journal-ref": "IEEE Transactions on Audio, Speech and Language Processing, Vol.\n  18, No. 1, pp. 58-67, 2010", "doi": "10.1109/TASL.2009.2023186", "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing quality requirements for multimedia communications, audio\ncodecs must maintain both high quality and low delay. Typically, audio codecs\noffer either low delay or high quality, but rarely both. We propose a codec\nthat simultaneously addresses both these requirements, with a delay of only 8.7\nms at 44.1 kHz. It uses gain-shape algebraic vector quantisation in the\nfrequency domain with time-domain pitch prediction. We demonstrate that the\nproposed codec operating at 48 kbit/s and 64 kbit/s out-performs both G.722.1C\nand MP3 and has quality comparable to AAC-LD, despite having less than one\nfourth of the algorithmic delay of these codecs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 18:41:16 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Valin", "Jean-Marc", ""], ["Terriberry", "Timothy B.", ""], ["Montgomery", "Christopher", ""], ["Maxwell", "Gregory", ""]]}, {"id": "1602.05920", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Manal H. Alassaf", "title": "Weighted Unsupervised Learning for 3D Object Detection", "comments": "IJACSA", "journal-ref": null, "doi": "10.14569/IJACSA.2016.070180", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel weighted unsupervised learning for object\ndetection using an RGB-D camera. This technique is feasible for detecting the\nmoving objects in the noisy environments that are captured by an RGB-D camera.\nThe main contribution of this paper is a real-time algorithm for detecting each\nobject using weighted clustering as a separate cluster. In a preprocessing\nstep, the algorithm calculates the pose 3D position X, Y, Z and RGB color of\neach data point and then it calculates each data point's normal vector using\nthe point's neighbor. After preprocessing, our algorithm calculates k-weights\nfor each data point; each weight indicates membership. Resulting in clustered\nobjects of the scene.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 19:40:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 23:51:27 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Alassaf", "Manal H.", ""]]}, {"id": "1602.05975", "submitter": "Jean-Marc Valin", "authors": "Steinar Midtskogen and Jean-Marc Valin", "title": "The AV1 Constrained Directional Enhancement Filter (CDEF)", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the constrained directional enhancement filter designed\nfor the AV1 royalty-free video codec. The in-loop filter is based on a\nnon-linear low-pass filter and is designed for vectorization efficiency. It\ntakes into account the direction of edges and patterns being filtered. The\nfilter works by identifying the direction of each block and then adaptively\nfiltering with a high degree of control over the filter strength along the\ndirection and across it. The proposed enhancement filter is shown to improve\nthe quality of the Alliance for Open Media (AOM) AV1 and Thor video codecs in\nparticular in low complexity configurations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 21:14:25 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 18:26:09 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 17:18:28 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Midtskogen", "Steinar", ""], ["Valin", "Jean-Marc", ""]]}, {"id": "1602.07453", "submitter": "Kota Naga Srinivasarao Batta", "authors": "B.K.N.Srinivasarao, Vinay Chakravarthi Gogineni, Subrahmanyam Mula and\n  Indrajit Chakrabarti", "title": "VLSI Friendly Framework for Scalable Video Coding based on Compressed\n  Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new VLSI friendly framework for scalable video coding\nbased on Compressed Sensing (CS). It achieves scalability through 3-Dimensional\nDiscrete Wavelet Transform (3-D DWT) and better compression ratio by exploiting\nthe inherent sparsity of the high-frequency wavelet sub-bands through CS. By\nusing 3-D DWT and a proposed adaptive measurement scheme called AMS at the\nencoder, one can succeed in improving the compression ratio and reducing the\ncomplexity of the decoder. The proposed video codec uses only 7% of the total\nnumber of multipliers needed in a conventional CS-based video coding system. A\ncodebook of Bernoulli matrices with different sizes corresponding to the\npredefined sparsity levels is maintained at both the encoder and the decoder.\nBased on the calculated l0-norm of the input vector, one of the sixteen\npossible Bernoulli matrices will be selected for taking the CS measurements and\nits index will be transmitted along with the measurements. Based on this index,\nthe corresponding Bernoulli matrix has been used in CS reconstruction algorithm\nto get back the high-frequency wavelet sub-bands at the decoder. At the\ndecoder, a new Enhanced Approximate Message Passing (EAMP) algorithm has been\nproposed to reconstruct the wavelet coefficients and apply the inverse wavelet\ntransform for restoring back the video frames. Simulation results have\nestablished the superiority of the proposed framework over the existing schemes\nand have increased its suitability for VLSI implementation. Moreover, the coded\nvideo is found to be scalable with an increase in a number of levels of wavelet\ndecomposition.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 10:08:19 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Srinivasarao", "B. K. N.", ""], ["Gogineni", "Vinay Chakravarthi", ""], ["Mula", "Subrahmanyam", ""], ["Chakrabarti", "Indrajit", ""]]}, {"id": "1602.07811", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Vincent Labatut (LIA), Serigne Gueye (LIA), Georges\n  Linar\\`es (LIA)", "title": "Narrative Smoothing: Dynamic Conversational Network for the Analysis of\n  TV Series Plots", "comments": null, "journal-ref": "DyNo: 2nd International Workshop on Dynamics in Networks, in\n  conjunction with the 2016 IEEE/ACM International Conference ASONAM, Aug 2016,\n  San Francisco, United States. pp.1111-1118,\n  \\&\\#x27E8;10.1109/ASONAM.2016.7752379\\&\\#x27E9", "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern popular TV series often develop complex storylines spanning several\nseasons, but are usually watched in quite a discontinuous way. As a result, the\nviewer generally needs a comprehensive summary of the previous season plot\nbefore the new one starts. The generation of such summaries requires first to\nidentify and characterize the dynamics of the series subplots. One way of doing\nso is to study the underlying social network of interactions between the\ncharacters involved in the narrative. The standard tools used in the Social\nNetworks Analysis field to extract such a network rely on an integration of\ntime, either over the whole considered period, or as a sequence of several\ntime-slices. However, they turn out to be inappropriate in the case of TV\nseries, due to the fact the scenes showed onscreen alternatively focus on\nparallel storylines, and do not necessarily respect a traditional chronology.\nThis makes existing extraction methods inefficient to describe the dynamics of\nrelationships between characters, or to get a relevant instantaneous view of\nthe current social state in the plot. This is especially true for characters\nshown as interacting with each other at some previous point in the plot but\ntemporarily neglected by the narrative. In this article, we introduce narrative\nsmoothing, a novel, still exploratory, network extraction method. It smooths\nthe relationship dynamics based on the plot properties, aiming at solving some\nof the limitations present in the standard approaches. In order to assess our\nmethod, we apply it to a new corpus of 3 popular TV series, and compare it to\nboth standard approaches. Our results are promising, showing narrative\nsmoothing leads to more relevant observations when it comes to the\ncharacterization of the protagonists and their relationships. It could be used\nas a basis for further modeling the intertwined storylines constituting TV\nseries plots.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 06:06:04 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 14:28:28 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 08:45:13 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2018 14:45:41 GMT"}, {"version": "v5", "created": "Thu, 30 Jan 2020 07:49:30 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Gueye", "Serigne", "", "LIA"], ["Linar\u00e8s", "Georges", "", "LIA"]]}, {"id": "1602.08185", "submitter": "Jean-Marc Valin", "authors": "Jean-Marc Valin", "title": "Extension spectrale d'un signal de parole de la bande t\\'el\\'ephonique\n  \\`a la bande AM", "comments": "61 pages, in French, Master's thesis, University of Sherbrooke, 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document proposes a bandwidth extension system producing a wideband\nsignal from a narrowband speech signal. The extension is performed\nindependently for high and low frequencies. High-frequency extension uses the\nexcitation-filter model. Extension of the excitation is performed in the time\ndomain using a non-linear function, while the spectral envelope is extended in\nthe cepstral domain using a multi-layer perceptron. Low-band extension is based\non the sinusoidal model. The amplitude of sinusoids is also estimated using a\nmulti-layer perceptron.\n  The results show that the sound quality after extension is higher than that\nof narrowband speech, with a significant variation across listeners. Some of\nthe techniques, including excitation extension, are of interest in the field of\nspeech coding.\n  -----\n  Le pr\\'esent m\\'emoire propose un syst\\`eme d'extension de la bande\npermettant de produire un signal en bande AM \\`a partir d'un signal de parole\nen bande t\\'el\\'ephonique. L'extension est effectu\\'ee de fa\\c{c}on\nind\\'ependante pour les hautes fr\\'equences et les basses fr\\'equences.\nL'extension des hautes fr\\'equences utilise le mod\\`ele filtre-excitation.\nL'extension de l'excitation est r\\'ealis\\'ee dans le domaine temporel par une\nfonction non lin\\'eaire, alors que l'extension de l'enveloppe spectrale\ns'effectue dans le domaine cepstral par un perceptron multi-couches.\nL'extension de la bande basse utilise le mod\\`ele sinuso\\\"idal. L'amplitude des\nsinuso\\\"ides est aussi estim\\'ee par un perceptron multi-couches.\n  Les r\\'esultats obtenus montrent que la qualit\\'e sonore apr\\`es extension\nest sup\\'erieure \\`a celle de la bande t\\'el\\'ephonique, avec une importante\ndiff\\'erence entre les auditeurs. Certaines techniques d\\'evelopp\\'ees, dont\nl'extension de l'excitation, pr\\'esentent un certain int\\'er\\^et pour le\ndomaine du codage de la parole.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 03:16:37 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Valin", "Jean-Marc", ""]]}]