[{"id": "1308.0315", "submitter": "Ali Wali", "authors": "Mohamed Chakroun, Ali Wali and Adel M. Alimi", "title": "MAS for video objects segmentation and tracking based on active contours\n  and SURF descriptor", "comments": "6 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 2, No 3, March 2013", "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, video segmentation and tracking is an important\nchallenging issue. In this paper, we describe a new video sequences\nsegmentation and tracking algorithm based on MAS \"multi-agent systems\" and SURF\n\"Speeded Up Robust Features\". Our approach consists in modelling a multi-agent\nsystem for segmenting the first image from a video sequence and tracking\nobjects in the video sequences. The used agents are supervisor and explorator\nagents, they are communicating between them and they inspire in their behavior\nfrom active contours approaches. The tracking of objects is based on SURF\ndescriptors \"Speed Up Robust Features\". We used the DIMA platform and \"API\nAteji PX\" (an extension of the Java language to facilitate parallel programming\non heterogeneous architectures) to implement this algorithm. The experimental\nresults indicate that the proposed algorithm is more robust and faster than\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 19:45:23 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Chakroun", "Mohamed", ""], ["Wali", "Ali", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1308.0435", "submitter": "Razafindradina Henri Bruno rhb", "authors": "Henri Bruno Razafindradina, Nicolas Raft Razafindrakoto, Paul Auguste\n  Randriamitantsoa", "title": "Improved Watermarking Scheme Using Discrete Cosine Transform and Schur\n  Decomposition", "comments": null, "journal-ref": "IJCSN International Journal of Computer Science and Network,\n  Volume 2, Issue 4, August 2013", "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking is a technique which consists in introducing a brand, the name\nor the logo of the author, in an image in order to protect it against illegal\ncopy. The capacity of the existing watermark channel is often limited. We\npropose in this paper a new robust method which consists in adding the\ntriangular matrix of the mark obtained after the Schur decomposition to the DCT\ntransform of the host image. The unitary matrix acts as secret key for the\nextraction of the mark. Unlike most watermarking algorithms, the host image and\nthe mark have the same size. The results show that our method is robust against\nattack techniques as : JPEG compression, colors reducing, adding noise,\nfiltering, cropping, low rotations, and histogram spreading.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 08:20:13 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Razafindradina", "Henri Bruno", ""], ["Razafindrakoto", "Nicolas Raft", ""], ["Randriamitantsoa", "Paul Auguste", ""]]}, {"id": "1308.0679", "submitter": "Li Xu", "authors": "Xu Li, Xingming Sun, Quansheng Liu", "title": "Image Integrity Authentication Scheme Based On Fixed Point Theory", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on fixed point theory, this paper proposes a new scheme for image\nintegrity authentication, which is different from Digital Signature and Fragile\nWatermarking. A realization of the new scheme is given based on Gaussian\nConvolution and Deconvolution (GCD) functions. For a given image, if it is\ninvariant under a GCD function, we call it GCD fixed point image. An existence\ntheorem of fixed points for GCD functions is proved and an iterative algorithm\nis presented for finding fixed points. Experiments show that GCD fixed point\nimages perform well in transparence, fragility, security and tampering\nlocalization.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 10:22:35 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Li", "Xu", ""], ["Sun", "Xingming", ""], ["Liu", "Quansheng", ""]]}, {"id": "1308.1150", "submitter": "Ali Wali", "authors": "Ali Wali and Adel M. Alimi", "title": "Multimodal Approach for Video Surveillance Indexing and Retrieval", "comments": "7 pages", "journal-ref": "Journal of Intelligent Computing, Volume: 1, Issue: 4 (December\n  2010), Page: 165-175", "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an overview of a multimodal system to indexing and\nsearching video sequence by the content that has been developed within the\nREGIMVid project. A large part of our system has been developed as part of\nTRECVideo evaluation. The MAVSIR platform provides High-level feature\nextraction from audio-visual content and concept/event-based video retrieval.\nWe illustrate the architecture of the system as well as provide an overview of\nthe descriptors supported to date. Then we demonstrate the usefulness of the\ntoolbox in the context of feature extraction, concepts/events learning and\nretrieval in large collections of video surveillance dataset. The results are\nencouraging as we are able to get good results on several event categories,\nwhile for all events we have gained valuable insights and experience.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 01:21:35 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Wali", "Ali", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1308.1224", "submitter": "Aleksandar Stupar", "authors": "Aleksandar Stupar and Sebastian Michel", "title": "Benchmarking Soundtrack Recommendation Systems with SRBench", "comments": "Extended version of the CIKM 2013 paper: SRbench-A Benchmark for\n  Soundtrack Recommendation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a benchmark to evaluate the retrieval performance of soundtrack\nrecommendation systems is proposed. Such systems aim at finding songs that are\nplayed as background music for a given set of images. The proposed benchmark is\nbased on preference judgments, where relevance is considered a continuous\nordinal variable and judgments are collected for pairs of songs with respect to\na query (i.e., set of images). To capture a wide variety of songs and images,\nwe use a large space of possible music genres, different emotions expressed\nthrough music, and various query-image themes. The benchmark consists of two\ntypes of relevance assessments: (i) judgments obtained from a user study, that\nserve as a \"gold standard\" for (ii) relevance judgments gathered through\nAmazon's Mechanical Turk. We report on an analysis of relevance judgments based\non different levels of user agreement and investigate the performance of two\nstate-of-the-art soundtrack recommendation systems using the proposed\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 10:10:22 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Stupar", "Aleksandar", ""], ["Michel", "Sebastian", ""]]}, {"id": "1308.1418", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O Nwana, Salman Avestimehr and Tsuhan Chen", "title": "A Latent Social Approach to YouTube Popularity Prediction", "comments": null, "journal-ref": null, "doi": "10.1109/GLOCOM.2013.6831554", "report-no": null, "categories": "cs.SI cs.MM cs.NI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current works on Information Centric Networking assume the spectrum of\ncaching strategies under the Least Recently/ Frequently Used (LRFU) scheme as\nthe de-facto standard, due to the ease of implementation and easier analysis of\nsuch strategies. In this paper we predict the popularity distribution of\nYouTube videos within a campus network. We explore two broad approaches in\npredicting the popularity of videos in the network: consensus approaches based\non aggregate behavior in the network, and social approaches based on the\ninformation diffusion over an implicit network. We measure the performance of\nour approaches under a simple caching framework by picking the k most popular\nvideos according to our predicted distribution and calculating the hit rate on\nthe cache. We develop our approach by first incorporating video inter-arrival\ntime (based on the power-law distribution governing the transmission time\nbetween two receivers of the same message in scale-free networks) to the\nbaseline (LRFU), then combining with an information diffusion model over the\ninferred latent social graph that governs diffusion of videos in the network.\nWe apply techniques from latent social network inference to learn the sharing\nprobabilities between users in the network and apply a virus propagation model\nborrowed from mathematical epidemiology to estimate the number of times a video\nwill be accessed in the future. Our approach gives rise to a 14% hit rate\nimprovement over the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 20:40:52 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Nwana", "Amandianeze O", ""], ["Avestimehr", "Salman", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1308.1817", "submitter": "Pasi Rikhard Saari", "authors": "Pasi Saari, Tuomas Eerola", "title": "Semantic Computing of Moods Based on Tags in Social Media of Music", "comments": "Preprint, 14 pages", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2013", "doi": "10.1109/TKDE.2013.128", "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social tags inherent in online music services such as Last.fm provide a rich\nsource of information on musical moods. The abundance of social tags makes this\ndata highly beneficial for developing techniques to manage and retrieve mood\ninformation, and enables study of the relationships between music content and\nmood representations with data substantially larger than that available for\nconventional emotion research. However, no systematic assessment has been done\non the accuracy of social tags and derived semantic models at capturing mood\ninformation in music. We propose a novel technique called Affective Circumplex\nTransformation (ACT) for representing the moods of music tracks in an\ninterpretable and robust fashion based on semantic computing of social tags and\nresearch in emotion modeling. We validate the technique by predicting listener\nratings of moods in music tracks, and compare the results to prediction with\nthe Vector Space Model (VSM), Singular Value Decomposition (SVD), Nonnegative\nMatrix Factorization (NMF), and Probabilistic Latent Semantic Analysis (PLSA).\nThe results show that ACT consistently outperforms the baseline techniques, and\nits performance is robust against a low number of track-level mood tags. The\nresults give validity and analytical insights for harnessing millions of music\ntracks and associated mood data available through social tags in application\ndevelopment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 11:29:24 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Saari", "Pasi", ""], ["Eerola", "Tuomas", ""]]}, {"id": "1308.1839", "submitter": "Xiaohong Peng", "authors": "Mirghiasaldin Seyedebrahimi, Colin Bailey, Xiao-Hong Peng", "title": "Model and Performance of a No-Reference Quality Assessment Metric for\n  Video Streaming", "comments": "To appear in IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2013.2270365", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video streaming via TCP networks has become a popular and highly demanded\nservice, but its quality assessment in both objective and subjective terms has\nnot been properly addressed. In this paper, based on statistical analysis a\nfull analytic model of a no-reference objective metric, namely Pause Intensity,\nfor video quality assessment is presented. The model characterizes the video\nplayout buffer behavior in connection with the network performance (throughput)\nand the video playout rate. This allows for instant quality measurement and\ncontrol without requiring a reference video. Pause intensity specifically\naddresses the need for assessing the quality issue in terms of the continuity\nin the playout of TCP streaming videos, which cannot be properly measured by\nother objective metrics such as PSNR, SSIM and buffer underrun or pause\nfrequency. The performance of the analytical model is rigidly verified by\nsimulation results and subjective tests using a range of video clips. It is\ndemonstrated that pause intensity is closely correlated with viewer opinion\nscores regardless of the vastly different composition of individual elements,\nsuch as pause duration and pause frequency which jointly constitute this new\nquality metric. It is also shown that the correlation performance of pause\nintensity is consistent and content independent.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 13:10:46 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Seyedebrahimi", "Mirghiasaldin", ""], ["Bailey", "Colin", ""], ["Peng", "Xiao-Hong", ""]]}, {"id": "1308.2393", "submitter": "Suresh Jaganathan", "authors": "Suresh Jaganathan and Srinivasan Arulanadam and Damodaram Avula", "title": "An Efficient Transport Protocol for delivery of Multimedia An Efficient\n  Transport Protocol for delivery of Multimedia Content in Wireless Grids", "comments": "20 pages, 15 figures, Peer Reviewed Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grid computing system is designed for solving complicated scientific and\ncommercial problems effectively,whereas mobile computing is a traditional\ndistributed system having computing capability with mobility and adopting\nwireless communications. Media and Entertainment fields can take advantage from\nboth paradigms by applying its usage in gaming applications and multimedia data\nmanagement. Multimedia data has to be stored and retrieved in an efficient and\neffective manner to put it in use. In this paper, we proposed an application\nlayer protocol for delivery of multimedia data in wireless girds i.e.\nmultimedia grid protocol (MMGP). To make streaming efficient a new video\ncompression algorithm called dWave is designed and embedded in the proposed\nprotocol. This protocol will provide faster, reliable access and render an\nimperceptible QoS in delivering multimedia in wireless grid environment and\ntackles the challenging issues such as i) intermittent connectivity, ii) device\nheterogeneity, iii) weak security and iv) device mobility.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 13:29:06 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Jaganathan", "Suresh", ""], ["Arulanadam", "Srinivasan", ""], ["Avula", "Damodaram", ""]]}, {"id": "1308.2600", "submitter": "Abdelali El Bouchti", "authors": "Mohamed Hanini, Abdelali El Bouchti, Abdelkrim Haqiq, Amine Berqia", "title": "An Enhanced Time Space Priority Scheme to Manage QoS for Multimedia\n  Flows transmitted to an end user in HSDPA Network", "comments": "5 pages", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 9, No. 2, February 2011", "doi": null, "report-no": null, "categories": "cs.NI cs.MM cs.SY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  When different type of packets with different needs of Quality of Service\n(QoS) requirements share the same network resources, it became important to use\nqueue management and scheduling schemes in order to maintain perceived quality\nat the end users at an acceptable level. Many schemes have been studied in the\nliterature, these schemes use time priority (to maintain QoS for Real Time (RT)\npackets) and/or space priority (to maintain QoS for Non Real Time (NRT)\npackets). In this paper, we study and show the drawback of a combined time and\nspace priority (TSP) scheme used to manage QoS for RT and NRT packets intended\nfor an end user in High Speed Downlink Packet Access (HSDPA) cell, and we\npropose an enhanced scheme (Enhanced Basic-TSP scheme) to improve QoS\nrelatively to the RT packets, and to exploit efficiently the network resources.\nA mathematical model for the EB-TSP scheme is done, and numerical results show\nthe positive impact of this scheme.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 15:48:04 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Hanini", "Mohamed", ""], ["Bouchti", "Abdelali El", ""], ["Haqiq", "Abdelkrim", ""], ["Berqia", "Amine", ""]]}, {"id": "1308.3225", "submitter": "Mohamed Ben Halima", "authors": "M. Ben Halima, M. Hamroun, S. Ben Moussa and A. M. Alimi", "title": "An interactive engine for multilingual video browsing using semantic\n  content", "comments": "4 pages, IGS 2013 Conference; IGS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of audio-visual information has increased dramatically with the\nadvent of High Speed Internet. Furthermore, technological advances in recent\nyears in the field of information technology, have simplified the use of video\ndata in various fields by the general public. This made it possible to store\nlarge collections of video documents into computer systems. To enable efficient\nuse of these collections, it is necessary to develop tools to facilitate access\nto these documents and handling them. In this paper we propose a method for\nindexing and retrieval of video sequences in a video database of large\ndimension, based on a weighting technique to calculate the degree of membership\nof a concept in a video also a structuring of the data of the audio-visual\n(context / concept / video) and a relevance feedback mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 19:54:11 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Halima", "M. Ben", ""], ["Hamroun", "M.", ""], ["Moussa", "S. Ben", ""], ["Alimi", "A. M.", ""]]}, {"id": "1308.3243", "submitter": "Mohamed Ben Halima", "authors": "M. Ben Halima, H. Karray and A. M. Alimi", "title": "Arabic Text Recognition in Video Sequences", "comments": "10 pages - International Journal of Computational Linguistics\n  Research. arXiv admin note: substantial text overlap with arXiv:1211.2150", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust approach for text extraction and\nrecognition from Arabic news video sequence. The text included in video\nsequences is an important needful for indexing and searching system. However,\nthis text is difficult to detect and recognize because of the variability of\nits size, their low resolution characters and the complexity of the\nbackgrounds. To solve these problems, we propose a system performing in two\nmain tasks: extraction and recognition of text. Our system is tested on a\nvaried database composed of different Arabic news programs and the obtained\nresults are encouraging and show the merits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 20:15:44 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Halima", "M. Ben", ""], ["Karray", "H.", ""], ["Alimi", "A. M.", ""]]}, {"id": "1308.4263", "submitter": "Angelo Ciaramella", "authors": "Angelo Ciaramella, Giulio Giunta", "title": "Compressive Sampling for the Packet Loss Recovery in Audio Multimedia\n  Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to introduce a new schema, based on a Compressive\nSampling technique, for the recovery of lost data in multimedia streaming. The\naudio streaming data are encapsuled in different packets by using an\ninterleaving technique. The Compressive Sampling technique is used to recover\naudio information in case of lost packets. Experimental results are presented\non speech and musical audio signals to illustrate the performances and the\ncapabilities of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 09:02:57 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Ciaramella", "Angelo", ""], ["Giunta", "Giulio", ""]]}, {"id": "1308.4458", "submitter": "Reza Pournaghi", "authors": "Reza Pournaghi and Xiaolin Wu", "title": "Coded Acquisition of High Frame Rate Video", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2368359", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High frame video (HFV) is an important investigational tool in sciences,\nengineering and military. In ultra-high speed imaging, the obtainable temporal,\nspatial and spectral resolutions are limited by the sustainable throughput of\nin-camera mass memory, the lower bound of exposure time, and illumination\nconditions. In order to break these bottlenecks, we propose a new coded video\nacquisition framework that employs K > 2 conventional cameras, each of which\nmakes random measurements of the 3D video signal in both temporal and spatial\ndomains. For each of the K cameras, this multi-camera strategy greatly relaxes\nthe stringent requirements in memory speed, shutter speed, and illumination\nstrength. The recovery of HFV from these random measurements is posed and\nsolved as a large scale l1 minimization problem by exploiting joint temporal\nand spatial sparsities of the 3D signal. Three coded video acquisition\ntechniques of varied trade offs between performance and hardware complexity are\ndeveloped: frame-wise coded acquisition, pixel-wise coded acquisition, and\ncolumn-row-wise coded acquisition. The performances of these techniques are\nanalyzed in relation to the sparsity of the underlying video signal.\nSimulations of these new HFV capture techniques are carried out and\nexperimental results are reported.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 01:13:46 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Pournaghi", "Reza", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1308.4908", "submitter": "Joel Kronander", "authors": "Joel Kronander, Stefan Gustavson, Gerhard Bonnet, Anders Ynnerman and\n  Jonas Unger", "title": "A Unified Framework for Multi-Sensor HDR Video Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most successful approaches to modern high quality HDR-video\ncapture is to use camera setups with multiple sensors imaging the scene through\na common optical system. However, such systems pose several challenges for HDR\nreconstruction algorithms. Previous reconstruction techniques have considered\ndebayering, denoising, resampling (align- ment) and exposure fusion as separate\nproblems. In contrast, in this paper we present a unifying approach, performing\nHDR assembly directly from raw sensor data. Our framework includes a camera\nnoise model adapted to HDR video and an algorithm for spatially adaptive HDR\nreconstruction based on fitting of local polynomial approximations to observed\nsensor data. The method is easy to implement and allows reconstruction to an\narbitrary resolution and output mapping. We present an implementation in CUDA\nand show real-time performance for an experimental 4 Mpixel multi-sensor HDR\nvideo system. We further show that our algorithm has clear advantages over\nexisting methods, both in terms of flexibility and reconstruction quality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 15:58:01 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Kronander", "Joel", ""], ["Gustavson", "Stefan", ""], ["Bonnet", "Gerhard", ""], ["Ynnerman", "Anders", ""], ["Unger", "Jonas", ""]]}, {"id": "1308.5326", "submitter": "Xu Li", "authors": "Xu Li, Xingming Sun, Quansheng Liu, Beijing Chen", "title": "A Novel Method for Image Integrity Authentication Based on Fixed Point\n  Theory", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on fixed point theory, this paper proposes a simple but efficient\nmethod for image integrity authentication, which is different from Digital\nSignature and Fragile Watermarking. By this method, any given image can be\ntransformed into a fixed point of a well-chosen function, which can be\nconstructed with periodic functions. The authentication can be realized due to\nthe fragility of the fixed points. The experiments show that 'Fixed Point\nImage' performs well in security, transparence, fragility and tampering\nlocalization.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 13:38:24 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Li", "Xu", ""], ["Sun", "Xingming", ""], ["Liu", "Quansheng", ""], ["Chen", "Beijing", ""]]}, {"id": "1308.6628", "submitter": "Kewei Tu", "authors": "Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, Song-Chun Zhu", "title": "Joint Video and Text Parsing for Understanding Events and Answering\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for parsing video and text jointly for understanding\nevents and answering user queries. Our framework produces a parse graph that\nrepresents the compositional structures of spatial information (objects and\nscenes), temporal information (actions and events) and causal information\n(causalities between events and fluents) in the video and text. The knowledge\nrepresentation of our framework is based on a spatial-temporal-causal And-Or\ngraph (S/T/C-AOG), which jointly models possible hierarchical compositions of\nobjects, scenes and events as well as their interactions and mutual contexts,\nand specifies the prior probabilistic distribution of the parse graphs. We\npresent a probabilistic generative model for joint parsing that captures the\nrelations between the input video/text, their corresponding parse graphs and\nthe joint parse graph. Based on the probabilistic model, we propose a joint\nparsing system consisting of three modules: video parsing, text parsing and\njoint inference. Video parsing and text parsing produce two parse graphs from\nthe input video and text respectively. The joint inference module produces a\njoint parse graph by performing matching, deduction and revision on the video\nand text parse graphs. The proposed framework has the following objectives:\nFirstly, we aim at deep semantic parsing of video and text that goes beyond the\ntraditional bag-of-words approaches; Secondly, we perform parsing and reasoning\nacross the spatial, temporal and causal dimensions based on the joint S/T/C-AOG\nrepresentation; Thirdly, we show that deep joint parsing facilitates subsequent\napplications such as generating narrative text descriptions and answering\nqueries in the forms of who, what, when, where and why. We empirically\nevaluated our system based on comparison against ground-truth as well as\naccuracy of query answering and obtained satisfactory results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 23:45:02 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2014 05:24:09 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Tu", "Kewei", ""], ["Meng", "Meng", ""], ["Lee", "Mun Wai", ""], ["Choe", "Tae Eun", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1308.6807", "submitter": "Joohwan Kim", "authors": "Joohwan Kim and R. Srikant", "title": "Achieving the Optimal Steaming Capacity and Delay Using Random Regular\n  Digraphs in P2P Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In earlier work, we showed that it is possible to achieve $O(\\log N)$\nstreaming delay with high probability in a peer-to-peer network, where each\npeer has as little as four neighbors, while achieving any arbitrary fraction of\nthe maximum possible streaming rate. However, the constant in the $O(log N)$\ndelay term becomes rather large as we get closer to the maximum streaming rate.\nIn this paper, we design an alternative pairing and chunk dissemination\nalgorithm that allows us to transmit at the maximum streaming rate while\nensuring that all, but a negligible fraction of the peers, receive the data\nstream with $O(\\log N)$ delay with high probability. The result is established\nby examining the properties of graph formed by the union of two or more random\n1-regular digraphs, i.e., directed graphs in which each node has an incoming\nand an outgoing node degree both equal to one.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 17:49:55 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Kim", "Joohwan", ""], ["Srikant", "R.", ""]]}]