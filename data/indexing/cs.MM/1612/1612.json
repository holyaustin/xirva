[{"id": "1612.00807", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, F. M. Bayer, V. A. Coutinho, S. Kulasekera, A.\n  Madanayake", "title": "Energy-efficient 8-point DCT Approximations: Theory and Hardware\n  Architectures", "comments": "21 pages, 7 figures, 5 tables", "journal-ref": "Circuits, Systems, and Signal Processing, November 2016, Volume\n  35, Issue 11, pp 4009-4029", "doi": "10.1007/s00034-015-0233-z", "report-no": null, "categories": "cs.MM cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its remarkable energy compaction properties, the discrete cosine\ntransform (DCT) is employed in a multitude of compression standards, such as\nJPEG and H.265/HEVC. Several low-complexity integer approximations for the DCT\nhave been proposed for both 1-D and 2-D signal analysis. The increasing demand\nfor low-complexity, energy efficient methods require algorithms with even lower\ncomputational costs. In this paper, new 8-point DCT approximations with very\nlow arithmetic complexity are presented. The new transforms are proposed based\non pruning state-of-the-art DCT approximations. The proposed algorithms were\nassessed in terms of arithmetic complexity, energy retention capability, and\nimage compression performance. In addition, a metric combining performance and\ncomputational complexity measures was proposed. Results showed good performance\nand extremely low computational complexity. Introduced algorithms were mapped\ninto systolic-array digital architectures and physically realized as digital\nprototype circuits using FPGA technology and mapped to 45nm CMOS technology.\nAll hardware-related metrics showed low resource consumption of the proposed\npruned approximate transforms. The best proposed transform according to the\nintroduced metric presents a reduction in power consumption of 21--25%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:47:28 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Coutinho", "V. A.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1612.01058", "submitter": "Margareta Ackerman", "authors": "Margareta Ackerman and David Loker", "title": "Algorithmic Songwriting with ALYSIA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.\nALYSIA is based on a machine learning model using Random Forests, and we\ndiscuss its success at pitch and rhythm prediction. Next, we show how ALYSIA\nwas used to create original pop songs that were subsequently recorded and\nproduced. Finally, we discuss our vision for the future of Automated\nSongwriting for both co-creative and autonomous systems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:36:51 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ackerman", "Margareta", ""], ["Loker", "David", ""]]}, {"id": "1612.01113", "submitter": "Seyed Hamid Safavi", "authors": "Seyed Hamid Safavi and Farah Torkamani-Azar", "title": "A novel Adaptive weighted Kronecker Compressive Sensing", "comments": "The 42nd IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP 2017), Ph.D. Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multidimensional signal reconstruction using a low number of\nmeasurements is of great interest. Therefore, an effective sampling scheme\nwhich should acquire the most information of signal using a low number of\nmeasurements is required. In this paper, we study a novel cube-based method for\nsampling and reconstruction of multidimensional signals. First, inspired by the\nblock-based compressive sensing (BCS), we divide a group of pictures (GoP) in a\nvideo sequence into cubes. By this way, we can easily store the measurement\nmatrix and also easily can generate the sparsifying basis. The reconstruction\nprocess also can be done in parallel. Second, along with the Kronecker\nstructure of the sampling matrix, we design a weight matrix based on the human\nvisuality system, i.e. perceptually. We will also benefit from different\nweighted $\\ell_1$-minimization methods for reconstruction. Furthermore,\nconventional methods for BCS consider an equal number of samples for all\nblocks. However, the sparsity order of blocks in natural images could be\ndifferent and, therefore, a various number of samples could be required for\ntheir reconstruction. Motivated by this point, we will adaptively allocate the\nsamples for each cube in a video sequence. Our aim is to show that our simple\nlinear sampling approach can be competitive with the other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 13:08:04 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 07:11:44 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Safavi", "Seyed Hamid", ""], ["Torkamani-Azar", "Farah", ""]]}, {"id": "1612.01657", "submitter": "Yang Yang", "authors": "Ruicong Xu, Yang Yang, Yadan Luo, Fumin Shen, Zi Huang, Heng Tao Shen", "title": "Binary Subspace Coding for Query-by-Image Video Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The query-by-image video retrieval (QBIVR) task has been attracting\nconsiderable research attention recently. However, most existing methods\nrepresent a video by either aggregating or projecting all its frames into a\nsingle datum point, which may easily cause severe information loss. In this\npaper, we propose an efficient QBIVR framework to enable an effective and\nefficient video search with image query. We first define a\nsimilarity-preserving distance metric between an image and its orthogonal\nprojection in the subspace of the video, which can be equivalently transformed\nto a Maximum Inner Product Search (MIPS) problem.\n  Besides, to boost the efficiency of solving the MIPS problem, we propose two\nasymmetric hashing schemes, which bridge the domain gap of images and videos.\nThe first approach, termed Inner-product Binary Coding (IBC), preserves the\ninner relationships of images and videos in a common Hamming space. To further\nimprove the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)\napproach, which employs compact bilinear projections instead of a single large\nprojection matrix. Extensive experiments have been conducted on four real-world\nvideo datasets to verify the effectiveness of our proposed approaches as\ncompared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 04:01:17 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Xu", "Ruicong", ""], ["Yang", "Yang", ""], ["Luo", "Yadan", ""], ["Shen", "Fumin", ""], ["Huang", "Zi", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1612.02198", "submitter": "Maarten Grachten", "authors": "Maarten Grachten, Carlos Eduardo Cancino-Chac\\'on, Thassilo\n  Gadermaier, Gerhard Widmer", "title": "Towards computer-assisted understanding of dynamics in symphonic music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people enjoy classical symphonic music. Its diverse instrumentation\nmakes for a rich listening experience. This diversity adds to the conductor's\nexpressive freedom to shape the sound according to their imagination. As a\nresult, the same piece may sound quite differently from one conductor to\nanother. Differences in interpretation may be noticeable subjectively to\nlisteners, but they are sometimes hard to pinpoint, presumably because of the\nacoustic complexity of the sound. We describe a computational model that\ninterprets dynamics---expressive loudness variations in performances---in terms\nof the musical score, highlighting differences between performances of the same\npiece. We demonstrate experimentally that the model has predictive power, and\ngive examples of conductor ideosyncrasies found by using the model as an\nexplanatory tool. Although the present model is still in active development, it\nmay pave the road for a consumer-oriented companion to interactive classical\nmusic understanding.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 11:18:21 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 11:14:30 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Grachten", "Maarten", ""], ["Cancino-Chac\u00f3n", "Carlos Eduardo", ""], ["Gadermaier", "Thassilo", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.03461", "submitter": "Renato J Cintra", "authors": "V. A. Coutinho, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "Low-complexity Pruned 8-point DCT Approximations for Image Encoding", "comments": "13 pages, 6 figures, 3 tables", "journal-ref": null, "doi": "10.1109/CONIELECOMP.2015.7086923", "report-no": null, "categories": "cs.MM cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two multiplierless pruned 8-point discrete cosine transform (DCT)\napproximation are presented. Both transforms present lower arithmetic\ncomplexity than state-of-the-art methods. The performance of such new methods\nwas assessed in the image compression context. A JPEG-like simulation was\nperformed, demonstrating the adequateness and competitiveness of the introduced\nmethods. Digital VLSI implementation in CMOS technology was also considered.\nBoth presented methods were realized in Berkeley Emulation Engine (BEE3).\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 19:41:44 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Coutinho", "V. A.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1612.04363", "submitter": "Anastasios Giovanidis", "authors": "Anastasios Giovanidis, Apostolos Avranas", "title": "Spatial multi-LRU: Distributed Caching for Wireless Networks with\n  Coverage Overlaps", "comments": "14 pages, double column, 5 figures, 15 sub-figures in total. arXiv\n  admin note: substantial text overlap with arXiv:1602.07623", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.MM cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a novel family of decentralised caching policies,\napplicable to wireless networks with finite storage at the edge-nodes\n(stations). These policies, that are based on the Least-Recently-Used\nreplacement principle, are here referred to as spatial multi-LRU. They update\ncache inventories in a way that provides content diversity to users who are\ncovered by, and thus have access to, more than one station. Two variations are\nproposed, the multi-LRU-One and -All, which differ in the number of replicas\ninserted in the involved caches. We analyse their performance under two types\nof traffic demand, the Independent Reference Model (IRM) and a model that\nexhibits temporal locality. For IRM, we propose a Che-like approximation to\npredict the hit probability, which gives very accurate results. Numerical\nevaluations show that the performance of multi-LRU increases the more the\nmulti-coverage areas increase, and it is close to the performance of\ncentralised policies, when multi-coverage is sufficient. For IRM traffic,\nmulti-LRU-One is preferable to multi-LRU-All, whereas when the traffic exhibits\ntemporal locality the -All variation can perform better. Both variations\noutperform the simple LRU. When popularity knowledge is not accurate, the new\npolicies can perform better than centralised ones.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:54:49 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Giovanidis", "Anastasios", ""], ["Avranas", "Apostolos", ""]]}, {"id": "1612.04688", "submitter": "Poorna Dasgupta", "authors": "Poorna Banerjee Dasgupta", "title": "Algorithmic Analysis of Invisible Video Watermarking using LSB Encoding\n  Over a Client-Server Framework", "comments": "4 pages, Published with International Journal of Computer Trends and\n  Technology (IJCTT), Volume-36 Number-3, June-2016", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V36(3):143-146, June 2016", "doi": "10.14445/22312803/IJCTT-V36P125", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video watermarking is extensively used in many media-oriented applications\nfor embedding watermarks, i.e. hidden digital data, in a video sequence to\nprotect the video from illegal copying and to identify manipulations made in\nthe video. In case of an invisible watermark, the human eye can not perceive\nany difference in the video, but a watermark extraction application can read\nthe watermark and obtain the embedded information. Although numerous\nmethodologies exist for embedding watermarks, many of them have shortcomings\nwith respect to performance efficiency, especially over a distributed network.\nThis paper proposes and analyses a 2-bit Least Significant Bit (LSB) parallel\nalgorithmic approach for achieving performance efficiency to watermark and\ndistribute videos over a client-server framework.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 12:39:53 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Dasgupta", "Poorna Banerjee", ""]]}, {"id": "1612.06753", "submitter": "Spencer Cappallo", "authors": "Spencer Cappallo, Thomas Mensink, Cees G. M. Snoek", "title": "Video Stream Retrieval of Unseen Queries using Semantic Memory", "comments": "Presented at BMVC 2016, British Machine Vision Conference, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval of live, user-broadcast video streams is an under-addressed and\nincreasingly relevant challenge. The on-line nature of the problem requires\ntemporal evaluation and the unforeseeable scope of potential queries motivates\nan approach which can accommodate arbitrary search queries. To account for the\nbreadth of possible queries, we adopt a no-example approach to query retrieval,\nwhich uses a query's semantic relatedness to pre-trained concept classifiers.\nTo adapt to shifting video content, we propose memory pooling and memory\nwelling methods that favor recent information over long past content. We\nidentify two stream retrieval tasks, instantaneous retrieval at any particular\ntime and continuous retrieval over a prolonged duration, and propose means for\nevaluating them. Three large scale video datasets are adapted to the challenge\nof stream retrieval. We report results for our search methods on the new stream\nretrieval tasks, as well as demonstrate their efficacy in a traditional,\nnon-streaming video task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:59:24 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Cappallo", "Spencer", ""], ["Mensink", "Thomas", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1612.07309", "submitter": "Li Li", "authors": "Li Li, Zhu Li, Bin Li, Dong Liu, and Houqiang Li", "title": "Pseudo Sequence based 2-D hierarchical reference structure for\n  Light-Field Image Compression", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2725198", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel pseudo sequence based 2-D hierarchical\nreference structure for light-field image compression. In the proposed scheme,\nwe first decompose the light-field image into multiple views and organize them\ninto a 2-D coding structure according to the spatial coordinates of the\ncorresponding microlens. Then we mainly develop three technologies to optimize\nthe 2-D coding structure. First, we divide all the views into four quadrants,\nand all the views are encoded one quadrant after another to reduce the\nreference buffer size as much as possible. Inside each quadrant, all the views\nare encoded hierarchically to fully exploit the correlations between different\nviews. Second, we propose to use the distance between the current view and its\nreference views as the criteria for selecting better reference frames for each\ninter view. Third, we propose to use the spatial relative positions between\ndifferent views to achieve more accurate motion vector scaling. The whole\nscheme is implemented in the reference software of High Efficiency Video\nCoding. The experimental results demonstrate that the proposed novel\npseudo-sequence based 2-D hierarchical structure can achieve maximum 14.2%\nbit-rate savings compared with the state-of-the-art light-field image\ncompression method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 20:31:57 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Li", "Li", ""], ["Li", "Zhu", ""], ["Li", "Bin", ""], ["Liu", "Dong", ""], ["Li", "Houqiang", ""]]}, {"id": "1612.07403", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Efficient Action Detection in Untrimmed Videos via Multi-Task Learning", "comments": "WACV 2017 camera ready, minor updates about test time efficiency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the joint learning of action recognition and temporal\nlocalization in long, untrimmed videos. We employ a multi-task learning\nframework that performs the three highly related steps of action proposal,\naction recognition, and action localization refinement in parallel instead of\nthe standard sequential pipeline that performs the steps in order. We develop a\nnovel temporal actionness regression module that estimates what proportion of a\nclip contains action. We use it for temporal localization but it could have\nother applications like video retrieval, surveillance, summarization, etc. We\nalso introduce random shear augmentation during training to simulate viewpoint\nchange. We evaluate our framework on three popular video benchmarks. Results\ndemonstrate that our joint model is efficient in terms of storage and\ncomputation in that we do not need to compute and cache dense trajectory\nfeatures, and that it is several times faster than its sequential ConvNets\ncounterpart. Yet, despite being more efficient, it outperforms state-of-the-art\nmethods with respect to accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 00:37:42 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 17:49:19 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1612.07872", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Gene Cheung, Patrick Le Callet, Pascal Frossard and Hong\n  Vicky Zhao", "title": "Object Shape Approximation & Contour Adaptive Depth Image Coding for\n  Virtual View Synthesis", "comments": "13 pages, submitted to IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A depth image provides partial geometric information of a 3D scene, namely\nthe shapes of physical objects as observed from a particular viewpoint. This\ninformation is important when synthesizing images of different virtual camera\nviewpoints via depth-image-based rendering (DIBR). It has been shown that depth\nimages can be efficiently coded using contour-adaptive codecs that preserve\nedge sharpness, resulting in visually pleasing DIBR-synthesized images.\nHowever, contours are typically losslessly coded as side information (SI),\nwhich is expensive if the object shapes are complex.\n  In this paper, we pursue a new paradigm in depth image coding for\ncolor-plus-depth representation of a 3D scene: we pro-actively simplify object\nshapes in a depth and color image pair to reduce depth coding cost, at a\npenalty of a slight increase in synthesized view distortion. Specifically, we\nfirst mathematically derive a distortion upper-bound proxy for 3DSwIM---a\nquality metric tailored for DIBR-synthesized images. This proxy reduces\ninterdependency among pixel rows in a block to ease optimization. We then\napproximate object contours via a dynamic programming (DP) algorithm to\noptimally trade off coding cost of contours using arithmetic edge coding (AEC)\nwith our proposed view synthesis distortion proxy. We modify the depth and\ncolor images according to the approximated object contours in an inter-view\nconsistent manner. These are then coded respectively using a contour-adaptive\nimage codec based on graph Fourier transform (GFT) for edge preservation and\nHEVC intra. Experimental results show that by maintaining sharp but simplified\nobject contours during contour-adaptive coding, for the same visual quality of\nDIBR-synthesized virtual views, our proposal can reduce depth image coding rate\nby up to 22% compared to alternative coding strategies such as HEVC intra.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 04:32:33 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Yuan", "Yuan", ""], ["Cheung", "Gene", ""], ["Callet", "Patrick Le", ""], ["Frossard", "Pascal", ""], ["Zhao", "Hong Vicky", ""]]}, {"id": "1612.07893", "submitter": "Lee Prangnell", "authors": "Lee Prangnell, Miguel Hern\\'andez-Cabronero and Victor Sanchez", "title": "Cross-Color Channel Perceptually Adaptive Quantization for HEVC", "comments": "Data Compression Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HEVC includes a Coding Unit (CU) level luminance-based perceptual\nquantization technique known as AdaptiveQP. AdaptiveQP perceptually adjusts the\nQuantization Parameter (QP) at the CU level based on the spatial activity of\nraw input video data in a luma Coding Block (CB). In this paper, we propose a\nnovel cross-color channel adaptive quantization scheme which perceptually\nadjusts the CU level QP according to the spatial activity of raw input video\ndata in the constituent luma and chroma CBs; i.e., the combined spatial\nactivity across all three color channels (the Y, Cb and Cr channels). Our\ntechnique is evaluated in HM 16 with 4:4:4, 4:2:2 and 4:2:0 YCbCr JCT-VC test\nsequences. Both subjective and objective visual quality evaluations are\nundertaken during which we compare our method with AdaptiveQP. Our technique\nachieves considerable coding efficiency improvements, with maximum BD-Rate\nreductions of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a maximum\ndecoding time reduction of 11.0%.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 07:57:34 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 13:59:41 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 16:14:50 GMT"}, {"version": "v4", "created": "Mon, 12 Feb 2018 18:49:24 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Prangnell", "Lee", ""], ["Hern\u00e1ndez-Cabronero", "Miguel", ""], ["Sanchez", "Victor", ""]]}, {"id": "1612.08350", "submitter": "Tarek El-Ganainy", "authors": "Tarek El-Ganainy, Mohamed Hefeeda", "title": "Streaming Virtual Reality Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent rise of interest in Virtual Reality (VR) came with the\navailability of commodity commercial VR prod- ucts, such as the Head Mounted\nDisplays (HMD) created by Oculus and other vendors. To accelerate the user\nadoption of VR headsets, content providers should focus on producing high\nquality immersive content for these devices. Similarly, multimedia streaming\nservice providers should enable the means to stream 360 VR content on their\nplatforms. In this study, we try to cover different aspects related to VR\ncontent representation, streaming, and quality assessment that will help\nestablishing the basic knowledge of how to build a VR streaming system.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 09:40:45 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["El-Ganainy", "Tarek", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "1612.08712", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo and\n  James Storer", "title": "Semantic Perceptual Image Compression using Deep Convolution Networks", "comments": "Accepted to Data Compression Conference, 11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been considered a significant problem to improve the visual\nquality of lossy image and video compression. Recent advances in computing\npower together with the availability of large training data sets has increased\ninterest in the application of deep learning cnns to address image recognition\nand image processing tasks. Here, we present a powerful cnn tailored to the\nspecific task of semantic image understanding to achieve higher visual quality\nin lossy compression. A modest increase in complexity is incorporated to the\nencoder which allows a standard, off-the-shelf jpeg decoder to be used. While\njpeg encoding may be optimized for generic images, the process is ultimately\nunaware of the specific content of the image to be compressed. Our technique\nmakes jpeg content-aware by designing and training a model to identify multiple\nsemantic regions in a given image. Unlike object detection techniques, our\nmodel does not require labeling of object positions and is able to identify\nobjects in a single pass. We present a new cnn architecture directed\nspecifically to image compression, which generates a map that highlights\nsemantically-salient regions so that they can be encoded at higher quality as\ncompared to background regions. By adding a complete set of features for every\nclass, and then taking a threshold over the sum of all feature activations, we\ngenerate a map that highlights semantically-salient regions so that they can be\nencoded at a better quality compared to background regions. Experiments are\npresented on the Kodak PhotoCD dataset and the MIT Saliency Benchmark dataset,\nin which our algorithm achieves higher visual quality for the same compressed\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 19:21:18 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 16:29:54 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Prakash", "Aaditya", ""], ["Moran", "Nick", ""], ["Garber", "Solomon", ""], ["DiLillo", "Antonella", ""], ["Storer", "James", ""]]}, {"id": "1612.08727", "submitter": "Bochen Li", "authors": "Bochen Li, Xinzhao Liu, Karthik Dinesh, Zhiyao Duan, Gaurav Sharma", "title": "Creating A Multi-track Classical Musical Performance Dataset for\n  Multimodal Music Analysis: Challenges, Insights, and Applications", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TMM.2018.2856090", "report-no": null, "categories": "cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dataset for facilitating audio-visual analysis of music\nperformances. The dataset comprises 44 simple multi-instrument classical music\npieces assembled from coordinated but separately recorded performances of\nindividual tracks. For each piece, we provide the musical score in MIDI format,\nthe audio recordings of the individual tracks, the audio and video recording of\nthe assembled mixture, and ground-truth annotation files including frame-level\nand note-level transcriptions. We describe our methodology for the creation of\nthe dataset, particularly highlighting our approaches for addressing the\nchallenges involved in maintaining synchronization and expressiveness. We\ndemonstrate the high quality of synchronization achieved with our proposed\napproach by comparing the dataset with existing widely-used music audio\ndatasets.\n  We anticipate that the dataset will be useful for the development and\nevaluation of existing music information retrieval (MIR) tasks, as well as for\nnovel multi-modal tasks. We benchmark two existing MIR tasks (multi-pitch\nanalysis and score-informed source separation) on the dataset and compare with\nother existing music audio datasets. Additionally, we consider two novel\nmulti-modal MIR tasks (visually informed multi-pitch analysis and polyphonic\nvibrato analysis) enabled by the dataset and provide evaluation measures and\nbaseline systems for future comparisons (from our recent work). Finally, we\npropose several emerging research directions that the dataset enables.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 20:27:24 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 20:40:20 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 00:59:35 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Li", "Bochen", ""], ["Liu", "Xinzhao", ""], ["Dinesh", "Karthik", ""], ["Duan", "Zhiyao", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1612.08882", "submitter": "Raphael Couturier", "authors": "Jean-Francois Couchot and Rapha\\\"el Couturier and Michel Salomon", "title": "Improving Blind Steganalysis in Spatial Domain using a Criterion to\n  Choose the Appropriate Steganalyzer between CNN and SRM+EC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional state-of-the-art image steganalysis approaches usually consist\nof a classifier trained with features provided by rich image models. As both\nfeatures extraction and classification steps are perfectly embodied in the deep\nlearning architecture called Convolutional Neural Network (CNN), different\nstudies have tried to design a CNN-based steganalyzer. The network designed by\nXu et al. is the first competitive CNN with the combination Spatial Rich Models\n(SRM) and Ensemble Classifier (EC) providing detection performances of the same\norder. In this work we propose a criterion to choose either the CNN or the\nSRM+EC method for a given input image. Our approach is studied with three\ndifferent steganographic spatial domain algorithms: S-UNIWARD, MiPOD, and HILL,\nusing the Tensorflow computing platform, and exhibits detection capabilities\nbetter than each method alone. Furthermore, as SRM+EC and the CNN are both only\ntrained with a single embedding algorithm, namely MiPOD, the proposed method\ncan be seen as an approach for blind steganalysis. In blind detection, error\nrates are respectively of 16% for S-UNIWARD, 16% for MiPOD, and 17% for HILL on\nthe BOSSBase with a payload of 0.4 bpp. For 0.1 bpp, the respective\ncorresponding error rates are of 39%, 38%, and 41%, and are always better than\nthe ones provided by SRM+EC.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 13:44:19 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 17:28:27 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Couchot", "Jean-Francois", ""], ["Couturier", "Rapha\u00ebl", ""], ["Salomon", "Michel", ""]]}]