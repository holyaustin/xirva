[{"id": "1703.00190", "submitter": "Krzysztof Wegner", "authors": "Krzysztof Wegner, Tomasz Grajek, Jakub Stankowski, Marek Domanski", "title": "Video transrating in AVC and HEVC transcoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HEVC (MPEG-H Part 2 and H.265) is a new coding technology which is expected\nto be deployed on the market along with new video services in the near future.\nHEVC is a successor of currently widely used AVC (MPEG-4 Part 10 and H.264). In\nthis paper, the quality coding gains obtained for the Cascaded Pixel Domain\nTranscoder of AVC-coded material to HEVC standard are reported. Extensive\nexperiments showed that transcoding with bitrate reduction allows the\nachievement of better rate-distortion performance than by compressing an\noriginal video sequence with the use of AVC at the same (reduced) bitrate.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 09:22:47 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Wegner", "Krzysztof", ""], ["Grajek", "Tomasz", ""], ["Stankowski", "Jakub", ""], ["Domanski", "Marek", ""]]}, {"id": "1703.00304", "submitter": "Angelos Valsamis", "authors": "Angelos Valsamis, Alexandros Psychas, Fotis Aisopos, Andreas Menychtas\n  and Theodora Varvarigou", "title": "Second Screen User Profiling and Multi-level Smart Recommendations in\n  the context of Social TVs", "comments": "In: Wu TT., Gennari R., Huang YM., Xie H., Cao Y. (eds) Emerging\n  Technologies for Education. SETE 2016", "journal-ref": "Lecture Notes in Computer Science, vol 10108. Springer, Cham,\n  2017, pp 514-525", "doi": "10.1007/978-3-319-52836-6_55", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Social TV, the increasing popularity of first and second\nscreen users, interacting and posting content online, illustrates new business\nopportunities and related technical challenges, in order to enrich user\nexperience on such environments. SAM (Socializing Around Media) project uses\nSocial Media-connected infrastructure to deal with the aforementioned\nchallenges, providing intelligent user context management models and mechanisms\ncapturing social patterns, to apply collaborative filtering techniques and\npersonalized recommendations towards this direction. This paper presents the\nContext Management mechanism of SAM, running in a Social TV environment to\nprovide smart recommendations for first and second screen content. Work\npresented is evaluated using real movie rating dataset found online, to\nvalidate the SAM's approach in terms of effectiveness as well as efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 14:06:44 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Valsamis", "Angelos", ""], ["Psychas", "Alexandros", ""], ["Aisopos", "Fotis", ""], ["Menychtas", "Andreas", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1703.00371", "submitter": "Jamie Hayes", "authors": "Jamie Hayes and George Danezis", "title": "Generating Steganographic Images via Adversarial Training", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training was recently shown to be competitive against supervised\nlearning methods on computer vision tasks, however, studies have mainly been\nconfined to generative tasks such as image synthesis. In this paper, we apply\nadversarial training techniques to the discriminative task of learning a\nsteganographic algorithm. Steganography is a collection of techniques for\nconcealing information by embedding it within a non-secret medium, such as\ncover texts or images. We show that adversarial training can produce robust\nsteganographic techniques: our unsupervised training scheme produces a\nsteganographic algorithm that competes with state-of-the-art steganographic\ntechniques, and produces a robust steganalyzer, which performs the\ndiscriminative task of deciding if an image contains secret information. We\ndefine a game between three parties, Alice, Bob and Eve, in order to\nsimultaneously train both a steganographic algorithm and a steganalyzer. Alice\nand Bob attempt to communicate a secret message contained within an image,\nwhile Eve eavesdrops on their conversation and attempts to determine if secret\ninformation is embedded within the image. We represent Alice, Bob and Eve by\nneural networks, and validate our scheme on two independent image datasets,\nshowing our novel method of studying steganographic problems is surprisingly\ncompetitive against established steganographic techniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 16:34:59 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 10:58:27 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 13:15:16 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hayes", "Jamie", ""], ["Danezis", "George", ""]]}, {"id": "1703.00383", "submitter": "Andjela Draganic", "authors": "Andjela Draganic, Milan Maric, Irena Orovic, Srdjan Stankovic", "title": "Identification of image source using serialnumber-based watermarking\n  under Compressive Sensing conditions", "comments": "submitted to MIPRO 2017 conference, Opatija, Croatia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the protection of ownership and the prevention of unauthorized\nmanipulation of digital images becomes an important concern, there is also a\nbig issue of image source origin authentication. This paper proposes a\nprocedure for the identification of the image source and content by using the\nPublic Key Cryptography Signature (PKCS). The procedure is based on the PKCS\nwatermarking of the images captured with numerous automatic observing cameras\nin the Trap View cloud system. Watermark is created based on 32-bit PKCS serial\nnumber and embedded into the captured image. Watermark detection on the\nreceiver side extracts the serial number and indicates the camera which\ncaptured the image by comparing the original and the extracted serial numbers.\nThe watermarking procedure is designed to provide robustness to image\noptimization based on the Compressive Sensing approach. Also, the procedure is\ntested under various attacks and shows successful identification of ownership.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 16:52:16 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Draganic", "Andjela", ""], ["Maric", "Milan", ""], ["Orovic", "Irena", ""], ["Stankovic", "Srdjan", ""]]}, {"id": "1703.00633", "submitter": "Christos Bampis", "authors": "Christos G. Bampis and Alan C. Bovik", "title": "Learning to Predict Streaming Video QoE: Distortions, Rebuffering and\n  Memory", "comments": "under review in Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile streaming video data accounts for a large and increasing percentage of\nwireless network traffic. The available bandwidths of modern wireless networks\nare often unstable, leading to difficulties in delivering smooth, high-quality\nvideo. Streaming service providers such as Netflix and YouTube attempt to adapt\ntheir systems to adjust in response to these bandwidth limitations by changing\nthe video bitrate or, failing that, allowing playback interruptions\n(rebuffering). Being able to predict end user' quality of experience (QoE)\nresulting from these adjustments could lead to perceptually-driven network\nresource allocation strategies that would deliver streaming content of higher\nquality to clients, while being cost effective for providers. Existing\nobjective QoE models only consider the effects on user QoE of video quality\nchanges or playback interruptions. For streaming applications, adaptive network\nstrategies may involve a combination of dynamic bitrate allocation along with\nplayback interruptions when the available bandwidth reaches a very low value.\nTowards effectively predicting user QoE, we propose Video Assessment of\nTemporaL Artifacts and Stalls (Video ATLAS): a machine learning framework where\nwe combine a number of QoE-related features, including objective quality\nfeatures, rebuffering-aware features and memory-driven features to make QoE\npredictions. We evaluated our learning-based QoE prediction model on the\nrecently designed LIVE-Netflix Video QoE Database which consists of practical\nplayout patterns, where the videos are afflicted by both quality changes and\nrebuffering events, and found that it provides improved performance over\nstate-of-the-art video quality metrics while generalizing well on different\ndatasets. The proposed algorithm is made publicly available at\nhttp://live.ece.utexas.edu/research/Quality/VideoATLAS release_v2.rar.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 05:45:26 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Bampis", "Christos G.", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1703.00796", "submitter": "Daniel Lerch Hostalot", "authors": "Daniel Lerch-Hostalot and David Meg\\'ias", "title": "Unsupervised Steganalysis Based on Artificial Training Sets", "comments": null, "journal-ref": null, "doi": "10.1016/j.engappai.2015.12.013", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an unsupervised steganalysis method that combines artificial\ntraining setsand supervised classification is proposed. We provide a formal\nframework for unsupervisedclassification of stego and cover images in the\ntypical situation of targeted steganalysis (i.e.,for a known algorithm and\napproximate embedding bit rate). We also present a completeset of experiments\nusing 1) eight different image databases, 2) image features based on\nRichModels, and 3) three different embedding algorithms: Least Significant Bit\n(LSB) matching,Highly undetectable steganography (HUGO) and Wavelet Obtained\nWeights (WOW). Weshow that the experimental results outperform previous methods\nbased on Rich Models inthe majority of the tested cases. At the same time, the\nproposed approach bypasses theproblem of Cover Source Mismatch -when the\nembedding algorithm and bit rate are known-, since it removes the need of a\ntraining database when we have a large enough testing set.Furthermore, we\nprovide a generic proof of the proposed framework in the machine\nlearningcontext. Hence, the results of this paper could be extended to other\nclassification problemssimilar to steganalysis.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 14:25:13 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Lerch-Hostalot", "Daniel", ""], ["Meg\u00edas", "David", ""]]}, {"id": "1703.00817", "submitter": "Daniel Lerch Hostalot PhD", "authors": "Daniel Lerch-Hostalot and David Meg\\'ias", "title": "LSB Matching Steganalysis Based on Patterns of Pixel Differences and\n  Random Embedding", "comments": null, "journal-ref": null, "doi": "10.1016/j.cose.2012.11.005.", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for detection of LSB matching steganogra-\nphy in grayscale images. This method is based on the analysis of the\ndifferences between neighboring pixels before and after random data embedding.\nIn natu- ral images, there is a strong correlation between adjacent pixels.\nThis correla- tion is disturbed by LSB matching generating new types of\ncorrelations. The pre- sented method generates patterns from these correlations\nand analyzes their varia- tion when random data are hidden. The experiments\nperformed for two different image databases show that the method yields better\nclassification accuracy com- pared to prior art for both LSB matching and HUGO\nsteganography. In addition, although the method is designed for the spatial\ndomain, some experiments show its applicability also for detecting JPEG\nsteganography.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 15:01:03 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Lerch-Hostalot", "Daniel", ""], ["Meg\u00edas", "David", ""]]}, {"id": "1703.00919", "submitter": "Krzysztof Wegner", "authors": "Krzysztof Wegner, Olgierd Stankiewicz, Marek Domanski", "title": "Depth Estimation using Modified Cost Function for Occlusion Handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel approach to occlusion handling problem in depth\nestimation using three views. A solution based on modification of similarity\ncost function is proposed. During the depth estimation via optimization\nalgorithms like Graph Cut similarity metric is constantly updated so that only\nnon-occluded fragments in side views are considered. At each iteration of the\nalgorithm non-occluded fragments are detected based on side view virtual depth\nmaps synthesized from the best currently estimated depth map of the center\nview. Then similarity metric is updated for correspondence search only in\nnon-occluded regions of the side views. The experimental results, conducted on\nwell-known 3D video test sequences, have proved that the depth maps estimated\nwith the proposed approach provide about 1.25 dB virtual view quality\nimprovement in comparison to the virtual view synthesized based on depth maps\ngenerated by the state-of-the-art MPEG Depth Estimation Reference Software.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 19:05:05 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 12:15:20 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Wegner", "Krzysztof", ""], ["Stankiewicz", "Olgierd", ""], ["Domanski", "Marek", ""]]}, {"id": "1703.01170", "submitter": "Huang-Chia Shih", "authors": "Huang-Chia Shih", "title": "A Survey on Content-Aware Video Analysis for Sports", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2655624", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports data analysis is becoming increasingly large-scale, diversified, and\nshared, but difficulty persists in rapidly accessing the most crucial\ninformation. Previous surveys have focused on the methodologies of sports video\nanalysis from the spatiotemporal viewpoint instead of a content-based\nviewpoint, and few of these studies have considered semantics. This study\ndevelops a deeper interpretation of content-aware sports video analysis by\nexamining the insight offered by research into the structure of content under\ndifferent scenarios. On the basis of this insight, we provide an overview of\nthe themes particularly relevant to the research on content-aware systems for\nbroadcast sports. Specifically, we focus on the video content analysis\ntechniques applied in sportscasts over the past decade from the perspectives of\nfundamentals and general review, a content hierarchical model, and trends and\nchallenges. Content-aware analysis methods are discussed with respect to\nobject-, event-, and context-oriented groups. In each group, the gap between\nsensation and content excitement must be bridged using proper strategies. In\nthis regard, a content-aware approach is required to determine user demands.\nFinally, the paper summarizes the future trends and challenges for sports video\nanalysis. We believe that our findings can advance the field of research on\ncontent-aware video analysis for broadcast sports.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 14:28:03 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Shih", "Huang-Chia", ""]]}, {"id": "1703.01331", "submitter": "Radu Arsinte", "authors": "Radu Arsinte", "title": "On The Automated Planning And Design Of SMATV Systems", "comments": "4 pages, 5 figures, Acta Technica Napocensis, Electronics and\n  Telecommunications, 2014, No.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents some theoretical and practical considerations regarding\nthe TV information distribution in local (small and medium) networks, using\ndifferent technologies and architectures. The SMATV concept is chosen to be\npresented extensively. The most important design formulae are presented with a\nsoftware package supporting the network planner to design and optimize the\nnetwork. A case study is realized, using standard components in SMATV, for a 5\nfloor building. The study proved that it is possible to design and optimize the\nentire network, without realizing first a costly experimental setup. It is also\npossible to run different architectures, optimizing also the costs of the final\nsolution of network.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 13:18:28 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Arsinte", "Radu", ""]]}, {"id": "1703.01789", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim, Juhan Nam", "title": "Sample-level Deep Convolutional Neural Networks for Music Auto-tagging\n  Using Raw Waveforms", "comments": "7 pages, Sound and Music Computing Conference (SMC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the end-to-end approach that learns hierarchical representations\nfrom raw data using deep convolutional neural networks has been successfully\nexplored in the image, text and speech domains. This approach was applied to\nmusical signals as well but has been not fully explored yet. To this end, we\npropose sample-level deep convolutional neural networks which learn\nrepresentations from very small grains of waveforms (e.g. 2 or 3 samples)\nbeyond typical frame-level input representations. Our experiments show how deep\narchitectures with sample-level filters improve the accuracy in music\nauto-tagging and they provide results comparable to previous state-of-the-art\nperformances for the Magnatagatune dataset and Million Song Dataset. In\naddition, we visualize filters learned in a sample-level DCNN in each layer to\nidentify hierarchically learned features and show that they are sensitive to\nlog-scaled frequency along layer, such as mel-frequency spectrogram that is\nwidely used in music classification systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:49:48 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 04:46:36 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Lee", "Jongpil", ""], ["Park", "Jiyoung", ""], ["Kim", "Keunhyoung Luke", ""], ["Nam", "Juhan", ""]]}, {"id": "1703.01793", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Juhan Nam", "title": "Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained\n  Convolutional Neural Networks for Music Auto-tagging", "comments": "5 pages, 5 figures, 2 tables", "journal-ref": null, "doi": "10.1109/LSP.2017.2713830", "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music auto-tagging is often handled in a similar manner to image\nclassification by regarding the 2D audio spectrogram as image data. However,\nmusic auto-tagging is distinguished from image classification in that the tags\nare highly diverse and have different levels of abstractions. Considering this\nissue, we propose a convolutional neural networks (CNN)-based architecture that\nembraces multi-level and multi-scaled features. The architecture is trained in\nthree steps. First, we conduct supervised feature learning to capture local\naudio features using a set of CNNs with different input sizes. Second, we\nextract audio features from each layer of the pre-trained convolutional\nnetworks separately and aggregate them altogether given a long audio clip.\nFinally, we put them into fully-connected networks and make final predictions\nof the tags. Our experiments show that using the combination of multi-level and\nmulti-scale features is highly effective in music auto-tagging and the proposed\nmethod outperforms previous state-of-the-arts on the MagnaTagATune dataset and\nthe Million Song Dataset. We further show that the proposed architecture is\nuseful in transfer learning.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:57:25 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 17:21:04 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Lee", "Jongpil", ""], ["Nam", "Juhan", ""]]}, {"id": "1703.01820", "submitter": "Amna Qureshi", "authors": "Amna Qureshi, David Meg\\'ias, Helena Rif\\`a-Pous", "title": "PSUM:Peer-to-peer multimedia content distribution using\n  collusion-resistant fingerprinting", "comments": "38 Pages, Journal", "journal-ref": null, "doi": "10.1016/j.jnca.2016.03.007", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of peer-to-peer (P2P) networks for multimedia distribution has spread\nout globally in recent years. The mass popularity is primarily driven by\ncost-effective distribution of content, also giving rise to piracy. An end user\n(buyer/peer) of a P2P content distribution system does not want to reveal\nhis/her identity during a transaction with a content owner (merchant), whereas\nthe merchant does not want the buyer to further distribute the content\nillegally. To date, different P2P distribution systems have been proposed that\nprovide copyright and privacy protection at a cost of high computational burden\nat the merchants and/or at the buyer's end and thus, making these systems\nimpractical. In this paper, we propose PSUM, a P2P content distribution system\nwhich allows efficient distribution of large-sized multimedia content while\npreserving the security and privacy of merchants and buyers. The security of\nPSUM is ensured by using an asymmetric fingerprinting protocol based on\ncollusion-resistant codes. In addition, PSUM enables buyers to obtain digital\ncontents anonymously, but this anonymity can be revoked as soon as he/she is\nfound guilty of copyright violation. The paper presents a thorough performance\nanalysis of PSUM, through different experiments and simulations, and also\nanalyzes several security compromising attacks and countermeasures.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 11:47:30 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Qureshi", "Amna", ""], ["Meg\u00edas", "David", ""], ["Rif\u00e0-Pous", "Helena", ""]]}, {"id": "1703.01986", "submitter": "Imen Triki", "authors": "Imen Triki, Quanyan Zhu, Rachid Elazouzi, Majed Haddad, Zhiheng Xu", "title": "Learning from Experience: A Dynamic Closed-Loop QoE Optimization for\n  Video Adaptation and Delivery", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of experience (QoE) is known to be subjective and\ncontext-dependent. Identifying and calculating the factors that affect QoE is\nindeed a difficult task. Recently, a lot of effort has been devoted to estimate\nthe users QoE in order to improve video delivery. In the literature, most of\nthe QoE-driven optimization schemes that realize trade-offs among different\nquality metrics have been addressed under the assumption of homogenous\npopulations. Nevertheless, people perceptions on a given video quality may not\nbe the same, which makes the QoE optimization harder. This paper aims at taking\na step further in order to address this limitation and meet users profiles. To\ndo so, we propose a closed-loop control framework based on the\nusers(subjective) feedbacks to learn the QoE function and optimize it at the\nsame time. Our simulation results show that our system converges to a steady\nstate, where the resulting QoE function noticeably improves the users\nfeedbacks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 17:21:31 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 14:15:08 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 16:47:34 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Triki", "Imen", ""], ["Zhu", "Quanyan", ""], ["Elazouzi", "Rachid", ""], ["Haddad", "Majed", ""], ["Xu", "Zhiheng", ""]]}, {"id": "1703.02437", "submitter": "Santiago Manen", "authors": "Santiago Manen, Michael Gygli, Dengxin Dai, Luc Van Gool", "title": "PathTrack: Fast Trajectory Annotation with Path Supervision", "comments": "10 pages, ICCV submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in Multiple Object Tracking (MOT) has been historically limited by\nthe size of the available datasets. We present an efficient framework to\nannotate trajectories and use it to produce a MOT dataset of unprecedented\nsize. In our novel path supervision the annotator loosely follows the object\nwith the cursor while watching the video, providing a path annotation for each\nobject in the sequence. Our approach is able to turn such weak annotations into\ndense box trajectories. Our experiments on existing datasets prove that our\nframework produces more accurate annotations than the state of the art, in a\nfraction of the time. We further validate our approach by crowdsourcing the\nPathTrack dataset, with more than 15,000 person trajectories in 720 sequences.\nTracking approaches can benefit training on such large-scale datasets, as did\nobject recognition. We prove this by re-training an off-the-shelf person\nmatching network, originally trained on the MOT15 dataset, almost halving the\nmisclassification rate. Additionally, training on our data consistently\nimproves tracking results, both on our dataset and on MOT15. On the latter, we\nimprove the top-performing tracker (NOMT) dropping the number of IDSwitches by\n18% and fragments by 5%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:36:39 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 07:08:34 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Manen", "Santiago", ""], ["Gygli", "Michael", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1703.03214", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura, Ozgur Ercetin, Hazer Inaltekin", "title": "Optimal Network-Assisted Multi-user DASH Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming video is becoming the predominant type of traffic over the Internet\nwith reports forecasting the video content to account for 80% of all traffic by\n2019. With significant investment on Internet backbone, the main bottleneck\nremains at the edge servers (e.g., WiFi access points, small cells, etc.). In\nthis work, we propose and prove the optimality of a multiuser resource\nallocation mechanism operating at the edge server that minimizes the\nprobability of stalling of video streams due to buffer under-flows. Our\nproposed policy utilizes Media Presentation Description (MPD) files of clients\nthat are sent in compliant to Dynamic Adaptive Streaming over HTTP (DASH)\nprotocol to be cognizant of the deadlines of each of the media file to be\ndisplayed by the clients. Then, the policy schedules the users in the order of\ntheir deadlines. After establishing the optimality of this policy to minimize\nthe stalling probability for a network with links associated with fixed loss\nrates, the utility of the algorithm is verified under realistic network\nconditions with detailed NS-3 simulations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 10:23:18 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 09:59:33 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ercetin", "Ozgur", ""], ["Inaltekin", "Hazer", ""]]}, {"id": "1703.03502", "submitter": "Dong Liu", "authors": "Ning Yan, Dong Liu, Houqiang Li, Feng Wu", "title": "A Convolutional Neural Network Approach for Half-Pel Interpolation in\n  Video Coding", "comments": "International Symposium on Circuits and Systems (ISCAS) 2017", "journal-ref": null, "doi": "10.1109/ISCAS.2017.8050458", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion compensation is a fundamental technology in video coding to remove the\ntemporal redundancy between video frames. To further improve the coding\nefficiency, sub-pel motion compensation has been utilized, which requires\ninterpolation of fractional samples. The video coding standards usually adopt\nfixed interpolation filters that are derived from the signal processing theory.\nHowever, as video signal is not stationary, the fixed interpolation filters may\nturn out less efficient. Inspired by the great success of convolutional neural\nnetwork (CNN) in computer vision, we propose to design a CNN-based\ninterpolation filter (CNNIF) for video coding. Different from previous studies,\none difficulty for training CNNIF is the lack of ground-truth since the\nfractional samples are actually not available. Our solution for this problem is\nto derive the \"ground-truth\" of fractional samples by smoothing high-resolution\nimages, which is verified to be effective by the conducted experiments.\nCompared to the fixed half-pel interpolation filter for luma in High Efficiency\nVideo Coding (HEVC), our proposed CNNIF achieves up to 3.2% and on average 0.9%\nBD-rate reduction under low-delay P configuration.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 01:19:47 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Yan", "Ning", ""], ["Liu", "Dong", ""], ["Li", "Houqiang", ""], ["Wu", "Feng", ""]]}, {"id": "1703.03530", "submitter": "Wen Hu", "authors": "Wen Hu, Yichao Jin, Yonggang Wen, Zhi Wang, Lifeng Sun", "title": "Towards Wi-Fi AP-Assisted Content Prefetching for On-Demand TV Series: A\n  Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of smart Wi-Fi APs (Access Point), which are equipped with huge\nstorage space, opens a new research area on how to utilize these resources at\nthe edge network to improve users' quality of experience (QoE) (e.g., a short\nstartup delay and smooth playback). One important research interest in this\narea is content prefetching, which predicts and accurately fetches contents\nahead of users' requests to shift the traffic away during peak periods.\nHowever, in practice, the different video watching patterns among users, and\nthe varying network connection status lead to the time-varying server load,\nwhich eventually makes the content prefetching problem challenging. To\nunderstand this challenge, this paper first performs a large-scale measurement\nstudy on users' AP connection and TV series watching patterns using\nreal-traces. Then, based on the obtained insights, we formulate the content\nprefetching problem as a Markov Decision Process (MDP). The objective is to\nstrike a balance between the increased prefetching&storage cost incurred by\nincorrect prediction and the reduced content download delay because of\nsuccessful prediction. A learning-based approach is proposed to solve this\nproblem and another three algorithms are adopted as baselines. In particular,\nfirst, we investigate the performance lower bound by using a random algorithm,\nand the upper bound by using an ideal offline approach. Then, we present a\nheuristic algorithm as another baseline. Finally, we design a reinforcement\nlearning algorithm that is more practical to work in the online manner. Through\nextensive trace-based experiments, we demonstrate the performance gain of our\ndesign. Remarkably, our learning-based algorithm achieves a better precision\nand hit ratio (e.g., 80%) with about 70% (resp. 50%) cost saving compared to\nthe random (resp. heuristic) algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 03:17:17 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Hu", "Wen", ""], ["Jin", "Yichao", ""], ["Wen", "Yonggang", ""], ["Wang", "Zhi", ""], ["Sun", "Lifeng", ""]]}, {"id": "1703.04574", "submitter": "Miles Hansard", "authors": "Kasim Terzic and Miles Hansard", "title": "Causes of discomfort in stereoscopic content: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the causes of discomfort in viewing stereoscopic content.\nThese include objective factors, such as misaligned images, as well as\nsubjective factors, such as excessive disparity. Different approaches to the\nmeasurement of visual discomfort are also reviewed, in relation to the\nunderlying physiological and psychophysical processes. The importance of\nunderstanding these issues, in the context of new display technologies, is\nemphasized.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 14:07:19 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Terzic", "Kasim", ""], ["Hansard", "Miles", ""]]}, {"id": "1703.05451", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Fumin Shen, Xiansheng Hua, Wankou Yang and\n  Zhenmin Tang", "title": "Refining Image Categorization by Exploiting Web Images and General\n  Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies show that refining real-world categories into semantic subcategories\ncontributes to better image modeling and classification. Previous image\nsub-categorization work relying on labeled images and WordNet's hierarchy is\nnot only labor-intensive, but also restricted to classify images into NOUN\nsubcategories. To tackle these problems, in this work, we exploit general\ncorpus information to automatically select and subsequently classify web images\ninto semantic rich (sub-)categories. The following two major challenges are\nwell studied: 1) noise in the labels of subcategories derived from the general\ncorpus; 2) noise in the labels of images retrieved from the web. Specifically,\nwe first obtain the semantic refinement subcategories from the text perspective\nand remove the noise by the relevance-based approach. To suppress the search\nerror induced noisy images, we then formulate image selection and classifier\nlearning as a multi-class multi-instance learning problem and propose to solve\nthe employed problem by the cutting-plane algorithm. The experiments show\nsignificant performance gains by using the generated data of our way on both\nimage categorization and sub-categorization tasks. The proposed approach also\nconsistently outperforms existing weakly supervised and web-supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 01:36:49 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Shen", "Fumin", ""], ["Hua", "Xiansheng", ""], ["Yang", "Wankou", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1703.05502", "submitter": "Evgeny Burnaev", "authors": "Denis Volkhonskiy and Ivan Nazarov and Evgeny Burnaev", "title": "Steganographic Generative Adversarial Networks", "comments": "15 pages, 10 figures, 5 tables, Workshop on Adversarial Training\n  (NIPS 2016, Barcelona, Spain)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is collection of methods to hide secret information (\"payload\")\nwithin non-secret information \"container\"). Its counterpart, Steganalysis, is\nthe practice of determining if a message contains a hidden payload, and\nrecovering it if possible. Presence of hidden payloads is typically detected by\na binary classifier. In the present study, we propose a new model for\ngenerating image-like containers based on Deep Convolutional Generative\nAdversarial Networks (DCGAN). This approach allows to generate more\nsetganalysis-secure message embedding using standard steganography algorithms.\nExperiment results demonstrate that the new model successfully deceives the\nsteganography analyzer, and for this reason, can be used in steganographic\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 08:28:11 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 19:56:14 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Volkhonskiy", "Denis", ""], ["Nazarov", "Ivan", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1703.05778", "submitter": "Ali Sharifara", "authors": "Ali Sharifara, and Amir Ghaderi", "title": "Medical Image Watermarking using 2D-DWT with Enhanced security and\n  capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teleradiology enables medical images to be transferred over the computer\nnetworks for many purposes including clinical interpretation, diagnosis,\narchive, etc. In telemedicine, medical images can be manipulated while\ntransferring. In addition, medical information security requirements are\nspecified by the legislative rules, and concerned entities must adhere to them.\nIn this research, we propose a new scheme based on 2-dimensional Discrete\nWavelet Transform (2D DWT) to improve the robustness and authentication of\nmedical images. In addition, the current research improves security and\ncapacity of watermarking using encryption and compression in medical images.\nThe evaluation is performed on the personal dataset, which contains 194 CTI and\n68 MRI cases.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 18:05:32 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Sharifara", "Ali", ""], ["Ghaderi", "Amir", ""]]}, {"id": "1703.06499", "submitter": "Atilla Elci Prof.", "authors": "Afrah Ramadhan, Firas Mahmood and Atilla Elci", "title": "Image denoising by median filter in wavelet domain", "comments": "10 pages, 9 figures, 2 tables", "journal-ref": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.9, No.1, pp: 31-40, February 2017", "doi": "10.5121/ijma.2017.9104", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The details of an image with noise may be restored by removing noise through\na suitable image de-noising method. In this research, a new method of image\nde-noising based on using median filter (MF) in the wavelet domain is proposed\nand tested. Various types of wavelet transform filters are used in conjunction\nwith median filter in experimenting with the proposed approach in order to\nobtain better results for image de-noising process, and, consequently to select\nthe best suited filter. Wavelet transform working on the frequencies of\nsub-bands split from an image is a powerful method for analysis of images.\nAccording to this experimental work, the proposed method presents better\nresults than using only wavelet transform or median filter alone. The MSE and\nPSNR values are used for measuring the improvement in de-noised images.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 19:47:49 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Ramadhan", "Afrah", ""], ["Mahmood", "Firas", ""], ["Elci", "Atilla", ""]]}, {"id": "1703.06891", "submitter": "Chris Donahue", "authors": "Chris Donahue, Zachary C. Lipton, Julian McAuley", "title": "Dance Dance Convolution", "comments": "Published as a conference paper at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players\nperform steps on a dance platform in synchronization with music as directed by\non-screen step charts. While many step charts are available in standardized\npacks, players may grow tired of existing charts, or wish to dance to a song\nfor which no chart exists. We introduce the task of learning to choreograph.\nGiven a raw audio track, the goal is to produce a new step chart. This task\ndecomposes naturally into two subtasks: deciding when to place steps and\ndeciding which steps to select. For the step placement task, we combine\nrecurrent and convolutional neural networks to ingest spectrograms of low-level\naudio features to predict steps, conditioned on chart difficulty. For step\nselection, we present a conditional LSTM generative model that substantially\noutperforms n-gram and fixed-window approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 18:00:13 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 07:44:55 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 00:45:51 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Donahue", "Chris", ""], ["Lipton", "Zachary C.", ""], ["McAuley", "Julian", ""]]}, {"id": "1703.06931", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Shen, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong\n  Wang, Ke Lu", "title": "Learning Correspondence Structures for Person Re-identification", "comments": "IEEE Trans. Image Processing, vol. 26, no. 5, pp. 2438-2453, 2017.\n  The project page for this paper is available at\n  http://min.sjtu.edu.cn/lwydemo/personReID.htm arXiv admin note: text overlap\n  with arXiv:1504.06243", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of handling spatial misalignments due to\ncamera-view changes or human-pose variations in person re-identification. We\nfirst introduce a boosting-based approach to learn a correspondence structure\nwhich indicates the patch-wise matching probabilities between images from a\ntarget camera pair. The learned correspondence structure can not only capture\nthe spatial correspondence pattern between cameras but also handle the\nviewpoint or human-pose variation in individual images. We further introduce a\nglobal constraint-based matching process. It integrates a global matching\nconstraint over the learned correspondence structure to exclude cross-view\nmisalignments during the image patch matching process, hence achieving a more\nreliable matching score between images. Finally, we also extend our approach by\nintroducing a multi-structure scheme, which learns a set of local\ncorrespondence structures to capture the spatial correspondence sub-patterns\nbetween a camera pair, so as to handle the spatial misalignments between\nindividual images in a more precise way. Experimental results on various\ndatasets demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 19:17:14 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 12:31:28 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 16:15:30 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Lin", "Weiyao", ""], ["Shen", "Yang", ""], ["Yan", "Junchi", ""], ["Xu", "Mingliang", ""], ["Wu", "Jianxin", ""], ["Wang", "Jingdong", ""], ["Lu", "Ke", ""]]}, {"id": "1703.07920", "submitter": "Kaori Abe", "authors": "Kaori Abe, Teppei Suzuki, Shunya Ueta, Akio Nakamura, Yutaka Satoh and\n  Hirokatsu Kataoka", "title": "Changing Fashion Cultures", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel concept that analyzes and visualizes worldwide\nfashion trends. Our goal is to reveal cutting-edge fashion trends without\ndisplaying an ordinary fashion style. To achieve the fashion-based analysis, we\ncreated a new fashion culture database (FCDB), which consists of 76 million\ngeo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of\nmixed fashion styles,the paper also proposes an unsupervised fashion trend\ndescriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal\nanalysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD\neffectively emphasizes consecutive features between two different times. In\nexperiments, we clearly show the analysis of fashion trends and fashion-based\ncity similarity. As the result of large-scale data collection and an\nunsupervised analyzer, the proposed approach achieves world-level fashion\nvisualization in a time series. The code, model, and FCDB will be publicly\navailable after the construction of the project page.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 03:48:08 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Abe", "Kaori", ""], ["Suzuki", "Teppei", ""], ["Ueta", "Shunya", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1703.08348", "submitter": "Vaneet Aggarwal", "authors": "Abubakr O. Al-Abbasi and Vaneet Aggarwal", "title": "Video Streaming in Distributed Erasure-coded Storage Systems: Stall\n  Duration Analysis", "comments": "18 pages, accepted to IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for global video has been burgeoning across industries. With the\nexpansion and improvement of video-streaming services, cloud-based video is\nevolving into a necessary feature of any successful business for reaching\ninternal and external audiences. This paper considers video streaming over\ndistributed systems where the video segments are encoded using an erasure code\nfor better reliability thus being the first work to our best knowledge that\nconsiders video streaming over erasure-coded distributed cloud systems. The\ndownload time of each coded chunk of each video segment is characterized and\nordered statistics over the choice of the erasure-coded chunks is used to\nobtain the playback time of different video segments. Using the playback times,\nbounds on the moment generating function on the stall duration is used to bound\nthe mean stall duration. Moment generating function based bounds on the ordered\nstatistics are also used to bound the stall duration tail probability which\ndetermines the probability that the stall time is greater than a pre-defined\nnumber. These two metrics, mean stall duration and the stall duration tail\nprobability, are important quality of experience (QoE) measures for the end\nusers. Based on these metrics, we formulate an optimization problem to jointly\nminimize the convex combination of both the QoE metrics averaged over all\nrequests over the placement and access of the video content. The non-convex\nproblem is solved using an efficient iterative algorithm. Numerical results\nshow significant improvement in QoE metrics for cloud-based video as compared\nto the considered baselines.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 10:39:05 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 11:32:48 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Al-Abbasi", "Abubakr O.", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1703.08717", "submitter": "Tommy Nilsson", "authors": "Tommy Nilsson", "title": "Smart Spaces: Challenges and Opportunities of BLE-Centered Mobile\n  Systems for Public Environments", "comments": "Nilsson, T., 2014. Smart spaces: challenges and opportunities of\n  BLE-centered mobile systems for public environments. arXiv admin note:\n  substantial text overlap with arXiv:1605.05528", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of mobile computing is currently altering patterns of our\nbehavior to a greater degree than perhaps any other invention. In combination\nwith the introduction of BLE (Bluetooth Low Energy) and similar technologies\nenabling context-awareness, designers are today finding themselves empowered to\nbuild experiences and facilitate interactions with our physical surroundings in\nways not possible before. The aim of this thesis is to present a research\nproject, currently underway at the University of Cambridge, which is dealing\nwith implementation of a BLE system into a museum environment. By assessing the\ntechnology, describing the design decisions as well as presenting a qualitative\nevaluation, this paper seeks to provide insight into some of the challenges and\npossible solutions connected to the process of developing ubiquitous BLE\ncomputing systems for public spaces. The project outcome revealed the potential\nuse of BLE to engage whole new groups of audiences as well as made me argue in\nfavor of a more seamful approach to the design of these systems.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 17:27:14 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Nilsson", "Tommy", ""]]}, {"id": "1703.09090", "submitter": "Zhi Liu", "authors": "Gene Cheung and Zhi Liu and Zhiyou Ma and Jack Z. G. Tan", "title": "Multi-Stream Switching for Interactive Virtual Reality Video Streaming", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) video provides an immersive 360 viewing experience to a\nuser wearing a head-mounted display: as the user rotates his head,\ncorrespondingly different fields-of-view (FoV) of the 360 video are rendered\nfor observation. Transmitting the entire 360 video in high quality over\nbandwidth-constrained networks from server to client for real-time playback is\nchallenging. In this paper we propose a multi-stream switching framework for VR\nvideo streaming: the server pre-encodes a set of VR video streams covering\ndifferent view ranges that account for server-client round trip time (RTT)\ndelay, and during streaming the server transmits and switches streams according\nto a user's detected head rotation angle. For a given RTT, we formulate an\noptimization to seek multiple VR streams of different view ranges and the\nhead-angle-to-stream mapping function simultaneously, in order to minimize the\nexpected distortion subject to bandwidth and storage constraints. We propose an\nalternating algorithm that, at each iteration, computes the optimal streams\nwhile keeping the mapping function fixed and vice versa. Experiments show that\nfor the same bandwidth, our multi-stream switching scheme outperforms a\nnon-switching single-stream approach by up to 2.9dB in PSNR.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 14:09:12 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Cheung", "Gene", ""], ["Liu", "Zhi", ""], ["Ma", "Zhiyou", ""], ["Tan", "Jack Z. G.", ""]]}, {"id": "1703.09103", "submitter": "Mohammad Reza Khosravi", "authors": "Mohammad Reza Khosravi, Mohammad Kazem Moghimi", "title": "Theoretical Evaluation of Li et al.'s Approach for Improving a Binary\n  Watermark-Based Scheme in Remote Sensing Data Communications", "comments": null, "journal-ref": "Majlesi Journal of Telecommunication Devices, 5, 4, 151-153 (2016)", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This letter is about a principal weakness of the published article by Li et\nal. in 2014. It seems that the mentioned work has a terrible conceptual mistake\nwhile presenting its theoretical approach. In fact, the work has tried to\ndesign a new attack and its effective solution for a basic watermarking\nalgorithm by Zhu et al. published in 2013, however in practice, we show the Li\net al.'s approach is not correct to obtain the aim. For disproof of the\nincorrect approach, we only apply a numerical example as the counterexample of\nthe Li et al.'s approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 14:29:15 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Khosravi", "Mohammad Reza", ""], ["Moghimi", "Mohammad Kazem", ""]]}, {"id": "1703.09179", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Gy\\\"orgy Fazekas, Mark Sandler and Kyunghyun Cho", "title": "Transfer learning for music classification and regression tasks", "comments": "18th International Society of Music Information Retrieval (ISMIR)\n  Conference, Suzhou, China, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a transfer learning approach for music\nclassification and regression tasks. We propose to use a pre-trained convnet\nfeature, a concatenated feature vector using the activations of feature maps of\nmultiple layers in a trained convolutional network. We show how this convnet\nfeature can serve as general-purpose music representation. In the experiments,\na convnet is trained for music tagging and then transferred to other\nmusic-related classification and regression tasks. The convnet feature\noutperforms the baseline MFCC feature in all the considered tasks and several\nprevious approaches that are aggregating MFCCs as well as low- and high-level\nmusic features.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 16:48:03 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 15:58:38 GMT"}, {"version": "v3", "created": "Sat, 15 Jul 2017 13:36:05 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 16:20:26 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "Gy\u00f6rgy", ""], ["Sandler", "Mark", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1703.09968", "submitter": "Abhishek Kashyap", "authors": "Abhishek Kashyap, Rajesh Singh Parmar, Megha Agrawal, Hariom Gupta", "title": "An Evaluation of Digital Image Forgery Detection Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the headway of the advanced image handling software and altering tools,\na computerized picture can be effectively controlled. The identification of\nimage manipulation is vital in light of the fact that an image can be utilized\nas legitimate confirmation, in crime scene investigation, and in numerous\ndifferent fields. The image forgery detection techniques intend to confirm the\ncredibility of computerized pictures with no prior information about the\noriginal image. There are numerous routes for altering a picture, for example,\nresampling, splicing, and copy-move. In this paper, we have examined different\ntype of image forgery and their detection techniques; mainly we focused on\npixel based image forgery detection techniques.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 10:59:33 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 08:06:10 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Kashyap", "Abhishek", ""], ["Parmar", "Rajesh Singh", ""], ["Agrawal", "Megha", ""], ["Gupta", "Hariom", ""]]}, {"id": "1703.10893", "submitter": "Jen-Cheng Hou", "authors": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang,\n  Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional\n  Neural Networks", "comments": "To appear in IEEE Transactions on Emerging Topics in Computational\n  Intelligence. Some audio samples can be reached in this link:\n  https://sites.google.com/view/avse2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus only on addressing audio information. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. We also propose a multi-task learning\nframework for reconstructing audio and visual signals at the output layer.\nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual\nencoder-decoder network, in which audio and visual data are first processed\nusing individual CNNs, and then fused into a joint network to generate enhanced\nspeech (the primary task) and reconstructed images (the secondary task) at the\noutput layer. The model is trained in an end-to-end manner, and parameters are\njointly learned through back-propagation. We evaluate enhanced speech using\nfive instrumental criteria. Results show that the AVDCNN model yields a notably\nsuperior performance compared with an audio-only CNN-based SE model and two\nconventional SE approaches, confirming the effectiveness of integrating visual\ninformation into the SE process. In addition, the AVDCNN model also outperforms\nan existing audio-visual SE model, confirming its capability of effectively\ncombining audio and visual information in SE.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:59:24 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 07:42:54 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 23:37:42 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 21:19:19 GMT"}, {"version": "v5", "created": "Mon, 18 Dec 2017 21:58:03 GMT"}, {"version": "v6", "created": "Wed, 24 Jan 2018 17:54:39 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Hou", "Jen-Cheng", ""], ["Wang", "Syu-Siang", ""], ["Lai", "Ying-Hui", ""], ["Tsao", "Yu", ""], ["Chang", "Hsiu-Wen", ""], ["Wang", "Hsin-Min", ""]]}]