[{"id": "2003.00414", "submitter": "Yixin Wang", "authors": "Yixin Wang, Xiaohong Guan, Youtian Du, Nan Nan", "title": "Harmonics Based Representation in Clarinet Tone Quality Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music tone quality evaluation is generally performed by experts. It could be\nsubjective and short of consistency and fairness as well as time-consuming. In\nthis paper we present a new method for identifying the clarinet reed quality by\nevaluating tone quality based on the harmonic structure and energy\ndistribution. We first decouple the quality of reed and clarinet pipe based on\nthe acoustic harmonics, and discover that the reed quality is strongly relevant\nto the even parts of the harmonics. Then we construct a features set consisting\nof the even harmonic envelope and the energy distribution of harmonics in\nspectrum. The annotated clarinet audio data are recorded from 3 levels of\nperformers and the tone quality is classified by machine learning. The results\nshow that our new method for identifying low and medium high tones\nsignificantly outperforms previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 06:06:32 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wang", "Yixin", ""], ["Guan", "Xiaohong", ""], ["Du", "Youtian", ""], ["Nan", "Nan", ""]]}, {"id": "2003.00418", "submitter": "Rudrabha Mukhopadhyay", "authors": "Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay\n  Namboodiri, C.V. Jawahar", "title": "Towards Automatic Face-to-Face Translation", "comments": "9 pages (including references), 5 figures, Published in ACM\n  Multimedia, 2019", "journal-ref": "MM '19: Proceedings of the 27th ACM International Conference on\n  Multimedia; October 2019; Pages 1428-1436", "doi": "10.1145/3343031.3351066", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In light of the recent breakthroughs in automatic machine translation\nsystems, we propose a novel approach that we term as \"Face-to-Face\nTranslation\". As today's digital communication becomes increasingly visual, we\nargue that there is a need for systems that can automatically translate a video\nof a person speaking in language A into a target language B with realistic lip\nsynchronization. In this work, we create an automatic pipeline for this problem\nand demonstrate its impact on multiple real-world applications. First, we build\na working speech-to-speech translation system by bringing together multiple\nexisting modules from speech and language. We then move towards \"Face-to-Face\nTranslation\" by incorporating a novel visual module, LipGAN for generating\nrealistic talking faces from the translated audio. Quantitative evaluation of\nLipGAN on the standard LRW test set shows that it significantly outperforms\nexisting approaches across all standard metrics. We also subject our\nFace-to-Face Translation pipeline, to multiple human evaluations and show that\nit can significantly improve the overall user experience for consuming and\ninteracting with multimodal content across languages. Code, models and demo\nvideo are made publicly available.\n  Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0\n  Code and models: https://github.com/Rudrabha/LipGAN\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 06:42:43 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["R", "Prajwal K", ""], ["Mukhopadhyay", "Rudrabha", ""], ["Philip", "Jerin", ""], ["Jha", "Abhishek", ""], ["Namboodiri", "Vinay", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2003.00451", "submitter": "Bo Fu", "authors": "Bo Fu, Liyan Wang, Yuechu Wu, Yufeng Wu, Shilin Fu, Yonggong Ren", "title": "Weak Texture Information Map Guided Image Super-resolution with Deep\n  Residual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) is an image processing task which\nobtains high-resolution (HR) image from a low-resolution (LR) image. Recently,\ndue to the capability in feature extraction, a series of deep learning methods\nhave brought important crucial improvement for SISR. However, we observe that\nno matter how deeper the networks are designed, they usually do not have good\ngeneralization ability, which leads to the fact that almost all of existing SR\nmethods have poor performances on restoration of the weak texture details. To\nsolve these problems, we propose a weak texture information map guided image\nsuper-resolution with deep residual networks. It contains three sub-networks,\none main network which extracts the main features and fuses weak texture\ndetails, another two auxiliary networks extract the weak texture details fallen\nin the main network. Two part of networks work cooperatively, the auxiliary\nnetworks predict and integrates week texture information into the main network,\nwhich is conducive to the main network learning more inconspicuous details.\nExperiments results demonstrate that our method's performs achieve the\nstate-of-the-art quantitatively. Specifically, the image super-resolution\nresults of our method own more weak texture details.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 09:45:01 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 13:43:30 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Fu", "Bo", ""], ["Wang", "Liyan", ""], ["Wu", "Yuechu", ""], ["Wu", "Yufeng", ""], ["Fu", "Shilin", ""], ["Ren", "Yonggong", ""]]}, {"id": "2003.00832", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Yunsheng Ma, Yang Gu, Jufeng Yang, Tengfei Xing, Pengfei\n  Xu, Runbo Hu, Hua Chai, Kurt Keutzer", "title": "An End-to-End Visual-Audio Attention Network for Emotion Recognition in\n  User-Generated Videos", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition in user-generated videos plays an important role in\nhuman-centered computing. Existing methods mainly employ traditional two-stage\nshallow pipeline, i.e. extracting visual and/or audio features and training\nclassifiers. In this paper, we propose to recognize video emotions in an\nend-to-end manner based on convolutional neural networks (CNNs). Specifically,\nwe develop a deep Visual-Audio Attention Network (VAANet), a novel architecture\nthat integrates spatial, channel-wise, and temporal attentions into a visual 3D\nCNN and temporal attentions into an audio 2D CNN. Further, we design a special\nclassification loss, i.e. polarity-consistent cross-entropy loss, based on the\npolarity-emotion hierarchy constraint to guide the attention generation.\nExtensive experiments conducted on the challenging VideoEmotion-8 and Ekman-6\ndatasets demonstrate that the proposed VAANet outperforms the state-of-the-art\napproaches for video emotion recognition. Our source code is released at:\nhttps://github.com/maysonma/VAANet.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:33:59 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhao", "Sicheng", ""], ["Ma", "Yunsheng", ""], ["Gu", "Yang", ""], ["Yang", "Jufeng", ""], ["Xing", "Tengfei", ""], ["Xu", "Pengfei", ""], ["Hu", "Runbo", ""], ["Chai", "Hua", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2003.01299", "submitter": "Wenhan Zhu", "authors": "Wenhan Zhu, Guangtao Zhai, Zongxi Han, Xiongkuo Min, Tao Wang, Zicheng\n  Zhang and Xiaokang Yang", "title": "A multiple attributes image quality database for smartphone camera photo\n  quality assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone is the superstar product in digital device market and the quality\nof smartphone camera photos (SCPs) is becoming one of the dominant\nconsiderations when consumers purchase smartphones. How to evaluate the quality\nof smartphone cameras and the taken photos is urgent issue to be solved. To\nbridge the gap between academic research accomplishment and industrial needs,\nin this paper, we establish a new Smartphone Camera Photo Quality Database\n(SCPQD2020) including 1800 images with 120 scenes taken by 15 smartphones.\nExposure, color, noise and texture which are four dominant factors influencing\nthe quality of SCP are evaluated in the subjective study, respectively. Ten\npopular no-reference (NR) image quality assessment (IQA) algorithms are tested\nand analyzed on our database. Experimental results demonstrate that the current\nobjective models are not suitable for SCPs, and quality metrics having high\ncorrelation with human visual perception are highly needed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 02:34:24 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhu", "Wenhan", ""], ["Zhai", "Guangtao", ""], ["Han", "Zongxi", ""], ["Min", "Xiongkuo", ""], ["Wang", "Tao", ""], ["Zhang", "Zicheng", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2003.01866", "submitter": "Eduardo Pavez", "authors": "Eduardo Pavez, Benjamin Girault, Antonio Ortega and Philip A. Chou", "title": "Region adaptive graph fourier transform for 3d point clouds", "comments": "5 pages, 3 figures, accepted ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Region Adaptive Graph Fourier Transform (RA-GFT) for\ncompression of 3D point cloud attributes. The RA-GFT is a multiresolution\ntransform, formed by combining spatially localized block transforms. We assume\nthe points are organized by a family of nested partitions represented by a\nrooted tree. At each resolution level, attributes are processed in clusters\nusing block transforms. Each block transform produces a single approximation\n(DC) coefficient, and various detail (AC) coefficients. The DC coefficients are\npromoted up the tree to the next (lower resolution) level, where the process\ncan be repeated until reaching the root. Since clusters may have a different\nnumbers of points, each block transform must incorporate the relative\nimportance of each coefficient. For this, we introduce the\n$\\mathbf{Q}$-normalized graph Laplacian, and propose using its eigenvectors as\nthe block transform. The RA-GFT achieves better complexity-performance\ntrade-offs than previous approaches. In particular, it outperforms the Region\nAdaptive Haar Transform (RAHT) by up to 2.5 dB, with a small complexity\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 02:47:44 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 21:45:58 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Pavez", "Eduardo", ""], ["Girault", "Benjamin", ""], ["Ortega", "Antonio", ""], ["Chou", "Philip A.", ""]]}, {"id": "2003.01958", "submitter": "Federico Simonetta", "authors": "Federico Simonetta, Stavros Ntalampiras, Federico Avanzini", "title": "ASMD: an automatic framework for compiling multimodal datasets with\n  audio and scores", "comments": "Accepted at the Sound and Music Computing Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes an open-source Python framework for handling datasets\nfor music processing tasks, built with the aim of improving the reproducibility\nof research projects in music computing and assessing the generalization\nabilities of machine learning models. The framework enables the automatic\ndownload and installation of several commonly used datasets for multimodal\nmusic processing. Specifically, we provide a Python API to access the datasets\nthrough Boolean set operations based on particular attributes, such as\nintersections and unions of composers, instruments, and so on. The framework is\ndesigned to ease the inclusion of new datasets and the respective ground-truth\nannotations so that one can build, convert, and extend one's own collection as\nwell as distribute it by means of a compliant format to take advantage of the\nAPI. All code and ground-truth are released under suitable open licenses.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:57:59 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 10:44:46 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Simonetta", "Federico", ""], ["Ntalampiras", "Stavros", ""], ["Avanzini", "Federico", ""]]}, {"id": "2003.02526", "submitter": "Serhan G\\\"ul", "authors": "Serhan G\\\"ul, Dimitri Podborski, Jangwoo Son, Gurdeep Singh Bhullar,\n  Thomas Buchholz, Thomas Schierl, Cornelius Hellge", "title": "Cloud Rendering-based Volumetric Video Streaming System for Mixed\n  Reality Services", "comments": "4 pages, 2 figures", "journal-ref": "11th ACM Multimedia Systems Conference (MMSys) 2020", "doi": "10.1145/3339825.3393583", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric video is an emerging technology for immersive representation of 3D\nspaces that captures objects from all directions using multiple cameras and\ncreates a dynamic 3D model of the scene. However, processing volumetric content\nrequires high amounts of processing power and is still a very demanding task\nfor today's mobile devices. To mitigate this, we propose a volumetric video\nstreaming system that offloads the rendering to a powerful cloud/edge server\nand only sends the rendered 2D view to the client instead of the full\nvolumetric content. We use 6DoF head movement prediction techniques, WebRTC\nprotocol and hardware video encoding to ensure low-latency in different parts\nof the processing chain. We demonstrate our system using both a browser-based\nclient and a Microsoft HoloLens client. Our application contains generic\ninterfaces that allow for easy deployment of various augmented/mixed reality\nclients using the same server implementation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 10:44:37 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 13:43:21 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["G\u00fcl", "Serhan", ""], ["Podborski", "Dimitri", ""], ["Son", "Jangwoo", ""], ["Bhullar", "Gurdeep Singh", ""], ["Buchholz", "Thomas", ""], ["Schierl", "Thomas", ""], ["Hellge", "Cornelius", ""]]}, {"id": "2003.03092", "submitter": "Hadi Hadizadeh", "authors": "Hadi Hadizadeh, Ivan V. bajic", "title": "Soft Video Multicasting Using Adaptive Compressed Sensing", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2020.2975420", "report-no": null, "categories": "cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, soft video multicasting has gained a lot of attention, especially\nin broadcast and mobile scenarios where the bit rate supported by the channel\nmay differ across receivers, and may vary quickly over time. Unlike the\nconventional designs that force the source to use a single bit rate according\nto the receiver with the worst channel quality, soft video delivery schemes\ntransmit the video such that the video quality at each receiver is commensurate\nwith its specific instantaneous channel quality. In this paper, we present a\nsoft video multicasting system using an adaptive block-based compressed sensing\n(BCS) method. The proposed system consists of an encoder, a transmission\nsystem, and a decoder. At the encoder side, each block in each frame of the\ninput video is adaptively sampled with a rate that depends on the texture\ncomplexity and visual saliency of the block. The obtained BCS samples are then\nplaced into several packets, and the packets are transmitted via a\nchannel-aware OFDM (orthogonal frequency division multiplexing) transmission\nsystem with a number of subchannels. At the decoder side, the received BCS\nsamples are first used to build an initial approximation of the transmitted\nframe. To further improve the reconstruction quality, an iterative BCS\nreconstruction algorithm is then proposed that uses an adaptive transform and\nan adaptive soft-thresholding operator, which exploits the temporal similarity\nbetween adjacent frames to achieve better reconstruction quality. The extensive\nobjective and subjective experimental results indicate the superiority of the\nproposed system over the state-of-the-art soft video multicasting systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 09:08:44 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Hadizadeh", "Hadi", ""], ["bajic", "Ivan V.", ""]]}, {"id": "2003.03320", "submitter": "Felix Sattler", "authors": "Felix Sattler, Thomas Wiegand, Wojciech Samek", "title": "Trends and Advancements in Deep Neural Network Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MM cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their great performance and scalability properties neural networks\nhave become ubiquitous building blocks of many applications. With the rise of\nmobile and IoT, these models now are also being increasingly applied in\ndistributed settings, where the owners of the data are separated by limited\ncommunication channels and privacy constraints. To address the challenges of\nthese distributed environments, a wide range of training and evaluation schemes\nhave been developed, which require the communication of neural network\nparametrizations. These novel approaches, which bring the \"intelligence to the\ndata\" have many advantages over traditional cloud solutions such as\nprivacy-preservation, increased security and device autonomy, communication\nefficiency and high training speed. This paper gives an overview over the\nrecent advancements and challenges in this new field of research at the\nintersection of machine learning and communications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 17:34:15 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Sattler", "Felix", ""], ["Wiegand", "Thomas", ""], ["Samek", "Wojciech", ""]]}, {"id": "2003.03703", "submitter": "Dongxu Li", "authors": "Dongxu Li, Xin Yu, Chenchen Xu, Lars Petersson, Hongdong Li", "title": "Transferring Cross-domain Knowledge for Video Sign Language Recognition", "comments": "CVPR2020 (oral) preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word-level sign language recognition (WSLR) is a fundamental task in sign\nlanguage interpretation. It requires models to recognize isolated sign words\nfrom videos. However, annotating WSLR data needs expert knowledge, thus\nlimiting WSLR dataset acquisition. On the contrary, there are abundant\nsubtitled sign news videos on the internet. Since these videos have no\nword-level annotation and exhibit a large domain gap from isolated signs, they\ncannot be directly used for training WSLR models. We observe that despite the\nexistence of a large domain gap, isolated and news signs share the same visual\nconcepts, such as hand gestures and body movements. Motivated by this\nobservation, we propose a novel method that learns domain-invariant visual\nconcepts and fertilizes WSLR models by transferring knowledge of subtitled news\nsign to them. To this end, we extract news signs using a base WSLR model, and\nthen design a classifier jointly trained on news and isolated signs to coarsely\nalign these two domain features. In order to learn domain-invariant features\nwithin each class and suppress domain-specific features, our method further\nresorts to an external memory to store the class centroids of the aligned news\nsigns. We then design a temporal attention based on the learnt descriptor to\nimprove recognition performance. Experimental results on standard WSLR datasets\nshow that our method outperforms previous state-of-the-art methods\nsignificantly. We also demonstrate the effectiveness of our method on\nautomatically localizing signs from sign news, achieving 28.1 for AP@0.5.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 03:05:21 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 14:53:06 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Li", "Dongxu", ""], ["Yu", "Xin", ""], ["Xu", "Chenchen", ""], ["Petersson", "Lars", ""], ["Li", "Hongdong", ""]]}, {"id": "2003.03955", "submitter": "Hao Wang", "authors": "Hao Wang, Doyen Sahoo, Chenghao Liu, Ke Shu, Palakorn Achananuparp,\n  Ee-peng Lim, Steven C. H. Hoi", "title": "Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images\n  and Recipes with Semantic Consistency and Attention Mechanism", "comments": "IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food retrieval is an important task to perform analysis of food-related\ninformation, where we are interested in retrieving relevant information about\nthe queried food item such as ingredients, cooking instructions, etc. In this\npaper, we investigate cross-modal retrieval between food images and cooking\nrecipes. The goal is to learn an embedding of images and recipes in a common\nfeature space, such that the corresponding image-recipe embeddings lie close to\none another. Two major challenges in addressing this problem are 1) large\nintra-variance and small inter-variance across cross-modal food data; and 2)\ndifficulties in obtaining discriminative recipe representations. To address\nthese two problems, we propose Semantic-Consistent and Attention-based Networks\n(SCAN), which regularize the embeddings of the two modalities through aligning\noutput semantic probabilities. Besides, we exploit a self-attention mechanism\nto improve the embedding of recipes. We evaluate the performance of the\nproposed method on the large-scale Recipe1M dataset, and show that we can\noutperform several state-of-the-art cross-modal retrieval strategies for food\nimages and cooking recipes by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 07:41:17 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 01:10:41 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Hao", ""], ["Sahoo", "Doyen", ""], ["Liu", "Chenghao", ""], ["Shu", "Ke", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-peng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2003.04169", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Yu Chen, Alexander Aved, Erik Blasch", "title": "I-ViSE: Interactive Video Surveillance as an Edge Service using\n  Unsupervised Feature Queries", "comments": "R1 is under review by the IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situation AWareness (SAW) is essential for many mission critical\napplications. However, SAW is very challenging when trying to immediately\nidentify objects of interest or zoom in on suspicious activities from thousands\nof video frames. This work aims at developing a queryable system to instantly\nselect interesting content. While face recognition technology is mature, in\nmany scenarios like public safety monitoring, the features of objects of\ninterest may be much more complicated than face features. In addition, human\noperators may not be always able to provide a descriptive, simple, and accurate\nquery. Actually, it is more often that there are only rough, general\ndescriptions of certain suspicious objects or accidents. This paper proposes an\nInteractive Video Surveillance as an Edge service (I-ViSE) based on\nunsupervised feature queries. Adopting unsupervised methods that do not reveal\nany private information, the I-ViSE scheme utilizes general features of a human\nbody and color of clothes. An I-ViSE prototype is built following the edge-fog\ncomputing paradigm and the experimental results verified the I-ViSE scheme\nmeets the design goal of scene recognition in less than two seconds.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 14:26:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""], ["Aved", "Alexander", ""], ["Blasch", "Erik", ""]]}, {"id": "2003.04210", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool", "title": "Semantic Object Prediction and Spatial Sound Super-Resolution with\n  Binaural Sounds", "comments": "Project page:\n  https://www.trace.ethz.ch/publications/2020/sound_perception/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can robustly recognize and localize objects by integrating visual and\nauditory cues. While machines are able to do the same now with images, less\nwork has been done with sounds. This work develops an approach for dense\nsemantic labelling of sound-making objects, purely based on binaural sounds. We\npropose a novel sensor setup and record a new audio-visual dataset of street\nscenes with eight professional binaural microphones and a 360 degree camera.\nThe co-existence of visual and audio cues is leveraged for supervision\ntransfer. In particular, we employ a cross-modal distillation framework that\nconsists of a vision `teacher' method and a sound `student' method -- the\nstudent method is trained to generate the same results as the teacher method.\nThis way, the auditory system can be trained without using human annotations.\nWe also propose two auxiliary tasks namely, a) a novel task on Spatial Sound\nSuper-resolution to increase the spatial resolution of sounds, and b) dense\ndepth prediction of the scene. We then formulate the three tasks into one\nend-to-end trainable multi-tasking network aiming to boost the overall\nperformance. Experimental results on the dataset show that 1) our method\nachieves promising results for semantic prediction and the two auxiliary tasks;\nand 2) the three tasks are mutually beneficial -- training them together\nachieves the best performance and 3) the number and orientations of microphones\nare both important. The data and code will be released to facilitate the\nresearch in this new direction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:49:01 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2003.04358", "submitter": "Rahul Sharma", "authors": "Rahul Sharma, Krishna Somandepalli and Shrikanth Narayanan", "title": "Crossmodal learning for audio-visual speech event localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An objective understanding of media depictions, such as about inclusive\nportrayals of how much someone is heard and seen on screen in film and\ntelevision, requires the machines to discern automatically who, when, how and\nwhere someone is talking. Media content is rich in multiple modalities such as\nvisuals and audio which can be used to learn speaker activity in videos. In\nthis work, we present visual representations that have implicit information\nabout when someone is talking and where. We propose a crossmodal neural network\nfor audio speech event detection using the visual frames. We use the learned\nrepresentations for two downstream tasks: i) audio-visual voice activity\ndetection ii) active speaker localization in video frames. We present a\nstate-of-the-art audio-visual voice activity detection system and demonstrate\nthat the learned embeddings can effectively localize to active speakers in the\nvisual frames.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 18:50:50 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Sharma", "Rahul", ""], ["Somandepalli", "Krishna", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2003.04679", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Chang Liu, Li Liu, Dongyan Zhao and Rui Yan", "title": "Learning to Respond with Stickers: A Framework of Unifying\n  Multi-Modality in Multi-Turn Dialog", "comments": "Accepted by The Web Conference 2020 (WWW 2020). Equal contribution\n  from first two authors. Dataset and code are released at\n  https://github.com/gsh199449/stickerchat", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stickers with vivid and engaging expressions are becoming increasingly\npopular in online messaging apps, and some works are dedicated to automatically\nselect sticker response by matching text labels of stickers with previous\nutterances. However, due to their large quantities, it is impractical to\nrequire text labels for the all stickers. Hence, in this paper, we propose to\nrecommend an appropriate sticker to user based on multi-turn dialog context\nhistory without any external labels. Two main challenges are confronted in this\ntask. One is to learn semantic meaning of stickers without corresponding text\nlabels. Another challenge is to jointly model the candidate sticker with the\nmulti-turn dialog context. To tackle these challenges, we propose a sticker\nresponse selector (SRS) model. Specifically, SRS first employs a convolutional\nbased sticker image encoder and a self-attention based multi-turn dialog\nencoder to obtain the representation of stickers and utterances. Next, deep\ninteraction network is proposed to conduct deep matching between the sticker\nwith each utterance in the dialog history. SRS then learns the short-term and\nlong-term dependency between all interaction results by a fusion network to\noutput the the final matching score. To evaluate our proposed method, we\ncollect a large-scale real-world dialog dataset with stickers from one of the\nmost popular online chatting platform. Extensive experiments conducted on this\ndataset show that our model achieves the state-of-the-art performance for all\ncommonly-used metrics. Experiments also verify the effectiveness of each\ncomponent of SRS. To facilitate further research in sticker selection field, we\nrelease this dataset of 340K multi-turn dialog and sticker pairs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:10:26 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Liu", "Chang", ""], ["Liu", "Li", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2003.05096", "submitter": "Peng Qi", "authors": "Juan Cao, Peng Qi, Qiang Sheng, Tianyun Yang, Junbo Guo, Jintao Li", "title": "Exploring the Role of Visual Content in Fake News Detection", "comments": "This is a preprint of a chapter published in Disinformation,\n  Misinformation, and Fake News in Social Media: Emerging Research Challenges\n  and Opportunities, edited by Kai, S., Suhang, W., Dongwon, L., Huan, L, 2020,\n  Springer reproduced with permission of Springer Nature Switzerland AG. The\n  final authenticated version is available online at:\n  https://www.springer.com/gp/book/9783030426989. arXiv admin note: text\n  overlap with arXiv:2001.00623, arXiv:1808.06686, arXiv:1903.00788 by other\n  authors", "journal-ref": "Disinformation, Misinformation, and Fake News in Social Media.\n  2020", "doi": "10.1007/978-3-030-42699-6", "report-no": null, "categories": "cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of social media promotes the proliferation of fake\nnews, which has caused significant negative societal effects. Therefore, fake\nnews detection on social media has recently become an emerging research area of\ngreat concern. With the development of multimedia technology, fake news\nattempts to utilize multimedia content with images or videos to attract and\nmislead consumers for rapid dissemination, which makes visual content an\nimportant part of fake news. Despite the importance of visual content, our\nunderstanding of the role of visual content in fake news detection is still\nlimited. This chapter presents a comprehensive review of the visual content in\nfake news, including the basic concepts, effective visual features,\nrepresentative detection methods and challenging issues of multimedia fake news\ndetection. This chapter can help readers to understand the role of visual\ncontent in fake news detection, and effectively utilize visual content to\nassist in detecting multimedia fake news.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 03:16:04 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Cao", "Juan", ""], ["Qi", "Peng", ""], ["Sheng", "Qiang", ""], ["Yang", "Tianyun", ""], ["Guo", "Junbo", ""], ["Li", "Jintao", ""]]}, {"id": "2003.06315", "submitter": "Maria Santamaria", "authors": "Maria Santamaria, Ebroul Izquierdo, Saverio Blasi, Marta Mrak", "title": "Estimation of Rate Control Parameters for Video Coding Using CNN", "comments": "5 pages, 5 figures, 4 tables", "journal-ref": "IEEE International Conference on Visual Communications and Image\n  Processing (VCIP 2018), Taichung, Taiwan, 9 -12 December 2018", "doi": "10.1109/VCIP.2018.8698721", "report-no": null, "categories": "eess.IV cs.LG cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rate-control is essential to ensure efficient video delivery. Typical\nrate-control algorithms rely on bit allocation strategies, to appropriately\ndistribute bits among frames. As reference frames are essential for exploiting\ntemporal redundancies, intra frames are usually assigned a larger portion of\nthe available bits. In this paper, an accurate method to estimate number of\nbits and quality of intra frames is proposed, which can be used for bit\nallocation in a rate-control scheme. The algorithm is based on deep learning,\nwhere networks are trained using the original frames as inputs, while\ndistortions and sizes of compressed frames after encoding are used as ground\ntruths. Two approaches are proposed where either local or global distortions\nare predicted.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 14:18:43 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Santamaria", "Maria", ""], ["Izquierdo", "Ebroul", ""], ["Blasi", "Saverio", ""], ["Mrak", "Marta", ""]]}, {"id": "2003.06576", "submitter": "Long Chen", "authors": "Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, Yueting\n  Zhuang", "title": "Counterfactual Samples Synthesizing for Robust Visual Question Answering", "comments": "Appear in CVPR 2020; Codes in https://github.com/yanxinzju/CSS-VQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite Visual Question Answering (VQA) has realized impressive progress over\nthe last few years, today's VQA models tend to capture superficial linguistic\ncorrelations in the train set and fail to generalize to the test set with\ndifferent QA distributions. To reduce the language biases, several recent works\nintroduce an auxiliary question-only model to regularize the training of\ntargeted VQA model, and achieve dominating performance on VQA-CP. However,\nsince the complexity of design, current methods are unable to equip the\nensemble-based models with two indispensable characteristics of an ideal VQA\nmodel: 1) visual-explainable: the model should rely on the right visual regions\nwhen making decisions. 2) question-sensitive: the model should be sensitive to\nthe linguistic variations in question. To this end, we propose a model-agnostic\nCounterfactual Samples Synthesizing (CSS) training scheme. The CSS generates\nnumerous counterfactual training samples by masking critical objects in images\nor words in questions, and assigning different ground-truth answers. After\ntraining with the complementary samples (ie, the original and generated\nsamples), the VQA models are forced to focus on all critical objects and words,\nwhich significantly improves both visual-explainable and question-sensitive\nabilities. In return, the performance of these models is further boosted.\nExtensive ablations have shown the effectiveness of CSS. Particularly, by\nbuilding on top of the model LMH, we achieve a record-breaking performance of\n58.95% on VQA-CP v2, with 6.5% gains.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 08:34:31 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Long", ""], ["Yan", "Xin", ""], ["Xiao", "Jun", ""], ["Zhang", "Hanwang", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2003.07505", "submitter": "Md Amiruzzaman", "authors": "Md Amiruzzaman and Rizal Mohd Nor", "title": "Hide Secret Information in Blocks: Minimum Distortion Embedding", "comments": "This paper is accepted for publication in IEEE SPIN 2020 conference", "journal-ref": "2020 7th International Conference on Signal Processing and\n  Integrated Networks (SPIN)", "doi": "10.1109/SPIN48934.2020.9071138", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new steganographic method is presented that provides minimum\ndistortion in the stego image. The proposed encoding algorithm focuses on DCT\nrounding error and optimizes that in a way to reduce distortion in the stego\nimage, and the proposed algorithm produces less distortion than existing\nmethods (e.g., F5 algorithm). The proposed method is based on DCT rounding\nerror which helps to lower distortion and higher embedding capacity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:49:35 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Amiruzzaman", "Md", ""], ["Nor", "Rizal Mohd", ""]]}, {"id": "2003.07544", "submitter": "Cunhang Fan", "authors": "Cunhang Fan and Jianhua Tao and Bin Liu and Jiangyan Yi and Zhengqi\n  Wen and Xuefei Liu", "title": "Deep Attention Fusion Feature for Speech Separation with End-to-End\n  Post-filter Method", "comments": "ACCEPTED by IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end post-filter method with deep\nattention fusion features for monaural speaker-independent speech separation.\nAt first, a time-frequency domain speech separation method is applied as the\npre-separation stage. The aim of pre-separation stage is to separate the\nmixture preliminarily. Although this stage can separate the mixture, it still\ncontains the residual interference. In order to enhance the pre-separated\nspeech and improve the separation performance further, the end-to-end\npost-filter (E2EPF) with deep attention fusion features is proposed. The E2EPF\ncan make full use of the prior knowledge of the pre-separated speech, which\ncontributes to speech separation. It is a fully convolutional speech separation\nnetwork and uses the waveform as the input features. Firstly, the 1-D\nconvolutional layer is utilized to extract the deep representation features for\nthe mixture and pre-separated signals in the time domain. Secondly, to pay more\nattention to the outputs of the pre-separation stage, an attention module is\napplied to acquire deep attention fusion features, which are extracted by\ncomputing the similarity between the mixture and the pre-separated speech.\nThese deep attention fusion features are conducive to reduce the interference\nand enhance the pre-separated speech. Finally, these features are sent to the\npost-filter to estimate each target signals. Experimental results on the\nWSJ0-2mix dataset show that the proposed method outperforms the\nstate-of-the-art speech separation method. Compared with the pre-separation\nmethod, our proposed method can acquire 64.1%, 60.2%, 25.6% and 7.5% relative\nimprovements in scale-invariant source-to-noise ratio (SI-SNR), the\nsignal-to-distortion ratio (SDR), the perceptual evaluation of speech quality\n(PESQ) and the short-time objective intelligibility (STOI) measures,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 05:43:12 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Fan", "Cunhang", ""], ["Tao", "Jianhua", ""], ["Liu", "Bin", ""], ["Yi", "Jiangyan", ""], ["Wen", "Zhengqi", ""], ["Liu", "Xuefei", ""]]}, {"id": "2003.07583", "submitter": "Wei Quan", "authors": "Wei Quan, Yuxuan Pan, Bin Xiang, Lin Zhang", "title": "Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow\n  Based QoE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the merit of containing full panoramic content in one camera, Virtual\nReality (VR) and 360-degree videos have attracted more and more attention in\nthe field of industrial cloud manufacturing and training. Industrial Internet\nof Things (IoT), where many VR terminals needed to be online at the same time,\ncan hardly guarantee VR's bandwidth requirement. However, by making use of\nusers' quality of experience (QoE) awareness factors, including the relative\nmoving speed and depth difference between the viewpoint and other content,\nbandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical\nFlow Based VR), an interactive method of VR streaming that can make use of VR\nusers' QoE awareness to ease the bandwidth pressure. The Just-Noticeable\nDifference through Optical Flow Estimation (JND-OFE) is explored to quantify\nusers' awareness of quality distortion in 360-degree videos. Accordingly, a\nnovel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is\nproposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling\nscheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is\nimplemented to make use of historical data to perform Adaptive BitRate(ABR).\nFor evaluation, we take two prior VR streaming schemes, Pano and Plato, as\nbaselines. Vast evaluations show that our system can increase the mean PSNR-OF\nscore by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano\nand Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that\nOFB-VR is a promising prototype for actual interactive industrial VR. A\nprototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 08:47:34 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Quan", "Wei", ""], ["Pan", "Yuxuan", ""], ["Xiang", "Bin", ""], ["Zhang", "Lin", ""]]}, {"id": "2003.07694", "submitter": "Siyu Huang", "authors": "Siyu Huang, Haoyi Xiong, Tianyang Wang, Qingzhong Wang, Zeyu Chen, Jun\n  Huan, Dejing Dou", "title": "Parameter-Free Style Projection for Arbitrary Style Transfer", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary image style transfer is a challenging task which aims to stylize a\ncontent image conditioned on an arbitrary style image. In this task the\ncontent-style feature transformation is a critical component for a proper\nfusion of features. Existing feature transformation algorithms often suffer\nfrom unstable learning, loss of content and style details, and non-natural\nstroke patterns. To mitigate these issues, this paper proposes a parameter-free\nalgorithm, Style Projection, for fast yet effective content-style\ntransformation. To leverage the proposed Style Projection~component, this paper\nfurther presents a real-time feed-forward model for arbitrary style transfer,\nincluding a regularization for matching the content semantics between inputs\nand outputs. Extensive experiments have demonstrated the effectiveness and\nefficiency of the proposed method in terms of qualitative analysis,\nquantitative evaluation, and user study.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 13:07:41 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Huang", "Siyu", ""], ["Xiong", "Haoyi", ""], ["Wang", "Tianyang", ""], ["Wang", "Qingzhong", ""], ["Chen", "Zeyu", ""], ["Huan", "Jun", ""], ["Dou", "Dejing", ""]]}, {"id": "2003.08355", "submitter": "Zehua Wang", "authors": "Wei Hu, Qianjiang Hu, Zehua Wang, Xiang Gao", "title": "Dynamic Point Cloud Denoising via Manifold-to-Manifold Distance", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3092826", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D dynamic point clouds provide a natural discrete representation of\nreal-world objects or scenes in motion, with a wide range of applications in\nimmersive telepresence, autonomous driving, surveillance, \\etc. Nevertheless,\ndynamic point clouds are often perturbed by noise due to hardware, software or\nother causes. While a plethora of methods have been proposed for static point\ncloud denoising, few efforts are made for the denoising of dynamic point\nclouds, which is quite challenging due to the irregular sampling patterns both\nspatially and temporally. In this paper, we represent dynamic point clouds\nnaturally on spatial-temporal graphs, and exploit the temporal consistency with\nrespect to the underlying surface (manifold). In particular, we define a\nmanifold-to-manifold distance and its discrete counterpart on graphs to measure\nthe variation-based intrinsic distance between surface patches in the temporal\ndomain, provided that graph operators are discrete counterparts of functionals\non Riemannian manifolds. Then, we construct the spatial-temporal graph\nconnectivity between corresponding surface patches based on the temporal\ndistance and between points in adjacent patches in the spatial domain.\nLeveraging the initial graph representation, we formulate dynamic point cloud\ndenoising as the joint optimization of the desired point cloud and underlying\ngraph representation, regularized by both spatial smoothness and temporal\nconsistency. We reformulate the optimization and present an efficient\nalgorithm. Experimental results show that the proposed method significantly\noutperforms independent denoising of each frame from state-of-the-art static\npoint cloud denoising approaches, on both Gaussian noise and simulated LiDAR\nnoise.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 04:54:20 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 07:21:36 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 12:53:04 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Hu", "Wei", ""], ["Hu", "Qianjiang", ""], ["Wang", "Zehua", ""], ["Gao", "Xiang", ""]]}, {"id": "2003.08473", "submitter": "Nikolaos Thomos", "authors": "Pantelis Maniotis and Nikolaos Thomos", "title": "Viewport-Aware Deep Reinforcement Learning Approach for 360$^o$ Video\n  Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT cs.LG cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360$^o$ video is an essential component of VR/AR/MR systems that provides\nimmersive experience to the users. However, 360$^o$ video is associated with\nhigh bandwidth requirements. The required bandwidth can be reduced by\nexploiting the fact that users are interested in viewing only a part of the\nvideo scene and that users request viewports that overlap with each other.\nMotivated by the findings of recent works where the benefits of caching video\ntiles at edge servers instead of caching entire 360$^o$ videos were shown, in\nthis paper, we introduce the concept of virtual viewports that have the same\nnumber of tiles with the original viewports. The tiles forming these viewports\nare the most popular ones for each video and are determined by the users'\nrequests. Then, we propose a proactive caching scheme that assumes unknown\nvideos' and viewports' popularity. Our scheme determines which videos to cache\nas well as which is the optimal virtual viewport per video. Virtual viewports\npermit to lower the dimensionality of the cache optimization problem. To solve\nthe problem, we first formulate the content placement of 360$^o$ videos in edge\ncache networks as a Markov Decision Process (MDP), and then we determine the\noptimal caching placement using the Deep Q-Network (DQN) algorithm. The\nproposed solution aims at maximizing the overall quality of the 360$^o$ videos\ndelivered to the end-users by caching the most popular 360$^o$ videos at base\nquality along with a virtual viewport in high quality. We extensively evaluate\nthe performance of the proposed system and compare it with that of known\nsystems such as LFU, LRU, FIFO, over both synthetic and real 360$^o$ video\ntraces. The results reveal the large benefits coming from proactive caching of\nvirtual viewports instead of the original ones in terms of the overall quality\nof the rendered viewports, the cache hit ratio, and the servicing cost.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 21:05:10 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 18:22:07 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Maniotis", "Pantelis", ""], ["Thomos", "Nikolaos", ""]]}, {"id": "2003.08574", "submitter": "Tho Nguyen Duc", "authors": "Tho Nguyen Duc, Chanh Minh Tran, Phan Xuan Tan, and Eiji Kamioka", "title": "Convolutional Neural Networks for Continuous QoE Prediction in Video\n  Streaming Services", "comments": null, "journal-ref": "IEEE Access 2020", "doi": "10.1109/ACCESS.2020.3004125", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video streaming services, predicting the continuous user's quality of\nexperience (QoE) plays a crucial role in delivering high quality streaming\ncontents to the user. However, the complexity caused by the temporal\ndependencies in QoE data and the non-linear relationships among QoE influence\nfactors has introduced challenges to continuous QoE prediction. To deal with\nthat, existing studies have utilized the Long Short-Term Memory model (LSTM) to\neffectively capture such complex dependencies, resulting in excellent QoE\nprediction accuracy. However, the high computational complexity of LSTM, caused\nby the sequential processing characteristic in its architecture, raises a\nserious question about its performance on devices with limited computational\npower. Meanwhile, Temporal Convolutional Network (TCN), a variation of\nconvolutional neural networks, has recently been proposed for sequence modeling\ntasks (e.g., speech enhancement), providing a superior prediction performance\nover baseline methods including LSTM in terms of prediction accuracy and\ncomputational complexity. Being inspired of that, in this paper, an improved\nTCN-based model, namely CNN-QoE, is proposed for continuously predicting the\nQoE, which poses characteristics of sequential data. The proposed model\nleverages the advantages of TCN to overcome the computational complexity\ndrawbacks of LSTM-based QoE models, while at the same time introducing the\nimprovements to its architecture to improve QoE prediction accuracy. Based on a\ncomprehensive evaluation, we demonstrate that the proposed CNN-QoE model can\nreach the state-of-the-art performance on both personal computers and mobile\ndevices, outperforming the existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 05:13:48 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Duc", "Tho Nguyen", ""], ["Tran", "Chanh Minh", ""], ["Tan", "Phan Xuan", ""], ["Kamioka", "Eiji", ""]]}, {"id": "2003.08619", "submitter": "Chanh Tran", "authors": "Chanh Minh Tran and Tho Nguyen Duc and Phan Xuan Tan and Eiji Kamioka", "title": "FAURAS: A Proxy-based Framework for Ensuring the Fairness of Adaptive\n  Video Streaming over HTTP/2 Server Push", "comments": null, "journal-ref": "Appl. Sci. 2020, 10, 2485", "doi": "10.3390/app10072485", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTTP/2 video streaming has caught a lot of attentions in the development of\nmultimedia technologies over the last few years. In HTTP/2, the server push\nmechanism allows the server to deliver more video segments to the client within\na single request in order to deal with the requests explosion problem. As a\nresult, recent research efforts have been focusing on utilizing such a feature\nto enhance the streaming experience while reducing the request-related\noverhead. However, current works only optimize the performance of a single\nclient, without necessary concerns of possible influences on other clients in\nthe same network. When multiple streaming clients compete for a shared\nbandwidth in HTTP/1.1, they are likely to suffer from unfairness, which is\ndefined as the inequality in their bitrate selections. For HTTP/1.1, existing\nworks have proven that the network-assisted solutions are effective in solving\nthe unfairness problem. However, the feasibility of utilizing such an approach\nfor the HTTP/2 server push has not been investigated. Therefore, in this paper,\na novel proxy-based framework is proposed to overcome the unfairness problem in\nadaptive streaming over HTTP/2 with the server push. Experimental results\nconfirm the outperformance of the proposed framework in ensuring the fairness,\nassisting the clients to avoid rebuffering events and lower bitrate degradation\namplitude, while maintaining the mechanism of the server push feature.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:14:30 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Tran", "Chanh Minh", ""], ["Duc", "Tho Nguyen", ""], ["Tan", "Phan Xuan", ""], ["Kamioka", "Eiji", ""]]}, {"id": "2003.08769", "submitter": "Nitish Nag", "authors": "Nitish Nag, Bindu Rajanna, Ramesh Jain", "title": "Personalized Taste and Cuisine Preference Modeling via Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the exponential growth in the usage of social media to share live\nupdates about life, taking pictures has become an unavoidable phenomenon.\nIndividuals unknowingly create a unique knowledge base with these images. The\nfood images, in particular, are of interest as they contain a plethora of\ninformation. From the image metadata and using computer vision tools, we can\nextract distinct insights for each user to build a personal profile. Using the\nunderlying connection between cuisines and their inherent tastes, we attempt to\ndevelop such a profile for an individual based solely on the images of his\nfood. Our study provides insights about an individual's inclination towards\nparticular cuisines. Interpreting these insights can lead to the development of\na more precise recommendation system. Such a system would avoid the generic\napproach in favor of a personalized recommendation system.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:07:56 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Nag", "Nitish", ""], ["Rajanna", "Bindu", ""], ["Jain", "Ramesh", ""]]}, {"id": "2003.08865", "submitter": "Yuan Gao", "authors": "Yuan Gao, Robert Bregovic, Reinhard Koch and Atanas Gotchev", "title": "DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one\nof the most effective methods for Densely-Sampled Light Field (DSLF)\nreconstruction. The ST-based DSLF reconstruction typically relies on an\niterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse\nregularization in shearlet domain, involving dozens of transformations between\nimage domain and shearlet domain, which are in general time-consuming. To\novercome this limitation, a novel learning-based ST approach, referred to as\nDeep Residual Shearlet Transform (DRST), is proposed in this paper.\nSpecifically, for an input sparsely-sampled EPI, DRST employs a deep fully\nConvolutional Neural Network (CNN) to predict the residuals of the shearlet\ncoefficients in shearlet domain in order to reconstruct a densely-sampled EPI\nin image domain. The DRST network is trained on synthetic Sparsely-Sampled\nLight Field (SSLF) data only by leveraging elaborately-designed masks.\nExperimental results on three challenging real-world light field evaluation\ndatasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the\nsuperiority of the proposed learning-based DRST approach over the\nnon-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a\n2.4x speedup over ST, at least.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:28:08 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Gao", "Yuan", ""], ["Bregovic", "Robert", ""], ["Koch", "Reinhard", ""], ["Gotchev", "Atanas", ""]]}, {"id": "2003.08897", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, and Hanqing\n  Lu", "title": "Normalized and Geometry-Aware Self-Attention Network for Image\n  Captioning", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention (SA) network has shown profound value in image captioning. In\nthis paper, we improve SA from two aspects to promote the performance of image\ncaptioning. First, we propose Normalized Self-Attention (NSA), a\nreparameterization of SA that brings the benefits of normalization inside SA.\nWhile normalization is previously only applied outside SA, we introduce a novel\nnormalization method and demonstrate that it is both possible and beneficial to\nperform it on the hidden activations inside SA. Second, to compensate for the\nmajor limit of Transformer that it fails to model the geometry structure of the\ninput objects, we propose a class of Geometry-aware Self-Attention (GSA) that\nextends SA to explicitly and efficiently consider the relative geometry\nrelations between the objects in the image. To construct our image captioning\nmodel, we combine the two modules and apply it to the vanilla self-attention\nnetwork. We extensively evaluate our proposals on MS-COCO image captioning\ndataset and superior results are achieved when comparing to state-of-the-art\napproaches. Further experiments on three challenging tasks, i.e. video\ncaptioning, machine translation, and visual question answering, show the\ngenerality of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 16:54:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["Yao", "Peng", ""], ["Lu", "Shichen", ""], ["Lu", "Hanqing", ""]]}, {"id": "2003.09249", "submitter": "Tho Nguyen Duc", "authors": "Phan Xuan Tan, Tho Nguyen Duc, Chanh Minh Tran, Eiji Kamioka", "title": "Continuous QoE Prediction Based on WaveNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous QoE prediction is crucial in the purpose of maximizing viewer\nsatisfaction, by which video service providers could improve the revenue.\nContinuously predicting QoE is challenging since it requires QoE models that\nare capable of capturing the complex dependencies among QoE influence factors.\nThe existing approaches that utilize Long-Short-Term-Memory (LSTM) network\nsuccessfully model such long-term dependencies, providing the superior QoE\nprediction performance. However, the inherent drawback of sequential computing\nof LSTM will result in high computational cost in training and prediction\ntasks. Recently, WaveNet, a deep neural network for generating raw audio\nwaveform, has been introduced. Immediately, it gains a great attention since it\nsuccessfully leverages the characteristic of parallel computing of causal\nconvolution and dilated convolution to deal with time-series data (e.g., audio\nsignal). Being inspired by the success of WaveNet, in this paper, we propose\nWaveNet-based QoE model for continuous QoE prediction in video streaming\nservices. The model is trained and tested upon on two publicly available\ndatabases, namely, LFOVIA Video QoE and LIVE Mobile Stall Video II. The\nexperimental results demonstrate that the proposed model outperforms the\nbaselines models in terms of processing time, while maintaining sufficient\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 13:06:55 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Tan", "Phan Xuan", ""], ["Duc", "Tho Nguyen", ""], ["Tran", "Chanh Minh", ""], ["Kamioka", "Eiji", ""]]}, {"id": "2003.09294", "submitter": "Yuan Gao", "authors": "Yuan Gao, Robert Bregovic and Atanas Gotchev", "title": "Self-Supervised Light Field Reconstruction Using Shearlet Transform and\n  Cycle Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image-based rendering approach using Shearlet Transform (ST) is one of\nthe state-of-the-art Densely-Sampled Light Field (DSLF) reconstruction methods.\nIt reconstructs Epipolar-Plane Images (EPIs) in image domain via an iterative\nregularization algorithm restoring their coefficients in shearlet domain.\nConsequently, the ST method tends to be slow because of the time spent on\ndomain transformations for dozens of iterations. To overcome this limitation,\nthis letter proposes a novel self-supervised DSLF reconstruction method,\nCycleST, which applies ST and cycle consistency to DSLF reconstruction.\nSpecifically, CycleST is composed of an encoder-decoder network and a residual\nlearning strategy that restore the shearlet coefficients of densely-sampled\nEPIs using EPI reconstruction and cycle consistency losses. Besides, CycleST is\na self-supervised approach that can be trained solely on Sparsely-Sampled Light\nFields (SSLFs) with small disparity ranges ($\\leqslant$ 8 pixels). Experimental\nresults of DSLF reconstruction on SSLFs with large disparity ranges (16 - 32\npixels) from two challenging real-world light field datasets demonstrate the\neffectiveness and efficiency of the proposed CycleST method. Furthermore,\nCycleST achieves ~ 9x speedup over ST, at least.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 14:27:38 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Gao", "Yuan", ""], ["Bregovic", "Robert", ""], ["Gotchev", "Atanas", ""]]}, {"id": "2003.09580", "submitter": "Tao Guo", "authors": "Tao Guo, Xikang Jiang, Bin Xiang, Lin Zhang", "title": "Edge-assisted Viewport Adaptive Scheme for real-time Omnidirectional\n  Video transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional applications are immersive and highly interactive, which can\nimprove the efficiency of remote collaborative work among factory workers. The\ntransmission of omnidirectional video (OV) is the most important step in\nimplementing virtual remote collaboration. Compared with the ordinary video\ntransmission, OV transmission requires more bandwidth, which is still a huge\nburden even under 5G networks. The tile-based scheme can reduce bandwidth\nconsumption. However, it neither accurately obtain the field of view(FOV) area,\nnor difficult to support real-time OV streaming. In this paper, we propose an\nedge-assisted viewport adaptive scheme (EVAS-OV) to reduce bandwidth\nconsumption during real-time OV transmission. First, EVAS-OV uses a Gated\nRecurrent Unit(GRU) model to predict users' viewport. Then, users were divided\ninto multicast clusters thereby further reducing the consumption of computing\nresources. EVAS-OV reprojects OV frames to accurately obtain users' FOV area\nfrom pixel level and adopt a redundant strategy to reduce the impact of\nviewport prediction errors. All computing tasks were offloaded to edge servers\nto reduce the transmission delay and improve bandwidth utilization.\nExperimental results show that EVAS-OV can save more than 60\\% of bandwidth\ncompared with the non-viewport adaptive scheme. Compared to a two-layer scheme\nwith viewport adaptive, EVAS-OV still saves 30\\% of bandwidth.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 05:48:43 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Guo", "Tao", ""], ["Jiang", "Xikang", ""], ["Xiang", "Bin", ""], ["Zhang", "Lin", ""]]}, {"id": "2003.10082", "submitter": "Th\\'eo Taburet", "authors": "Th\\'eo Taburet, Patrick Bas, Wadih Sawaya, Remi Cogranne", "title": "JPEG Steganography and Synchronization of DCT Coefficients for a Given\n  Development Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper proposes to use the statistical analysis of the correlation\nbetween DCT coefficients to design a new synchronization strategy that can be\nused for cost-based steganographic schemes in the JPEG domain. First, an\nanalysis is performed on the covariance matrix of DCT coefficients of\nneighboring blocks after a development similar to the one used to generate\nBossBase. This analysis exhibits groups of uncorrelated coefficients: 4 groups\nper block and 2 groups of uncorrelated diagonal neighbors together with groups\nof mutually correlated coefficients groups of 6 coefficients per blocs and 8\ncoefficients between 2 adjacent blocks. Using the uncorrelated groups, an\nembedding scheme can be designed using only 8 disjoint lattices. The cost map\nfor each lattice is updated firstly by using an implicit underlying Gaussian\ndistribution with a variance directly computed from the embedding costs, and\nsecondly by deriving conditional distributions from multivariate distributions.\nThe covariance matrix of these distributions takes into account both the\ncorrelations exhibited by the analysis of the covariance matrix and the\nvariance derived from the cost. This synchronization scheme enables to obtain a\ngain of PE of 5% at QF 95 for an embedding rate close to 0.3 bnzac coefficient\nusing DCTR feature sets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 04:52:21 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 00:45:37 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Taburet", "Th\u00e9o", ""], ["Bas", "Patrick", ""], ["Sawaya", "Wadih", ""], ["Cogranne", "Remi", ""]]}, {"id": "2003.10414", "submitter": "Venkatesh Shenoy Kadandale", "authors": "Venkatesh S. Kadandale, Juan F. Montesinos, Gloria Haro, Emilia\n  G\\'omez", "title": "Multi-channel U-Net for Music Source Separation", "comments": "The paper has been accepted at IEEE MMSP2020. Project Page:\n  https://vskadandale.github.io/multi-channel-unet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fairly straightforward approach for music source separation is to train\nindependent models, wherein each model is dedicated for estimating only a\nspecific source. Training a single model to estimate multiple sources generally\ndoes not perform as well as the independent dedicated models. However,\nConditioned U-Net (C-U-Net) uses a control mechanism to train a single model\nfor multi-source separation and attempts to achieve a performance comparable to\nthat of the dedicated models. We propose a multi-channel U-Net (M-U-Net)\ntrained using a weighted multi-task loss as an alternative to the C-U-Net. We\ninvestigate two weighting strategies for our multi-task loss: 1) Dynamic\nWeighted Average (DWA), and 2) Energy Based Weighting (EBW). DWA determines the\nweights by tracking the rate of change of loss of each task during training.\nEBW aims to neutralize the effect of the training bias arising from the\ndifference in energy levels of each of the sources in a mixture. Our methods\nprovide three-fold advantages compared to C-UNet: 1) Fewer effective training\niterations per epoch, 2) Fewer trainable network parameters (no control\nparameters), and 3) Faster processing at inference. Our methods achieve\nperformance comparable to that of C-U-Net and the dedicated U-Nets at a much\nlower training cost.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:42:35 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 10:14:44 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 13:37:58 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Kadandale", "Venkatesh S.", ""], ["Montesinos", "Juan F.", ""], ["Haro", "Gloria", ""], ["G\u00f3mez", "Emilia", ""]]}, {"id": "2003.10421", "submitter": "Eric M\\\"uller-Budack", "authors": "Eric M\\\"uller-Budack, Jonas Theiner, Sebastian Diering, Maximilian\n  Idahl, Ralph Ewerth", "title": "Multimodal Analytics for Real-world News using Measures of Cross-modal\n  Entity Consistency", "comments": "Accepted for publication in: International Conference on Multimedia\n  Retrieval (ICMR), Dublin, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web has become a popular source for gathering information and\nnews. Multimodal information, e.g., enriching text with photos, is typically\nused to convey the news more effectively or to attract attention. Photo content\ncan range from decorative, depict additional important information, or can even\ncontain misleading information. Therefore, automatic approaches to quantify\ncross-modal consistency of entity representation can support human assessors to\nevaluate the overall multimodal message, for instance, with regard to bias or\nsentiment. In some cases such measures could give hints to detect fake news,\nwhich is an increasingly important topic in today's society. In this paper, we\nintroduce a novel task of cross-modal consistency verification in real-world\nnews and present a multimodal approach to quantify the entity coherence between\nimage and text. Named entity linking is applied to extract persons, locations,\nand events from news texts. Several measures are suggested to calculate\ncross-modal similarity for these entities using state of the art approaches. In\ncontrast to previous work, our system automatically gathers example data from\nthe Web and is applicable to real-world news. Results on two novel datasets\nthat cover different languages, topics, and domains demonstrate the feasibility\nof our approach. Datasets and code are publicly available to foster research\ntowards this new direction.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:49:06 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 09:22:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["M\u00fcller-Budack", "Eric", ""], ["Theiner", "Jonas", ""], ["Diering", "Sebastian", ""], ["Idahl", "Maximilian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2003.10546", "submitter": "Hyunji Chung", "authors": "Hyunji Chung, Jungheum Park, Sangjin Lee", "title": "Forensic Analysis of Residual Information in Adobe PDF Files", "comments": "11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, as electronic files include personal records and business\nactivities, these files can be used as important evidences in a digital\nforensic investigation process. In general, the data that can be verified using\nits own application programs is largely used in the investigation of document\nfiles. However, in the case of the PDF file that has been largely used at the\npresent time, certain data, which include the data before some modifications,\nexist in electronic document files unintentionally. Because such residual\ninformation may present the writing process of a file, it can be usefully used\nin a forensic viewpoint. This paper introduces why the residual information is\nstored inside the PDF file and explains a way to extract the information. In\naddition, we demonstrate the attributes of PDF files can be used to hide data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 05:47:05 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Chung", "Hyunji", ""], ["Park", "Jungheum", ""], ["Lee", "Sangjin", ""]]}, {"id": "2003.10820", "submitter": "Emna Baccour", "authors": "Emna Baccour and Aiman Erbad and Kashif Bilal and Amr Mohamed and\n  Mohsen Guizani and Mounir Hamdi", "title": "FacebookVideoLive18: A Live Video Streaming Dataset for Streams Metadata\n  and Online Viewers Locations", "comments": "Manuscript accepted in ICIOT 2020", "journal-ref": "ICIOT 2020", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in personal smart devices and pervasive network\nconnectivity, users are no longer passive content consumers, but also\ncontributors in producing new contents. This expansion in live services\nrequires a detailed analysis of broadcasters' and viewers' behavior to maximize\nusers' Quality of Experience (QoE). In this paper, we present a dataset\ngathered from one of the popular live streaming platforms: Facebook. In this\ndataset, we stored more than 1,500,000 live stream records collected in June\nand July 2018. These data include public live videos from all over the world.\nHowever, Facebook live API does not offer the possibility to collect online\nvideos with their fine grained data. The API allows to get the general data of\na stream, only if we know its ID (identifier). Therefore, using the live map\nwebsite provided by Facebook and showing the locations of online streams and\nlocations of viewers, we extracted video IDs and different coordinates along\nwith general metadata. Then, having these IDs and using the API, we can collect\nthe fine grained metadata of public videos that might be useful for the\nresearch community. We also present several preliminary analyses to describe\nand identify the patterns of the streams and viewers. Such fine grained details\nwill enable the multimedia community to recreate real-world scenarios\nparticularly for resource allocation, caching, computation, and transcoding in\nedge networks. Existing datasets do not provide the locations of the viewers,\nwhich limits the efforts made to allocate the multimedia resources as close as\npossible to viewers and to offer better QoE.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:15:31 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Bilal", "Kashif", ""], ["Mohamed", "Amr", ""], ["Guizani", "Mohsen", ""], ["Hamdi", "Mounir", ""]]}, {"id": "2003.11100", "submitter": "Helard Becerra Martinez Dr", "authors": "Helard Martinez and Andrew Hines and Mylene C. Q. Farias", "title": "How deep is your encoder: an analysis of features descriptors for an\n  autoencoder-based audio-visual quality metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of audio-visual quality assessment models poses a number of\nchallenges in order to obtain accurate predictions. One of these challenges is\nthe modelling of the complex interaction that audio and visual stimuli have and\nhow this interaction is interpreted by human users. The No-Reference\nAudio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with\nthis problem from a machine learning perspective. The metric receives two sets\nof audio and video features descriptors and produces a low-dimensional set of\nfeatures used to predict the audio-visual quality. A basic implementation of\nNAViDAd was able to produce accurate predictions tested with a range of\ndifferent audio-visual databases. The current work performs an ablation study\non the base architecture of the metric. Several modules are removed or\nre-trained using different configurations to have a better understanding of the\nmetric functionality. The results presented in this study provided important\nfeedback that allows us to understand the real capacity of the metric's\narchitecture and eventually develop a much better audio-visual quality metric.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 20:15:12 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Martinez", "Helard", ""], ["Hines", "Andrew", ""], ["Farias", "Mylene C. Q.", ""]]}, {"id": "2003.11300", "submitter": "Tobias Hossfeld", "authors": "Babak Naderi, Tobias Hossfeld, Matthias Hirth, Florian Metzger,\n  Sebastian M\\\"oller, Rafael Zequeira Jim\\'enez", "title": "Impact of the Number of Votes on the Reliability and Validity of\n  Subjective Speech Quality Assessment in the Crowdsourcing Approach", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": "10.1109/QoMEX48832.2020.9123115", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subjective quality of transmitted speech is traditionally assessed in a\ncontrolled laboratory environment according to ITU-T Rec. P.800. In turn, with\ncrowdsourcing, crowdworkers participate in a subjective online experiment using\ntheir own listening device, and in their own working environment. Despite such\nless controllable conditions, the increased use of crowdsourcing micro-task\nplatforms for quality assessment tasks has pushed a high demand for\nstandardized methods, resulting in ITU-T Rec. P.808. This work investigates the\nimpact of the number of judgments on the reliability and the validity of\nquality ratings collected through crowdsourcing-based speech quality\nassessments, as an input to ITU-T Rec. P.808 . Three crowdsourcing experiments\non different platforms were conducted to evaluate the overall quality of three\ndifferent speech datasets, using the Absolute Category Rating procedure. For\neach dataset, the Mean Opinion Scores (MOS) are calculated using differing\nnumbers of crowdsourcing judgements. Then the results are compared to MOS\nvalues collected in a standard laboratory experiment, to assess the validity of\ncrowdsourcing approach as a function of number of votes. In addition, the\nreliability of the average scores is analyzed by checking inter-rater\nreliability, gain in certainty, and the confidence of the MOS. The results\nprovide a suggestion on the required number of votes per condition, and allow\nto model its impact on validity and reliability.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 10:15:27 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Naderi", "Babak", ""], ["Hossfeld", "Tobias", ""], ["Hirth", "Matthias", ""], ["Metzger", "Florian", ""], ["M\u00f6ller", "Sebastian", ""], ["Jim\u00e9nez", "Rafael Zequeira", ""]]}, {"id": "2003.12265", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Sergiu Gordea, Peter Knees", "title": "Unsupervised Cross-Modal Audio Representation Learning from Unstructured\n  Multilingual Text", "comments": "This is the long version of our SAC2020 poster presentation", "journal-ref": "In Proceedings of the 35th ACM/SIGAPP Symposium On Applied\n  Computing (SAC2020), March 30-April 3, 2020, Brno, Czech Republic", "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to unsupervised audio representation learning. Based\non a triplet neural network architecture, we harnesses semantically related\ncross-modal information to estimate audio track-relatedness. By applying Latent\nSemantic Indexing (LSI) we embed corresponding textual information into a\nlatent vector space from which we derive track relatedness for online triplet\nselection. This LSI topic modelling facilitates fine-grained selection of\nsimilar and dissimilar audio-track pairs to learn the audio representation\nusing a Convolution Recurrent Neural Network (CRNN). By this we directly\nproject the semantic context of the unstructured text modality onto the learned\nrepresentation space of the audio modality without deriving structured\nground-truth annotations from it. We evaluate our approach on the Europeana\nSounds collection and show how to improve search in digital audio libraries by\nharnessing the multilingual meta-data provided by numerous European digital\nlibraries. We show that our approach is invariant to the variety of annotation\nstyles as well as to the different languages of this collection. The learned\nrepresentations perform comparable to the baseline of handcrafted features,\nrespectively exceeding this baseline in similarity retrieval precision at\nhigher cut-offs with only 15\\% of the baseline's feature vector length.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 07:37:15 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Schindler", "Alexander", ""], ["Gordea", "Sergiu", ""], ["Knees", "Peter", ""]]}, {"id": "2003.12428", "submitter": "Yurui Ming", "authors": "Yurui Ming, Weiping Ding, Zehong Cao, Chin-Teng Lin", "title": "A General Approach for Using Deep Neural Network for Digital\n  Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technologies of the Internet of Things (IoT) facilitate digital contents such\nas images being acquired in a massive way. However, consideration from the\nprivacy or legislation perspective still demands the need for intellectual\ncontent protection. In this paper, we propose a general deep neural network\n(DNN) based watermarking method to fulfill this goal. Instead of training a\nneural network for protecting a specific image, we train on an image set and\nuse the trained model to protect a distinct test image set in a bulk manner.\nRespective evaluations both from the subjective and objective aspects confirm\nthe supremacy and practicability of our proposed method. To demonstrate the\nrobustness of this general neural watermarking mechanism, commonly used\nmanipulations are applied to the watermarked image to examine the corresponding\nextracted watermark, which still retains sufficient recognizable traits. To the\nbest of our knowledge, we are the first to propose a general way to perform\nwatermarking using DNN. Considering its performance and economy, it is\nconcluded that subsequent studies that generalize our work on utilizing DNN for\nintellectual content protection is a promising research trend.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 06:22:04 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ming", "Yurui", ""], ["Ding", "Weiping", ""], ["Cao", "Zehong", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "2003.12742", "submitter": "Tobias Hossfeld", "authors": "Tobias Hossfeld, Poul E. Heegaard, Martin Varela, Lea Skorin-Kapov,\n  Markus Fiedler", "title": "From QoS Distributions to QoE Distributions: a System's Perspective", "comments": "4th International Workshop on Quality of Experience Management (QoE\n  Management 2020), featured by IEEE Conference on Network Softwarization (IEEE\n  NetSoft 2020), Ghent, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of QoE management, network and service providers commonly rely\non models that map system QoS conditions (e.g., system response time, paket\nloss, etc.) to estimated end user QoE values. Observable QoS conditions in the\nsystem may be assumed to follow a certain distribution, meaning that different\nend users will experience different conditions. On the other hand, drawing from\nthe results of subjective user studies, we know that user diversity leads to\ndistributions of user scores for any given test conditions (in this case\nreferring to the QoS parameters of interest). Our previous studies have shown\nthat to correctly derive various QoE metrics (e.g., Mean Opinion Score (MOS),\nquantiles, probability of users rating \"good or better\", etc.) in a system\nunder given conditions, there is a need to consider rating distributions\nobtained from user studies, which are often times not available. In this paper\nwe extend these findings to show how to approximate user rating distributions\ngiven a QoS-to-MOS mapping function and second order statistics. Such a user\nrating distribution may then be combined with a QoS distribution observed in a\nsystem to finally derive corresponding distributions of QoE scores. We provide\ntwo examples to illustrate this process: 1) analytical results using a Web QoE\nmodel relating waiting times to QoE, and 2) numerical results using\nmeasurements relating packet losses to video stall pattern, which are in turn\nmapped to QoE estimates. The results in this paper provide a solution to the\nproblem of understanding the QoE distribution in a system, in cases where the\nnecessary data is not directly available in the form of models going beyond the\nMOS, or where the full details of subjective experiments are not available.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 08:31:30 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hossfeld", "Tobias", ""], ["Heegaard", "Poul E.", ""], ["Varela", "Martin", ""], ["Skorin-Kapov", "Lea", ""], ["Fiedler", "Markus", ""]]}, {"id": "2003.13217", "submitter": "Shivam Agarwal Mr", "authors": "Shivam Agarwal and Siddarth Venkatraman", "title": "Deep Residual Neural Networks for Image in Speech Steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is the art of hiding a secret message inside a publicly visible\ncarrier message. Ideally, it is done without modifying the carrier, and with\nminimal loss of information in the secret message. Recently, various deep\nlearning based approaches to steganography have been applied to different\nmessage types. We propose a deep learning based technique to hide a source RGB\nimage message inside finite length speech segments without perceptual loss. To\nachieve this, we train three neural networks; an encoding network to hide the\nmessage in the carrier, a decoding network to reconstruct the message from the\ncarrier and an additional image enhancer network to further improve the\nreconstructed message. We also discuss future improvements to the algorithm\nproposed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 04:49:45 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Agarwal", "Shivam", ""], ["Venkatraman", "Siddarth", ""]]}, {"id": "2003.13669", "submitter": "Alireza Javaheri", "authors": "Alireza Javaheri, Catarina Brites, Fernando Pereira and Joao Ascenso", "title": "A generalized Hausdorff distance based quality metric for point cloud\n  geometry", "comments": "This article is accepted to 12th International Conference on Quality\n  of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": "10.1109/QoMEX48832.2020.9123087", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable quality assessment of decoded point cloud geometry is essential to\nevaluate the compression performance of emerging point cloud coding solutions\nand guarantee some target quality of experience. This paper proposes a novel\npoint cloud geometry quality assessment metric based on a generalization of the\nHausdorff distance. To achieve this goal, the so-called generalized Hausdorff\ndistance for multiple rankings is exploited to identify the best performing\nquality metric in terms of correlation with the MOS scores obtained from a\nsubjective test campaign. The experimental results show that the quality metric\nderived from the classical Hausdorff distance leads to low objective-subjective\ncorrelation and, thus, fails to accurately evaluate the quality of decoded\npoint clouds for emerging codecs. However, the quality metric derived from the\ngeneralized Hausdorff distance with an appropriately selected ranking,\noutperforms the MPEG adopted geometry quality metrics when decoded point clouds\nwith different types of coding distortions are considered.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:53:02 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Javaheri", "Alireza", ""], ["Brites", "Catarina", ""], ["Pereira", "Fernando", ""], ["Ascenso", "Joao", ""]]}, {"id": "2003.13684", "submitter": "Hai Dong", "authors": "Tooba Aamir, Hai Dong, and Athman Bouguettaya", "title": "Social-Sensor Composition for Tapestry Scenes", "comments": "15 pages. IEEE Transactions on Services Computing", "journal-ref": null, "doi": "10.1109/TSC.2020.2974741", "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive use of social media platforms and overwhelming amounts of\nimagery data creates unique opportunities for sensing, gathering and sharing\ninformation about events. One of its potential applications is to leverage\ncrowdsourced social media images to create a tapestry scene for scene analysis\nof designated locations and time intervals. The existing attempts however\nignore the temporal-semantic relevance and spatio-temporal evolution of the\nimages and direction-oriented scene reconstruction. We propose a novel\nsocial-sensor cloud (SocSen) service composition approach to form tapestry\nscenes for scene analysis. The novelty lies in utilising images and image\nmeta-information to bypass expensive traditional image processing techniques to\nreconstruct scenes. Metadata, such as geolocation, time and angle of view of an\nimage are modelled as non-functional attributes of a SocSen service. Our major\ncontribution lies on proposing a context and direction-aware spatio-temporal\nclustering and recommendation approach for selecting a set of temporally and\nsemantically similar services to compose the best available SocSen services.\nAnalytical results based on real datasets are presented to demonstrate the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 00:07:34 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Aamir", "Tooba", ""], ["Dong", "Hai", ""], ["Bouguettaya", "Athman", ""]]}]