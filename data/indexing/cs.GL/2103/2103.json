[{"id": "2103.06304", "submitter": "Letitia Parcalabescu", "authors": "Letitia Parcalabescu, Nils Trost, Anette Frank", "title": "What is Multimodality?", "comments": "Paper accepted for publication at MMSR 2021; 10 pages, 5 figures", "journal-ref": "Proceedings of the 1st Workshop on Multimodal Semantic\n  Representations (MMSR), 2021, Groningen, Netherlands (Online), Association\n  for Computational Linguistics, p. 1--10", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.GL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last years have shown rapid developments in the field of multimodal\nmachine learning, combining e.g., vision, text or speech. In this position\npaper we explain how the field uses outdated definitions of multimodality that\nprove unfit for the machine learning era. We propose a new task-relative\ndefinition of (multi)modality in the context of multimodal machine learning\nthat focuses on representations and information that are relevant for a given\nmachine learning task. With our new definition of multimodality we aim to\nprovide a missing foundation for multimodal research, an important component of\nlanguage grounding and a crucial milestone towards NLU.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 19:14:07 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 09:17:44 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:32:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Parcalabescu", "Letitia", ""], ["Trost", "Nils", ""], ["Frank", "Anette", ""]]}, {"id": "2103.06312", "submitter": "Daniel Zhang", "authors": "Daniel Zhang, Saurabh Mishra, Erik Brynjolfsson, John Etchemendy, Deep\n  Ganguli, Barbara Grosz, Terah Lyons, James Manyika, Juan Carlos Niebles,\n  Michael Sellitto, Yoav Shoham, Jack Clark, Raymond Perrault", "title": "The AI Index 2021 Annual Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Welcome to the fourth edition of the AI Index Report. This year we\nsignificantly expanded the amount of data available in the report, worked with\na broader set of external organizations to calibrate our data, and deepened our\nconnections with the Stanford Institute for Human-Centered Artificial\nIntelligence (HAI). The AI Index Report tracks, collates, distills, and\nvisualizes data related to artificial intelligence. Its mission is to provide\nunbiased, rigorously vetted, and globally sourced data for policymakers,\nresearchers, executives, journalists, and the general public to develop\nintuitions about the complex field of AI. The report aims to be the most\ncredible and authoritative source for data and insights about AI in the world.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:29:44 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Daniel", ""], ["Mishra", "Saurabh", ""], ["Brynjolfsson", "Erik", ""], ["Etchemendy", "John", ""], ["Ganguli", "Deep", ""], ["Grosz", "Barbara", ""], ["Lyons", "Terah", ""], ["Manyika", "James", ""], ["Niebles", "Juan Carlos", ""], ["Sellitto", "Michael", ""], ["Shoham", "Yoav", ""], ["Clark", "Jack", ""], ["Perrault", "Raymond", ""]]}]