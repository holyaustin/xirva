[{"id": "1306.0159", "submitter": "Scott Aaronson", "authors": "Scott Aaronson", "title": "The Ghost in the Quantum Turing Machine", "comments": "85 pages (more a short book than a long essay!), 2 figures. To appear\n  in \"The Once and Future Turing: Computing the World,\" a collection edited by\n  S. Barry Cooper and Andrew Hodges. And yes, I know Turing is 101 by now. v2:\n  Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.GL physics.hist-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In honor of Alan Turing's hundredth birthday, I unwisely set out some\nthoughts about one of Turing's obsessions throughout his life, the question of\nphysics and free will. I focus relatively narrowly on a notion that I call\n\"Knightian freedom\": a certain kind of in-principle physical unpredictability\nthat goes beyond probabilistic unpredictability. Other, more metaphysical\naspects of free will I regard as possibly outside the scope of science. I\nexamine a viewpoint, suggested independently by Carl Hoefer, Cristi Stoica, and\neven Turing himself, that tries to find scope for \"freedom\" in the universe's\nboundary conditions rather than in the dynamical laws. Taking this viewpoint\nseriously leads to many interesting conceptual problems. I investigate how far\none can go toward solving those problems, and along the way, encounter (among\nother things) the No-Cloning Theorem, the measurement problem, decoherence,\nchaos, the arrow of time, the holographic principle, Newcomb's paradox,\nBoltzmann brains, algorithmic information theory, and the Common Prior\nAssumption. I also compare the viewpoint explored here to the more radical\nspeculations of Roger Penrose. The result of all this is an unusual perspective\non time, quantum mechanics, and causation, of which I myself remain skeptical,\nbut which has several appealing features. Among other things, it suggests\ninteresting empirical questions in neuroscience, physics, and cosmology; and\ntakes a millennia-old philosophical debate into some underexplored territory.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 00:36:34 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2013 16:52:29 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Aaronson", "Scott", ""]]}, {"id": "1306.3295", "submitter": "Jeff M Phillips", "authors": "Mary Hall, Robert M. Kirby, Feifei Li, Miriah Meyer, Valerio Pascucci,\n  Jeff M. Phillips, Rob Ricci, Jacobus Van der Merwe, Suresh Venkatasubramanian", "title": "Rethinking Abstractions for Big Data: Why, Where, How, and What", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "UUCS-13-002", "categories": "cs.GL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data refers to large and complex data sets that, under existing\napproaches, exceed the capacity and capability of current compute platforms,\nsystems software, analytical tools and human understanding. Numerous lessons on\nthe scalability of big data can already be found in asymptotic analysis of\nalgorithms and from the high-performance computing (HPC) and applications\ncommunities. However, scale is only one aspect of current big data trends;\nfundamentally, current and emerging problems in big data are a result of\nunprecedented complexity--in the structure of the data and how to analyze it,\nin dealing with unreliability and redundancy, in addressing the human factors\nof comprehending complex data sets, in formulating meaningful analyses, and in\nmanaging the dense, power-hungry data centers that house big data.\n  The computer science solution to complexity is finding the right\nabstractions, those that hide as much triviality as possible while revealing\nthe essence of the problem that is being addressed. The \"big data challenge\"\nhas disrupted computer science by stressing to the very limits the familiar\nabstractions which define the relevant subfields in data analysis, data\nmanagement and the underlying parallel systems. As a result, not enough of\nthese challenges are revealed by isolating abstractions in a traditional\nsoftware stack or standard algorithmic and analytical techniques, and attempts\nto address complexity either oversimplify or require low-level management of\ndetails. The authors believe that the abstractions for big data need to be\nrethought, and this reorganization needs to evolve and be sustained through\ncontinued cross-disciplinary collaboration.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 05:11:34 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Hall", "Mary", ""], ["Kirby", "Robert M.", ""], ["Li", "Feifei", ""], ["Meyer", "Miriah", ""], ["Pascucci", "Valerio", ""], ["Phillips", "Jeff M.", ""], ["Ricci", "Rob", ""], ["Van der Merwe", "Jacobus", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1306.5215", "submitter": "Andreas Tolk", "authors": "Andreas Tolk, Saikou Y. Diallo, Jose J. Padilla, Ross Gore", "title": "Epistemology of Modeling and Simulation: How can we gain Knowledge from\n  Simulations?", "comments": "MODSIM World 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epistemology is the branch of philosophy that deals with gaining knowledge.\nIt is closely related to ontology. The branch that deals with questions like\n\"What is real?\" and \"What do we know?\" as it provides these components. When\nusing modeling and simulation, we usually imply that we are doing so to either\napply knowledge, in particular when we are using them for training and\nteaching, or that we want to gain new knowledge, for example when doing\nanalysis or conducting virtual experiments. This paper looks at the history of\nscience to give a context to better cope with the question, how we can gain\nknowledge from simulation. It addresses aspects of computability and the\ngeneral underlying mathematics, and applies the findings to validation and\nverification and development of federations. As simulations are understood as\ncomputable executable hypotheses, validation can be understood as hypothesis\ntesting and theory building. The mathematical framework allows furthermore\naddressing some challenges when developing federations and the potential\nintroduction of contradictions when composing different theories, as they are\nrepresented by the federated simulation systems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 19:15:51 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Tolk", "Andreas", ""], ["Diallo", "Saikou Y.", ""], ["Padilla", "Jose J.", ""], ["Gore", "Ross", ""]]}]