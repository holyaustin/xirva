[{"id": "2101.02342", "submitter": "Zhabiz Gharibshah", "authors": "Zhabiz Gharibshah, Xingquan Zhu", "title": "User Response Prediction in Online Advertising", "comments": "ACM Computing Surveys (CSUR), 2021, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online advertising, as the vast market, has gained significant attention in\nvarious platforms ranging from search engines, third-party websites, social\nmedia, and mobile apps. The prosperity of online campaigns is a challenge in\nonline marketing and is usually evaluated by user response through different\nmetrics, such as clicks on advertisement (ad) creatives, subscriptions to\nproducts, purchases of items, or explicit user feedback through online surveys.\nRecent years have witnessed a significant increase in the number of studies\nusing computational approaches, including machine learning methods, for user\nresponse prediction. However, existing literature mainly focuses on\nalgorithmic-driven designs to solve specific challenges, and no comprehensive\nreview exists to answer many important questions. What are the parties involved\nin the online digital advertising eco-systems? What type of data are available\nfor user response prediction? How to predict user response in a reliable and/or\ntransparent way? In this survey, we provide a comprehensive review of user\nresponse prediction in online advertising and related recommender applications.\nOur essential goal is to provide a thorough understanding of online advertising\nplatforms, stakeholders, data availability, and typical ways of user response\nprediction. We propose a taxonomy to categorize state-of-the-art user response\nprediction methods, primarily focus on the current progress of machine learning\nmethods used in different online platforms. In addition, we also review\napplications of user response prediction, benchmark datasets, and open-source\ncodes in the field.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:00:44 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 00:50:09 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gharibshah", "Zhabiz", ""], ["Zhu", "Xingquan", ""]]}, {"id": "2101.09385", "submitter": "Joshua Kroll", "authors": "Joshua A. Kroll", "title": "Outlining Traceability: A Principle for Operationalizing Accountability\n  in Computing Systems", "comments": "To be published in the Proceedings of the 2021 ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT'21)", "journal-ref": null, "doi": "10.1145/3442188.3445937", "report-no": null, "categories": "cs.CY cs.AI cs.GL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accountability is widely understood as a goal for well governed computer\nsystems, and is a sought-after value in many governance contexts. But how can\nit be achieved? Recent work on standards for governable artificial intelligence\nsystems offers a related principle: traceability. Traceability requires\nestablishing not only how a system worked but how it was created and for what\npurpose, in a way that explains why a system has particular dynamics or\nbehaviors. It connects records of how the system was constructed and what the\nsystem did mechanically to the broader goals of governance, in a way that\nhighlights human understanding of that mechanical operation and the decision\nprocesses underlying it. We examine the various ways in which the principle of\ntraceability has been articulated in AI principles and other policy documents\nfrom around the world, distill from these a set of requirements on software\nsystems driven by the principle, and systematize the technologies available to\nmeet those requirements. From our map of requirements to supporting tools,\ntechniques, and procedures, we identify gaps and needs separating what\ntraceability requires from the toolbox available for practitioners. This map\nreframes existing discussions around accountability and transparency, using the\nprinciple of traceability to show how, when, and why transparency can be\ndeployed to serve accountability goals and thereby improve the normative\nfidelity of systems and their development processes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 00:13:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kroll", "Joshua A.", ""]]}, {"id": "2101.10098", "submitter": "Michael Lissack", "authors": "Michael Lissack", "title": "The Slodderwetenschap (Sloppy Science) of Stochastic Parrots -- A Plea\n  for Science to NOT take the Route Advocated by Gebru and Bender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.GL physics.hist-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article is a position paper written in reaction to the now-infamous\npaper titled \"On the Dangers of Stochastic Parrots: Can Language Models Be Too\nBig?\" by Timnit Gebru, Emily Bender, and others who were, as of the date of\nthis writing, still unnamed. I find the ethics of the Parrot Paper lacking, and\nin that lack, I worry about the direction in which computer science, machine\nlearning, and artificial intelligence are heading. At best, I would describe\nthe argumentation and evidentiary practices embodied in the Parrot Paper as\nSlodderwetenschap (Dutch for Sloppy Science) -- a word which the academic world\nlast widely used in conjunction with the Diederik Stapel affair in psychology\n[2]. What is missing in the Parrot Paper are three critical elements: 1)\nacknowledgment that it is a position paper/advocacy piece rather than research,\n2) explicit articulation of the critical presuppositions, and 3) explicit\nconsideration of cost/benefit trade-offs rather than a mere recitation of\npotential \"harms\" as if benefits did not matter. To leave out these three\nelements is not good practice for either science or research.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:55:09 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lissack", "Michael", ""]]}]