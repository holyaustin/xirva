[{"id": "1608.00177", "submitter": "EPTCS", "authors": "Thomas Brihaye, Beno\\^it Delahaye, Lo\\\"ig Jezequel, Nicolas Markey,\n  Ji\\v{r}\\'i Srba", "title": "Proceedings Cassting Workshop on Games for the Synthesis of Complex\n  Systems and 3rd International Workshop on Synthesis of Complex Parameters", "comments": null, "journal-ref": "EPTCS 220, 2016", "doi": "10.4204/EPTCS.220", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the joint proceedings of the Workshop on Games for the\nSynthesis of Complex Systems (CASSTING'16) and of the 3rd International\nWorkshop on Synthesis of Complex Parameters (SynCoP'16). The workshops were\nheld in Eindhoven, The Netherlands, as satellite events of the 19th European\nJoint Conferences on Theory and Practice of Software (ETAPS'16). Both workshops\nare closely related in their topics as well as target audience and they shared\na joint invited talk given by Giorgio Delzanno.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 01:53:16 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Brihaye", "Thomas", ""], ["Delahaye", "Beno\u00eet", ""], ["Jezequel", "Lo\u00efg", ""], ["Markey", "Nicolas", ""], ["Srba", "Ji\u0159\u00ed", ""]]}, {"id": "1608.00255", "submitter": "Marek Zawadowski", "authors": "Justyna Grudzinska and Marek Zawadowski", "title": "Continuation semantics for multi-quantifier sentences: operation-based\n  approaches", "comments": "19 pages, corrections in the table on page 3. arXiv admin note: text\n  overlap with arXiv:1605.03981", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical scope-assignment strategies for multi-quantifier sentences involve\nquantifier phrase (QP)-movement. More recent continuation-based approaches\nprovide a compelling alternative, for they interpret QP's in situ - without\nresorting to Logical Forms or any structures beyond the overt syntax. The\ncontinuation-based strategies can be divided into two groups: those that locate\nthe source of scope-ambiguity in the rules of semantic composition and those\nthat attribute it to the lexical entries for the quantifier words. In this\npaper, we focus on the former operation-based approaches and the nature of the\nsemantic operations involved. More specifically, we discuss three such possible\noperation-based strategies for multi-quantifier sentences, together with their\nrelative merits and costs.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 20:01:04 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 22:50:41 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Grudzinska", "Justyna", ""], ["Zawadowski", "Marek", ""]]}, {"id": "1608.00533", "submitter": "Joao Rasga", "authors": "J. Rasga, C. Sernadas, P. Mateus and A. Sernadas", "title": "Decision and optimization problems in the Unreliable-Circuit Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ambition constrained validity and the model witness problems in the logic\nUCL, for reasoning about circuits with unreliable gates, are analyzed.\nMoreover, two additional problems, motivated by the applications, are studied.\nOne consists of finding bounds on the reliability rate of the gates that ensure\nthat a given circuit has an intended success rate. The other consists of\nfinding a reliability rate of the gates that maximizes the success rate of a\ngiven circuit. Sound and complete algorithms are developed for these problems\nand their computational complexity is studied.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:41:44 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Rasga", "J.", ""], ["Sernadas", "C.", ""], ["Mateus", "P.", ""], ["Sernadas", "A.", ""]]}, {"id": "1608.00653", "submitter": "EPTCS", "authors": "Benedikt Br\\\"utsch (RWTH Aachen University), Wolfgang Thomas (RWTH\n  Aachen University)", "title": "Playing Games in the Baire Space", "comments": "In Proceedings Cassting'16/SynCoP'16, arXiv:1608.00177", "journal-ref": "EPTCS 220, 2016, pp. 13-25", "doi": "10.4204/EPTCS.220.2", "report-no": null, "categories": "cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve a generalized version of Church's Synthesis Problem where a play is\ngiven by a sequence of natural numbers rather than a sequence of bits; so a\nplay is an element of the Baire space rather than of the Cantor space. Two\nplayers Input and Output choose natural numbers in alternation to generate a\nplay. We present a natural model of automata (\"N-memory automata\") equipped\nwith the parity acceptance condition, and we introduce also the corresponding\nmodel of \"N-memory transducers\". We show that solvability of games specified by\nN-memory automata (i.e., existence of a winning strategy for player Output) is\ndecidable, and that in this case an N-memory transducer can be constructed that\nimplements a winning strategy for player Output.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 00:36:20 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Br\u00fctsch", "Benedikt", "", "RWTH Aachen University"], ["Thomas", "Wolfgang", "", "RWTH\n  Aachen University"]]}, {"id": "1608.00657", "submitter": "EPTCS", "authors": "Louise Foshammer (Aalborg University), Kim Guldstrand Larsen (Aalborg\n  University), Anders Mariegaard (Aalborg University)", "title": "Weighted Branching Simulation Distance for Parametric Weighted Kripke\n  Structures", "comments": "In Proceedings Cassting'16/SynCoP'16, arXiv:1608.00177", "journal-ref": "EPTCS 220, 2016, pp. 63-75", "doi": "10.4204/EPTCS.220.6", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns branching simulation for weighted Kripke structures with\nparametric weights. Concretely, we consider a weighted extension of branching\nsimulation where a single transitions can be matched by a sequence of\ntransitions while preserving the branching behavior. We relax this notion to\nallow for a small degree of deviation in the matching of weights, inducing a\ndirected distance on states. The distance between two states can be used\ndirectly to relate properties of the states within a sub-fragment of weighted\nCTL. The problem of relating systems thus changes to minimizing the distance\nwhich, in the general parametric case, corresponds to finding suitable\nparameter valuations such that one system can approximately simulate another.\nAlthough the distance considers a potentially infinite set of transition\nsequences we demonstrate that there exists an upper bound on the length of\nrelevant sequences, thereby establishing the computability of the distance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 00:37:12 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Foshammer", "Louise", "", "Aalborg University"], ["Larsen", "Kim Guldstrand", "", "Aalborg\n  University"], ["Mariegaard", "Anders", "", "Aalborg University"]]}, {"id": "1608.00658", "submitter": "EPTCS", "authors": "Bharath Siva Kumar Tati, Markus Siegle (UniBw)", "title": "Rate Reduction for State-labelled Markov Chains with Upper Time-bounded\n  CSL Requirements", "comments": "In Proceedings Cassting'16/SynCoP'16, arXiv:1608.00177", "journal-ref": "EPTCS 220, 2016, pp. 77-89", "doi": "10.4204/EPTCS.220.7", "report-no": null, "categories": "cs.SY cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents algorithms for identifying and reducing a dedicated set\nof controllable transition rates of a state-labelled continuous-time Markov\nchain model. The purpose of the reduction is to make states to satisfy a given\nrequirement, specified as a CSL upper time-bounded Until formula. We\ndistinguish two different cases, depending on the type of probability bound. A\nnatural partitioning of the state space allows us to develop possible\nsolutions, leading to simple algorithms for both cases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 00:37:24 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Tati", "Bharath Siva Kumar", "", "UniBw"], ["Siegle", "Markus", "", "UniBw"]]}, {"id": "1608.00678", "submitter": "Stephen Chong", "authors": "Stephen Chong, Joshua Guttman, Anupam Datta, Andrew Myers, Benjamin\n  Pierce, Patrick Schaumont, Tim Sherwood, Nickolai Zeldovich", "title": "Report on the NSF Workshop on Formal Methods for Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Report on the NSF Workshop on Formal Methods for Security, held 19-20\nNovember 2015.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 02:29:01 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 15:29:25 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Chong", "Stephen", ""], ["Guttman", "Joshua", ""], ["Datta", "Anupam", ""], ["Myers", "Andrew", ""], ["Pierce", "Benjamin", ""], ["Schaumont", "Patrick", ""], ["Sherwood", "Tim", ""], ["Zeldovich", "Nickolai", ""]]}, {"id": "1608.00692", "submitter": "George Barmpalias Dr", "authors": "George Barmpalias and Rodney G. Downey", "title": "Kobayashi compressibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kobayashi introduced a uniform notion of compressibility of infinite binary\nsequences in terms of relative Turing computations with sub-identity use of the\noracle. Kobayashi compressibility has remained a relatively obscure notion,\nwith the exception of some work on resource bounded Kolmogorov complexity. The\nmain goal of this note is to show that it is relevant to a number of topics in\ncurrent research on algorithmic randomness. We prove that Kobayashi\ncompressibility can be used in order to define Martin-Loef randomness, a strong\nversion of finite randomness and Kurtz randomness, strictly in terms of Turing\nreductions. Moreover these randomness notions naturally correspond to Turing\nreducibility, weak truth-table reducibility and truth-table reducibility\nrespectively. Finally we discuss Kobayashi's main result from his 1981\ntechnical report regarding the compressibility of computably enumerable sets,\nand provide additional related original results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 04:12:08 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 00:17:52 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Barmpalias", "George", ""], ["Downey", "Rodney G.", ""]]}, {"id": "1608.00730", "submitter": "Carmine Dodaro", "authors": "Carmine Dodaro, Philip Gasteiger, Nicola Leone, Benjamin Musitsch,\n  Francesco Ricca, Kostyantyn Shchekotykhin", "title": "Combining Answer Set Programming and Domain Heuristics for Solving Hard\n  Industrial Problems (Application Paper)", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a popular logic programming paradigm that has\nbeen applied for solving a variety of complex problems. Among the most\nchallenging real-world applications of ASP are two industrial problems defined\nby Siemens: the Partner Units Problem (PUP) and the Combined Configuration\nProblem (CCP). The hardest instances of PUP and CCP are out of reach for\nstate-of-the-art ASP solvers. Experiments show that the performance of ASP\nsolvers could be significantly improved by embedding domain-specific\nheuristics, but a proper effective integration of such criteria in\noff-the-shelf ASP implementations is not obvious. In this paper the combination\nof ASP and domain-specific heuristics is studied with the goal of effectively\nsolving real-world problem instances of PUP and CCP. As a byproduct of this\nactivity, the ASP solver WASP was extended with an interface that eases\nembedding new external heuristics in the solver. The evaluation shows that our\ndomain-heuristic-driven ASP solver finds solutions for all the real-world\ninstances of PUP and CCP ever provided by Siemens. This paper is under\nconsideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:36:08 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Dodaro", "Carmine", ""], ["Gasteiger", "Philip", ""], ["Leone", "Nicola", ""], ["Musitsch", "Benjamin", ""], ["Ricca", "Francesco", ""], ["Shchekotykhin", "Kostyantyn", ""]]}, {"id": "1608.00731", "submitter": "Carmine Dodaro", "authors": "Mario Alviano and Carmine Dodaro", "title": "Anytime answer set optimization via unsatisfiable core shrinking", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsatisfiable core analysis can boost the computation of optimum stable\nmodels for logic programs with weak constraints. However, current solvers\nemploying unsatisfiable core analysis either run to completion, or provide no\nsuboptimal stable models but the one resulting from the preliminary disjoint\ncores analysis. This drawback is circumvented here by introducing a progression\nbased shrinking of the analyzed unsatisfiable cores. In fact, suboptimal stable\nmodels are possibly found while shrinking unsatisfiable cores, hence resulting\ninto an anytime algorithm. Moreover, as confirmed empirically, unsatisfiable\ncore analysis also benefits from the shrinking process in terms of solved\ninstances. This paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:36:53 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Alviano", "Mario", ""], ["Dodaro", "Carmine", ""]]}, {"id": "1608.00867", "submitter": "Jorge Fandinno", "authors": "Jorge Fandinno", "title": "Deriving Conclusions From Non-Monotonic Cause-Effect Relations", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of Logic Programming (under stable models semantics)\nthat, not only allows concluding whether a true atom is a cause of another\natom, but also deriving new conclusions from these causal-effect relations.\nThis is expressive enough to capture informal rules like \"if some agent's\nactions $\\mathcal{A}$ have been necessary to cause an event $E$ then conclude\natom $caused(\\mathcal{A},E)$,\" something that, to the best of our knowledge,\nhad not been formalised in the literature. To this aim, we start from a first\nattempt that proposed extending the syntax of logic programs with so-called\ncausal literals. These causal literals are expressions that can be used in rule\nbodies and allow inspecting the derivation of some atom $A$ in the program with\nrespect to some query function $\\phi$. Depending on how these query functions\nare defined, we can model different types of causal relations such as\nsufficient, necessary or contributory causes, for instance. The initial\napproach was specifically focused on monotonic query functions. This was enough\nto cover sufficient cause-effect relations but, unfortunately, necessary and\ncontributory are essentially non-monotonic. In this work, we define a semantics\nfor non-monotonic causal literals showing that, not only extends the stable\nmodel semantics for normal logic programs, but also preserves many of its usual\ndesirable properties for the extended syntax. Using this new semantics, we\nprovide precise definitions of necessary and contributory causal relations and\nbriefly explain their behaviour on a pair of typical examples from the\nKnowledge Representation literature. (Under consideration for publication in\nTheory and Practice of Logic Programming)\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:30:34 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Fandinno", "Jorge", ""]]}, {"id": "1608.00870", "submitter": "Jorge Fandinno", "authors": "Pedro Cabalar and Jorge Fandinno", "title": "Justifications for Programs with Disjunctive and Causal-choice Rules", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an extension of the stable model semantics for\ndisjunctive logic programs where each true atom in a model is associated with\nan algebraic expression (in terms of rule labels) that represents its\njustifications. As in our previous work for non-disjunctive programs, these\njustifications are obtained in a purely semantic way, by algebraic operations\n(product, addition and application) on a lattice of causal values. Our new\ndefinition extends the concept of causal stable model to disjunctive logic\nprograms and satisfies that each (standard) stable model corresponds to a\ndisjoint class of causal stable models sharing the same truth assignments, but\npossibly varying the obtained explanations. We provide a pair of illustrative\nexamples showing the behaviour of the new semantics and discuss the need of\nintroducing a new type of rule, which we call causal-choice. This type of rule\nintuitively captures the idea of \"$A$ may cause $B$\" and, when causal\ninformation is disregarded, amounts to a usual choice rule under the standard\nstable model semantics. (Under consideration for publication in Theory and\nPractice of Logic Programming)\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:36:01 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 15:21:45 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Cabalar", "Pedro", ""], ["Fandinno", "Jorge", ""]]}, {"id": "1608.01338", "submitter": "Tiantian Gao", "authors": "Tiantian Gao, Paul Fodor and Michael Kifer", "title": "Paraconsistency and Word Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word puzzles and the problem of their representations in logic languages have\nreceived considerable attention in the last decade (Ponnuru et al. 2004;\nShapiro 2011; Baral and Dzifcak 2012; Schwitter 2013). Of special interest is\nthe problem of generating such representations directly from natural language\n(NL) or controlled natural language (CNL). An interesting variation of this\nproblem, and to the best of our knowledge, scarcely explored variation in this\ncontext, is when the input information is inconsistent. In such situations, the\nexisting encodings of word puzzles produce inconsistent representations and\nbreak down. In this paper, we bring the well-known type of paraconsistent\nlogics, called Annotated Predicate Calculus (APC) (Kifer and Lozinskii 1992),\nto bear on the problem. We introduce a new kind of non-monotonic semantics for\nAPC, called consistency preferred stable models and argue that it makes APC\ninto a suitable platform for dealing with inconsistency in word puzzles and,\nmore generally, in NL sentences. We also devise a number of general principles\nto help the user choose among the different representations of NL sentences,\nwhich might seem equivalent but, in fact, behave differently when inconsistent\ninformation is taken into account. These principles can be incorporated into\nexisting CNL translators, such as Attempto Controlled English (ACE) (Fuchs et\nal. 2008) and PENG Light (White and Schwitter 2009). Finally, we show that APC\nwith the consistency preferred stable model semantics can be equivalently\nembedded in ASP with preferences over stable models, and we use this embedding\nto implement this version of APC in Clingo (Gebser et al. 2011) and its Asprin\nadd-on (Brewka et al. 2015).\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 20:26:20 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 19:21:19 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Gao", "Tiantian", ""], ["Fodor", "Paul", ""], ["Kifer", "Michael", ""]]}, {"id": "1608.01401", "submitter": "EPTCS", "authors": "Daniela Ashoush (Univesity of Oxford), Bob Coecke (Univesity of\n  Oxford)", "title": "Dual Density Operators and Natural Language Meaning", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 1-10", "doi": "10.4204/EPTCS.221.1", "report-no": null, "categories": "cs.CL cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density operators allow for representing ambiguity about a vector\nrepresentation, both in quantum theory and in distributional natural language\nmeaning. Formally equivalently, they allow for discarding part of the\ndescription of a composite system, where we consider the discarded part to be\nthe context. We introduce dual density operators, which allow for two\nindependent notions of context. We demonstrate the use of dual density\noperators within a grammatical-compositional distributional framework for\nnatural language meaning. We show that dual density operators can be used to\nsimultaneously represent: (i) ambiguity about word meanings (e.g. queen as a\nperson vs. queen as a band), and (ii) lexical entailment (e.g. tiger ->\nmammal). We provide a proof-of-concept example.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:12 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Ashoush", "Daniela", "", "Univesity of Oxford"], ["Coecke", "Bob", "", "Univesity of\n  Oxford"]]}, {"id": "1608.01402", "submitter": "EPTCS", "authors": "Josef Bolt (Univesity of Oxford), Bob Coecke (Univesity of Oxford),\n  Fabrizio Genovese (Univesity of Oxford), Martha Lewis (Univesity of Oxford),\n  Daniel Marsden (Univesity of Oxford), Robin Piedeleu (Univesity of Oxford)", "title": "Interacting Conceptual Spaces", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 11-19", "doi": "10.4204/EPTCS.221.2", "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose applying the categorical compositional scheme of [6] to conceptual\nspace models of cognition. In order to do this we introduce the category of\nconvex relations as a new setting for categorical compositional semantics,\nemphasizing the convex structure important to conceptual space applications. We\nshow how conceptual spaces for composite types such as adjectives and verbs can\nbe constructed. We illustrate this new model on detailed examples.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:21 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Bolt", "Josef", "", "Univesity of Oxford"], ["Coecke", "Bob", "", "Univesity of Oxford"], ["Genovese", "Fabrizio", "", "Univesity of Oxford"], ["Lewis", "Martha", "", "Univesity of Oxford"], ["Marsden", "Daniel", "", "Univesity of Oxford"], ["Piedeleu", "Robin", "", "Univesity of Oxford"]]}, {"id": "1608.01404", "submitter": "EPTCS", "authors": "Mehrnoosh Sadrzadeh (Queen Mary University of London)", "title": "Quantifier Scope in Categorical Compositional Distributional Semantics", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 49-57", "doi": "10.4204/EPTCS.221.6", "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work with J. Hedges, we formalised a generalised quantifiers\ntheory of natural language in categorical compositional distributional\nsemantics with the help of bialgebras. In this paper, we show how quantifier\nscope ambiguity can be represented in that setting and how this representation\ncan be generalised to branching quantifiers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:57 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Sadrzadeh", "Mehrnoosh", "", "Queen Mary University of London"]]}, {"id": "1608.01433", "submitter": "Julia Sapi\\~na", "authors": "M. Alpuente, D. Ballis, F. Frechina, J. Sapi\\~na", "title": "Assertion-based Analysis via Slicing with ABETS (System Description)", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ABETS, an assertion-based, dynamic analyzer that helps diagnose\nerrors in Maude programs. ABETS uses slicing to automatically create reduced\nversions of both a run's execution trace and executed program, reduced versions\nin which any information that is not relevant to the bug currently being\ndiagnosed is removed. In addition, ABETS employs runtime assertion checking to\nautomate the identification of bugs so that whenever an assertion is violated,\nthe system automatically infers accurate slicing criteria from the failure. We\nsummarize the main services provided by ABETS, which also include a novel\nassertion-based facility for program repair that generates suitable program\nfixes when a state invariant is violated. Finally, we provide an experimental\nevaluation that shows the performance and effectiveness of the system. This\npaper is under consideration for publication in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 06:02:00 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Alpuente", "M.", ""], ["Ballis", "D.", ""], ["Frechina", "F.", ""], ["Sapi\u00f1a", "J.", ""]]}, {"id": "1608.01560", "submitter": "Sergey Slavnov A", "authors": "Sergey Slavnov", "title": "On partial traces and compactification of $*$-autonomous Mix-categories", "comments": "The author's preceding paper on this subject [arXiv:1607.03877] was\n  withdrawn shortly after publication because of a fatal error in the proof as\n  well as in the announced result. This is the second try with results and\n  proofs corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question when a $*$-autonomous Mix-category has a representation\nas a $*$-autonomous Mix-subcategory of a compact one. We define certain partial\ntrace-like operation on morphisms of a Mix-category, which we call a mixed\ntrace, and show that any structure preserving embedding of a Mix-category into\na compact one induces a mixed trace on the former. We also show that,\nconversely, if a Mix-category ${\\bf K}$ has a mixed trace, then we can\nconstruct a compact category and structure preserving embedding of ${\\bf K}$\ninto it, which induces the same mixed trace.\n  Finally, we find a specific condition expressed in terms of interaction of\nMix- and coevaluation maps on a Mix-category ${\\bf K}$, which is necessary and\nsufficient for a structure preserving embedding of ${\\bf K}$ into a compact one\nto exist. When this condition is satisfied, we construct a \"free\" or \"minimal\"\nmixed trace on ${\\bf K}$ directly from the Mix-category structure, which gives\nus also a \"free\" compactification of ${\\bf K}$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 14:44:19 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Slavnov", "Sergey", ""]]}, {"id": "1608.01594", "submitter": "K. Tuncay Tekle", "authors": "K. Tuncay Tekle, Yanhong A. Liu", "title": "Precise Complexity Guarantees for Pointer Analysis via Datalog with\n  Extensions", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 19 pages,\n  LaTeX", "journal-ref": "Theory and Practice of Logic Programming, Volume 16, Special Issue\n  5-6, September 2016, pp. 916-932", "doi": "10.1017/S1471068416000405", "report-no": null, "categories": "cs.PL cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pointer analysis is a fundamental static program analysis for computing the\nset of objects that an expression can refer to. Decades of research has gone\ninto developing methods of varying precision and efficiency for pointer\nanalysis for programs that use different language features, but determining\nprecisely how efficient a particular method is has been a challenge in itself.\n  For programs that use different language features, we consider methods for\npointer analysis using Datalog and extensions to Datalog. When the rules are in\nDatalog, we present the calculation of precise time complexities from the rules\nusing a new algorithm for decomposing rules for obtaining the best\ncomplexities. When extensions such as function symbols and universal\nquantification are used, we describe algorithms for efficiently implementing\nthe extensions and the complexities of the algorithms.\n  This paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:05:13 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 15:46:20 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Tekle", "K. Tuncay", ""], ["Liu", "Yanhong A.", ""]]}, {"id": "1608.01603", "submitter": "Amelia  Harrison", "authors": "Amelia Harrison and Vladimir Lifschitz", "title": "Stable Models for Infinitary Formulas with Extensional Atoms", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The definition of stable models for propositional formulas with infinite\nconjunctions and disjunctions can be used to describe the semantics of answer\nset programming languages. In this note, we enhance that definition by\nintroducing a distinction between intensional and extensional atoms. The\nsymmetric splitting theorem for first-order formulas is then extended to\ninfinitary formulas and used to reason about infinitary definitions. This note\nis under consideration for publication in Theory and Practice of Logic\nProgramming.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:32:57 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Harrison", "Amelia", ""], ["Lifschitz", "Vladimir", ""]]}, {"id": "1608.01626", "submitter": "Amelia  Harrison", "authors": "Amelia Harrison, Vladimir Lifschitz, and Julian Michael", "title": "Proving Infinitary Formulas", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infinitary propositional logic of here-and-there is important for the\ntheory of answer set programming in view of its relation to strongly equivalent\ntransformations of logic programs. We know a formal system axiomatizing this\nlogic exists, but a proof in that system may include infinitely many formulas.\nIn this note we describe a relationship between the validity of infinitary\nformulas in the logic of here-and-there and the provability of formulas in some\nfinite deductive systems. This relationship allows us to use finite proofs to\njustify the validity of infinitary formulas. This note is under consideration\nfor publication in Theory and Practice of Logic Programming.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 17:46:05 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Harrison", "Amelia", ""], ["Lifschitz", "Vladimir", ""], ["Michael", "Julian", ""]]}, {"id": "1608.01755", "submitter": "Waqar  Ahmed", "authors": "Waqar Ahmed and Osman Hasan", "title": "Formal Availability Analysis using Theorem Proving", "comments": "16 pages. arXiv admin note: text overlap with arXiv:1505.02648", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability analysis is used to assess the possible failures and their\nrestoration process for a given system. This analysis involves the calculation\nof instantaneous and steady-state availabilities of the individual system\ncomponents and the usage of this information along with the commonly used\navailability modeling techniques, such as Availability Block Diagrams (ABD) and\nFault Trees (FTs) to determine the system-level availability. Traditionally,\navailability analyses are conducted using paper-and-pencil methods and\nsimulation tools but they cannot ascertain absolute correctness due to their\ninaccuracy limitations. As a complementary approach, we propose to use the\nhigher-order-logic theorem prover HOL4 to conduct the availability analysis of\nsafety-critical systems. For this purpose, we present a higher-order-logic\nformalization of instantaneous and steady-state availability, ABD\nconfigurations and generic unavailability FT gates. For illustration purposes,\nthese formalizations are utilized to conduct formal availability analysis of a\nsatellite solar array, which is used as the main source of power for the Dong\nFang Hong-3 (DFH-3) satellite.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 04:13:14 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Ahmed", "Waqar", ""], ["Hasan", "Osman", ""]]}, {"id": "1608.01785", "submitter": "Weijun Zhu", "authors": "Weijun Zhu", "title": "Molecular Model Checking a Temporal Logic", "comments": "14 pages, 10 figures and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The molecular computing has been successfully employed to solve more and more\ncomplex computation problems. However, as an important complex problem, the\nmodel checking are still far from fully resolved under the circumstance of\nmolecular computing, since it is still a lack of method. To address this issue,\na model checking method is presented for checking the basic constructs in a\ngiven temporal logic using molecular computing. Through the design of the new\nencoding and calling this process, we get a molecule-based approach for\nchecking all of the basic constructs of this logic.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 07:25:19 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 00:44:19 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 06:17:38 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Zhu", "Weijun", ""]]}, {"id": "1608.01856", "submitter": "Michael Morak", "authors": "Manuel Bichler, Michael Morak and Stefan Woltran", "title": "The Power of Non-Ground Rules in Answer Set Programming", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer set programming (ASP) is a well-established logic programming language\nthat offers an intuitive, declarative syntax for problem solving. In its\ntraditional application, a fixed ASP program for a given problem is designed\nand the actual instance of the problem is fed into the program as a set of\nfacts. This approach typically results in programs with comparably short and\nsimple rules. However, as is known from complexity analysis, such an approach\nlimits the expressive power of ASP; in fact, an entire NP-check can be encoded\ninto a single large rule body of bounded arity that performs both a guess and a\ncheck within the same rule.\n  Here, we propose a novel paradigm for encoding hard problems in ASP by making\nexplicit use of large rules which depend on the actual instance of the problem.\nWe illustrate how this new encoding paradigm can be used, providing examples of\nproblems from the first, second, and even third level of the polynomial\nhierarchy. As state-of-the-art solvers are tuned towards short rules, rule\ndecomposition is a key technique in the practical realization of our approach.\nWe also provide some preliminary benchmarks which indicate that giving up the\nconvenient way of specifying a fixed program can lead to a significant\nspeed-up.\n  This paper is under consideration for acceptance into TPLP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 12:26:22 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Bichler", "Manuel", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1608.02082", "submitter": "Daniela Inclezan", "authors": "Daniela Inclezan", "title": "COREALMLIB: An ALM Library Translated from the Component Library", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 figures (2 of which in PDF format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents COREALMLIB, an ALM library of commonsense knowledge about\ndynamic domains. The library was obtained by translating part of the COMPONENT\nLIBRARY (CLIB) into the modular action language ALM. CLIB consists of general\nreusable and composable commonsense concepts, selected based on a thorough\nstudy of ontological and lexical resources. Our translation targets CLIB states\n(i.e., fluents) and actions. The resulting ALM library contains the\ndescriptions of 123 action classes grouped into 43 reusable modules that are\norganized into a hierarchy. It is made available online and of interest to\nresearchers in the action language, answer-set programming, and natural\nlanguage understanding communities. We believe that our translation has two\nmain advantages over its CLIB counterpart: (i) it specifies axioms about\nactions in a more elaboration tolerant and readable way, and (ii) it can be\nseamlessly integrated with ASP reasoning algorithms (e.g., for planning and\npostdiction). In contrast, axioms are described in CLIB using STRIPS-like\noperators, and CLIB's inference engine cannot handle planning nor postdiction.\nUnder consideration for publication in TPLP.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 08:59:21 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 16:56:18 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Inclezan", "Daniela", ""]]}, {"id": "1608.02327", "submitter": "Petr Jancar", "authors": "Petr Jancar", "title": "Deciding structural liveness of Petri nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place/transition Petri nets are a standard model for a class of distributed\nsystems whose reachability spaces might be infinite. One of well-studied topics\nis the verification of safety and liveness properties in this model; despite\nthe extensive research effort, some basic problems remain open, which is\nexemplified by the open complexity status of the reachability problem. The\nliveness problems are known to be closely related to the reachability problem,\nand many structural properties of nets that are related to liveness have been\nstudied. Somewhat surprisingly, the decidability status of the problem if a net\nis structurally live, i.e. if there is an initial marking for which it is live,\nhas remained open, as also a recent paper (Best and Esparza, 2016) emphasizes.\nHere we show that the structural liveness problem for Petri nets is decidable.\nA crucial ingredient of the proof is the result by Leroux (LiCS 2013) showing\nthat we can compute a finite (Presburger) description of the reachability set\nfor a marked Petri net if this set is semilinear.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 06:05:59 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Jancar", "Petr", ""]]}, {"id": "1608.02534", "submitter": "Pedro Lopez-Garcia", "authors": "Manuel V. Hermenegildo and Pedro Lopez-Garcia", "title": "Pre-proceedings of the 26th International Symposium on Logic-Based\n  Program Synthesis and Transformation (LOPSTR 2016)", "comments": "Papers selected for presentation at LOPSTR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the pre-proceedings of the 26th International\nSymposium on Logic-Based Program Synthesis and Transformation (LOPSTR 2016),\nheld on 6-8th September 2016 in Edinburgh, Scotland UK, and co-located with the\n18th International Symposium on Principles and Practice of Declarative\nProgramming (PPDP 2016) and the 23rd Static Analysis Symposium (SAS 2016).\nAfter discussion at the symposium papers will go through a second round of\nrefereeing and selection for the formal proceedings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 17:57:30 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 15:52:11 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Hermenegildo", "Manuel V.", ""], ["Lopez-Garcia", "Pedro", ""]]}, {"id": "1608.02636", "submitter": "J\\\"urgen Koslowski", "authors": "Michael Blondin, Alain Finkel, Pierre McKenzie", "title": "Well Behaved Transition Systems", "comments": "19 pages, 3 figures", "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 3 (September\n  13, 2017) lmcs:3928", "doi": "10.23638/LMCS-13(3:24)2017", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-quasi-ordering (i.e., a well-founded quasi-ordering such that all\nantichains are finite) that defines well-structured transition systems (WSTS)\nis shown not to be the weakest hypothesis that implies decidability of the\ncoverability problem. We show coverability decidable for monotone transition\nsystems that only require the absence of infinite antichains and call well\nbehaved transitions systems (WBTS) the new strict superclass of the class of\nWSTS that arises. By contrast, we confirm that boundedness and termination are\nundecidable for WBTS under the usual hypotheses, and show that stronger\nmonotonicity conditions can enforce decidability. Proofs are similar or even\nidentical to existing proofs but the surprising message is that a hypothesis\nimplicitely assumed minimal for twenty years in the theory of WSTS can\nmeaningfully be relaxed, allowing more orderings to be handled in an abstract\nway.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 22:02:29 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 13:18:38 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 15:14:35 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Blondin", "Michael", ""], ["Finkel", "Alain", ""], ["McKenzie", "Pierre", ""]]}, {"id": "1608.02644", "submitter": "Daniel Whalen", "authors": "Daniel Whalen", "title": "Holophrasm: a neural Automated Theorem Prover for higher-order logic", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a system for Automated Theorem Proving in higher order logic using\ndeep learning and eschewing hand-constructed features. Holophrasm exploits the\nformalism of the Metamath language and explores partial proof trees using a\nneural-network-augmented bandit algorithm and a sequence-to-sequence model for\naction enumeration. The system proves 14% of its test theorems from Metamath's\nset.mm module.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 22:33:13 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 03:22:11 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Whalen", "Daniel", ""]]}, {"id": "1608.02681", "submitter": "Amelia  Harrison", "authors": "Amelia Harrison and Yuliya Lierler", "title": "First-Order Modular Logic Programs and their Conservative Extensions", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modular logic programs provide a way of viewing logic programs as consisting\nof many independent, meaningful modules. This paper introduces first-order\nmodular logic programs, which can capture the meaning of many answer set\nprograms. We also introduce conservative extensions of such programs. This\nconcept helps to identify strong relationships between modular programs as well\nas between traditional programs. We show how the notion of a conservative\nextension can be used to justify the common projection rewriting. This note is\nunder consideration for publication in Theory and Practice of Logic\nProgramming.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 03:06:40 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 20:05:29 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Harrison", "Amelia", ""], ["Lierler", "Yuliya", ""]]}, {"id": "1608.02688", "submitter": "Jo Devriendt", "authors": "Jo Devriendt, Bart Bogaerts, Maurice Bruynooghe, Marc Denecker", "title": "On Local Domain Symmetry for Model Expansion", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures (arXiv:YYMM.NNNNN)", "journal-ref": null, "doi": "10.1017/S1471068416000508", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry in combinatorial problems is an extensively studied topic. We\ncontinue this research in the context of model expansion problems, with the aim\nof automating the workflow of detecting and breaking symmetry. We focus on\nlocal domain symmetry, which is induced by permutations of domain elements, and\nwhich can be detected on a first-order level. As such, our work is a\ncontinuation of the symmetry exploitation techniques of model generation\nsystems, while it differs from more recent symmetry breaking techniques in\nanswer set programming which detect symmetry on ground programs. Our main\ncontributions are sufficient conditions for symmetry of model expansion\nproblems, the identification of local domain interchangeability, which can\noften be broken completely, and efficient symmetry detection algorithms for\nboth local domain interchangeability as well as local domain symmetry in\ngeneral. Our approach is implemented in the model expansion system IDP, and we\npresent experimental results showcasing the strong and weak points of our\napproach compared to SBASS , a symmetry breaking technique for answer set\nprogramming.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 04:33:26 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Devriendt", "Jo", ""], ["Bogaerts", "Bart", ""], ["Bruynooghe", "Maurice", ""], ["Denecker", "Marc", ""]]}, {"id": "1608.02692", "submitter": "EPTCS", "authors": "Daniel Gebler, Kirstin Peters", "title": "Proceedings Combined 23rd International Workshop on Expressiveness in\n  Concurrency and 13th Workshop on Structural Operational Semantics", "comments": null, "journal-ref": "EPTCS 222, 2016", "doi": "10.4204/EPTCS.222", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the Combined 23nd International\nWorkshop on Expressiveness in Concurrency and the 13th Workshop on Structural\nOperational Semantics (EXPRESS/SOS 2016) which was held on 22 August 2016 in\nQu\\'ebec City, Canada, as an affiliated workshop of CONCUR 2016, the 27th\nInternational Conference on Concurrency Theory. The EXPRESS workshops aim at\nbringing together researchers interested in the expressiveness of various\nformal systems and semantic notions, particularly in the field of concurrency.\nTheir focus has traditionally been on the comparison between programming\nconcepts (such as concurrent, functional, imperative, logic and object-oriented\nprogramming) and between mathematical models of computation (such as process\nalgebras, Petri nets, event structures, modal logics, and rewrite systems) on\nthe basis of their relative expressive power. The EXPRESS workshop series has\nrun successfully since 1994 and over the years this focus has become broadly\nconstrued. The SOS workshops aim at being a forum for researchers, students and\npractitioners interested in new developments, and directions for future\ninvestigation, in the field of structural operational semantics. One of the\nspecific goals of the SOS workshop series is to establish synergies between the\nconcurrency and programming language communities working on the theory and\npractice of SOS. Since 2012, the EXPRESS and SOS communities have organized an\nannual combined EXPRESS/SOS workshop on the expressiveness of mathematical\nmodels of computation and the formal semantics of systems and programming\nconcepts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 05:24:04 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Gebler", "Daniel", ""], ["Peters", "Kirstin", ""]]}, {"id": "1608.02693", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Carl Schultz", "title": "Deeply Semantic Inductive Spatio-Temporal Learning", "comments": "Accepted for publication at ILP 2016: 26th International Conference\n  on Inductive Logic Programming 4th - 6th September 2016, London. Keywords:\n  Spatio-Temporal Learning; Dynamic Visuo-Spatial Imagery; Declarative Spatial\n  Reasoning; Inductive Logic Programming; AI and Art", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an inductive spatio-temporal learning framework rooted in\ninductive logic programming. With an emphasis on visuo-spatial language, logic,\nand cognition, the framework supports learning with relational spatio-temporal\nfeatures identifiable in a range of domains involving the processing and\ninterpretation of dynamic visuo-spatial imagery. We present a prototypical\nsystem, and an example application in the domain of computing for visual arts\nand computational cognitive science.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 05:48:51 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Schultz", "Carl", ""]]}, {"id": "1608.02807", "submitter": "Emanuele De Angelis", "authors": "Emanuele De Angelis (1), Fabio Fioravanti (1), Maria Chiara Meo (1),\n  Alberto Pettorossi (2), and Maurizio Proietti (3) ((1) DEC, University G.\n  d'Annunzio, Pescara, Italy, (2) DICII, University of Rome Tor Vergata, Rome,\n  Italy,(3) IASI-CNR, Rome, Italy)", "title": "Verification of Time-Aware Business Processes using Constrained Horn\n  Clauses", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/14", "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for verifying properties of time-aware business\nprocesses, that is, business process where time constraints on the activities\nare explicitly taken into account. Business processes are specified using an\nextension of the Business Process Modeling Notation (BPMN) and durations are\ndefined by constraints over integer numbers. The definition of the operational\nsemantics is given by a set OpSem of constrained Horn clauses (CHCs). Our\nverification method consists of two steps. (Step 1) We specialize OpSem with\nrespect to a given business process and a given temporal property to be\nverified, whereby getting a set of CHCs whose satisfiability is equivalent to\nthe validity of the given property. (Step 2) We use state-of-the-art solvers\nfor CHCs to check the satisfiability of such sets of clauses. We have\nimplemented our verification method using the VeriMAP transformation system,\nand the Eldarica and Z3 solvers for CHCs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 14:15:31 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["De Angelis", "Emanuele", ""], ["Fioravanti", "Fabio", ""], ["Meo", "Maria Chiara", ""], ["Pettorossi", "Alberto", ""], ["Proietti", "Maurizio", ""]]}, {"id": "1608.02896", "submitter": "Enrique Martin-Martin", "authors": "Elvira Albert, Nikolaos Bezirgiannis, Frank de Boer, Enrique\n  Martin-Martin", "title": "A Formal, Resource Consumption-Preserving Translation of Actors to\n  Haskell", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/35", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal translation of an actor-based language with cooperative\nscheduling to the functional language Haskell. The translation is proven\ncorrect with respect to a formal semantics of the source language and a\nhigh-level operational semantics of the target, i.e. a subset of Haskell. The\nmain correctness theorem is expressed in terms of a simulation relation between\nthe operational semantics of actor programs and their translation. This allows\nus to then prove that the resource consumption is preserved over this\ntranslation, as we establish an equivalence of the cost of the original and\nHaskell-translated execution traces.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 18:13:45 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 07:56:38 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Albert", "Elvira", ""], ["Bezirgiannis", "Nikolaos", ""], ["de Boer", "Frank", ""], ["Martin-Martin", "Enrique", ""]]}, {"id": "1608.03054", "submitter": "Frederic Mesnard", "authors": "Fred Mesnard, Etienne Payet, German Vidal", "title": "On the Completeness of Selective Unification in Concolic Testing of\n  Logic Programs", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/12", "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concolic testing is a popular dynamic validation technique that can be used\nfor both model checking and automatic test case generation. We have recently\nintroduced concolic testing in the context of logic programming. In contrast to\nprevious approaches, the key ingredient in this setting is a technique to\ngenerate appropriate run-time goals by considering all possible ways an atom\ncan unify with the heads of some program clauses. This is called \"selective\"\nunification. In this paper, we show that the existing algorithm is not complete\nand explore different alternatives in order to have a sound and complete\nalgorithm for selective unification.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 06:04:53 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Mesnard", "Fred", ""], ["Payet", "Etienne", ""], ["Vidal", "German", ""]]}, {"id": "1608.03125", "submitter": "EPTCS", "authors": "Eduard Baranov (Ecole polytechnique f\\'ed\\'erale de Lausanne), Simon\n  Bliudze (Ecole polytechnique f\\'ed\\'erale de Lausanne)", "title": "A Note on the Expressiveness of BIP", "comments": "In Proceedings EXPRESS/SOS 2016, arXiv:1608.02692", "journal-ref": "EPTCS 222, 2016, pp. 1-14", "doi": "10.4204/EPTCS.222.1", "report-no": null, "categories": "cs.DC cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend our previous algebraic formalisation of the notion of\ncomponent-based framework in order to formally define two forms, strong and\nweak, of the notion of full expressiveness. Our earlier result shows that the\nBIP (Behaviour-Interaction-Priority) framework does not possess the strong full\nexpressiveness. In this paper, we show that BIP has the weak form of this\nnotion and provide results detailing weak and strong full expressiveness for\nclassical BIP and several modifications, obtained by relaxing the constraints\nimposed on priority models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:10:33 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Baranov", "Eduard", "", "Ecole polytechnique f\u00e9d\u00e9rale de Lausanne"], ["Bliudze", "Simon", "", "Ecole polytechnique f\u00e9d\u00e9rale de Lausanne"]]}, {"id": "1608.03126", "submitter": "EPTCS", "authors": "Xian Xu (East China University of Science and Technology, China)", "title": "Higher-order Processes with Parameterization over Names and Processes", "comments": "In Proceedings EXPRESS/SOS 2016, arXiv:1608.02692", "journal-ref": "EPTCS 222, 2016, pp. 15-29", "doi": "10.4204/EPTCS.222.2", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterization extends higher-order processes with the capability of\nabstraction and application (like those in lambda-calculus). This extension is\nstrict, i.e., higher-order processes equipped with parameterization is\ncomputationally more powerful. This paper studies higher-order processes with\ntwo kinds of parameterization: one on names and the other on processes\nthemselves. We present two results. One is that in presence of\nparameterization, higher-order processes can encode first-order (name-passing)\nprocesses in a quite neat fashion, in contrast to the fact that higher-order\nprocesses without parameterization cannot encode first-order processes at all.\nIn the other result, we provide a simpler characterization of the (standard)\ncontext bisimulation for higher-order processes with parameterization, in terms\nof the normal bisimulation that stems from the well-known normal\ncharacterization for higher-order calculus. These two results demonstrate more\nessence of the parameterization method in the higher-order paradigm toward\nexpressiveness and behavioural equivalence.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:10:41 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Xu", "Xian", "", "East China University of Science and Technology, China"]]}, {"id": "1608.03127", "submitter": "EPTCS", "authors": "Sanjiva Prasad (Indian Institute of Technology Delhi), Lenore D. Zuck\n  (University of Illinois at Chicago)", "title": "Self-Similarity Breeds Resilience", "comments": "In Proceedings EXPRESS/SOS 2016, arXiv:1608.02692", "journal-ref": "EPTCS 222, 2016, pp. 30-44", "doi": "10.4204/EPTCS.222.3", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-similarity is the property of a system being similar to a part of\nitself. We posit that a special class of behaviourally self-similar systems\nexhibits a degree of resilience to adversarial behaviour. We formalise the\nnotions of system, adversary and resilience in operational terms, based on\ntransition systems and observations. While the general problem of proving\nsystems to be behaviourally self-similar is undecidable, we show, by casting\nthem in the framework of well-structured transition systems, that there is an\ninteresting class of systems for which the problem is decidable. We illustrate\nour prescriptive framework for resilience with some small examples, e.g.,\nsystems robust to failures in a fail-stop model, and those avoiding\nside-channel attacks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:10:49 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Prasad", "Sanjiva", "", "Indian Institute of Technology Delhi"], ["Zuck", "Lenore D.", "", "University of Illinois at Chicago"]]}, {"id": "1608.03128", "submitter": "EPTCS", "authors": "Matias David Lee (Univ. Lyon, ENS de Lyon, CNRS, UCB Lyon 1, LIP,\n  France.), Bas Luttik (Eindhoven University of Technology, The Netherlands.)", "title": "Unique Parallel Decomposition for the Pi-calculus", "comments": "In Proceedings EXPRESS/SOS 2016, arXiv:1608.02692", "journal-ref": "EPTCS 222, 2016, pp. 45-59", "doi": "10.4204/EPTCS.222.4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A (fragment of a) process algebra satisfies unique parallel decomposition if\nthe definable behaviours admit a unique decomposition into indecomposable\nparallel components. In this paper we prove that finite processes of the\npi-calculus, i.e. processes that perform no infinite executions, satisfy this\nproperty modulo strong bisimilarity and weak bisimilarity. Our results are\nobtained by an application of a general technique for establishing unique\nparallel decomposition using decomposition orders.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:10:58 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Lee", "Matias David", "", "Univ. Lyon, ENS de Lyon, CNRS, UCB Lyon 1, LIP,\n  France."], ["Luttik", "Bas", "", "Eindhoven University of Technology, The Netherlands."]]}, {"id": "1608.03129", "submitter": "EPTCS", "authors": "Mariangiola Dezani-Ciancaglini (Dipartimento di Informatica,\n  Universita' di Torino), Paola Giannini (Computer Science Institute, DiSIT,\n  Universita' del Piemente Orientale)", "title": "Reversible Multiparty Sessions with Checkpoints", "comments": "In Proceedings EXPRESS/SOS 2016, arXiv:1608.02692", "journal-ref": "EPTCS 222, 2016, pp. 60-74", "doi": "10.4204/EPTCS.222.5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible interactions model different scenarios, like biochemical systems\nand human as well as automatic negotiations. We abstract interactions via\nmultiparty sessions enriched with named checkpoints. Computations can either go\nforward or roll back to some checkpoints, where possibly different choices may\nbe taken. In this way communications can be undone and different conversations\nmay be tried. Interactions are typed with global types, which control also\nrollbacks. Typeability of session participants in agreement with global types\nensures session fidelity and progress of reversible communications.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:11:06 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Dezani-Ciancaglini", "Mariangiola", "", "Dipartimento di Informatica,\n  Universita' di Torino"], ["Giannini", "Paola", "", "Computer Science Institute, DiSIT,\n  Universita' del Piemente Orientale"]]}, {"id": "1608.03131", "submitter": "EPTCS", "authors": "Massimo Bartoletti, Ludovic Henrio, Sophia Knight, Hugo Torres Vieira", "title": "Proceedings 9th Interaction and Concurrency Experience", "comments": null, "journal-ref": "EPTCS 223, 2016", "doi": "10.4204/EPTCS.223", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of ICE 2016, the 9th Interaction and\nConcurrency Experience, which was held in Heraklion, Greece on the 8th and 9th\nof June 2016 as a satellite event of DisCoTec 2016. The ICE procedure for paper\nselection allows PC members to interact, anonymously, with authors. During the\nreview phase, each submitted paper is published on a discussion forum whose\naccess is restricted to the authors and to all the PC members not declaring a\nconflict of interest. The PC members post comments and questions that the\nauthors reply to. For the first time, the 2016 edition of ICE included a\nfeature targeting review transparency: reviews of accepted papers were made\npublic on the workshop website and workshop participants in particular were\nable to access them during the workshop. Each paper was reviewed by three PC\nmembers, and altogether nine papers were accepted for publication (the workshop\nalso featured three brief announcements which are not part of this volume). We\nwere proud to host two invited talks, by Alexandra Silva and Uwe Nestmann. The\nabstracts of these two talks are included in this volume together with the\nregular papers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:24:02 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Henrio", "Ludovic", ""], ["Knight", "Sophia", ""], ["Vieira", "Hugo Torres", ""]]}, {"id": "1608.03269", "submitter": "J\\\"urgen Koslowski", "authors": "Takayuki Kihara (University of California, Berkeley)", "title": "Borel-piecewise continuous reducibility for uniformization problems", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 12, Issue 4 (April 27,\n  2017) lmcs:2173", "doi": "10.2168/LMCS-12(4:4)2016", "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a fine hierarchy of Borel-piecewise continuous functions,\nespecially, between closed-piecewise continuity and $G_\\delta$-piecewise\ncontinuity. Our aim is to understand how a priority argument in computability\ntheory is connected to the notion of $G_\\delta$-piecewise continuity, and then\nwe utilize this connection to obtain separation results on subclasses of\n$G_\\delta$-piecewise continuous reductions for uniformization problems on\nset-valued functions with compact graphs. This method is also applicable for\nseparating various non-constructive principles in the Weihrauch lattice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 19:54:30 GMT"}, {"version": "v2", "created": "Sun, 4 Sep 2016 17:15:20 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 14:20:03 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Kihara", "Takayuki", "", "University of California, Berkeley"]]}, {"id": "1608.03319", "submitter": "EPTCS", "authors": "Henryk Michalewski (University of Warsaw, Poland), Matteo Mio (CNRS\n  and ENS-Lyon, France), Miko{\\l}aj Boja\\'nczyk (University of Warsaw, Poland)", "title": "On the Regular Emptiness Problem of Subzero Automata", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 1-23", "doi": "10.4204/EPTCS.223.1", "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subzero automata is a class of tree automata whose acceptance condition can\nexpress probabilistic constraints. Our main result is that the problem of\ndetermining if a subzero automaton accepts some regular tree is decidable.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:25:25 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Michalewski", "Henryk", "", "University of Warsaw, Poland"], ["Mio", "Matteo", "", "CNRS\n  and ENS-Lyon, France"], ["Boja\u0144czyk", "Miko\u0142aj", "", "University of Warsaw, Poland"]]}, {"id": "1608.03320", "submitter": "EPTCS", "authors": "Tommaso Bolognesi (Istituto di Scienza e Tecnologie dell'Informazione\n  \"A. Faedo\", Consiglio Nazionale delle Ricerche), Vincenzo Ciancia (Istituto\n  di Scienza e Tecnologie dell'Informazione \"A. Faedo\", Consiglio Nazionale\n  delle Ricerche)", "title": "Nominal Cellular Automata", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 24-35", "doi": "10.4204/EPTCS.223.2", "report-no": null, "categories": "cs.FL cs.LO nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging field of Nominal Computation Theory is concerned with the theory\nof Nominal Sets and its applications to Computer Science. We investigate here\nthe impact of nominal sets on the definition of Cellular Automata and on their\ncomputational capabilities, with a special focus on the emergent behavioural\nproperties of this new model and their significance in the context of\ncomputation-oriented interpretations of physical phenomena. A preliminary\ninvestigation of the relations between Nominal Cellular Automata and Wolfram's\nElementary Cellular Automata is also carried out.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:25:35 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Bolognesi", "Tommaso", "", "Istituto di Scienza e Tecnologie dell'Informazione\n  \"A. Faedo\", Consiglio Nazionale delle Ricerche"], ["Ciancia", "Vincenzo", "", "Istituto\n  di Scienza e Tecnologie dell'Informazione \"A. Faedo\", Consiglio Nazionale\n  delle Ricerche"]]}, {"id": "1608.03323", "submitter": "EPTCS", "authors": "Roberto Guanciale (KTH Royal Institute of Technology, Sweden), Emilio\n  Tuosto (University of Leicester, UK)", "title": "An Abstract Semantics of the Global View of Choreographies", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 67-82", "doi": "10.4204/EPTCS.223.5", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an abstract semantics of the global view of choreographies. Our\nsemantics is given in terms of pre-orders and can accommodate different lower\nlevel semantics. We discuss the adequacy of our model by considering its\nrelation with communicating machines, that we use to formalise the local view.\nInterestingly, our framework seems to be more expressive than others where\nsemantics of global views have been considered. This will be illustrated by\ndiscussing some interesting examples.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:26:06 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Guanciale", "Roberto", "", "KTH Royal Institute of Technology, Sweden"], ["Tuosto", "Emilio", "", "University of Leicester, UK"]]}, {"id": "1608.03325", "submitter": "EPTCS", "authors": "Alexis Bernadet (Dalhousie University, Canada), Ivan Lanese (Focus\n  Team, University of Bologna/INRIA, Italy)", "title": "A Modular Formalization of Reversibility for Concurrent Models and\n  Languages", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 98-112", "doi": "10.4204/EPTCS.223.7", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal-consistent reversibility is the reference notion of reversibility for\nconcurrency. We introduce a modular framework for defining causal-consistent\nreversible extensions of concurrent models and languages. We show how our\nframework can be used to define reversible extensions of formalisms as\ndifferent as CCS and concurrent X-machines. The generality of the approach\nallows for the reuse of theories and techniques in different settings.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:26:24 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Bernadet", "Alexis", "", "Dalhousie University, Canada"], ["Lanese", "Ivan", "", "Focus\n  Team, University of Bologna/INRIA, Italy"]]}, {"id": "1608.03326", "submitter": "EPTCS", "authors": "Seyed Hossein Haeri (Universite catholique de Louvain, Belgium), Peter\n  Van Roy (Universite catholique de Louvain, Belgium), Carlos Baquero\n  (Universidade do Minho, Portugal), Christopher Meiklejohn (Universite\n  catholique de Louvain, Belgium)", "title": "Worlds of Events: Deduction with Partial Knowledge about Causality", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 113-127", "doi": "10.4204/EPTCS.223.8", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between internet users are mediated by their devices and the\ncommon support infrastructure in data centres. Keeping track of causality\namongst actions that take place in this distributed system is key to provide a\nseamless interaction where effects follow causes. Tracking causality in large\nscale interactions is difficult due to the cost of keeping large quantities of\nmetadata; even more challenging when dealing with resource-limited devices. In\nthis paper, we focus on keeping partial knowledge on causality and address\ndeduction from that knowledge.\n  We provide the first proof-theoretic causality modelling for distributed\npartial knowledge. We prove computability and consistency results. We also\nprove that the partial knowledge gives rise to a weaker model than classical\ncausality. We provide rules for offline deduction about causality and refute\nsome related folklore. We define two notions of forward and backward\nbisimilarity between devices, using which we prove two important results.\nNamely, no matter the order of addition/removal, two devices deduce similarly\nabout causality so long as: (1) the same causal information is fed to both. (2)\nthey start bisimilar and erase the same causal information. Thanks to our\nestablishment of forward and backward bisimilarity, respectively, proofs of the\nlatter two results work by simple induction on length.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:26:34 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Haeri", "Seyed Hossein", "", "Universite catholique de Louvain, Belgium"], ["Van Roy", "Peter", "", "Universite catholique de Louvain, Belgium"], ["Baquero", "Carlos", "", "Universidade do Minho, Portugal"], ["Meiklejohn", "Christopher", "", "Universite\n  catholique de Louvain, Belgium"]]}, {"id": "1608.03771", "submitter": "Manfred Schmidt-Schauss", "authors": "Manfred Schmidt-Schau{\\ss} and Temur Kutsia and Jordi Levy and Mateu\n  Villaret", "title": "Nominal Unification of Higher Order Expressions with Recursive Let", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/34", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sound and complete algorithm for nominal unification of higher-order\nexpressions with a recursive let is described, and shown to run in\nnon-deterministic polynomial time. We also explore specializations like nominal\nletrec-matching for plain expressions and for DAGs and determine the complexity\nof corresponding unification problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 12:21:30 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Schmidt-Schau\u00df", "Manfred", ""], ["Kutsia", "Temur", ""], ["Levy", "Jordi", ""], ["Villaret", "Mateu", ""]]}, {"id": "1608.03814", "submitter": "Jonathan Sterling", "authors": "Jonathan Sterling", "title": "Higher-Order Functions and Brouwer's Thesis", "comments": "Journal of Functional Programming, Bob Harper Festschrift Collection", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending Mart\\'in Escard\\'o's effectful forcing technique, we give a new\nproof of a well-known result: Brouwer's monotone bar theorem holds for any bar\nthat can be realized by a functional of type $(\\mathbb{N} \\to \\mathbb{N}) \\to\n\\mathbb{N}$ in G\\\"odel's System T. Effectful forcing is an elementary\nalternative to standard sheaf-theoretic forcing arguments, using ideas from\nprogramming languages, including computational effects, monads, the algebra\ninterpretation of call-by-name ${\\lambda}$-calculus, and logical relations. Our\nargument proceeds by interpreting System T programs as well-founded dialogue\ntrees whose nodes branch on a query to an oracle of type\n$\\mathbb{N}\\to\\mathbb{N}$, lifted to higher type along a call-by-name\ntranslation. To connect this interpretation to the bar theorem, we then show\nthat Brouwer's famous \"mental constructions\" of barhood constitute an invariant\nform of these dialogue trees in which queries to the oracle are made maximally\nand in order.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 15:23:28 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 19:23:41 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 15:02:20 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2021 21:04:44 GMT"}, {"version": "v5", "created": "Mon, 19 Apr 2021 14:22:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sterling", "Jonathan", ""]]}, {"id": "1608.03912", "submitter": "Paul Tarau", "authors": "Paul Tarau", "title": "A Hiking Trip Through the Orders of Magnitude: Deriving Efficient\n  Generators for Closed Simply-Typed Lambda Terms and Normal Forms", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/16", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to several other families of lambda terms, no closed formula or\ngenerating function is known and none of the sophisticated techniques devised\nin analytic combinatorics can currently help with counting or generating the\nset of {\\em simply-typed closed lambda terms} of a given size.\n  Moreover, their asymptotic scarcity among the set of closed lambda terms\nmakes counting them via brute force generation and type inference quickly\nintractable, with previous published work showing counts for them only up to\nsize 10.\n  By taking advantage of the synergy between logic variables, unification with\noccurs check and efficient backtracking in today's Prolog systems, we climb 4\norders of magnitude above previously known counts by deriving progressively\nfaster Horn Clause programs that generate and/or count the set of closed\nsimply-typed lambda terms of sizes up to 14. A similar count for {\\em closed\nsimply-typed normal forms} is also derived up to size 14.\n  {\\em {\\bf Keywords:} logic programming transformations, type inference,\ncombinatorics of lambda terms, simply-typed lambda calculus, simply-typed\nnormal forms. }\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 21:11:33 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Tarau", "Paul", ""]]}, {"id": "1608.04301", "submitter": "Thorsten Wissmann", "authors": "Miika Hannula", "title": "Validity and Entailment in Modal and Propositional Dependence Logics", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (April 26,\n  2019) lmcs:5403", "doi": "10.23638/LMCS-15(2:4)2019", "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The computational properties of modal and propositional dependence logics\nhave been extensively studied over the past few years, starting from a result\nby Sevenster showing NEXPTIME-completeness of the satisfiability problem for\nmodal dependence logic. Thus far, however, the validity and entailment\nproperties of these logics have remained mostly unaddressed. This paper\nprovides a comprehensive classification of the complexity of validity and\nentailment in various modal and propositional dependence logics. The logics\nexamined are obtained by extending the standard modal and propositional logics\nwith notions of dependence, independence, and inclusion in the team semantics\ncontext. In particular, we address the question of the complexity of validity\nin modal dependence logic. By showing that it is NEXPTIME-complete we refute an\nearlier conjecture proposing a higher complexity for the problem.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 18:41:12 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 02:09:28 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 09:20:33 GMT"}, {"version": "v4", "created": "Thu, 10 Jan 2019 15:05:59 GMT"}, {"version": "v5", "created": "Thu, 25 Apr 2019 09:30:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hannula", "Miika", ""]]}, {"id": "1608.04688", "submitter": "Gines Moreno Dr.", "authors": "Gin\\'es Moreno, Jaime Penabad, and Germ\\'an Vidal", "title": "Tuning Fuzzy Logic Programs with Symbolic Execution", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/15", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy logic programming is a growing declarative paradigm aiming to integrate\nfuzzy logic into logic programming. One of the most difficult tasks when\nspecifying a fuzzy logic program is determining the right weights for each\nrule, as well as the most appropriate fuzzy connectives and operators. In this\npaper, we introduce a symbolic extension of fuzzy logic programs in which some\nof these parameters can be left unknown, so that the user can easily see the\nimpact of their possible values. Furthermore, given a number of test cases, the\nmost appropriate values for these parameters can be automatically computed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:51:07 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Moreno", "Gin\u00e9s", ""], ["Penabad", "Jaime", ""], ["Vidal", "Germ\u00e1n", ""]]}, {"id": "1608.05252", "submitter": "Carlos Olarte", "authors": "Moreno Falaschi, Maurizio Gabbrielli, Carlos Olarte, Catuscia\n  Palamidessi", "title": "Slicing Concurrent Constraint Programs", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/21", "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent Constraint Programming (CCP) is a declarative model for\nconcurrency where agents interact by telling and asking constraints (pieces of\ninformation) in a shared store. Some previous works have developed\n(approximated) declarative debuggers for CCP languages. However, the task of\ndebugging concurrent programs remains difficult. In this paper we define a\ndynamic slicer for CCP and we show it to be a useful companion tool for the\nexisting debugging techniques. Our technique starts by considering a partial\ncomputation (a trace) that shows the presence of bugs. Often, the quantity of\ninformation in such a trace is overwhelming, and the user gets easily lost,\nsince she cannot focus on the sources of the bugs. Our slicer allows for\nmarking part of the state of the computation and assists the user to eliminate\nmost of the redundant information in order to highlight the errors. We show\nthat this technique can be tailored to timed variants of CCP. We also develop a\nprototypical implementation freely available for making experiments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 13:36:21 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 14:43:44 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Falaschi", "Moreno", ""], ["Gabbrielli", "Maurizio", ""], ["Olarte", "Carlos", ""], ["Palamidessi", "Catuscia", ""]]}, {"id": "1608.05327", "submitter": "Igor Konnov", "authors": "Igor Konnov, Marijana Lazic, Helmut Veith and Josef Widder", "title": "A Short Counterexample Property for Safety and Liveness Verification of\n  Fault-tolerant Distributed Algorithms", "comments": "16 pages, 11 pages appendix", "journal-ref": null, "doi": "10.1145/3009837.3009860", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed algorithms have many mission-critical applications ranging from\nembedded systems and replicated databases to cloud computing. Due to\nasynchronous communication, process faults, or network failures, these\nalgorithms are difficult to design and verify. Many algorithms achieve fault\ntolerance by using threshold guards that, for instance, ensure that a process\nwaits until it has received an acknowledgment from a majority of its peers.\nConsequently, domain-specific languages for fault-tolerant distributed systems\noffer language support for threshold guards.\n  We introduce an automated method for model checking of safety and liveness of\nthreshold-guarded distributed algorithms in systems where the number of\nprocesses and the fraction of faulty processes are parameters. Our method is\nbased on a short counterexample property: if a distributed algorithm violates a\ntemporal specification (in a fragment of LTL), then there is a counterexample\nwhose length is bounded and independent of the parameters. We prove this\nproperty by (i) characterizing executions depending on the structure of the\ntemporal formula, and (ii) using commutativity of transitions to accelerate and\nshorten executions. We extended the ByMC toolset (Byzantine Model Checker) with\nour technique, and verified liveness and safety of 10 prominent fault-tolerant\ndistributed algorithms, most of which were out of reach for existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 16:43:03 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 10:37:16 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Konnov", "Igor", ""], ["Lazic", "Marijana", ""], ["Veith", "Helmut", ""], ["Widder", "Josef", ""]]}, {"id": "1608.05368", "submitter": "Anushri Jana", "authors": "Anushri Jana, Uday P. Khedker, Advaita Datar, R Venkatesh, C Niyas", "title": "Scaling Bounded Model Checking By Transforming Programs With Arrays", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/23", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded Model Checking is one the most successful techniques for finding bugs\nin program. However, for programs with loops iterating over large-sized arrays,\nbounded model checkers often exceed the limit of resources available to them.\nWe present a transformation that enables bounded model checkers to verify a\ncertain class of array properties. Our technique transforms an\narray-manipulating program in ANSI-C to an array-free and loop-free program.\nThe transformed program can efficiently be verified by an off-the-shelf bounded\nmodel checker. Though the transformed program is, in general, an abstraction of\nthe original program, we formally characterize the properties for which the\ntransformation is precise. We demonstrate the applicability and usefulness of\nour technique on both industry code as well as academic benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 17:35:49 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Jana", "Anushri", ""], ["Khedker", "Uday P.", ""], ["Datar", "Advaita", ""], ["Venkatesh", "R", ""], ["Niyas", "C", ""]]}, {"id": "1608.05440", "submitter": "Manuel Carro", "authors": "Manuel Carro and Andy King (Eds.)", "title": "Papers presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016)", "comments": "To be published at Theory and Practive of Logic Programming", "journal-ref": "Theory and Practice of Logic Programming, Vol. 16, 2016", "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the list of the full papers accepted for presentation at the 32nd\nInternational Conference on Logic Programming, New York City, USA, October\n18-21, 2016.\n  In addition to the main conference itself, ICLP hosted four pre-conference\nworkshops, the Autumn School on Logic Programing, and a Doctoral Consortium.\n  The final versions of the full papers will be published in a special issue of\nthe journal Theory and Practice of Logic Programming (TPLP). We received eighty\neight abstract submissions, of which twenty seven papers were accepted for\npublication as TPLP rapid communications.\n  Papers deemed of sufficiently high quality to be presented as the conference,\nbut not enough to be appear in TPLP, will be published as Technical\nCommunications in the OASIcs series. Fifteen papers fell into this category.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 21:58:34 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Carro", "Manuel", "", "Eds."], ["King", "Andy", "", "Eds."]]}, {"id": "1608.05521", "submitter": "Germ\\'an Vidal", "authors": "Naoki Nishida and Adri\\'an Palacios and Germ\\'an Vidal", "title": "Towards Reversible Computation in Erlang", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/20", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a reversible language, any forward computation can be undone by a finite\nsequence of backward steps. Reversible computing has been studied in the\ncontext of different programming languages and formalisms, where it has been\nused for debugging and for enforcing fault-tolerance, among others. In this\npaper, we consider a subset of Erlang, a concurrent language based on the actor\nmodel. We formally introduce a reversible semantics for this language. To the\nbest of our knowledge, this is the first attempt to define a reversible\nsemantics for Erlang.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 08:01:25 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Nishida", "Naoki", ""], ["Palacios", "Adri\u00e1n", ""], ["Vidal", "Germ\u00e1n", ""]]}, {"id": "1608.05548", "submitter": "Lo\\\"ic Paulev\\'e", "authors": "Lo\\\"ic Paulev\\'e", "title": "Goal-Oriented Reduction of Automata Networks", "comments": "Accepted at CMSB 2016", "journal-ref": null, "doi": "10.1007/978-3-319-45177-0_16", "report-no": null, "categories": "cs.LO cs.DM cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider networks of finite-state machines having local transitions\nconditioned by the current state of other automata. In this paper, we depict a\nreduction procedure tailored for a given reachability property of the form\n``from global state s there exists a sequence of transitions leading to a state\nwhere an automaton g is in a local state T'. By exploiting a causality analysis\nof the transitions within the individual automata, the proposed reduction\nremoves local transitions while preserving all the minimal traces that satisfy\nthe reachability property. The complexity of the procedure is polynomial in the\ntotal number of local states and transitions, and exponential in the number of\nlocal states within one automaton. Applied to automata networks modelling\ndynamics of biological systems, we observe that the reduction shrinks down\nsignificantly the reachable state space, enhancing the tractability of the\nmodel-checking of large networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 09:31:45 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Paulev\u00e9", "Lo\u00efc", ""]]}, {"id": "1608.05617", "submitter": "Michael Hanus", "authors": "Michael Hanus", "title": "CurryCheck: Checking Properties of Curry Programs", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/43", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CurryCheck, a tool to automate the testing of programs written in\nthe functional logic programming language Curry. CurryCheck executes unit tests\nas well as property tests which are parameterized over one or more arguments.\nIn the latter case, CurryCheck tests these properties by systematically\nenumerating test cases so that, for smaller finite domains, CurryCheck can\nactually prove properties. Unit tests and properties can be defined in a Curry\nmodule without being exported. Thus, they are also useful to document the\nintended semantics of the source code. Furthermore, CurryCheck also supports\nthe automated checking of specifications and contracts occurring in source\nprograms. Hence, CurryCheck is a useful tool that contributes to the property-\nand specification-based development of reliable and well tested declarative\nprograms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:40:02 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Hanus", "Michael", ""]]}, {"id": "1608.05619", "submitter": "Alicia Villanueva", "authors": "Mar\\'ia Alpuente, Daniel Pardo, Alicia Villanueva", "title": "Symbolic Abstract Contract Synthesis in a Rewriting Framework", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/1", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automated technique for inferring software contracts from\nprograms that are written in a non-trivial fragment of C, called KernelC, that\nsupports pointer-based structures and heap manipulation. Starting from the\nsemantic definition of KernelC in the K framework, we enrich the symbolic\nexecution facilities recently provided by K with novel capabilities for\nassertion synthesis that are based on abstract subsumption. Roughly speaking,\nwe define an abstract symbolic technique that explains the execution of a\n(modifier) C function by using other (observer) routines in the same program.\nWe implemented our technique in the automated tool KindSpec 2.0, which\ngenerates logical axioms that express pre- and post-condition assertions by\ndefining the precise input/output behaviour of the C routines.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:43:25 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Alpuente", "Mar\u00eda", ""], ["Pardo", "Daniel", ""], ["Villanueva", "Alicia", ""]]}, {"id": "1608.05675", "submitter": "Michael Morak", "authors": "Manuel Bichler, Michael Morak and Stefan Woltran", "title": "lpopt: A Rule Optimization Tool for Answer Set Programming", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534), 14 pages, LaTeX, 2\n  figures", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/40", "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art answer set programming (ASP) solvers rely on a program\ncalled a grounder to convert non-ground programs containing variables into\nvariable-free, propositional programs. The size of this grounding depends\nheavily on the size of the non-ground rules, and thus, reducing the size of\nsuch rules is a promising approach to improve solving performance. To this end,\nin this paper we announce lpopt, a tool that decomposes large logic programming\nrules into smaller rules that are easier to handle for current solvers. The\ntool is specifically tailored to handle the standard syntax of the ASP language\n(ASP-Core) and makes it easier for users to write efficient and intuitive ASP\nprograms, which would otherwise often require significant hand-tuning by expert\nASP engineers. It is based on an idea proposed by Morak and Woltran (2012) that\nwe extend significantly in order to handle the full ASP syntax, including\ncomplex constructs like aggregates, weak constraints, and arithmetic\nexpressions. We present the algorithm, the theoretical foundations on how to\ntreat these constructs, as well as an experimental evaluation showing the\nviability of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 17:20:03 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 07:59:54 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Bichler", "Manuel", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1608.05676", "submitter": "Bin Fang", "authors": "Bin Fang, Mihaela Sighireanu", "title": "Hierarchical Shape Abstraction for Analysis of Free-List Memory\n  Allocators", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/26", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchical abstract domain for the analysis of free-list\nmemory allocators that tracks shape and numerical properties about both the\nheap and the free lists. Our domain is based on Separation Logic extended with\npredicates that capture the pointer arithmetics constraints for the heap-list\nand the shape of the free-list. These predicates are combined using a\nhierarchical composition operator to specify the overlapping of the heap-list\nby the free-list. In addition to expressiveness, this operator leads to a\ncompositional and compact representation of abstract values and simplifies the\nimplementation of the abstract domain. The shape constraints are combined with\nnumerical constraints over integer arrays to track properties about the\nallocation policies (best-fit, first-fit, etc). Such properties are out of the\nscope of the existing analyzers. We implemented this domain and we show its\neffectiveness on several implementations of free-list allocators.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 17:20:11 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Fang", "Bin", ""], ["Sighireanu", "Mihaela", ""]]}, {"id": "1608.05698", "submitter": "Maciej Zielenkiewicz", "authors": "Maciej Zielenkiewicz, Aleksy Schubert", "title": "Automata Theory Approach to Predicate Intuitionistic Logic", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/32", "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicate intuitionistic logic is a well established fragment of dependent\ntypes. According to the Curry-Howard isomorphism proof construction in the\nlogic corresponds well to synthesis of a program the type of which is a given\nformula. We present a model of automata that can handle proof construction in\nfull intuitionistic first-order logic. The automata are constructed in such a\nway that any successful run corresponds directly to a normal proof in the\nlogic. This makes it possible to discuss formal languages of proofs or\nprograms, the closure properties of the automata and their connections with the\ntraditional logical connectives.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 19:08:39 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zielenkiewicz", "Maciej", ""], ["Schubert", "Aleksy", ""]]}, {"id": "1608.05763", "submitter": "Arun Nampally", "authors": "Arun Nampally and C. R. Ramakrishnan", "title": "Inference in Probabilistic Logic Programs using Lifted Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of lifted inference in the context of\nPrism-like probabilistic logic programming languages. Traditional inference in\nsuch languages involves the construction of an explanation graph for the query\nand computing probabilities over this graph. When evaluating queries over\nprobabilistic logic programs with a large number of instances of random\nvariables, traditional methods treat each instance separately. For many\nprograms and queries, we observe that explanations can be summarized into\nsubstantially more compact structures, which we call lifted explanation graphs.\nIn this paper, we define lifted explanation graphs and operations over them. In\ncontrast to existing lifted inference techniques, our method for constructing\nlifted explanations naturally generalizes existing methods for constructing\nexplanation graphs. To compute probability of query answers, we solve\nrecurrences generated from the lifted graphs. We show examples where the use of\nour technique reduces the asymptotic complexity of inference.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 00:37:20 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Nampally", "Arun", ""], ["Ramakrishnan", "C. R.", ""]]}, {"id": "1608.05787", "submitter": "Martin Ziegler", "authors": "Sewon Park, Franz Brau{\\ss}e, Pieter Collins, SunYoung Kim, Michal\n  Kone\\v{c}n\\'y, Gyesik Lee, Norbert M\\\"uller, Eike Neumann, Norbert Preining,\n  Martin Ziegler", "title": "Foundation of Computer (Algebra) ANALYSIS Systems: Semantics, Logic,\n  Programming, Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a semantics of operating on real numbers that is sound,\nTuring-complete, and practical. It modifies the intuitive but super-recursive\nBlum-Shub-Smale model (formalizing Computer ALGEBRA Systems), to coincide in\npower with the realistic but inconvenient Type-2 Turing machine underlying\nComputable Analysis: reconciling both as foundation to a Computer ANALYSIS\nSystem.\n  Several examples illustrate the elegance of rigorous numerical coding in this\nframework, formalized as a simple imperative programming language ERC with\ndenotational semantics for REALIZING a real function $f$: arguments $x$ are\ngiven as exact real numbers, while values $y=f(x)$ suffice to be returned\napproximately up to absolute error $2^p$ with respect to an additionally given\ninteger parameter $p\\to-\\infty$. Real comparison (necessarily) becomes partial,\npossibly 'returning' the lazy Kleenean value UNKNOWN (subtly different from\n$\\bot$ for classically undefined expressions like 1/0). This asserts closure\nunder composition, and in fact 'Turing-completeness over the reals': All and\nonly functions computable in the sense of Computable Analysis can be realized\nin ERC. Programs thus operate on a many-sorted structure involving real numbers\nand integers, the latter connected via the 'error' embedding $Z\\ni p\\mapsto\n2^p\\in R$, whose first-order theory is proven decidable and model-complete.\nThis logic serves for formally specifying and formally verifying correctness of\nERC programs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 06:12:45 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 19:12:18 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2018 14:24:05 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 06:25:18 GMT"}, {"version": "v5", "created": "Mon, 14 Sep 2020 01:59:03 GMT"}, {"version": "v6", "created": "Mon, 7 Jun 2021 15:55:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Park", "Sewon", ""], ["Brau\u00dfe", "Franz", ""], ["Collins", "Pieter", ""], ["Kim", "SunYoung", ""], ["Kone\u010dn\u00fd", "Michal", ""], ["Lee", "Gyesik", ""], ["M\u00fcller", "Norbert", ""], ["Neumann", "Eike", ""], ["Preining", "Norbert", ""], ["Ziegler", "Martin", ""]]}, {"id": "1608.05893", "submitter": "Tatsuya Abe", "authors": "Tatsuya Abe, Tomoharu Ugawa, Toshiyuki Maeda, and Kousuke Matsumoto", "title": "Reducing State Explosion for Software Model Checking with Relaxed Memory\n  Consistency Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software model checking suffers from the so-called state explosion problem,\nand relaxed memory consistency models even worsen this situation. What is\nworse, parameterizing model checking by memory consistency models, that is, to\nmake the model checker as flexible as we can supply definitions of memory\nconsistency models as an input, intensifies state explosion. This paper\nexplores specific reasons for state explosion in model checking with multiple\nmemory consistency models, provides some optimizations intended to mitigate the\nproblem, and applies them to McSPIN, a model checker for memory consistency\nmodels that we are developing. The effects of the optimizations and the\nusefulness of McSPIN are demonstrated experimentally by verifying copying\nprotocols of concurrent copying garbage collection algorithms. To the best of\nour knowledge, this is the first model checking of the concurrent copying\nprotocols under relaxed memory consistency models.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 04:08:45 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Abe", "Tatsuya", ""], ["Ugawa", "Tomoharu", ""], ["Maeda", "Toshiyuki", ""], ["Matsumoto", "Kousuke", ""]]}, {"id": "1608.06084", "submitter": "Igor Sedl\\'ar", "authors": "Igor Sedl\\'ar", "title": "Propositional dynamic logic with Belnapian truth values", "comments": "To appear in the Proceedings of Advances in Modal Logic 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BPDL, a combination of propositional dynamic logic PDL with the\nbasic four-valued modal logic BK studied by Odintsov and Wansing (`Modal logics\nwith Belnapian truth values', J. Appl. Non-Class. Log. 20, 279--301 (2010)). We\nmodify the standard arguments based on canonical models and filtration to suit\nthe four-valued context and prove weak completeness and decidability of BPDL.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 08:41:51 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Sedl\u00e1r", "Igor", ""]]}, {"id": "1608.06130", "submitter": "Andr\\'e Frochaux", "authors": "Andr\\'e Frochaux and Nicole Schweikardt", "title": "Monadic Datalog Containment on Trees Using the Descendant-Axis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their AMW14-paper, Frochaux, Grohe, and Schweikardt showed that the query\ncontainment problem for monadic datalog on finite unranked labeled trees is\nExptime-complete when (a) considering unordered trees using the child-axis, and\nwhen (b) considering ordered trees using the axes firstchild, nextsibling, and\nchild. Furthermore, when allowing to use also the descendant-axis, the query\ncontainment problem was shown to be solvable in 2-fold exponential time, but it\nremained open to determine the problems exact complexity in presence of the\ndescendant-axis. The present paper closes this gap by showing that, in the\npresence of the descendant-axis, the problem is 2Exptime-hard.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 11:37:20 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Frochaux", "Andr\u00e9", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1608.06212", "submitter": "Antoine Amarilli", "authors": "Jan A. Bergstra and Alban Ponse", "title": "Datatype defining rewrite systems for naturals and integers", "comments": "arXiv admin note: text overlap with arXiv:1406.3280", "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 1 (February\n  18, 2021) lmcs:7192", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A datatype defining rewrite system (DDRS) is an algebraic (equational)\nspecification intended to specify a datatype. When interpreting the equations\nfrom left-to-right, a DDRS defines a term rewriting system that must be\nground-complete. First we define two DDRSs for the ring of integers, each\ncomprising twelve rewrite rules, and prove their ground-completeness. Then we\nintroduce natural number and integer arithmetic specified according to unary\nview, that is, arithmetic based on a postfix unary append constructor (a form\nof tallying). Next we specify arithmetic based on two other views: binary and\ndecimal notation. The binary and decimal view have as their characteristic that\neach normal form resembles common number notation, that is, either a digit, or\na string of digits without leading zero, or the negated versions of the latter.\nInteger arithmetic in binary and decimal notation is based on (postfix) digit\nappend functions. For each view we define a DDRS, and in each case the\nresulting datatype is a canonical term algebra that extends a corresponding\ncanonical term algebra for natural numbers. Then, for each view, we consider an\nalternative DDRS based on tree constructors that yields comparable normal\nforms, which for that view admits expressions that are algorithmically more\ninvolved. For all DDRSs considered, ground-completeness is proven.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 16:25:23 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 15:29:52 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 17:51:54 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2021 13:58:28 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 14:52:55 GMT"}, {"version": "v6", "created": "Wed, 17 Feb 2021 13:57:55 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Bergstra", "Jan A.", ""], ["Ponse", "Alban", ""]]}, {"id": "1608.06392", "submitter": "Waqar  Ahmed", "authors": "Waqar Ahmed and Osman Hasan", "title": "Formalization of Fault Trees in Higher-order Logic: A Deep Embedding\n  Approach", "comments": "arXiv admin note: text overlap with arXiv:1505.02648", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault Tree (FT) is a standard failure modeling technique that has been\nextensively used to predict reliability, availability and safety of many\ncomplex engineering systems. In order to facilitate the formal analysis of FT\nbased analyses, a higher-order-logic formalization of FTs has been recently\nproposed. However, this formalization is quite limited in terms of handling\nlarge systems and transformation of FT models into their corresponding\nReliability Block Diagram (RBD) structures, i.e., a frequently used\ntransformation in reliability and availability analyses. In order to overcome\nthese limitations, we present a deep embedding based formalization of FTs. In\nparticular, the paper presents a formalization of AND, OR and NOT FT gates,\nwhich are in turn used to formalize other commonly used FT gates, i.e., NAND,\nNOR, XOR, Inhibit, Comparator and majority Voting, and the formal verification\nof their failure probability expressions. For illustration purposes, we present\na formal failure analysis of a communication gateway software for the next\ngeneration air traffic management system.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 06:11:10 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Ahmed", "Waqar", ""], ["Hasan", "Osman", ""]]}, {"id": "1608.06567", "submitter": "Shaull Almagor", "authors": "Shaull Almagor and Orna Kupferman", "title": "High-Quality Synthesis Against Stochastic Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical synthesis problem, we are given an LTL formula psi over sets\nof input and output signals, and we synthesize a transducer that realizes psi.\nOne weakness of automated synthesis in practice is that it pays no attention to\nthe quality of the synthesized system. Indeed, the classical setting is\nBoolean: a computation satisfies a specification or does not satisfy it.\nAccordingly, while the synthesized system is correct, there is no guarantee\nabout its quality. In recent years, researchers have considered extensions of\nthe classical Boolean setting to a quantitative one. The logic LTL[F] is a\nmulti-valued logic that augments LTL with quality operators. The satisfaction\nvalue of an LTL[F] formula is a real value in [0,1], where the higher the value\nis, the higher is the quality in which the computation satisfies the\nspecification.\n  Decision problems for LTL become search or optimization problems for LFL[F].\nIn particular, in the synthesis problem, the goal is to generate a transducer\nthat satisfies the specification in the highest possible quality.\n  Previous work considered the worst-case setting, where the goal is to\nmaximize the quality of the computation with the minimal quality. We introduce\nand solve the stochastic setting, where the goal is to generate a transducer\nthat maximizes the expected quality of a computation, subject to a given\ndistribution of the input signals. Thus, rather than being hostile, the\nenvironment is assumed to be probabilistic, which corresponds to many realistic\nsettings. We show that the problem is 2EXPTIME-complete, like classical LTL\nsynthesis, and remains so in two extensions we consider: one that maximizes the\nexpected quality while guaranteeing that the minimal quality is, with\nprobability $1$, above a given threshold, and one that allows assumptions on\nthe environment.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 16:21:19 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Almagor", "Shaull", ""], ["Kupferman", "Orna", ""]]}, {"id": "1608.06729", "submitter": "Zhe Hou", "authors": "Zhe Hou and Alwen Tiu", "title": "Completeness for a First-order Abstract Separation Logic", "comments": "This is an extended version of the APLAS 2016 paper with the same\n  title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work on theorem proving for the assertion language of separation\nlogic (SL) either focuses on abstract semantics which are not readily available\nin most applications of program verification, or on concrete models for which\ncompleteness is not possible. An important element in concrete SL is the\npoints-to predicate which denotes a singleton heap. SL with the points-to\npredicate has been shown to be non-recursively enumerable. In this paper, we\ndevelop a first-order SL, called FOASL, with an abstracted version of the\npoints-to predicate. We prove that FOASL is sound and complete with respect to\nan abstract semantics, of which the standard SL semantics is an instance. We\nalso show that some reasoning principles involving the points-to predicate can\nbe approximated as FOASL theories, thus allowing our logic to be used for\nreasoning about concrete program verification problems. We give some example\ntheories that are sound with respect to different variants of separation logics\nfrom the literature, including those that are incompatible with Reynolds's\nsemantics. In the experiment we demonstrate our FOASL based theorem prover\nwhich is able to handle a large fragment of separation logic with heap\nsemantics as well as non-standard semantics.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 06:59:41 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Hou", "Zhe", ""], ["Tiu", "Alwen", ""]]}, {"id": "1608.07163", "submitter": "David Cerna", "authors": "David Cerna", "title": "Total Recursion over Lexicographical Orderings: Elementary Recursive\n  Operators Beyond PR", "comments": "Remains too incomplete and I would like to avoid future reference to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we generalize primitive recursion in order to construct a\nhierarchy of terminating total recursive operators which we refer to as {\\em\nleveled primitive recursion of order $i$}($\\mathbf{PR}_{i}$). Primitive\nrecursion is equivalent to leveled primitive recursion of order $1$\n($\\mathbf{PR}_{1}$). The functions constructable from the basic functions make\nup $\\mathbf{PR}_{0}$. Interestingly, we show that $\\mathbf{PR}_{2}$ is a\nconservative extension of $\\mathbf{PR}_{1}$. However, members of the hierarchy\nbeyond $\\mathbf{PR}_{2}$, that is $\\mathbf{PR}_{i}$ where $i\\geq 3$, can\nformalize the Ackermann function, and thus are more expressive than primitive\nrecursion. It remains an open question which members of the hierarchy are more\nexpressive than the previous members and which are conservative extensions. We\nconjecture that for all $i\\geq 1$ $\\mathbf{PR}_{2i} \\subset\n\\mathbf{PR}_{2i+1}$. Investigation of further extensions is left for future\nwork.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 13:34:32 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 06:06:31 GMT"}, {"version": "v3", "created": "Wed, 3 Jan 2018 11:56:34 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Cerna", "David", ""]]}, {"id": "1608.07398", "submitter": "EPTCS", "authors": "Gregor G\\\"ossler (INRIA, France), Oleg Sokolsky", "title": "Proceedings First Workshop on Causal Reasoning for Embedded and\n  safety-critical Systems Technologies", "comments": null, "journal-ref": "EPTCS 224, 2016", "doi": "10.4204/EPTCS.224", "report-no": null, "categories": "cs.LO cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal approaches for automated causality analysis, fault localization,\nexplanation of events, accountability and blaming have been proposed\nindependently by several communities --- in particular, AI, concurrency,\nmodel-based diagnosis, formal methods. Work on these topics has significantly\ngained speed during the last years. The goals of CREST are to bring together\nand foster exchange between researchers from the different communities, and to\npresent and discuss recent advances and new ideas in the field.\n  The workshop program consisted of a set of invited and contributed\npresentations that illustrate different techniques for, and applications of,\ncausality analysis and fault localization.\n  The program was anchored by two keynote talks. The keynote by Hana Chockler\n(King's College) provided a broad perspective on the application of causal\nreasoning based on Halpern and Pearl's definitions of actual causality to a\nvariety of application domains ranging from formal verification to legal\nreasoning. The keynote by Chao Wang (Virginia Tech) concentrated on\nconstraint-based analysis techniques for debugging and verifying concurrent\nprograms.\n  Workshop papers deal with compositional causality analysis and a wide\nspectrum of application for causal reasoning, such as debugging of\nprobabilistic models, accountability and responsibility, hazard analysis in\npractice based on Lewis' counterfactuals, and fault localization and repair.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 09:13:22 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["G\u00f6ssler", "Gregor", "", "INRIA, France"], ["Sokolsky", "Oleg", ""]]}, {"id": "1608.07514", "submitter": "Thorsten Wissmann", "authors": "Leszek Ko{\\l}odziejczyk, Henryk Michalewski, Pierre Pradic, Micha{\\l}\n  Skrzypczak", "title": "The logical strength of B\\\"uchi's decidability theorem", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (May 23,\n  2019) lmcs:5503", "doi": "10.23638/LMCS-15(2:16)2019", "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the strength of axioms needed to prove various results related to\nautomata on infinite words and B\\\"uchi's theorem on the decidability of the MSO\ntheory of $(N, {\\le})$. We prove that the following are equivalent over the\nweak second-order arithmetic theory $RCA_0$:\n  (1) the induction scheme for $\\Sigma^0_2$ formulae of arithmetic,\n  (2) a variant of Ramsey's Theorem for pairs restricted to so-called additive\ncolourings,\n  (3) B\\\"uchi's complementation theorem for nondeterministic automata on\ninfinite words,\n  (4) the decidability of the depth-$n$ fragment of the MSO theory of $(N,\n{\\le})$, for each $n \\ge 5$.\n  Moreover, each of (1)-(4) implies McNaughton's determinisation theorem for\nautomata on infinite words, as well as the \"bounded-width\" version of K\\\"onig's\nLemma, often used in proofs of McNaughton's theorem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 16:39:03 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 11:46:55 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 17:03:32 GMT"}, {"version": "v4", "created": "Wed, 22 May 2019 06:14:41 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ko\u0142odziejczyk", "Leszek", ""], ["Michalewski", "Henryk", ""], ["Pradic", "Pierre", ""], ["Skrzypczak", "Micha\u0142", ""]]}, {"id": "1608.07703", "submitter": "Juan P. Aguilera", "authors": "Juan P. Aguilera and Matthias Baaz", "title": "Unsound Inferences Make Proofs Shorter", "comments": "21 pages. July 2017 preprint", "journal-ref": "J. symb. log. 84 (2019) 102-122", "doi": "10.1017/jsl.2018.51", "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give examples of calculi that extend Gentzen's sequent calculus LK by\nunsound quantifier inferences in such a way that (i) derivations lead only to\ntrue sequents, and (ii) proofs therein are non-elementarily shorter than\nLK-proofs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 12:25:32 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 08:55:08 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Aguilera", "Juan P.", ""], ["Baaz", "Matthias", ""]]}, {"id": "1608.07708", "submitter": "Ekaterina Komendantskaya Dr", "authors": "Ekaterina Komendantskaya and John Power", "title": "Logic programming: laxness and saturation", "comments": "30 pages, submitted to Journal of Logic and Algebraic Methods in\n  Programming. arXiv admin note: substantial text overlap with arXiv:1602.05400", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A propositional logic program $P$ may be identified with a $P_fP_f$-coalgebra\non the set of atomic propositions in the program. The corresponding\n$C(P_fP_f)$-coalgebra, where $C(P_fP_f)$ is the cofree comonad on $P_fP_f$,\ndescribes derivations by resolution. That correspondence has been developed to\nmodel first-order programs in two ways, with lax semantics and saturated\nsemantics, based on locally ordered categories and right Kan extensions\nrespectively. We unify the two approaches, exhibiting them as complementary\nrather than competing, reflecting the theorem-proving and proof-search aspects\nof logic programming. While maintaining that unity, we further refine lax\nsemantics to give finitary models of logic programs with existential variables,\nand to develop a precise semantic relationship between variables in logic\nprogramming and worlds in local state.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 13:23:19 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 11:54:08 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Komendantskaya", "Ekaterina", ""], ["Power", "John", ""]]}, {"id": "1608.07792", "submitter": "David Cerna", "authors": "David M. Cerna", "title": "A Generalized Resolution Proof Schema and the Pigeonhole Principle", "comments": "Work is progress. This is the current extension of the the schematic\n  resolution refutation formalism. A better formalism is being investigated.\n  arXiv admin note: substantial text overlap with arXiv:1503.08551,\n  arXiv:1601.06548", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The schematic CERES method is a method of cut elimination for proof schemata,\nthat is a sequence of proofs with a recursive construction. Proof schemata can\nbe thought of as a way to circumvent the addition of an induction rule to the\nLK-calculus. In this work, we formalize a schematic version of the Infinitary\nPigeonhole Principle (IPP), in the LKS-calculus, and analyse the extracted\nclause set schema. However, the refutation we find cannot be expressed as a\nresolution proof schema because there is no clear ordering of the terms\nindexing the recursion, every ordering is used in the refutation. Interesting\nenough, the clause set and its refutation is very close to a canonical form\nfound in cut elimination of LK-proofs. Not being able to handle refutations of\nthis form is problematic in that proof schema, when instantiated, are\nLK-proofs. Based on the structure of our refutation and structural results, we\ndevelop a generalized resolution proof schema based on recursion over a special\ntype of list, and provide a refutation, using our generalization, of the clause\nset extracted from our formal proof of IPP. We also extract a Herbrand System\nfrom the refutation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 09:40:18 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Cerna", "David M.", ""]]}, {"id": "1608.07880", "submitter": "EPTCS", "authors": "Georgiana Caltais (Department for Computer and Information Science,\n  University of Konstanz, Germany), Stefan Leue (Department for Computer and\n  Information Science, University of Konstanz, Germany), Mohammad Reza Mousavi\n  (Centre for Research on Embedded Systems, Halmstad University, Sweden)", "title": "(De-)Composing Causality in Labeled Transition Systems", "comments": "In Proceedings CREST 2016, arXiv:1608.07398", "journal-ref": "EPTCS 224, 2016, pp. 10-24", "doi": "10.4204/EPTCS.224.3", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a notion of counterfactual causality in the\nHalpern and Pearl sense that is compositional with respect to the interleaving\nof transition systems. The formal framework for reasoning on what caused the\nviolation of a safety property is established in the context of labeled\ntransition systems and Hennessy Milner logic. The compositionality results are\ndevised for non-communicating systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:35:55 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Caltais", "Georgiana", "", "Department for Computer and Information Science,\n  University of Konstanz, Germany"], ["Leue", "Stefan", "", "Department for Computer and\n  Information Science, University of Konstanz, Germany"], ["Mousavi", "Mohammad Reza", "", "Centre for Research on Embedded Systems, Halmstad University, Sweden"]]}, {"id": "1608.07881", "submitter": "EPTCS", "authors": "Hichem Debbi", "title": "Debugging of Markov Decision Processes (MDPs) Models", "comments": "In Proceedings CREST 2016, arXiv:1608.07398", "journal-ref": "EPTCS 224, 2016, pp. 25-39", "doi": "10.4204/EPTCS.224.4", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model checking, a counterexample is considered as a valuable tool for\ndebugging. In Probabilistic Model Checking (PMC), counterexample generation has\na quantitative aspect. The counterexample in PMC is a set of paths in which a\npath formula holds, and their accumulative probability mass violates the\nprobability threshold. However, understanding the counterexample is not an easy\ntask. In this paper we address the task of counterexample analysis for Markov\nDecision Processes (MDPs). We propose an aided-diagnostic method for\nprobabilistic counterexamples based on the notions of causality, responsibility\nand blame. Given a counterexample for a Probabilistic CTL (PCTL) formula that\ndoes not hold over an MDP model, this method guides the user to the most\nrelevant parts of the model that led to the violation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:36:04 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Debbi", "Hichem", ""]]}, {"id": "1608.07883", "submitter": "EPTCS", "authors": "Sylvain Hall\\'e (Universit\\'e du Qu\\'ebec \\`a Chicoutimi), Oussama\n  Beroual (Universit\\'e du Qu\\'ebec \\`a Chicoutimi)", "title": "Fault Localization in Web Applications via Model Finding", "comments": "In Proceedings CREST 2016, arXiv:1608.07398", "journal-ref": "EPTCS 224, 2016, pp. 55-67", "doi": "10.4204/EPTCS.224.6", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a generic technique for fault localization independent from the\nnature of the object or the specification language used to declare its expected\nproperties. This technique is based on the concept of \"repair\", a minimal set\nof transformations which, when applied to the original object, restores its\nsatisfiability with respect to the specification. We show how this technique\ncan be applied with various specification languages, including propositional\nand finite first-order logic. In particular, we focus on its use in the\ndetection of layout faults in web applications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:36:24 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Hall\u00e9", "Sylvain", "", "Universit\u00e9 du Qu\u00e9bec \u00e0 Chicoutimi"], ["Beroual", "Oussama", "", "Universit\u00e9 du Qu\u00e9bec \u00e0 Chicoutimi"]]}, {"id": "1608.08033", "submitter": "Van Hung Le", "authors": "Van Hung Le", "title": "Fuzzy Logic in Narrow Sense with Hedges", "comments": "10 pages, International Journal of Computer Science & Information\n  Technology (IJCSIT) Vol 8, No 3, June 2016", "journal-ref": null, "doi": "10.5121/ijcsit.2016.8310", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical logic has a serious limitation in that it cannot cope with the\nissues of vagueness and uncertainty into which fall most modes of human\nreasoning. In order to provide a foundation for human knowledge representation\nand reasoning in the presence of vagueness, imprecision, and uncertainty, fuzzy\nlogic should have the ability to deal with linguistic hedges, which play a very\nimportant role in the modification of fuzzy predicates. In this paper, we\nextend fuzzy logic in narrow sense with graded syntax, introduced by Novak et\nal., with many hedge connectives. In one case, each hedge does not have any\ndual one. In the other case, each hedge can have its own dual one. The\nresulting logics are shown to also have the Pavelka-style completeness\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 12:55:15 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Le", "Van Hung", ""]]}, {"id": "1608.08472", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "ALLSAT compressed with wildcards: From CNF's to orthogonal DNF's by\n  imposing the clauses one by one", "comments": "To appear in The Computer Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for converting a Boolean CNF into an orthogonal\nDNF, aka exclusive sum of products. Our method (which will be pitted against a\nhardwired command from Mathematica) zooms in on the models of the CNF by\nimposing its clauses one by one. Clausal Imposition invites parallelization,\nand wildcards beyond the common don't-care symbol compress the output. The\nmethod is most efficient for few but large clauses. Generalizing clauses one\ncan in fact impose superclauses. By definition, superclauses are obtained from\nclauses by substituting each positive litereal by an arbitrary conjunction of\npositive literals.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:32:41 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 15:57:49 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 14:48:43 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2020 09:46:04 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1608.08704", "submitter": "Christoph Berkholz", "authors": "Christoph Berkholz and Jakob Nordstr\\\"om", "title": "Near-Optimal Lower Bounds on Quantifier Depth and Weisfeiler-Leman\n  Refinement Steps", "comments": "This is the full-length version of a paper with the same title which\n  appeared in Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in\n  Computer Science (LICS '16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove near-optimal trade-offs for quantifier depth versus number of\nvariables in first-order logic by exhibiting pairs of $n$-element structures\nthat can be distinguished by a $k$-variable first-order sentence but where\nevery such sentence requires quantifier depth at least $n^{\\Omega(k/\\log k)}$.\nOur trade-offs also apply to first-order counting logic, and by the known\nconnection to the $k$-dimensional Weisfeiler--Leman algorithm imply\nnear-optimal lower bounds on the number of refinement iterations.\n  A key component in our proof is the hardness condensation technique recently\nintroduced by [Razborov '16] in the context of proof complexity. We apply this\nmethod to reduce the domain size of relational structures while maintaining the\nminimal quantifier depth to distinguish them in finite variable logics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 01:58:56 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 14:53:13 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Berkholz", "Christoph", ""], ["Nordstr\u00f6m", "Jakob", ""]]}, {"id": "1608.08754", "submitter": "Pingfan Kong", "authors": "Pingfan Kong, Yi Li, Xiaohong Chen, Jun Sun, Meng Sun, Jingyi Wang", "title": "Towards Concolic Testing for Hybrid Systems", "comments": "Full paper accepted by Formal Methods Europe 2016 containing appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid systems exhibit both continuous and discrete behavior. Analyzing\nhybrid systems is known to be hard. Inspired by the idea of concolic testing\n(of programs), we investigate whether we can combine random sampling and\nsymbolic execution in order to effectively verify hybrid systems. We identify a\nsufficient condition under which such a combination is more effective than\nrandom sampling. Furthermore, we analyze different strategies of combining\nrandom sampling and symbolic execution and propose an algorithm which allows us\nto dynamically switch between them so as to reduce the overall cost. Our method\nhas been implemented as a web-based checker named HyChecker. HyChecker has been\nevaluated with benchmark hybrid systems and a water treatment system in order\nto test its effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 07:42:53 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Kong", "Pingfan", ""], ["Li", "Yi", ""], ["Chen", "Xiaohong", ""], ["Sun", "Jun", ""], ["Sun", "Meng", ""], ["Wang", "Jingyi", ""]]}, {"id": "1608.08779", "submitter": "Joelle Despeyroux", "authors": "Jo\\\"elle Despeyroux (CRISAM), Carlos Olarte (UFRN), Elaine Pimentel\n  (UFRN)", "title": "Hybrid and Subexponential Linear Logics Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HyLL (Hybrid Linear Logic) and SELL (Subexponential Linear Logic) are logical\nframeworks that have been extensively used for specifying systems that exhibit\nmodalities such as temporal or spatial ones. Both frameworks have linear logic\n(LL) as a common ground and they admit (cut-free) complete focused proof\nsystems. The difference between the two logics relies on the way modalities are\nhandled. In HyLL, truth judgments are labelled by worlds and hybrid connectives\nrelate worlds with formulas. In SELL, the linear logic exponentials (!, ?) are\ndecorated with labels representing locations, and an ordering on such labels\ndefines the provability relation among resources in those locations. It is well\nknown that SELL, as a logical framework, is strictly more expressive than LL.\nHowever, so far, it was not clear whether HyLL is more expressive than LL\nand/or SELL. In this paper, we show an encoding of the HyLL's logical rules\ninto LL with the highest level of adequacy, hence showing that HyLL is as\nexpressive as LL. We also propose an encoding of HyLL into SELL $\\doublecup$\n(SELL plus quantification over locations) that gives better insights about the\nmeaning of worlds in HyLL. We conclude our expressiveness study by showing that\nprevious attempts of encoding Computational Tree Logic (CTL) operators into\nHyLL cannot be extended to consider the whole set of temporal connectives. We\nshow that a system of LL with fixed points is indeed needed to faithfully\nencode the behavior of such temporal operators.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 09:11:12 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 13:37:02 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Despeyroux", "Jo\u00eblle", "", "CRISAM"], ["Olarte", "Carlos", "", "UFRN"], ["Pimentel", "Elaine", "", "UFRN"]]}, {"id": "1608.08918", "submitter": "J\\\"urgen Koslowski", "authors": "Claude Sureson", "title": "Subcomputable Schnorr Randomness", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 2 (April 28,\n  2017) lmcs:3290", "doi": "10.23638/LMCS-13(2:2)2017", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of Schnorr randomness refers to computable reals or computable\nfunctions. We propose a version of Schnorr randomness for subcomputable classes\nand characterize it in different ways: by Martin L\\\"of tests, martingales or\nmeasure computable machines.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 16:04:37 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 11:49:16 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Sureson", "Claude", ""]]}, {"id": "1608.08956", "submitter": "Matthias Van Der Hallen", "authors": "Matthias van der Hallen, Sergey Paramonov, Michael Leuschel, Gerda\n  Janssens", "title": "Knowledge Representation Analysis of Graph Mining", "comments": "Paper presented at the 9th Workshop on Answer Set Programming and\n  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems, especially those with a composite structure, can naturally be\nexpressed in higher order logic. From a KR perspective modeling these problems\nin an intuitive way is a challenging task. In this paper we study the graph\nmining problem as an example of a higher order problem. In short, this problem\nasks us to find a graph that frequently occurs as a subgraph among a set of\nexample graphs. We start from the problem's mathematical definition to solve it\nin three state-of-the-art specification systems. For IDP and ASP, which have no\nnative support for higher order logic, we propose the use of encoding\ntechniques such as the disjoint union technique and the saturation technique.\nProB benefits from the higher order support for sets. We compare the\nperformance of the three approaches to get an idea of the overhead of the\nhigher order support.\n  We propose higher-order language extensions for IDP-like specification\nlanguages and discuss what kind of solver support is needed. Native higher\norder shifts the burden of rewriting specifications using encoding techniques\nfrom the user to the solver itself.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:23:58 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["van der Hallen", "Matthias", ""], ["Paramonov", "Sergey", ""], ["Leuschel", "Michael", ""], ["Janssens", "Gerda", ""]]}]