[{"id": "1906.00107", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan, Mehul Bhatt, and Srikrishna Varadarajan", "title": "Out of Sight But Not Out of Mind: An Answer Set Programming Based Online\n  Abduction Framework for Visual Sensemaking in Autonomous Driving", "comments": "IJCAI 2019: the 28th International Joint Conference on Artificial\n  Intelligence (IJCAI) 2019, August 10 - 16, Macao. (Preprint / to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the need and potential of systematically integrated vision and\nsemantics} solutions for visual sensemaking (in the backdrop of autonomous\ndriving). A general method for online visual sensemaking using answer set\nprogramming is systematically formalised and fully implemented. The method\nintegrates state of the art in (deep learning based) visual computing, and is\ndeveloped as a modular framework usable within hybrid architectures for\nperception & control. We evaluate and demo with community established\nbenchmarks KITTIMOD and MOT. As use-case, we focus on the significance of\nhuman-centred visual sensemaking ---e.g., semantic representation and\nexplainability, question-answering, commonsense interpolation--- in\nsafety-critical autonomous driving situations.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 22:23:15 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Varadarajan", "Srikrishna", ""]]}, {"id": "1906.00197", "submitter": "Stefano Forti", "authors": "Stefano Forti, Federica Paganelli, Antonio Brogi", "title": "Probabilistic QoS-aware Placement of VNF chains at the Edge", "comments": null, "journal-ref": "Theory and Practice of Logic Programming (2021)", "doi": "10.1017/S1471068421000016", "report-no": null, "categories": "cs.NI cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying IoT-enabled Virtual Network Function (VNF) chains to Cloud-Edge\ninfrastructures requires determining a placement for each VNF that satisfies\nall set deployment requirements as well as a software-defined routing of\ntraffic flows between consecutive functions that meets all set communication\nrequirements. In this article, we present a declarative solution, EdgeUsher, to\nthe problem of how to best place VNF chains to Cloud-Edge infrastructures.\nEdgeUsher can determine all eligible placements for a set of VNF chains to a\nCloud-Edge infrastructure so to satisfy all of their hardware, IoT, security,\nbandwidth, and latency requirements. It exploits probability distributions to\nmodel the dynamic variations in the available Cloud-Edge infrastructure, and to\nassess output eligible placements against those variations.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 10:08:01 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 17:10:48 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 13:15:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Forti", "Stefano", ""], ["Paganelli", "Federica", ""], ["Brogi", "Antonio", ""]]}, {"id": "1906.00217", "submitter": "Kirill Krinkin", "authors": "Ren\\'e Haberland, Kirill Krinkin, Sergey Ivanovskiy", "title": "Abstract Predicate Entailment over Points-To Heaplets is Syntax\n  Recognition", "comments": "9 pages, 3 figures", "journal-ref": "IEEE Xplore, 18th Conf. of Open Innovations (FRUCT), 2016,\n  pp.66-74", "doi": "10.1109/FRUCT-ISPIT.2016.7561510", "report-no": null, "categories": "cs.LO cs.FL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract predicates are considered in this paper as abstraction technique for\nheap-separated configurations, and as genuine Prolog predicates which are\ntranslated straight into a corresponding formal language grammar used as\nvalidation scheme for intermediate heap states. The approach presented is\nrule-based because the abstract predicates are rule-based, the parsing\ntechnique can be interpreted as an automated fold/unfold of the corresponding\nheap graph.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 12:36:37 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Haberland", "Ren\u00e9", ""], ["Krinkin", "Kirill", ""], ["Ivanovskiy", "Sergey", ""]]}, {"id": "1906.00259", "submitter": "Paige Randall North", "authors": "Paige Randall North", "title": "Type-theoretic weak factorization systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents three characterizations of the weak factorization\nsystems on finitely complete categories that interpret intensional dependent\ntype theory with Sigma-, Pi-, and Id-types. The first characterization is that\nthe weak factorization system (L,R) has the properties that L is stable under\npullback along R and that all maps to a terminal object are in R. We call such\nweak factorization systems type-theoretic. The second is that the weak\nfactorization system has an Id-presentation: roughly, it is generated by\nId-types in the empty context. The third is that the weak factorization system\n(L, R) is generated by a Moore relation system, a generalization of the notion\nof Moore paths.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 17:01:25 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["North", "Paige Randall", ""]]}, {"id": "1906.00624", "submitter": "Louis Jachiet", "authors": "Michael Benedikt, Pierre Bourhis (CRIStAL, CNRS, SPIRALS), Louis\n  Jachiet (CRIStAL, CNRS, SPIRALS), Micha\\\"el Thomazo (DI-ENS, ENS Paris, CNRS,\n  PSL, VALDA )", "title": "Reasoning about disclosure in data integration in the presence of source\n  constraints", "comments": null, "journal-ref": "28th International Joint Conference on Artificial Intelligence\n  (IJCAI-19), Aug 2019, Macau, China", "doi": "10.24963/ijcai.2019/215", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration systems allow users to access data sitting in multiple\nsources by means of queries over a global schema, related to the sources via\nmappings. Data sources often contain sensitive information, and thus an\nanalysis is needed to verify that a schema satisfies a privacy policy, given as\na set of queries whose answers should not be accessible to users. Such an\nanalysis should take into account not only knowledge that an attacker may have\nabout the mappings, but also what they may know about the semantics of the\nsources. In this paper, we show that source constraints can have a dramatic\nimpact on disclosure analysis. We study the problem of determining whether a\ngiven data integration system discloses a source query to an attacker in the\npresence of constraints, providing both lower and upper bounds on source-aware\ndisclosure analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 08:18:12 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 12:50:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Benedikt", "Michael", "", "CRIStAL, CNRS, SPIRALS"], ["Bourhis", "Pierre", "", "CRIStAL, CNRS, SPIRALS"], ["Jachiet", "Louis", "", "CRIStAL, CNRS, SPIRALS"], ["Thomazo", "Micha\u00ebl", "", "DI-ENS, ENS Paris, CNRS,\n  PSL, VALDA"]]}, {"id": "1906.00703", "submitter": "Arne Meier", "authors": "Yasir Mahmood, Arne Meier, Johannes Schmidt", "title": "Parameterised Complexity of Abduction in Schaefer's Framework", "comments": "gave a more precise title and corrected proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abductive reasoning is a non-monotonic formalism stemming from the work of\nPeirce. It describes the process of deriving the most plausible explanations of\nknown facts. Considering the positive version asking for sets of variables as\nexplanations, we study, besides asking for existence of the set of\nexplanations, two explanation size limited variants of this reasoning problem\n(less than or equal to, and equal to). In this paper, we present a thorough\ntwo-dimensional classification of these problems. The first dimension is\nregarding the parameterised complexity under a wealth of different\nparameterisations. The second dimension spans through all possible Boolean\nfragments of these problems in Schaefer's constraint satisfaction framework\nwith co-clones (STOC 1978). Thereby, we almost complete the parameterised\npicture started by Fellows et al. (AAAI 2012), partially building on results of\nNordh and Zanuttini (Artif. Intell. 2008). In this process, we outline a\nfine-grained analysis of the inherent parameterised intractability of these\nproblems and pinpoint their FPT parts. As the standard algebraic approach is\nnot applicable to our problems, we develop an alternative method that makes the\nalgebraic tools partially available again.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 10:58:35 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:03:03 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mahmood", "Yasir", ""], ["Meier", "Arne", ""], ["Schmidt", "Johannes", ""]]}, {"id": "1906.00715", "submitter": "Kamellia Reshadi", "authors": "Thorsten Ehlers, Florin Manea, Dirk Nowotka, Kamellia Reshadi", "title": "On Modelling the Avoidability of Patterns as CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving avoidability problems in the area of string combinatorics often\nrequires, in an initial step, the construction, via a computer program, of a\nvery long word that does not contain any word that matches a given pattern. It\nis well known that this is a computationally hard task. Despite being rather\nstraightforward that, ultimately, all such tasks can be formalized as\nconstraints satisfaction problems, no unified approach to solving them was\nproposed so far, and very diverse ad-hoc methods were used. We aim to fill this\ngap: we show how several relevant avoidability problems can be modelled, and\nconsequently solved, in an uniform way as constraint satisfaction problems,\nusing the framework of MiniZinc. The main advantage of this approach is that\none is now required only to formulate the avoidability problem in the MiniZinc\nlanguage, and then the actual search for a solution does not have to be\nimplemented ad-hoc, being instead carried out by a standard CSP-solver.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 11:32:34 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ehlers", "Thorsten", ""], ["Manea", "Florin", ""], ["Nowotka", "Dirk", ""], ["Reshadi", "Kamellia", ""]]}, {"id": "1906.00763", "submitter": "Joshua Moerman", "authors": "Joshua Moerman, Jurriaan Rot", "title": "Separation and Renaming in Nominal Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nominal sets provide a foundation for reasoning about names. They are used\nprimarily in syntax with binders, but also, e.g., to model automata over\ninfinite alphabets. In this paper, nominal sets are related to nominal renaming\nsets, which involve arbitrary substitutions rather than permutations, through a\ncategorical adjunction. In particular, the left adjoint relates the separated\nproduct of nominal sets to the Cartesian product of nominal renaming sets.\nBased on these results, we define the new notion of separated nominal automata.\nThese automata can be exponentially smaller than classical nominal automata, if\nthe semantics is closed under substitutions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 12:54:58 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Moerman", "Joshua", ""], ["Rot", "Jurriaan", ""]]}, {"id": "1906.00766", "submitter": "Karoliina Lehtinen", "authors": "Luca Aceto, Antonis Achilleos, Adrian Francalanza, Anna\n  Ing\\'olfsd\\'ottir, Karoliina Lehtinen", "title": "An Operational Guide to Monitorability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitorability delineates what properties can be verified at runtime.\nAlthough many monitorability definitions exist, few are defined explicitly in\nterms of the guarantees provided by monitors, i.e., the computational entities\ncarrying out the verification. We view monitorability as a spectrum: the fewer\nmonitor guarantees that are required, the more properties become monitorable.\nWe present a monitorability hierarchy and provide operational and syntactic\ncharacterisations for its levels. Existing monitorability definitions are\nmapped into our hierarchy, providing a unified framework that makes the\noperational assumptions and guarantees of each definition explicit. This\nprovides a rigorous foundation that can inform design choices and correctness\nclaims for runtime verification tools.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:01:55 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Aceto", "Luca", ""], ["Achilleos", "Antonis", ""], ["Francalanza", "Adrian", ""], ["Ing\u00f3lfsd\u00f3ttir", "Anna", ""], ["Lehtinen", "Karoliina", ""]]}, {"id": "1906.00784", "submitter": "Paul Wild", "authors": "Paul Wild and Lutz Schr\\\"oder and Dirk Pattinson and Barbara K\\\"onig", "title": "A Modal Characterization Theorem for a Probabilistic Fuzzy Description\n  Logic", "comments": "arXiv admin note: text overlap with arXiv:1810.04722", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fuzzy modality `probably` is interpreted over probabilistic type spaces\nby taking expected truth values. The arising probabilistic fuzzy description\nlogic is invariant under probabilistic bisimilarity; more informatively, it is\nnon-expansive wrt. a suitable notion of behavioural distance. In the present\npaper, we provide a characterization of the expressive power of this logic\nbased on this observation: We prove a probabilistic analogue of the classical\nvan Benthem theorem, which states that modal logic is precisely the\nbisimulation-invariant fragment of first-order logic. Specifically, we show\nthat every formula in probabilistic fuzzy first-order logic that is\nnon-expansive wrt. behavioural distance can be approximated by concepts of\nbounded rank in probabilistic fuzzy description logic.\n  For a modal logic perspective on the same result, see arXiv:1810.04722.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 04:37:45 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 12:57:53 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Wild", "Paul", ""], ["Schr\u00f6der", "Lutz", ""], ["Pattinson", "Dirk", ""], ["K\u00f6nig", "Barbara", ""]]}, {"id": "1906.00798", "submitter": "Christopher Hahn", "authors": "Bernd Finkbeiner, Christopher Hahn, Marvin Stenger, Leander Tentrup", "title": "RVHyper: A Runtime Verification Tool for Temporal Hyperproperties", "comments": "arXiv admin note: substantial text overlap with arXiv:1807.00758", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RVHyper, a runtime verification tool for hyperproperties.\nHyperproperties, such as non-interference and observational determinism, relate\nmultiple computation traces with each other. Specifications are given as\nformulas in the temporal logic HyperLTL, which extends linear-time temporal\nlogic (LTL) with trace quantifiers and trace variables. RVHyper processes\nexecution traces sequentially until a violation of the specification is\ndetected. In this case, a counter example, in the form of a set of traces, is\nreturned. As an example application, we show how RVHyper can be used to detect\nspurious dependencies in hardware designs.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 11:26:08 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Finkbeiner", "Bernd", ""], ["Hahn", "Christopher", ""], ["Stenger", "Marvin", ""], ["Tentrup", "Leander", ""]]}, {"id": "1906.00931", "submitter": "Jeffrey Uhlmann", "authors": "Jeffrey Uhlmann and Jie Wang", "title": "On Radically Expanding the Landscape of Potential Applications for\n  Automated Proof Methods", "comments": "Expanded narrative", "journal-ref": "SN Computer Science (2021) 2:259", "doi": "10.1007/s42979-021-00674-w", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the potential of computer-assisted proof methods to\nbe applied much more broadly than commonly recognized. More specifically, we\ncontend that there are vast opportunities to derive useful mathematical results\nand properties that are extremely narrow in scope, and of practical relevance\nonly to highly-specialized engineering applications, that are presently\noverlooked because they have characteristics atypical of those that are\nconventionally pursued in the areas of pure and applied mathematics. As a\nconcrete example, we demonstrate use of automated methods for certifying\npolynomial nonnegativity as a part of a dimension-pinning strategy to prove\nthat the inverse of the relative gain array (RGA) of a d-dimensional\npositive-definite matrix is doubly-stochastic for $d\\leq 4$.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:09:32 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 18:42:18 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 17:05:40 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Uhlmann", "Jeffrey", ""], ["Wang", "Jie", ""]]}, {"id": "1906.01519", "submitter": "Fabio Zanasi", "authors": "Filippo Bonchi, Robin Piedeleu, Pawel Sobocinski, and Fabio Zanasi", "title": "Bialgebraic Semantics for String Diagrams", "comments": "Accepted for publications in the proceedings of the 30th\n  International Conference on Concurrency Theory (CONCUR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turi and Plotkin's bialgebraic semantics is an abstract approach to\nspecifying the operational semantics of a system, by means of a distributive\nlaw between its syntax (encoded as a monad) and its dynamics (an endofunctor).\nThis setup is instrumental in showing that a semantic specification (a\ncoalgebra) satisfies desirable properties: in particular, that it is\ncompositional.\n  In this work, we use the bialgebraic approach to derive well-behaved\nstructural operational semantics of string diagrams, a graphical syntax that is\nincreasingly used in the study of interacting systems across different\ndisciplines. Our analysis relies on representing the two-dimensional operations\nunderlying string diagrams in various categories as a monad, and their\nbialgebraic semantics in terms of a distributive law over that monad.\n  As a proof of concept, we provide bialgebraic compositional semantics for a\nversatile string diagrammatic language which has been used to model both signal\nflow graphs (control theory) and Petri nets (concurrency theory). Moreover, our\napproach reveals a correspondence between two different interpretations of the\nFrobenius equations on string diagrams and two synchronisation mechanisms for\nprocesses, \\`a la Hoare and \\`a la Milner.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:31:35 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 09:25:16 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Bonchi", "Filippo", ""], ["Piedeleu", "Robin", ""], ["Sobocinski", "Pawel", ""], ["Zanasi", "Fabio", ""]]}, {"id": "1906.01583", "submitter": "Hari Govind Vediramana Krishnan", "authors": "Hari Govind V K, Yakir Vizel, Vijay Ganesh and Arie Gurfinkel", "title": "Interpolating Strong Induction", "comments": "Accepted to CAV 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of strong induction, also known as k-induction is one of the\nfirst techniques for unbounded SAT-based Model Checking (SMC). While elegant\nand simple to apply, properties as such are rarely k-inductive and when they\ncan be strengthened, there is no effective strategy to guess the depth of\ninduction. It has been mostly displaced by techniques that compute inductive\nstrengthenings based on interpolation and property directed reachability (Pdr).\nIn this paper, we present kAvy, an SMC algorithm that effectively uses\nk-induction to guide interpolation and Pdr-style inductive generalization.\nUnlike pure k-induction, kAvy uses Pdr-style generalization to compute and\nstrengthen an inductive trace. Unlike pure Pdr, kAvy uses relative k-induction\nto construct an inductive invariant. The depth of induction is adjusted\ndynamically by minimizing a proof of unsatisfiability. We have implemented kAvy\nwithin the Avy Model Checker and evaluated it on HWMCC instances. Our results\nshow that kAvy is more effective than both Avy and Pdr, and that using\nk-induction leads to faster running time and solving more instances. Further,\non a class of benchmarks, called shift, kAvy is orders of magnitude faster than\nAvy, Pdr and k-induction.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:54:42 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["K", "Hari Govind V", ""], ["Vizel", "Yakir", ""], ["Ganesh", "Vijay", ""], ["Gurfinkel", "Arie", ""]]}, {"id": "1906.01734", "submitter": "EPTCS", "authors": "Alexander Cowtan, Silas Dilkes, Ross Duncan, Will Simmons, Seyon\n  Sivarajah", "title": "Phase Gadget Synthesis for Shallow Circuits", "comments": "In Proceedings QPL 2019, arXiv:2004.14750", "journal-ref": "EPTCS 318, 2020, pp. 213-228", "doi": "10.4204/EPTCS.318.13", "report-no": null, "categories": "quant-ph cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of the circuit optimisation methods used by tket, a\ncompiler system for quantum software developed by Cambridge Quantum Computing\nLtd. We focus on a novel technique based around phase gadgets, a family of\nmulti-qubit quantum operations which occur naturally in a wide range of quantum\ncircuits of practical interest. The phase gadgets have a simple presentation in\nthe ZX-calculus, which makes it easy to reason about them. Taking advantage of\nthis, we present an efficient method to translate the phase gadgets back to\nCNOT gates and single qubit operations suitable for execution on a quantum\ncomputer with significant reductions in gate count and circuit depth. We\ndemonstrate the effectiveness of these methods on a quantum chemistry\nbenchmarking set based on variational circuits for ground state estimation of\nsmall molecules.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 21:37:13 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 04:14:19 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Cowtan", "Alexander", ""], ["Dilkes", "Silas", ""], ["Duncan", "Ross", ""], ["Simmons", "Will", ""], ["Sivarajah", "Seyon", ""]]}, {"id": "1906.02111", "submitter": "Yuyu Zhang", "authors": "Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi,\n  Le Song", "title": "Can Graph Neural Networks Help Logic Reasoning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effectively combining logic reasoning and probabilistic inference has been a\nlong-standing goal of machine learning: the former has the ability to\ngeneralize with small training data, while the latter provides a principled\nframework for dealing with noisy data. However, existing methods for combining\nthe best of both worlds are typically computationally intensive. In this paper,\nwe focus on Markov Logic Networks and explore the use of graph neural networks\n(GNNs) for representing probabilistic logic inference. It is revealed from our\nanalysis that the representation power of GNN alone is not enough for such a\ntask. We instead propose a more expressive variant, called ExpressGNN, which\ncan perform effective probabilistic logic inference while being able to scale\nto a large number of entities. We demonstrate by several benchmark datasets\nthat ExpressGNN has the potential to advance probabilistic logic reasoning to\nthe next stage.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 16:40:47 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 19:13:16 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 20:43:15 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zhang", "Yuyu", ""], ["Chen", "Xinshi", ""], ["Yang", "Yuan", ""], ["Ramamurthy", "Arun", ""], ["Li", "Bo", ""], ["Qi", "Yuan", ""], ["Song", "Le", ""]]}, {"id": "1906.02282", "submitter": "Shiqi Wang", "authors": "Shiqi Wang, Yizheng Chen, Ahmed Abdou, Suman Jana", "title": "Enhancing Gradient-based Attacks with Symbolic Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in defenses against adversarial examples, like\nadversarial training, make the neural networks robust against various classes\nof attackers (e.g., first-order gradient-based attacks). However, it is an open\nquestion whether the adversarially trained networks are truly robust under\nunknown attacks. In this paper, we present interval attacks, a new technique to\nfind adversarial examples to evaluate the robustness of neural networks.\nInterval attacks leverage symbolic interval propagation, a bound propagation\ntechnique that can exploit a broader view around the current input to locate\npromising areas containing adversarial instances, which in turn can be searched\nwith existing gradient-guided attacks. We can obtain such a broader view using\nsound bound propagation methods to track and over-approximate the errors of the\nnetwork within given input ranges. Our results show that, on state-of-the-art\nadversarially trained networks, interval attack can find on average 47%\nrelatively more violations than the state-of-the-art gradient-guided PGD\nattack.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:58:07 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wang", "Shiqi", ""], ["Chen", "Yizheng", ""], ["Abdou", "Ahmed", ""], ["Jana", "Suman", ""]]}, {"id": "1906.02427", "submitter": "Vishal Sunder", "authors": "Vishal Sunder, Ashwin Srinivasan, Lovekesh Vig, Gautam Shroff, Rohit\n  Rahul", "title": "One-shot Information Extraction from Document Images using\n  Neuro-Deductive Program Synthesis", "comments": "11 pages, appears in the 13th International Workshop on\n  Neural-Symbolic Learning and Reasoning at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our interest in this paper is in meeting a rapidly growing industrial demand\nfor information extraction from images of documents such as invoices, bills,\nreceipts etc. In practice users are able to provide a very small number of\nexample images labeled with the information that needs to be extracted. We\nadopt a novel two-level neuro-deductive, approach where (a) we use pre-trained\ndeep neural networks to populate a relational database with facts about each\ndocument-image; and (b) we use a form of deductive reasoning, related to\nmeta-interpretive learning of transition systems to learn extraction programs:\nGiven task-specific transitions defined using the entities and relations\nidentified by the neural detectors and a small number of instances (usually 1,\nsometimes 2) of images and the desired outputs, a resource-bounded\nmeta-interpreter constructs proofs for the instance(s) via logical deduction; a\nset of logic programs that extract each desired entity is easily synthesized\nfrom such proofs. In most cases a single training example together with a\nnoisy-clone of itself suffices to learn a program-set that generalizes well on\ntest documents, at which time the value of each entity is determined by a\nmajority vote across its program-set. We demonstrate our two-level\nneuro-deductive approach on publicly available datasets (\"Patent\" and \"Doctor's\nBills\") and also describe its use in a real-life industrial problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:48:21 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Sunder", "Vishal", ""], ["Srinivasan", "Ashwin", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""], ["Rahul", "Rohit", ""]]}, {"id": "1906.02808", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland and Sergey Ivanovskiy", "title": "Dynamically Allocated Memory Verification in Object-Oriented Programs\n  using Prolog", "comments": "5 pages, 4 figures", "journal-ref": "ISP-RAS, 8th Spring/Summer Young Researchers' Colloquium on\n  Software Engineering (SYRCoSE), 2014, pp.46-50, ISBN 978-5-91474-020-4, ISSN\n  2311-7230", "doi": "10.15514/SYRCOSE-2014-8-7", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Prolog-based framework for fully automated verification currently under\ndevelopment for heap-based object-oriented data is introduced. Dynamically\nallocated issues are discussed, recent approaches and criteria are analysed.\nThe architecture and its components are introduced by example. Finally,\npropositions to further and related work are given.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 20:57:53 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Haberland", "Ren\u00e9", ""], ["Ivanovskiy", "Sergey", ""]]}, {"id": "1906.03085", "submitter": "Weilin Luo", "authors": "Weilin Luo and Hai Wan and Hongzhen Zhong and Ou Wei", "title": "CoAPI: An Efficient Two-Phase Algorithm Using Core-Guided\n  Over-Approximate Cover for Prime Compilation of Non-Clausal Formulae", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prime compilation, i.e., the generation of all prime implicates or implicants\n(primes for short) of formulae, is a prominent fundamental issue for AI.\nRecently, the prime compilation for non-clausal formulae has received great\nattention. The state-of-the-art approaches generate all primes along with a\nprime cover constructed by prime implicates using dual rail encoding. However,\nthe dual rail encoding potentially expands search space. In addition,\nconstructing a prime cover, which is necessary for their methods, is\ntime-consuming. To address these issues, we propose a novel two-phase method --\nCoAPI. The two phases are the key to construct a cover without using dual rail\nencoding. Specifically, given a non-clausal formula, we first propose a\ncore-guided method to rewrite the non-clausal formula into a cover constructed\nby over-approximate implicates in the first phase. Then, we generate all the\nprimes based on the cover in the second phase. In order to reduce the size of\nthe cover, we provide a multi-order based shrinking method, with a good\ntradeoff between the small size and efficiency, to compress the size of cover\nconsiderably. The experimental results show that CoAPI outperforms\nstate-of-the-art approaches. Particularly, for generating all prime implicates,\nCoAPI consumes about one order of magnitude less time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 13:26:39 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Luo", "Weilin", ""], ["Wan", "Hai", ""], ["Zhong", "Hongzhen", ""], ["Wei", "Ou", ""]]}, {"id": "1906.03452", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "Algebra of Concurrent Games", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce parallelism into the basic algebra of games to model concurrent\ngame algebraically. Parallelism is treated as a new kind of game operation. The\nresulted algebra of concurrent games can be used widely to reason the parallel\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 13:28:40 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 06:23:25 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 04:33:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1906.03476", "submitter": "Michal Walicki", "authors": "Michal Walicki and Sjur Dyrkolbotn", "title": "Paraconsistency, resolution and relevance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digraphs provide an alternative syntax for propositional logic, with digraph\nkernels corresponding to classical models. Semikernels generalize kernels and\nwe identify a subset of well-behaved semikernels that provides nontrivial\nmodels for inconsistent theories, specializing to the classical semantics for\nthe consistent ones. Direct (instead of refutational) reasoning with classical\nresolution is sound and complete for this semantics, when augmented with a\nspecific weakening which, in particular, excludes Ex Falso. Dropping all forms\nof weakening yields reasoning which also avoids typical fallacies of relevance.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 15:30:12 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Walicki", "Michal", ""], ["Dyrkolbotn", "Sjur", ""]]}, {"id": "1906.03523", "submitter": "Ali Payani", "authors": "Ali Payani and Faramarz Fekri", "title": "Inductive Logic Programming via Differentiable Deep Neural Logic\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a novel paradigm for solving Inductive Logic Programming (ILP)\nproblems via deep recurrent neural networks. This proposed ILP solver is\ndesigned based on differentiable implementation of the deduction via forward\nchaining. In contrast to the majority of past methods, instead of searching\nthrough the space of possible first-order logic rules by using some restrictive\nrule templates, we directly learn the symbolic logical predicate rules by\nintroducing a novel differentiable Neural Logic (dNL) network. The proposed dNL\nnetwork is able to learn and represent Boolean functions efficiently and in an\nexplicit manner. We show that the proposed dNL-ILP solver supports desirable\nfeatures such as recursion and predicate invention. Further, we investigate the\nperformance of the proposed ILP solver in classification tasks involving\nbenchmark relational datasets. In particular, we show that our proposed method\noutperforms the state of the art ILP solvers in classification tasks for\nMutagenesis, Cora and IMDB datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 21:23:21 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Payani", "Ali", ""], ["Fekri", "Faramarz", ""]]}, {"id": "1906.03557", "submitter": "David Naumann", "authors": "David A. Naumann and Minh Ngo", "title": "Whither Programs as Specifications", "comments": "To appear in 7th International Symposium on Unifying Theories of\n  Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unifying theories distil common features of programming languages and design\nmethods by means of algebraic operators and their laws. Several practical\nconcerns --- e.g., improvement of a program, conformance of code with design,\ncorrectness with respect to specified requirements --- are subsumed by the\nbeautiful notion that programs and designs are special forms of specification\nand their relationships are instances of logical implication between\nspecifications. Mathematical development of this idea has been fruitful but\nlimited to an impoverished notion of specification: trace properties. Some\nmathematically precise properties of programs, dubbed hyperproperties, refer to\ntraces collectively. For example, confidentiality involves knowledge of\npossible traces. This article reports on both obvious and surprising results\nabout lifting algebras of programming to hyperproperties, especially in\nconnection with loops, and suggests directions for further research. The\ntechnical results are: a compositional semantics, at the hyper level, of\nimperative programs with loops, and proof that this semantics coincides with\nthe direct image of a standard semantics, for subset closed hyperproperties.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 03:55:10 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 14:14:58 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Naumann", "David A.", ""], ["Ngo", "Minh", ""]]}, {"id": "1906.03611", "submitter": "Anupam Das", "authors": "Anupam Das", "title": "From QBFs to MALL and back via focussing: fragments of multiplicative\n  additive linear logic for each level of the polynomial hierarchy", "comments": "25 pages, 5 figures. Extended version of IJCAR '18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate how to extract alternating time bounds from\n'focussed' proof systems. Our main result is the obtention of fragments of\nMALLw (MALL with weakening) complete for each level of the polynomial\nhierarchy. In one direction we encode QBF satisfiability and in the other we\nencode focussed proof search, and we show that the composition of the two\nencodings preserves quantifier alternation, yielding the required result. By\ncarefully composing with well-known embeddings of MALLw into MALL, we obtain a\nsimilar delineation of MALL formulas, again carving out fragments complete for\neach level of the polynomial hierarchy. This refines the well-known results\nthat both MALLw and MALL are PSPACE-complete.\n  A key insight is that we have to refine the usual presentation of focussing\nto account for deterministic computations in proof search, which correspond to\ninvertible rules that do not branch. This is so that we may more faithfully\nassociate phases of focussed proof search to their alternating time complexity.\nThis presentation seems to uncover further dualities at the level of proof\nsearch than usual presentations, so could be of further proof theoretic\ninterest in its own right.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 10:07:58 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 15:38:15 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 20:15:40 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Das", "Anupam", ""]]}, {"id": "1906.03836", "submitter": "Jorge A. P\\'erez", "authors": "Alen Arslanagi\\'c, Jorge A. P\\'erez, and Erik Voogd", "title": "Minimal Session Types (Extended Version)", "comments": "Extended version of an ECOOP 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types are a type-based approach to the verification of\nmessage-passing programs. They have been much studied as type systems for the\npi-calculus and for languages such as Java. A session type specifies what and\nwhen should be exchanged through a channel. Central to session-typed languages\nare constructs in types and processes that specify sequencing in protocols.\nHere we study minimal session types, session types without sequencing. This is\narguably the simplest form of session types. By relying on a core process\ncalculus with sessions and higher-order concurrency (abstraction passing), we\nprove that every process typable with usual (non minimal) session types can be\ncompiled down into a process typed with minimal session types. This means that\nhaving sequencing constructs in both processes and session types is redundant;\nonly sequentiality in processes is indispensable, as it can precisely codify\nsequentiality in types. Our developments draw inspiration from work by Parrow\non behavior-preserving decompositions of untyped processes. By casting Parrow's\nresults in the realm of typed processes, our results reveal a conceptually\nsimple formulation of session types and a principled avenue to the integration\nof session types into languages without sequencing in types.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 08:20:56 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 13:30:34 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Arslanagi\u0107", "Alen", ""], ["P\u00e9rez", "Jorge A.", ""], ["Voogd", "Erik", ""]]}, {"id": "1906.03930", "submitter": "Tianyu Sun", "authors": "Tianyu Sun and Wensheng Yu", "title": "Formalization of the Axiom of Choice and its Equivalent Theorems", "comments": "26 pages, 2 figures, 2 tables, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the formalization of the axiom of choice and\nseveral of its famous equivalent theorems in Morse-Kelley set theory. These\ntheorems include Tukey's lemma, the Hausdorff maximal principle, the maximal\nprinciple, Zermelo's postulate, Zorn's lemma and the well-ordering theorem. We\nprove the above theorems by the axiom of choice in turn, and finally prove the\naxiom of choice by Zermelo's postulate and the well-ordering theorem, thus\ncompleting the cyclic proof of equivalence between them. The proofs are checked\nformally using the Coq proof assistant in which Morse-Kelley set theory is\nformalized. The whole process of formal proof demonstrates that the Coq-based\nmachine proving of mathematics theorem is highly reliable and rigorous. The\nformal work of this paper is enough for most applications, especially in set\ntheory, topology and algebra.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:17:19 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sun", "Tianyu", ""], ["Yu", "Wensheng", ""]]}, {"id": "1906.03978", "submitter": "Thakur Neupane", "authors": "Thakur Neupane and Chris J. Myers and Curtis Madsen and Hao Zheng and\n  Zhen Zhang", "title": "STAMINA: STochastic Approximate Model-checker for INfinite-state\n  Analysis", "comments": "CAV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic model checking is a technique for analyzing systems that possess\nprobabilistic characteristics. However, its scalability is limited as\nprobabilistic models of real-world applications typically have very large or\ninfinite state space. This paper presents a new infinite state CTMC model\nchecker, STAMINA, with improved scalability. It uses a novel state space\napproximation method to reduce large and possibly infinite state CTMC models to\nfinite state representations that are amenable to existing stochastic model\ncheckers. It is integrated with a new property-guided state expansion approach\nthat improves the analysis accuracy. Demonstration of the tool on several\nbenchmark examples shows promising results in terms of analysis efficiency and\naccuracy compared with a state-of-the-art CTMC model checker that deploys a\nsimilar approximation method.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 01:01:25 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Neupane", "Thakur", ""], ["Myers", "Chris J.", ""], ["Madsen", "Curtis", ""], ["Zheng", "Hao", ""], ["Zhang", "Zhen", ""]]}, {"id": "1906.04199", "submitter": "Shankara Narayanan Krishna", "authors": "V. Dave, E. Filiot, S. Krishna and N. Lhote", "title": "Deciding the Computability of Regular Functions over Infinite Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regular functions from infinite words to infinite words can be equivalently\nspecified by MSO-transducers, streaming $\\omega$-string transducers as well as\ndeterministic two-way transducers with look-ahead. In their one-way\nrestriction, the latter transducers define the class of rational functions.\nEven though regular functions are robustly characterised by several\nfinite-state devices, even the subclass of rational functions may contain\nfunctions which are not computable (by a Turing machine with infinite input).\nThis paper proposes a decision procedure for the following synthesis problem:\ngiven a regular function $f$ (equivalently specified by one of the\naforementioned transducer model), is $f$ computable and if it is, synthesize a\nTuring machine computing it.\n  For regular functions, we show that computability is equivalent to\ncontinuity, and therefore the problem boils down to deciding continuity. We\nestablish a generic characterisation of continuity for functions preserving\nregular languages under inverse image (such as regular functions). We exploit\nthis characterisation to show the decidability of continuity (and hence\ncomputability) of rational and regular functions. For rational functions, we\nshow that this can be done in \\textsc{NLogSpace} (it was already known to be in\n\\textsc{PTime} by Prieur). In a similar fashion, we also effectively\ncharacterise uniform continuity of regular functions, and relate it to the\nnotion of uniform computability, which offers stronger efficiency guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 11:35:35 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 14:37:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Dave", "V.", ""], ["Filiot", "E.", ""], ["Krishna", "S.", ""], ["Lhote", "N.", ""]]}, {"id": "1906.04491", "submitter": "Kees Middelburg", "authors": "J. A. Bergstra, C. A. Middelburg", "title": "Using Hoare logic in a process algebra setting", "comments": "24 pages; presentation improved, examples added", "journal-ref": "Fundamenta Informaticae 179(4):321--344, 2021", "doi": "10.3233/FI-2021-2026", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the relation between process algebra and Hoare logic. We\ninvestigate the question whether and how a Hoare logic can be used for\nreasoning about how data change in the course of a process when reasoning\nequationally about that process. We introduce an extension of ACP (Algebra of\nCommunicating Processes) with features that are relevant to processes in which\ndata are involved, present a Hoare logic for the processes considered in this\nprocess algebra, and discuss the use of this Hoare logic as a complement to\npure equational reasoning with the equational axioms of the process algebra.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:50:01 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 10:37:00 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 11:23:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bergstra", "J. A.", ""], ["Middelburg", "C. A.", ""]]}, {"id": "1906.04765", "submitter": "W{\\l}odzimierz Drabent", "authors": "W{\\l}odzimierz Drabent", "title": "The Prolog debugger and declarative programming", "comments": "15 pages. This version: a reference added to a companion report with\n  examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programming is a declarative programming paradigm. Programming language\nProlog makes logic programming possible, at least to a substantial extent.\nHowever the Prolog debugger works solely in terms of the operational semantics.\nSo it is incompatible with declarative programming. This report discusses this\nissue and tries to find how the debugger may be used from the declarative point\nof view. The results are rather not encouraging.\n  Also, the box model of Byrd, used by the debugger, is explained in terms of\nSLD-resolution.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 18:32:15 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 01:08:36 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 16:30:50 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 19:07:37 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Drabent", "W\u0142odzimierz", ""]]}, {"id": "1906.04984", "submitter": "Pablo Gordillo", "authors": "Elvira Albert, Jes\\'us Correas, Pablo Gordillo, Guillermo\n  Rom\\'an-D\\'iez and Albert Rubio", "title": "SAFEVM: A Safety Verifier for Ethereum Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethereum smart contracts are public, immutable and distributed and, as such,\nthey are prone to vulnerabilities sourcing from programming mistakes of\ndevelopers. This paper presents SAFEVM, a verification tool for Ethereum smart\ncontracts that makes use of state-of-the-art verification engines for C\nprograms. SAFEVM takes as input an Ethereum smart contract (provided either in\nSolidity source code, or in compiled EVM bytecode), optionally with assert and\nrequire verification annotations, and produces in the output a report with the\nverification results. Besides general safety annotations, SAFEVM handles the\nverification of array accesses: it automatically generates SV-COMP verification\nassertions such that C verification engines can prove safety of array accesses.\nOur experimental evaluation has been undertaken on all contracts pulled from\netherscan.io (more than 24,000) by using as back-end verifiers CPAchecker,\nSeaHorn and VeryMax.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 07:49:06 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Albert", "Elvira", ""], ["Correas", "Jes\u00fas", ""], ["Gordillo", "Pablo", ""], ["Rom\u00e1n-D\u00edez", "Guillermo", ""], ["Rubio", "Albert", ""]]}, {"id": "1906.05170", "submitter": "Graham Campbell", "authors": "Graham Campbell", "title": "Efficient Graph Rewriting", "comments": "BSc Thesis, Department of Computer Science, University of York, 54\n  pages, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL cs.SC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph transformation is the rule-based modification of graphs, and is a\ndiscipline dating back to the 1970s. The declarative nature of graph rewriting\nrules comes at a cost. In general, to match the left-hand graph of a fixed rule\nwithin a host graph requires polynomial time. To improve matching performance,\nD\\\"orr proposed to equip rules and host graphs with distinguished root nodes.\nThis model was implemented by Plump and Bak, but unfortunately, is not\ninvertible. We address this problem by defining rootedness using a partial\nfunction onto a two-point set rather than pointing graphs with root nodes. We\nshow a new result that the graph class of trees can be recognised by a rooted\nGT system in linear time, given an input graph of bounded degree. Finally, we\ndefine a new notion of confluence modulo garbage and non-garbage critical\npairs, showing it is sufficient to require strong joinability of only the\nnon-garbage critical pairs to establish confluence modulo garbage.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:50:57 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 11:46:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Campbell", "Graham", ""]]}, {"id": "1906.05340", "submitter": "Bill Stoddart", "authors": "Bill Stoddart", "title": "The Halting Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The halting problem is considered to be an essential part of the theoretical\nbackground to computing. That halting is not in general computable has\nsupposedly been proved in many text books and taught on many computer science\ncourses, in order to illustrate the limits of computation. However, Eric Hehner\nhas a dissenting view, in which the specification of the halting problem is\ncalled into question.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 09:47:19 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Stoddart", "Bill", ""]]}, {"id": "1906.05573", "submitter": "Marco Peressotti", "authors": "Tomasz Brengos and Marco Peressotti", "title": "Two modes of recognition: algebra, coalgebra, and languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to build a connection between two approaches towards\ncategorical language theory: the coalgebraic and algebraic language theory for\nmonads. For a pair of monads modelling the branching and the linear type we\ndefined regular maps that generalize regular languages known in classical\nnon-deterministic automata theory. These maps are behaviours of certain\nautomata (i.e. they possess a coalgebraic nature), yet they arise from\nEilenberg-Moore algebras and their homomorphisms (by exploiting duality between\nthe category of Eilenberg-Moore algebras and saturated coalgebras).\n  Given some additional assumptions, we show that regular maps form a certain\nsubcategory of the Kleisli category for the monad which is the composition of\nthe branching and linear type. Moreover, we state a Kleene-like theorem\ncharacterising the regular morphisms category in terms of the smallest\nsubcategory closed under certain operations. Additionally, whenever the\nbranching type monad is taken to be the powerset monad, we show that regular\nmaps are described as maps recognized by certain functors whose codomains are\ncategories with all finite hom-sets.\n  We instantiate our framework on classical non-deterministic automata, tree\nautomata, fuzzy automata and weighted automata.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:44:54 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Brengos", "Tomasz", ""], ["Peressotti", "Marco", ""]]}, {"id": "1906.05590", "submitter": "Luigi Santocanale", "authors": "Luigi Santocanale (LIS)", "title": "On discrete idempotent paths", "comments": null, "journal-ref": "Words 2019, Sep 2019, Loughborough, United Kingdom", "doi": null, "report-no": null, "categories": "math.LO cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set of discrete lattice paths from (0, 0) to (n, n) with North and East\nsteps (i.e. words w $\\in$ { x, y } * such that |w| x = |w| y = n) has a\ncanonical monoid structure inherited from the bijection with the set of\njoin-continuous maps from the chain { 0, 1,. .. , n } to itself. We explicitly\ndescribe this monoid structure and, relying on a general characterization of\nidempotent join-continuous maps from a complete lattice to itself, we\ncharacterize idempotent paths as upper zigzag paths. We argue that these paths\nare counted by the odd Fibonacci numbers. Our method yields a\ngeometric/combinatorial proof of counting results, due to Howie and to Laradji\nand Umar, for idempotents in monoids of monotone endomaps on finite chains.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 10:33:05 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Santocanale", "Luigi", "", "LIS"]]}, {"id": "1906.05593", "submitter": "Thomas Ehrhard", "authors": "Thomas Ehrhard (IRIF), Farzad Jafar-Rahmani", "title": "On the denotational semantics of Linear Logic with least and greatest\n  fixed points of formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a denotational semantics of Linear Logic with least and greatest\nfixed points in coherence spaces (where both fixed points are interpreted in\nthe same way) and in coherence spaces with totality (where they have different\ninterpretations). These constructions can be carried out in many different\ndenotational models of LL (hypercoherences, Scott semantics, finiteness spaces\netc). We also present a natural embedding of G{\\\"o}del System T in LL with\nfixed points thus enforcing the expressive power of this system as a\nprogramming language featuring both normalization and a huge expressive power\nin terms of data types.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 10:40:42 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Ehrhard", "Thomas", "", "IRIF"], ["Jafar-Rahmani", "Farzad", ""]]}, {"id": "1906.05704", "submitter": "Eduard Kamburjan", "authors": "Eduard Kamburjan, Stefan Mitsch, Martina Kettenbach, and Reiner\n  H\\\"ahnle", "title": "Modeling and Verifying Cyber-Physical Systems with Hybrid Active Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LO cs.PL cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal modeling of cyber-physical systems (CPS) is hard, because they pose\nthe double challenge of combined discrete-continuous dynamics and concurrent\nbehavior. Existing formal specification and verification languages for CPS are\ndesigned on top of their underlying proof search technology. They lack\nhigh-level structuring elements. In addition, they are not efficiently\nexecutable. This makes formal CPS models hard to understand and to validate,\nhence impairs their usability. Instead, we suggest to model CPS in an Active\nObjects (AO) language designed for concise, intuitive modeling of concurrent\nsystems. To this end, we extend the AO language ABS and its runtime environment\nwith Hybrid Active Objects (HAO). CPS models and requirements formalized in HAO\nmust follow certain communication patterns that permit automatic translation\ninto differential dynamic logic, a sequential hybrid program logic.\nVerification is achieved by discharging the resulting formulas with the theorem\nprover KeYmaera X. We demonstrate the practicality of our approach with case\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:01:16 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Kamburjan", "Eduard", ""], ["Mitsch", "Stefan", ""], ["Kettenbach", "Martina", ""], ["H\u00e4hnle", "Reiner", ""]]}, {"id": "1906.05729", "submitter": "Daniel O. Martinez-Rivillas", "authors": "Daniel O. Mart\\'inez-Rivillas and Ruy J.G.B. de Queiroz", "title": "The $\\infty$-groupoid generated by an arbitrary topological\n  $\\lambda$-model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.AT math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lambda calculus is a universal programming language. It can represent the\ncomputable functions, and such offers a formal counterpart to the point of view\nof functions as rules. Terms represent functions and this allows for the\napplication of a term/function to any other term/function, including itself.\nThe calculus can be seen as a formal theory with certain pre-established axioms\nand inference rules, which can be interpreted by models. Dana Scott proposed\nthe first non-trivial model of the extensional lambda calculus, known as $\nD_\\infty$, to represent the $\\lambda$-terms as the typical functions of set\ntheory, where it is not allowed to apply a function to itself. Here we propose\na construction of an $\\infty$-groupoid from any lambda model endowed with a\ntopology. We apply this construction for the particular case $D_\\infty$, and we\nsee that the Scott topology does not provide enough information about the\nrelationship between higher homotopies. This motivates a new line of research\nfocused on the exploration of $\\lambda$-models with the structure of a\nnon-trivial $\\infty$-groupoid to generalize the proofs of term conversion\n(e.g., $\\beta$-equality, $\\eta$-equality) to higher-proofs in\n$\\lambda$-calculus.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:46:33 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 18:06:09 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 12:34:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mart\u00ednez-Rivillas", "Daniel O.", ""], ["de Queiroz", "Ruy J. G. B.", ""]]}, {"id": "1906.05847", "submitter": "Abhishek Kulkarni", "authors": "Abhishek N. Kulkarni, Jie Fu", "title": "Opportunistic Synthesis in Reactive Games under Information Asymmetry", "comments": "Submitted to Conference on Decision and Control 2019", "journal-ref": null, "doi": "10.1109/CDC40024.2019.9029851", "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive synthesis is a class of methods to construct a provably-correct\ncontrol system, referred to as a robot, with respect to a temporal logic\nspecification in the presence of a dynamic and uncontrollable environment. This\nis achieved by modeling the interaction between the robot and its environment\nas a two-player zero-sum game. However, existing reactive synthesis methods\nassume both players to have complete information, which is not the case in many\nstrategic interactions. In this paper, we use a variant of hypergames to model\nthe interaction between the robot and its environment; which has incomplete\ninformation about the specification of the robot. This model allows us to\nidentify a subset of game states from where the robot can leverage the\nasymmetrical information to achieve a better outcome, which is not possible if\nboth players have symmetrical and complete information. We then introduce a\nnovel method of opportunistic synthesis by defining a Markov Decision Process\n(MDP) using the hypergame under temporal logic specifications. When the\nenvironment plays some stochastic strategy in its perceived sure-winning and\nsure-losing regions of the game, we show that by following the opportunistic\nstrategy, the robot is ensured to only improve the outcome of the game -\nmeasured by satisfaction of sub-specifications - whenever an opportunity\nbecomes available. We demonstrate the correctness and optimality of this method\nusing a robot motion planning example in the presence of an adversary.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:48:21 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Kulkarni", "Abhishek N.", ""], ["Fu", "Jie", ""]]}, {"id": "1906.05937", "submitter": "EPTCS", "authors": "Antonin Delpeuch (University of Oxford)", "title": "A Complete Language for Faceted Dataflow Programs", "comments": "In Proceedings ACT 2019, arXiv:2009.06334", "journal-ref": "EPTCS 323, 2020, pp. 1-14", "doi": "10.4204/EPTCS.323.1", "report-no": null, "categories": "cs.LO cs.DB math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete categorical axiomatization of a wide class of dataflow\nprograms. This gives a three-dimensional diagrammatic language for workflows,\nmore expressive than the directed acyclic graphs generally used for this\npurpose. This calls for an implementation of these representations in data\ntransformation tools.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:35:19 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 02:12:39 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Delpeuch", "Antonin", "", "University of Oxford"]]}, {"id": "1906.06047", "submitter": "Rasmus Kr{\\ae}mmer Rendsvig", "authors": "Andr\\'es Occhipinti Liberman, Andreas Achen, Rasmus Kr{\\ae}mmer\n  Rendsvig", "title": "Dynamic Term-Modal Logics for First-Order Epistemic Planning", "comments": null, "journal-ref": null, "doi": "10.1016/j.artint.2020.103305", "report-no": null, "categories": "cs.LO cs.AI cs.MA math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical planning frameworks are built on first-order languages. The\nfirst-order expressive power is desirable for compactly representing actions\nvia schemas, and for specifying quantified conditions such as $\\neg\\exists\nx\\mathsf{blocks\\_door}(x)$. In contrast, several recent epistemic planning\nframeworks are built on propositional epistemic logic. The epistemic language\nis useful to describe planning problems involving higher-order reasoning or\nepistemic goals such as $K_{a}\\neg\\mathsf{problem}$.\n  This paper develops a first-order version of Dynamic Epistemic Logic (DEL).\nIn this framework, for example, $\\exists xK_{x}\\exists\ny\\mathsf{blocks\\_door}(y)$ is a formula. The formalism combines the strengths\nof DEL (higher-order reasoning) with those of first-order logic (lifted\nrepresentation) to model multi-agent epistemic planning. The paper introduces\nan epistemic language with a possible-worlds semantics, followed by novel\ndynamics given by first-order action models and their execution via product\nupdates. Taking advantage of the first-order machinery, epistemic action\nschemas are defined to provide compact, problem-independent domain\ndescriptions, in the spirit of PDDL.\n  Concerning metatheory, the paper defines axiomatic normal term-modal logics,\nshows a Canonical Model Theorem-like result which allows establishing\ncompleteness through frame characterization formulas, shows decidability for\nthe finite agent case, and shows a general completeness result for the dynamic\nextension by reduction axioms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 06:51:25 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 22:56:16 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Liberman", "Andr\u00e9s Occhipinti", ""], ["Achen", "Andreas", ""], ["Rendsvig", "Rasmus Kr\u00e6mmer", ""]]}, {"id": "1906.06187", "submitter": "Leon Weber", "authors": "Leon Weber, Pasquale Minervini, Jannes M\\\"unchmeyer, Ulf Leser, Tim\n  Rockt\\\"aschel", "title": "NLProlog: Reasoning with Weak Unification for Question Answering in\n  Natural Language", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based models are attractive for various tasks because they inherently\nlead to interpretable and explainable decisions and can easily incorporate\nprior knowledge. However, such systems are difficult to apply to problems\ninvolving natural language, due to its linguistic variability. In contrast,\nneural models can cope very well with ambiguity by learning distributed\nrepresentations of words and their composition from data, but lead to models\nthat are difficult to interpret. In this paper, we describe a model combining\nneural networks with logic programming in a novel manner for solving multi-hop\nreasoning tasks over natural language. Specifically, we propose to use a Prolog\nprover which we extend to utilize a similarity function over pretrained\nsentence encoders. We fine-tune the representations for the similarity function\nvia backpropagation. This leads to a system that can apply rule-based reasoning\nto natural language, and induce domain-specific rules from training data. We\nevaluate the proposed system on two different question answering tasks, showing\nthat it outperforms two baselines -- BIDAF (Seo et al., 2016a) and FAST QA\n(Weissenborn et al., 2017b) on a subset of the WikiHop corpus and achieves\ncompetitive results on the MedHop data set (Welbl et al., 2017).\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 13:05:08 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Weber", "Leon", ""], ["Minervini", "Pasquale", ""], ["M\u00fcnchmeyer", "Jannes", ""], ["Leser", "Ulf", ""], ["Rockt\u00e4schel", "Tim", ""]]}, {"id": "1906.06251", "submitter": "Curtis Bright", "authors": "Curtis Bright, J\\\"urgen Gerhard, Ilias Kotsireas, Vijay Ganesh", "title": "Effective problem solving using SAT solvers", "comments": "To appear in Proceedings of the Maple Conference 2019", "journal-ref": null, "doi": "10.1007/978-3-030-41258-6_15", "report-no": null, "categories": "cs.AI cs.LO cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we demonstrate how to solve a variety of problems and puzzles\nusing the built-in SAT solver of the computer algebra system Maple. Once the\nproblems have been encoded into Boolean logic, solutions can be found (or shown\nto not exist) automatically, without the need to implement any search\nalgorithm. In particular, we describe how to solve the $n$-queens problem, how\nto generate and solve Sudoku puzzles, how to solve logic puzzles like the\nEinstein riddle, how to solve the 15-puzzle, how to solve the maximum clique\nproblem, and finding Graeco-Latin squares.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 15:32:17 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 17:20:51 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Bright", "Curtis", ""], ["Gerhard", "J\u00fcrgen", ""], ["Kotsireas", "Ilias", ""], ["Ganesh", "Vijay", ""]]}, {"id": "1906.06275", "submitter": "Mihai Codescu", "authors": "Mihai Codescu and Bernd Krieg-Br\\\"uckner and Till Mossakowski", "title": "Extensions of Generic DOL for Generic Ontology Design Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic ontologies were introduced as an extension (Generic DOL) of the\nDistributed Ontology, Modeling and Specification Language, DOL, with the aim to\nprovide a language for Generic Ontology Design Patterns. In this paper we\npresent a number of new language constructs that increase the expressivity and\nthe generality of Generic DOL, among them sequential and optional parameters,\nlist parameters with recursion, and local sub-patterns. These are illustrated\nwith non-trivial patterns: generic value sets and (nested) qualitatively graded\nrelations, demonstrated as definitional building blocks in an application\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 16:25:43 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Codescu", "Mihai", ""], ["Krieg-Br\u00fcckner", "Bernd", ""], ["Mossakowski", "Till", ""]]}, {"id": "1906.06582", "submitter": "Christoph Benzm\\\"uller", "authors": "David Fuenmayor and Christoph Benzm\\\"uller", "title": "A Computational-Hermeneutic Approach for Conceptual Explicitation", "comments": "29 pages, 9 figures, to appear in A. Nepomuceno, L. Magnani, F.\n  Salguero, C. Bar\\'es, M. Fontaine (eds.), Model-Based Reasoning in Science\n  and Technology. Inferential Models for Logic, Language, Cognition and\n  Computation, Series \"Sapere\", Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computer-supported approach for the logical analysis and\nconceptual explicitation of argumentative discourse. Computational hermeneutics\nharnesses recent progresses in automated reasoning for higher-order logics and\naims at formalizing natural-language argumentative discourse using flexible\ncombinations of expressive non-classical logics. In doing so, it allows us to\nrender explicit the tacit conceptualizations implicit in argumentative\ndiscursive practices. Our approach operates on networks of structured arguments\nand is iterative and two-layered. At one layer we search for logically correct\nformalizations for each of the individual arguments. At the next layer we\nselect among those correct formalizations the ones which honor the argument's\ndialectic role, i.e. attacking or supporting other arguments as intended. We\noperate at these two layers in parallel and continuously rate sentences'\nformalizations by using, primarily, inferential adequacy criteria. An\ninterpretive, logical theory will thus gradually evolve. This theory is\ncomposed of meaning postulates serving as explications for concepts playing a\nrole in the analyzed arguments. Such a recursive, iterative approach to\ninterpretation does justice to the inherent circularity of understanding: the\nwhole is understood compositionally on the basis of its parts, while each part\nis understood only in the context of the whole (hermeneutic circle). We\nsummarily discuss previous work on exemplary applications of human-in-the-loop\ncomputational hermeneutics in metaphysical discourse. We also discuss some of\nthe main challenges involved in fully-automating our approach. By sketching\nsome design ideas and reviewing relevant technologies, we argue for the\ntechnological feasibility of a highly-automated computational hermeneutics.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 15:57:57 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 09:49:19 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Fuenmayor", "David", ""], ["Benzm\u00fcller", "Christoph", ""]]}, {"id": "1906.06684", "submitter": "Martin Ziegler", "authors": "Willem Fouch\\'e, Hyunwoo Lee, Donghyun Lim, Sewon Park, Matthias\n  Schr\\\"oder, Martin Ziegler", "title": "Randomized Computation of Continuous Data: Is Brownian Motion\n  Computable?", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider randomized computation of continuous data in the sense of\nComputable Analysis. Our first contribution formally confirms that it is no\nloss of generality to take as sample space the Cantor space of infinite FAIR\ncoin flips. This extends [Schr\\\"oder&Simpson'05] and [Hoyrup&Rojas'09]\nconsidering sequences of suitably and adaptively BIASED coins.\n  Our second contribution is concerned with 1D Brownian Motion (aka Wiener\nProcess), a probability distribution on the space of continuous functions\nf:[0,1]->R with f(0)=0 whose computability has been conjectured\n[Davie&Fouch\\'e'13; arXiv:1409.4667,S6]. We establish that this (higher-type)\nrandom variable is computable iff some/every computable family of moduli of\ncontinuity (as ordinary random variables) has a computable probability\ndistribution with respect to the Wiener Measure.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 13:32:14 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Fouch\u00e9", "Willem", ""], ["Lee", "Hyunwoo", ""], ["Lim", "Donghyun", ""], ["Park", "Sewon", ""], ["Schr\u00f6der", "Matthias", ""], ["Ziegler", "Martin", ""]]}, {"id": "1906.06931", "submitter": "Thorsten Wissmann", "authors": "Jan K\\v{r}et\\'insk\\'y and Tobias Meggendorfer", "title": "Of Cores: A Partial-Exploration Framework for Markov Decision Processes", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (October\n  9, 2020) lmcs:6833", "doi": "10.23638/LMCS-16(4:3)2020", "report-no": null, "categories": "eess.SY cs.AI cs.LO cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a framework for approximate analysis of Markov decision\nprocesses (MDP) with bounded-, unbounded-, and infinite-horizon properties. The\nmain idea is to identify a \"core\" of an MDP, i.e., a subsystem where we\nprovably remain with high probability, and to avoid computation on the less\nrelevant rest of the state space. Although we identify the core using\nsimulations and statistical techniques, it allows for rigorous error bounds in\nthe analysis. Consequently, we obtain efficient analysis algorithms based on\npartial exploration for various settings, including the challenging case of\nstrongly connected systems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 10:07:31 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 14:02:46 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 07:45:18 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2020 09:40:11 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 13:07:38 GMT"}, {"version": "v6", "created": "Thu, 8 Oct 2020 13:49:41 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["K\u0159et\u00ednsk\u00fd", "Jan", ""], ["Meggendorfer", "Tobias", ""]]}, {"id": "1906.07083", "submitter": "Philipp Berger", "authors": "Philipp Berger, Johanna Nellen, Joost-Pieter Katoen, Erika Abraham, Md\n  Tawhid Bin Waez and Thomas Rambow", "title": "Multiple Analyses, Requirements Once: simplifying testing & verification\n  in automotive model-based development", "comments": "With full appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial model-based development (MBD) frameworks, requirements are\ntypically specified informally using textual descriptions. To enable the\napplication of formal methods, these specifications need to be formalized in\nthe input languages of all formal tools that should be applied to analyse the\nmodels at different development levels. In this paper we propose a unified\napproach for the computer-assisted formal specification of requirements and\ntheir fully automated translation into the specification languages of different\nverification tools. We consider a two-stage MBD scenario where first Simulink\nmodels are developed from which executable code is generated automatically. We\n(i) propose a specification language and a prototypical tool for the formal but\nstill textual specification of requirements, (ii) show how these requirements\ncan be translated automatically into the input languages of Simulink Design\nVerifier for verification of Simulink models and BTC EmbeddedValidator for\nsource code verification, and (iii) show how our unified framework enables\nbesides automated formal verification also the automated generation of test\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 15:24:28 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Berger", "Philipp", ""], ["Nellen", "Johanna", ""], ["Katoen", "Joost-Pieter", ""], ["Abraham", "Erika", ""], ["Waez", "Md Tawhid Bin", ""], ["Rambow", "Thomas", ""]]}, {"id": "1906.07253", "submitter": "Yu Wang", "authors": "Yu Wang, Mojtaba Zarei, Borzoo Bonakdarpour, Miroslav Pajic", "title": "Statistical Verification of Hyperproperties for Cyber-Physical System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important properties of cyber-physical systems (CPS) are defined upon\nthe relationship between multiple executions simultaneously in continuous time.\nExamples include probabilistic fairness and sensitivity to modeling errors\n(i.e., parameters changes) for real-valued signals. These requirements can only\nbe specified by hyperproperties. In this work, we focus on verifying\nprobabilistic hyperproperties for CPS. To cover a wide range of modeling\nformalisms, we first propose a general model of probabilistic uncertain systems\n(PUSs) that unify commonly studied CPS models such as continuous-time Markov\nchains (CTMCs) and probabilistically parametrized Hybrid I/O Automata. To\nformally specify hyperproperties, we propose a new temporal logic, hyper\nprobabilistic signal temporal logic (HyperPSTL) that serves as a hyper and\nprobabilistic version of the conventional signal temporal logic (STL).\nConsidering complexity of real-world systems that can be captured as PUSs, we\nadopt a statistical model checking (SMC) approach for their verification. We\ndevelop a new SMC technique based on the direct computation of the significance\nlevels of statistical assertions for HyperPSTL specifications, which requires\nno a priori knowledge on the indifference margin. Then, we introduce SMC\nalgorithms for HyperPSTL specifications on the joint probabilistic distribution\nof multiple paths, as well as specifications with nested probabilistic\noperators quantifying different paths, which cannot be handled by existing SMC\nalgorithms. Finally, we show the effectiveness of our SMC algorithms on CPS\nbenchmarks with varying levels of complexity, including the Toyota Powertrain\nControl~System.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 20:30:37 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 15:48:39 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Wang", "Yu", ""], ["Zarei", "Mojtaba", ""], ["Bonakdarpour", "Borzoo", ""], ["Pajic", "Miroslav", ""]]}, {"id": "1906.07508", "submitter": "Olivier Bailleux", "authors": "Olivier Bailleux", "title": "Subsumption-driven clause learning with DPLL+restarts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use a DPLL+restart to solve SAT instances by successive\nsimplifications based on the production of clauses that subsume the initial\nclauses. We show that this approach allows the refutation of pebbling formulae\nin polynomial time and linear space, as effectively as with a CDCL solver.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:53:52 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Bailleux", "Olivier", ""]]}, {"id": "1906.07811", "submitter": "Alessandro Gianola", "authors": "Diego Calvanese, Silvio Ghilardi, Alessandro Gianola, Marco Montali,\n  Andrey Rivkin", "title": "Formal Modeling and SMT-Based Parameterized Verification of Data-Aware\n  BPMN (Extended Version)", "comments": "long version of a paper accepted at the BPM conference. arXiv admin\n  note: substantial text overlap with arXiv:1905.12991", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DAB -- a data-aware extension of BPMN where the process operates\nover case and persistent data (partitioned into a read-only database called\ncatalog and a read-write database called repository). The model trades off\nbetween expressiveness and the possibility of supporting parameterized\nverification of safety properties on top of it. Specifically, taking\ninspiration from the literature on verification of artifact systems, we study\nverification problems where safety properties are checked irrespectively of the\ncontent of the read-only catalog, and accepting the potential presence of\nunboundedly many tuples in the catalog and repository. We tackle such problems\nusing an array-based backward reachability procedure fully implemented in MCMT\n-- a state-of-the-art array-based SMT model checker. Notably, we prove that the\nprocedure is sound and complete for checking safety of DABs, and single out\nadditional conditions that guarantee its termination and, in turn, show\ndecidability of checking safety.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 02:31:03 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 14:46:49 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 11:53:16 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Calvanese", "Diego", ""], ["Ghilardi", "Silvio", ""], ["Gianola", "Alessandro", ""], ["Montali", "Marco", ""], ["Rivkin", "Andrey", ""]]}, {"id": "1906.07886", "submitter": "Jonathan Rawski", "authors": "Jane Chandlee, Remi Eyraud, Jeffrey Heinz, Adam Jardine, Jonathan\n  Rawski", "title": "Learning with Partially Ordered Representations", "comments": "to appear in Proceedings of Mathematics of Language (ACL SIGMOL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper examines the characterization and learning of grammars defined\nwith enriched representational models. Model-theoretic approaches to formal\nlanguage theory traditionally assume that each position in a string belongs to\nexactly one unary relation. We consider unconventional string models where\npositions can have multiple, shared properties, which are arguably useful in\nmany applications. We show the structures given by these models are partially\nordered, and present a learning algorithm that exploits this ordering relation\nto effectively prune the hypothesis space. We prove this learning algorithm,\nwhich takes positive examples as input, finds the most general grammar which\ncovers the data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 02:43:50 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 16:42:24 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Chandlee", "Jane", ""], ["Eyraud", "Remi", ""], ["Heinz", "Jeffrey", ""], ["Jardine", "Adam", ""], ["Rawski", "Jonathan", ""]]}, {"id": "1906.08084", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima", "title": "LiFtEr: Language to Encode Induction Heuristics for Isabelle/HOL", "comments": "This is the pre-print of our paper of the same title accepted at\n  APLAS2019 (https://doi.org/10.1007/978-3-030-34175-6_14). We updated the\n  draft after fixing the errata found by Kenji Miyamoto", "journal-ref": null, "doi": "10.1007/978-3-030-34175-6_14", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof assistants, such as Isabelle/HOL, offer tools to facilitate inductive\ntheorem proving. Isabelle experts know how to use these tools effectively;\nhowever, there is a little tool support for transferring this expert knowledge\nto a wider user audience. To address this problem, we present our\ndomain-specific language, LiFtEr. LiFtEr allows experienced Isabelle users to\nencode their induction heuristics in a style independent of any problem domain.\nLiFtEr's interpreter mechanically checks if a given application of induction\ntool matches the heuristics, thus automating the knowledge transfer loop.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:04:54 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 09:22:43 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 21:31:46 GMT"}, {"version": "v4", "created": "Sun, 24 May 2020 07:34:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Nagashima", "Yutaka", ""]]}, {"id": "1906.08178", "submitter": "Viktor Toman", "authors": "Pranav Ashok, Tom\\'a\\v{s} Br\\'azdil, Krishnendu Chatterjee, Jan\n  K\\v{r}et\\'insk\\'y, Christoph H. Lampert, Viktor Toman", "title": "Strategy Representation by Decision Trees with Linear Classifiers", "comments": "Full version of the paper. To appear in QEST 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph games and Markov decision processes (MDPs) are standard models in\nreactive synthesis and verification of probabilistic systems with\nnondeterminism. The class of $\\omega$-regular winning conditions; e.g., safety,\nreachability, liveness, parity conditions; provides a robust and expressive\nspecification formalism for properties that arise in analysis of reactive\nsystems. The resolutions of nondeterminism in games and MDPs are represented as\nstrategies, and we consider succinct representation of such strategies. The\ndecision-tree data structure from machine learning retains the flavor of\ndecisions of strategies and allows entropy-based minimization to obtain\nsuccinct trees. However, in contrast to traditional machine-learning problems\nwhere small errors are allowed, for winning strategies in graph games and MDPs\nno error is allowed, and the decision tree must represent the entire strategy.\nIn this work we propose decision trees with linear classifiers for\nrepresentation of strategies in graph games and MDPs. We have implemented\nstrategy representation using this data structure and we present experimental\nresults for problems on graph games and MDPs, which show that this new data\nstructure presents a much more efficient strategy representation as compared to\nstandard decision trees.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 15:49:03 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 08:42:36 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Ashok", "Pranav", ""], ["Br\u00e1zdil", "Tom\u00e1\u0161", ""], ["Chatterjee", "Krishnendu", ""], ["K\u0159et\u00ednsk\u00fd", "Jan", ""], ["Lampert", "Christoph H.", ""], ["Toman", "Viktor", ""]]}, {"id": "1906.08361", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland and Igor L. Bratchikov", "title": "Transformation of XML Documents with Prolog", "comments": "5 pages, 1 figure, 4 appendices", "journal-ref": "SCOPUS PURE/Elsevier/FDPW'2008, Advances in Methods of Information\n  and Communication Technology (AMICT2018). vol.10, 2008, pp.99-111, RSCI\n  26444969, ISBN 975-5-8021-1020-1", "doi": null, "report-no": null, "categories": "cs.LO cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transforming XML documents with conventional XML languages, like XSL-T, is\ndisadvantageous because there is too lax abstraction on the target language and\nit is rather difficult to recognize rule-oriented transformations. Prolog as a\nprogramming language of declarative paradigm is especially good for\nimplementation of analysis of formal languages. Prolog seems also to be good\nfor term manipulation, complex schema-transformation and text retrieval. In\nthis report an appropriate model for XML documents is proposed, the basic\ntransformation language for Prolog LTL is defined and the expressiveness power\ncompared with XSL-T is demonstrated, the implementations used throughout are\nmulti paradigmatic.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:20:14 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Haberland", "Ren\u00e9", ""], ["Bratchikov", "Igor L.", ""]]}, {"id": "1906.08369", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland", "title": "Unification of Template-Expansion and XML-Validation", "comments": "4 pages (with Extended Abstract)", "journal-ref": "39th Int.Conf. on Control Processes and Stability (CPaS08),\n  vol.34, Saint Petersburg State University, 2008, pp.389-394, RSCI\n  517.51:517.9:518.9, ISBN 978-5-288-04680-3, ISSN 2313-7304", "doi": null, "report-no": null, "categories": "cs.LO cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The processing of XML documents often includes creation and validation. These\ntwo operations are typically performed in two different nodes within a computer\nnetwork that do not correlate with each other. The process of creation is also\ncalled instantiation of a template and can be described by filling a template\nwith data from external repositories. Initial access to arbitrary sources can\nbe formulated as an expression of certain command languages like XPath. Filling\nmeans copying invariant element nodes to the target document and unfolding\nvariable parts from a given template. Validation is a descision problem\nreturning true if a given XML document satisfies a schema and false otherwise.\nThe main subject is to find a language that unions the template expansion and\nthe validation. [..].\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:38:05 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Haberland", "Ren\u00e9", ""]]}, {"id": "1906.08731", "submitter": "Sandro Stucki", "authors": "Sandro Stucki, C\\'esar S\\'anchez, Gerardo Schneider and Borzoo\n  Bonakdarpour", "title": "Gray-box Monitoring of Hyperproperties (Extended Version)", "comments": "This is an extended version of a paper presented at the 23rd\n  International Symposium on Formal Methods (FM '19). This version contains\n  full proofs, a description of the proof-of-concept monitor for DDM, and\n  experimental results that were not included in the original publication", "journal-ref": "ter Beek M., McIver A., Oliveira J. (eds), Formal Methods - The\n  Next 30 Years. FM 2019. Lecture Notes in Computer Science, vol 11800.\n  Springer, Cham", "doi": "10.1007/978-3-030-30942-8_25", "report-no": null, "categories": "cs.LO cs.CR cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important system properties, particularly in security and privacy,\ncannot be verified statically. Therefore, runtime verification is an appealing\nalternative. Logics for hyperproperties, such as HyperLTL, support a rich set\nof such properties. We first show that black-box monitoring of HyperLTL is in\ngeneral unfeasible, and suggest a gray-box approach. Gray-box monitoring\nimplies performing analysis of the system at run-time, which brings new\nlimitations to monitorabiliy (the feasibility of solving the monitoring\nproblem). Thus, as another contribution of this paper we refine the classic\nnotions of monitorability, both for trace properties and hyperproperties,\ntaking into account the computability of the monitor. We then apply our\napproach to monitor a privacy hyperproperty called distributed data minimality,\nexpressed as a HyperLTL property, by using an SMT-based static verifier at\nruntime.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:23:44 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 13:41:38 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 16:32:08 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Stucki", "Sandro", ""], ["S\u00e1nchez", "C\u00e9sar", ""], ["Schneider", "Gerardo", ""], ["Bonakdarpour", "Borzoo", ""]]}, {"id": "1906.09105", "submitter": "Tiago Veras Sr.", "authors": "Tiago M. L.Veras, Arthur F. Ramos, Ruy J. G. B. de Queiroz and\n  Anjolina G. de Oliveira", "title": "A Topological Application of Labelled Natural Deduction", "comments": "42 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1804.01413, arXiv:1803.01709, arXiv:1906.09107", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a labelled deduction system based on the concept of computational\npaths (sequences of rewrites) as equalities between two terms of the same type.\nWe also define a term rewriting system that is used to make computations\nbetween these computational paths, establishing equalities between equalities.\nWe then proceed to show the main result here: using this system to obtain the\ncalculation of the fundamental group of the circle, of the torus and the real\nprojective plane.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 19:34:40 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 16:04:06 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 16:16:48 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 14:25:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Veras", "Tiago M. L.", ""], ["Ramos", "Arthur F.", ""], ["de Queiroz", "Ruy J. G. B.", ""], ["de Oliveira", "Anjolina G.", ""]]}, {"id": "1906.09107", "submitter": "Tiago Veras Sr.", "authors": "Tiago M. L. de Veras and Arthur F. Ramos and Ruy J. G. B. de Queiroz\n  and Anjolina G. de Oliveira", "title": "An alternative approach to the calculation of fundamental groups based\n  on labeled natural deduction", "comments": "28 pages, 17 figures arXiv admin note: text overlap with\n  arXiv:1804.01413, arXiv:1803.01709, arXiv:1906.09105", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use a labelled deduction system based on the concept of\ncomputational paths (sequence of rewrites) as equalities between two terms of\nthe same type. We also define a term rewriting system that is used to make\ncomputations between these computational paths, establishing equalities between\nequalities. We use a labelled deduction system based on the concept of\ncomputational paths (sequence of rewrites) to obtain some results of algebraic\ntopology and with support of the Seifet-Van Kampen Theorem we will calculate,\nin a way less complex than the one made in mathematics \\cite{Munkres} and the\ntechnique of homotopy type theory \\cite{hott}, the fundamental group of Klein\nBlottle $\\mathbb{K}^2$, of the Torus $\\mathbb{T}^2$ and Two holed Torus\n$\\mathbb{M}_2=\\mathbb{T}^2\\# \\mathbb{T}^2$ (the connected sum two torus).\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 19:34:50 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["de Veras", "Tiago M. L.", ""], ["Ramos", "Arthur F.", ""], ["de Queiroz", "Ruy J. G. B.", ""], ["de Oliveira", "Anjolina G.", ""]]}, {"id": "1906.09131", "submitter": "Lidia Tendera", "authors": "Ian Pratt-Hartmann and Lidia Tendera", "title": "The Fluted Fragment with Transitivity", "comments": "Extended version of MFCS 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the satisfiability problem for the fluted fragment extended with\ntransitive relations. We show that the logic enjoys the finite model property\nwhen only one transitive relation is available. On the other hand we show that\nthe satisfiability problem is undecidable already for the two-variable fragment\nof the logic in the presence of three transitive relations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 13:26:31 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Pratt-Hartmann", "Ian", ""], ["Tendera", "Lidia", ""]]}, {"id": "1906.09142", "submitter": "Gethin Norman", "authors": "Marta Kwiatkowska and Gethin Norman and David Parker", "title": "Verification and Control of Turn-Based Probabilistic Real-Time Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative verification techniques have been developed for the formal\nanalysis of a variety of probabilistic models, such as Markov chains, Markov\ndecision process and their variants. They can be used to produce guarantees on\nquantitative aspects of system behaviour, for example safety, reliability and\nperformance, or to help synthesise controllers that ensure such guarantees are\nmet. We propose the model of turn-based probabilistic timed multi-player games,\nwhich incorporates probabilistic choice, real-time clocks and nondeterministic\nbehaviour across multiple players. Building on the digital clocks approach for\nthe simpler model of probabilistic timed automata, we show how to compute the\nkey measures that underlie quantitative verification, namely the probability\nand expected cumulative price to reach a target. We illustrate this on case\nstudies from computer security and task scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 13:52:03 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 12:46:42 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 11:56:04 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kwiatkowska", "Marta", ""], ["Norman", "Gethin", ""], ["Parker", "David", ""]]}, {"id": "1906.09198", "submitter": "Paolo Papotti", "authors": "Naser Ahmadi, Joohyung Lee, Paolo Papotti, Mohammed Saeed", "title": "Explainable Fact Checking with Probabilistic Answer Set Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenge in fact checking is the ability to improve the transparency of\nthe decision. We present a fact checking method that uses reference information\nin knowledge graphs (KGs) to assess claims and explain its decisions. KGs\ncontain a formal representation of knowledge with semantic descriptions of\nentities and their relationships. We exploit such rich semantics to produce\ninterpretable explanations for the fact checking output. As information in a KG\nis inevitably incomplete, we rely on logical rule discovery and on Web text\nmining to gather the evidence to assess a given claim. Uncertain rules and\nfacts are turned into logical programs and the checking task is modeled as an\ninference problem in a probabilistic extension of answer set programs.\nExperiments show that the probabilistic inference enables the efficient\nlabeling of claims with interpretable explanations, and the quality of the\nresults is higher than state of the art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 15:35:03 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Ahmadi", "Naser", ""], ["Lee", "Joohyung", ""], ["Papotti", "Paolo", ""], ["Saeed", "Mohammed", ""]]}, {"id": "1906.09370", "submitter": "Andr\\'es Ezequiel Viso", "authors": "Eduardo Bonelli and Delia Kesner and Andr\\'es Viso", "title": "Strong Bisimulation for Control Operators", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.CSL.2020.4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to identify programs with control operators\nwhose reduction semantics are in exact correspondence. This is achieved by\nintroducing a relation $\\simeq$, defined over a revised presentation of\nParigot's $\\lambda\\mu$-calculus we dub $\\Lambda M$. Our result builds on two\nfundamental ingredients: (1) factorization of $\\lambda\\mu$-reduction into\nmultiplicative and exponential steps by means of explicit term operators of\n$\\Lambda M$, and (2) translation of $\\Lambda M$-terms into Laurent's polarized\nproof-nets (PPN) such that cut-elimination in PPN simulates our calculus. Our\nproposed relation $\\simeq$ is shown to characterize structural equivalence in\nPPN. Most notably, $\\simeq$ is shown to be a strong bisimulation with respect\nto reduction in $\\Lambda M$, i.e. two $\\simeq$-equivalent terms have the exact\nsame reduction semantics, a result which fails for Regnier's\n$\\sigma$-equivalence in $\\lambda$-calculus as well as for Laurent's\n$\\sigma$-equivalence in $\\lambda\\mu$.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 02:35:39 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 15:11:34 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bonelli", "Eduardo", ""], ["Kesner", "Delia", ""], ["Viso", "Andr\u00e9s", ""]]}, {"id": "1906.09503", "submitter": "Vladimir Zamdzhiev", "authors": "Bert Lindenhovius and Michael Mislove and Vladimir Zamdzhiev", "title": "LNL-FPC: The Linear/Non-linear Fixpoint Calculus", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 2 (April 22,\n  2021) lmcs:7390", "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a type system with mixed linear and non-linear recursive types\ncalled LNL-FPC (the linear/non-linear fixpoint calculus). The type system\nsupports linear typing, which enhances the safety properties of programs, but\nalso supports non-linear typing as well, which makes the type system more\nconvenient for programming. Just as in FPC, we show that LNL-FPC supports\ntype-level recursion, which in turn induces term-level recursion. We also\nprovide sound and computationally adequate categorical models for LNL-FPC that\ndescribe the categorical structure of the substructural operations of\nIntuitionistic Linear Logic at all non-linear types, including the recursive\nones. In order to do so, we describe a new technique for solving recursive\ndomain equations within cartesian categories by constructing the solutions over\npre-embeddings. The type system also enjoys implicit weakening and contraction\nrules that we are able to model by identifying the canonical comonoid structure\nof all non-linear types. We also show that the requirements of our abstract\nmodel are reasonable by constructing a large class of concrete models that have\nfound applications not only in classical functional programming, but also in\nemerging programming paradigms that incorporate linear types, such as quantum\nprogramming and circuit description programming languages.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 20:50:27 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 14:30:24 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 23:21:45 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 20:37:57 GMT"}, {"version": "v5", "created": "Tue, 23 Mar 2021 15:07:34 GMT"}, {"version": "v6", "created": "Wed, 21 Apr 2021 06:44:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lindenhovius", "Bert", ""], ["Mislove", "Michael", ""], ["Zamdzhiev", "Vladimir", ""]]}, {"id": "1906.09541", "submitter": "Yuxi Fu", "authors": "Yuxi Fu", "title": "A Uniform Approach to Random Process Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lot of research on probabilistic transition systems. There are not\nmany studies in probabilistic process models. The lack of investigation into\nthe interactive aspect of probabilistic processes is mainly due to the\ndifficulty caused by the discrepancy between probabilistic actions and\nnondeterministic behaviours. The paper proposes a uniform approach to\nprobabilistic process models and a bisimulation congruence for probabilistic\nconcurrency.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 02:45:19 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Fu", "Yuxi", ""]]}, {"id": "1906.09649", "submitter": "EPTCS", "authors": "Vladimir Zamdzhiev", "title": "Reflecting Algebraically Compact Functors", "comments": "In Proceedings ACT 2019, arXiv:2009.06334", "journal-ref": "EPTCS 323, 2020, pp. 15-23", "doi": "10.4204/EPTCS.323.2", "report-no": null, "categories": "cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compact T-algebra is an initial T-algebra whose inverse is a final\nT-coalgebra. Functors with this property are said to be algebraically compact.\nThis is a very strong property used in programming semantics which allows one\nto interpret recursive datatypes involving mixed-variance functors, such as\nfunction space. The construction of compact algebras is usually done in\ncategories with a zero object where some form of a limit-colimit coincidence\nexists. In this paper we consider a more abstract approach and show how one can\nconstruct compact algebras in categories which have neither a zero object, nor\na (standard) limit-colimit coincidence by reflecting the compact algebras from\ncategories which have both. In doing so, we provide a constructive description\nof a large class of algebraically compact functors (satisfying a\ncompositionality principle) and show our methods compare quite favorably to\nother approaches from the literature.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 20:55:26 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 02:12:59 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zamdzhiev", "Vladimir", ""]]}, {"id": "1906.09709", "submitter": "Jeremy Siek", "authors": "Jeremy G. Siek", "title": "Transitivity of Subtyping for Intersection Types", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subtyping rules for intersection types traditionally employ a\ntransitivity rule (Barendregt et al. 1983), which means that subtyping does not\nsatisfy the subformula property, making it more difficult to use in filter\nmodels for compiler verification. Laurent develops a sequent-style subtyping\nsystem, without transitivity, and proves transitivity via a sequence of six\nlemmas that culminate in cut-elimination (2018). This article develops a\nsubtyping system in regular style that omits transitivity and provides a direct\nproof of transitivity, significantly reducing the length of the proof,\nexchanging the six lemmas for just one. Inspired by Laurent's system, the rule\nfor function types is essentially the $\\beta$-soundness property. The new\nsystem satisfies the \"subformula conjunction property\": every type occurring in\nthe derivation of $A <: B$ is a subformula of $A$ or $B$, or an intersection of\nsuch subformulas. The article proves that the new subtyping system is\nequivalent to that of Barendregt, Coppo, and Dezani-Ciancaglini.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 03:41:50 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 21:38:44 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Siek", "Jeremy G.", ""]]}, {"id": "1906.09873", "submitter": "Rasoul Ramezanian", "authors": "Rasoul Ramezanian", "title": "Computer-Simulation Model Theory (P= NP is not provable)", "comments": "18 pages. arXiv admin note: text overlap with arXiv:1205.5994", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation hypothesis says that all the materials and events in the\nreality (including the universe, our body, our thinking, walking and etc) are\ncomputations, and the reality is a computer simulation program like a video\ngame. All works we do (talking, reasoning, seeing and etc) are computations\nperformed by the universe-computer which runs the simulation program. Inspired\nby the view of the simulation hypothesis (but independent of this hypothesis),\nwe propose a new method of logical reasoning named \"Computer-Simulation Model\nTheory\", CSMT. Computer-Simulation Model Theory is an extension of Mathematical\nModel Theory where instead of mathematical-structures, computer-simulations are\nreplaced, and the activity of reasoning and computing of the reasoner is also\nsimulated in the model. (CSMT) argues that:\n  For a formula $\\phi$, construct a computer simulation model $S$, such that\n  1- $\\phi$ does not hold in $S$, and\n  2- the reasoner $I$ $($human being, the one who lives inside the reality$)$\ncannot distinguish $S$ from the reality $(R)$,\n  then $I$ cannot prove $\\phi$ in reality.\n  Although $\\mathrm{CSMT}$ is inspired by the simulation hypothesis, but this\nreasoning method is independent of the acceptance of this hypothesis. As we\nargue in this part, one may do not accept the simulation hypothesis, but knows\n$\\mathrm{CSMT}$ a valid reasoning method. As an application of\nComputer-Simulation Model Theory, we study the famous problem P vs NP. We let\n$\\phi \\equiv\\mathrm{ [P= NP]} $ and construct a computer simulation model $E$\nsuch that $\\mathrm{P= NP}$ does not hold in $E$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 05:11:45 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ramezanian", "Rasoul", ""]]}, {"id": "1906.09899", "submitter": "Bernhard Gleiss", "authors": "Gilles Barthe, Renate Eilers, Pamina Georgiou, Bernhard Gleiss, Laura\n  Kovacs, Matteo Maffei", "title": "Verifying Relational Properties using Trace Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a logical framework for the verification of relational properties\nin imperative programs. Our work is motivated by relational properties which\ncome from security applications and often require reasoning about formulas with\nquantifier-alternations. Our framework reduces verification of relational\nproperties of imperative programs to a validity problem into trace logic, an\nexpressive instance of first-order predicate logic. Trace logic draws its\nexpressiveness from its syntax, which allows expressing properties over\ncomputation traces. Its axiomatization supports fine-grained reasoning about\nintermediate steps in program execution, notably loop iterations. We present an\nalgorithm to encode the semantics of programs as well as their relational\nproperties in trace logic, and then show how first-order theorem proving can be\nused to reason about the resulting trace logic formulas. Our work is\nimplemented in the tool Rapid and evaluated with examples coming from the\nsecurity field.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 12:51:12 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 08:20:46 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 09:10:57 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Barthe", "Gilles", ""], ["Eilers", "Renate", ""], ["Georgiou", "Pamina", ""], ["Gleiss", "Bernhard", ""], ["Kovacs", "Laura", ""], ["Maffei", "Matteo", ""]]}, {"id": "1906.10005", "submitter": "Francois Laroussinie", "authors": "Akash Hossain and Francois Laroussinie", "title": "From Quantified CTL to QBF", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QCTL extends the temporal logic CTL with quantifications over atomic\npropositions. This extension is known to be very expressive: QCTL allows us to\nexpress complex properties over Kripke structures (it is as expressive as MSO).\nSeveral semantics exist for the quantifications: here, we work with the\nstructure semantics, where the extra propositions label the Kripke structure\n(and not its execution tree), and the model-checking problem is known to be\nPSPACE-complete in this framework. We propose a model-checking algorithm for\nQCTL based on a reduction to QBF. We consider several reduction strategies, and\nwe compare them with a prototype (based on the SMT-solver Z3) on several\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:58:37 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Hossain", "Akash", ""], ["Laroussinie", "Francois", ""]]}, {"id": "1906.10047", "submitter": "Geoffrey Hamilton", "authors": "Amir M. Ben-Amram and Geoff Hamilton", "title": "Tight Polynomial Worst-Case Bounds for Loop Programs", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 2 (May 14,\n  2020) lmcs:6477", "doi": "10.23638/LMCS-16(2:4)2020", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2008, Ben-Amram, Jones and Kristiansen showed that for a simple\nprogramming language - representing non-deterministic imperative programs with\nbounded loops, and arithmetics limited to addition and multiplication - it is\npossible to decide precisely whether a program has certain growth-rate\nproperties, in particular whether a computed value, or the program's running\ntime, has a polynomial growth rate.\n  A natural and intriguing problem was to move from answering the decision\nproblem to giving a quantitative result, namely, a tight polynomial upper\nbound. This paper shows how to obtain asymptotically-tight, multivariate,\ndisjunctive polynomial bounds for this class of programs. This is a complete\nsolution: whenever a polynomial bound exists it will be found.\n  A pleasant surprise is that the algorithm is quite simple; but it relies on\nsome subtle reasoning. An important ingredient in the proof is the forest\nfactorization theorem, a strong structural result on homomorphisms into a\nfinite monoid.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:11:40 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 20:29:17 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 12:57:29 GMT"}, {"version": "v4", "created": "Tue, 12 May 2020 08:06:30 GMT"}, {"version": "v5", "created": "Wed, 13 May 2020 12:45:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ben-Amram", "Amir M.", ""], ["Hamilton", "Geoff", ""]]}, {"id": "1906.10073", "submitter": "Josephine Lamp", "authors": "Josephine Lamp, Simone Silvetti, Marc Breton, Laura Nenzi, and Lu Feng", "title": "A Logic-Based Learning Approach to Explore Diabetes Patient Behaviors", "comments": "18 pages, 10 figures, submitted to 17th International Conference on\n  Computational Methods in Systems Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type I Diabetes (T1D) is a chronic disease in which the body's ability to\nsynthesize insulin is destroyed. It can be difficult for patients to manage\ntheir T1D, as they must control a variety of behavioral factors that affect\nglycemic control outcomes. In this paper, we explore T1D patient behaviors\nusing a Signal Temporal Logic (STL) based learning approach. STL formulas\nlearned from real patient data characterize behavior patterns that may result\nin varying glycemic control. Such logical characterizations can provide\nfeedback to clinicians and their patients about behavioral changes that\npatients may implement to improve T1D control. We present both individual- and\npopulation-level behavior patterns learned from a clinical dataset of 21 T1D\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:51:34 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Lamp", "Josephine", ""], ["Silvetti", "Simone", ""], ["Breton", "Marc", ""], ["Nenzi", "Laura", ""], ["Feng", "Lu", ""]]}, {"id": "1906.10261", "submitter": "Temitope Ajileye", "authors": "Temitope Ajileye, Boris Motik, Ian Horrocks", "title": "Datalog Materialisation in Distributed RDF Stores with Dynamic Data\n  Exchange", "comments": "16 pages, ISWC conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several centralised RDF systems support datalog reasoning by precomputing and\nstoring all logically implied triples using the wellknown seminaive algorithm.\nLarge RDF datasets often exceed the capacity of centralised RDF systems, and a\ncommon solution is to distribute the datasets in a cluster of shared-nothing\nservers. While numerous distributed query answering techniques are known,\ndistributed seminaive evaluation of arbitrary datalog rules is less understood.\nIn fact, most distributed RDF stores either support no reasoning or can handle\nonly limited datalog fragments. In this paper we extend the dynamic data\nexchange approach for distributed query answering by Potter et al. [12] to a\nreasoning algorithm that can handle arbitrary rules while preserving important\nproperties such as nonrepetition of inferences. We also show empirically that\nour algorithm scales well to very large RDF datasets\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:52:09 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ajileye", "Temitope", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1906.10357", "submitter": "Eugene Goldberg", "authors": "Eugene Goldberg", "title": "Partial Quantifier Elimination With Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a modification of the Quantifier Elimination (QE) problem called\nPartial QE (PQE). In PQE, only a small part of the formula is taken out of the\nscope of quantifiers. The appeal of PQE is that many verification problems,\ne.g. equivalence checking and model checking, reduce to PQE and the latter is\nmuch easier than complete QE. Earlier, we introduced a PQE algorithm based on\nthe machinery of D-sequents. A D-sequent is a record stating that a clause is\nredundant in a quantified CNF formula in a specified subspace. To make this\nalgorithm efficient, it is important to reuse learned D-sequents. However,\nreusing D-sequents is not as easy as conflict clauses in SAT-solvers because\nredundancy is a structural rather than a semantic property. Earlier, we\nmodified the definition of D-sequents to enable their safe reusing. In this\npaper, we present a PQE algorithm based on new D-sequents. It is different from\nits predecessor in two aspects. First, the new algorithm can learn and reuse\nD-sequents. Second, it proves clauses redundant one by one and thus backtracks\nas soon as the current target clause is proved redundant in the current\nsubspace. This makes the new PQE algorithm similar to a SAT-solver that\nbacktracks as soon as just one clause is falsified. We show experimentally that\nthe new PQE algorithm outperforms its predecessor.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 07:21:14 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 15:59:27 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Goldberg", "Eugene", ""]]}, {"id": "1906.10395", "submitter": "Teodora Baluta", "authors": "Teodora Baluta and Shiqi Shen and Shweta Shinde and Kuldeep S. Meel\n  and Prateek Saxena", "title": "Quantitative Verification of Neural Networks And its Security\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are increasingly employed in safety-critical domains. This\nhas prompted interest in verifying or certifying logically encoded properties\nof neural networks. Prior work has largely focused on checking existential\nproperties, wherein the goal is to check whether there exists any input that\nviolates a given property of interest. However, neural network training is a\nstochastic process, and many questions arising in their analysis require\nprobabilistic and quantitative reasoning, i.e., estimating how many inputs\nsatisfy a given property. To this end, our paper proposes a novel and\nprincipled framework to quantitative verification of logical properties\nspecified over neural networks. Our framework is the first to provide PAC-style\nsoundness guarantees, in that its quantitative estimates are within a\ncontrollable and bounded error from the true count. We instantiate our\nalgorithmic framework by building a prototype tool called NPAQ that enables\nchecking rich properties over binarized neural networks. We show how emerging\nsecurity analyses can utilize our framework in 3 concrete point applications:\nquantifying robustness to adversarial inputs, efficacy of trojan attacks, and\nfairness/bias of given neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:08:03 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Baluta", "Teodora", ""], ["Shen", "Shiqi", ""], ["Shinde", "Shweta", ""], ["Meel", "Kuldeep S.", ""], ["Saxena", "Prateek", ""]]}, {"id": "1906.10640", "submitter": "Pranav Ashok", "authors": "Pranav Ashok, Jan K\\v{r}et\\'insk\\'y, Kim Guldstrand Larsen, Adrien Le\n  Co\\\"ent, Jakob Haahr Taankvist, Maximilian Weininger", "title": "SOS: Safe, Optimal and Small Strategies for Hybrid Markov Decision\n  Processes", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30281-8_9", "report-no": null, "categories": "eess.SY cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For hybrid Markov decision processes, UPPAAL Stratego can compute strategies\nthat are safe for a given safety property and (in the limit) optimal for a\ngiven cost function. Unfortunately, these strategies cannot be exported easily\nsince they are computed as a very long list. In this paper, we demonstrate\nmethods to learn compact representations of the strategies in the form of\ndecision trees. These decision trees are much smaller, more understandable, and\ncan easily be exported as code that can be loaded into embedded systems.\nDespite the size compression and actual differences to the original strategy,\nwe provide guarantees on both safety and optimality of the decision-tree\nstrategy. On the top, we show how to obtain yet smaller representations, which\nare still guaranteed safe, but achieve a desired trade-off between size and\noptimality.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 16:35:09 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ashok", "Pranav", ""], ["K\u0159et\u00ednsk\u00fd", "Jan", ""], ["Larsen", "Kim Guldstrand", ""], ["Co\u00ebnt", "Adrien Le", ""], ["Taankvist", "Jakob Haahr", ""], ["Weininger", "Maximilian", ""]]}, {"id": "1906.10719", "submitter": "Thorsten Wissmann", "authors": "Thomas Powell", "title": "A unifying framework for continuity and complexity in higher types", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 3 (September\n  9, 2020) lmcs:6769", "doi": "10.23638/LMCS-16(3:17)2020", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We set up a parametrised monadic translation for a class of call-by-value\nfunctional languages, and prove a corresponding soundness theorem. We then\npresent a series of concrete instantiations of our translation, demonstrating\nthat a number of fundamental notions concerning higher-order computation,\nincluding termination, continuity and complexity, can all be subsumed into our\nframework. Our main goal is to provide a unifying scheme which brings together\nseveral concepts which are often treated separately in the literature. However,\nas a by-product, we also obtain (i) a method for extracting moduli of\ncontinuity for closed functionals of type\n$(\\mathbb{N}\\to\\mathbb{N})\\to\\mathbb{N}$ definable in (extensions of) System T,\nand (ii) a characterisation of the time complexity of bar recursion.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 18:39:38 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 14:26:36 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 14:58:16 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 08:39:54 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Powell", "Thomas", ""]]}, {"id": "1906.10793", "submitter": "Yehia Abd Alrahman", "authors": "Yehia Abd Alrahman, Giuseppe Perelli, Nir Piterman", "title": "Reconfigurable Interaction for MAS Modelling", "comments": "This is a final and revised version. This research is funded by the\n  ERC consolidator grant D-SynMA under the European Union's Horizon 2020\n  research and innovation programme (grant agreement No 772459)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formalism to model and reason about multi-agent systems. We\nallow agents to interact and communicate in different modes so that they can\npursue joint tasks; agents may dynamically synchronize, exchange data, adapt\ntheir behaviour, and reconfigure their communication interfaces. The formalism\ndefines a local behaviour based on shared variables and a global one based on\nmessage passing. We extend LTL to be able to reason explicitly about the\nintentions of the different agents and their interaction protocols. We also\nstudy the complexity of satisfiability and model-checking of this extension.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 00:31:46 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 11:51:59 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Alrahman", "Yehia Abd", ""], ["Perelli", "Giuseppe", ""], ["Piterman", "Nir", ""]]}, {"id": "1906.11033", "submitter": "Yanis Sellami", "authors": "Mnacho Echenim, Nicolas Peltier, Yanis Sellami", "title": "Ilinva: Using Abduction to Generate Loop Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system to prove properties of programs. The key feature of this\napproach is a method to automatically synthesize inductive invariants of the\nloops contained in the program. The method is generic, i.e., it applies to a\nlarge set of programming languages and application domains; and lazy, in the\nsense that it only generates invariants that allow one to derive the required\nproperties. It relies on an existing system called GPiD for abductive reasoning\nmodulo theories, and on the platform for program verification Why3. Experiments\nshow evidence of the practical relevance of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:36:46 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Echenim", "Mnacho", ""], ["Peltier", "Nicolas", ""], ["Sellami", "Yanis", ""]]}, {"id": "1906.11166", "submitter": "Jiri Adamek", "authors": "Jiri Adamek", "title": "On free completely iterative algebras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For every finitary set functor F we demonstrate that free algebras carry a\ncanonical partial order. In case F is bicontinuous, we prove that the cpo\nobtained as the conservative completion of the free algebra is the free\ncompletely iterative algebra. Moreover, the algebra structure of the latter is\nthe unique continuous extension of the algebra structure of the free algebra.\nFor general finitary functors the free algebra and the free completely\niterative algebra are proved to be posets sharing the same conservative\ncompletion. And for every recursive equation e in the free completely iterative\nalgebra we present an omega-chain of approximate solutions in the free algebra\nwhose join is the solution of e.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 15:32:44 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 11:16:41 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Adamek", "Jiri", ""]]}, {"id": "1906.11203", "submitter": "Zhe Hou", "authors": "Zhe Hou, David Sanan, Alwen Tiu, Yang Liu and Jin Song Dong", "title": "A formalisation of the SPARC TSO memory model for multi-core machine\n  code", "comments": "15 pages + 2 pages of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SPARC processors have many applications in mission-critical industries such\nas aviation and space engineering. Hence, it is important to provide formal\nframeworks that facilitate the verification of hardware and software that run\non or interface with these processors. This paper presents the first mechanised\nSPARC Total Store Ordering (TSO) memory model which operates on top of an\nabstract model of the SPARC Instruction Set Architecture (ISA) for multi-core\nprocessors. Both models are specified in the theorem prover Isabelle/HOL. We\nformalise two TSO memory models: one is an adaptation of the axiomatic SPARC\nTSO model, the other is a novel operational TSO model which is suitable for\nverifying execution results. We prove that the operational model is sound and\ncomplete with respect to the axiomatic model. Finally, we give verification\nexamples with two case studies drawn from the SPARCv9 manual.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 05:08:39 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Hou", "Zhe", ""], ["Sanan", "David", ""], ["Tiu", "Alwen", ""], ["Liu", "Yang", ""], ["Dong", "Jin Song", ""]]}, {"id": "1906.11319", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland and Kirill Krinkin", "title": "A Stricter Heap Separating Points-To Logic", "comments": "2 pages (Extended Abstract)", "journal-ref": "3rd Int.Scientific Symposium Sense Enable (SPITSE2016)/Int.\n  Academic Forum AMO-SPITSE-NESEFF, 2016, pp.103-104, RSCI 26444969, ISBN\n  978-5-91412-313-7", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic memory issues are hard to locate and may cost much of a development\nproject's efforts and was repeatedly reported similarly afterwards\nindependently by different persons. Verification as one formal method may proof\na given program's heap matches a specified dynamic behaviour. Dynamic (or heap)\nmemory, is the region within main memory that is manipulated by program\nstatements like alloc, free and pointer manipulation during program execution.\nUsually, heap memory is allocated for problems where the amount of used memory\nis unknown prior to execution. Regions within the heap may be related \"somehow\"\nwith each other, often, but not always, by pointers containing absolute\naddresses of related heap cells. The data structure described by all valid\npointer variables manifests heap graphs.\n  A heap graph is a directed connected simple graph within the dynamic memory\nwhich may contain cycles, and where each vertex represents an unique memory\naddress and every edge links two heap vertices. The heap graph must be pointed\nby at least one variable from the local stack or a chain of other heap graphs\nwhich is finally pointed by at least one stacked variable. Heap vertices may\nnot overlap. A heap formula expresses the assertion on dynamic memory and can\neither be a heaplet, or a recursively defined heap-spatial or logical formula.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 19:55:30 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Haberland", "Ren\u00e9", ""], ["Krinkin", "Kirill", ""]]}, {"id": "1906.11425", "submitter": "EPTCS", "authors": "Per Lindgren (Lule{\\aa} University of Technology), Marcus Lindner\n  (Lule{\\aa} University of Technology), Nils Fitinghoff (Lule{\\aa} University\n  of Technology)", "title": "Introducing Certified Compilation in Education by a Functional Language\n  Approach", "comments": "In Proceedings TFPIE 2018, arXiv:1906.10757", "journal-ref": "EPTCS 295, 2019, pp. 65-78", "doi": "10.4204/EPTCS.295.5", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classes on compiler technology are commonly found in Computer Science\ncurricula, covering aspects of parsing, semantic analysis, intermediate\ntransformations and target code generation. This paper reports on introducing\ncertified compilation techniques through a functional language approach in an\nintroductory course on Compiler Construction. Targeting students with little or\nno experience in formal methods, the proof process is highly automated using\nthe Why3 framework. Underlying logic, semantic modelling and proofs are\nintroduced along with exercises and assignments leading up to a formally\nverified compiler for a simplistic imperative language.\n  This paper covers the motivation, course design, tool selection, and teaching\nmethods, together with evaluations and suggested improvements from the\nperspectives of both students and teachers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 03:33:54 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lindgren", "Per", "", "Lule\u00e5 University of Technology"], ["Lindner", "Marcus", "", "Lule\u00e5 University of Technology"], ["Fitinghoff", "Nils", "", "Lule\u00e5 University\n  of Technology"]]}, {"id": "1906.11488", "submitter": "Omar Alhawi", "authors": "Omar M. Alhawi, Mustafa A. Mustafa and Lucas C. Cordeiro", "title": "Finding Security Vulnerabilities in Unmanned Aerial Vehicles Using\n  Software Verification", "comments": "16 pages, 7 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of Unmanned Aerial Vehicles (UAVs) embedded with vulnerable\nmonolithic software has recently raised serious concerns about their security\ndue to concurrency aspects and fragile communication links. However, verifying\nsecurity in UAV software based on traditional testing remains an open challenge\nmainly due to scalability and deployment issues. Here we investigate software\nverification techniques to detect security vulnerabilities in typical UAVs. In\nparticular, we investigate existing software analyzers and verifiers, which\nimplement fuzzing and bounded model checking (BMC) techniques, to detect memory\nsafety and concurrency errors. We also investigate fragility aspects related to\nthe UAV communication link. All UAV components (e.g., position, velocity, and\nattitude control) heavily depend on the communication link. Our preliminary\nresults show that fuzzing and BMC techniques can detect various software\nvulnerabilities, which are of particular interest to ensure security in UAVs.\nWe were able to perform successful cyber-attacks via penetration testing\nagainst the UAV both connection and software system. As a result, we\ndemonstrate real cyber-threats with the possibility of exploiting further\nsecurity vulnerabilities in real-world UAV software in the foreseeable future.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 08:15:39 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 11:57:57 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Alhawi", "Omar M.", ""], ["Mustafa", "Mustafa A.", ""], ["Cordeiro", "Lucas C.", ""]]}, {"id": "1906.11649", "submitter": "Frederic Blanqui", "authors": "Fr\\'ed\\'eric Blanqui (DEDUCTEAM, LSV, ENS Paris Saclay), Guillaume\n  Genestier (DEDUCTEAM, LSV, ENS Paris Saclay, CRI), Olivier Hermant (CRI)", "title": "Dependency Pairs Termination in Dependent Type Theory Modulo Rewriting", "comments": "FSCD 2019 - 4th International Conference on Formal Structures for\n  Computation and Deduction, Jun 2019, Dortmund, Germany", "journal-ref": null, "doi": "10.4230/LIPIcs.FSCD.2019.9", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependency pairs are a key concept at the core of modern automated\ntermination provers for first-order term rewriting systems. In this paper, we\nintroduce an extension of this technique for a large class of dependently-typed\nhigher-order rewriting systems. This extends previous resultsby Wahlstedt on\nthe one hand and the first author on the other hand to strong normalization and\nnon-orthogonal rewriting systems. This new criterion is implemented in the\ntype-checker Dedukti.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:48:50 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 12:02:11 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Blanqui", "Fr\u00e9d\u00e9ric", "", "DEDUCTEAM, LSV, ENS Paris Saclay"], ["Genestier", "Guillaume", "", "DEDUCTEAM, LSV, ENS Paris Saclay, CRI"], ["Hermant", "Olivier", "", "CRI"]]}, {"id": "1906.11742", "submitter": "Carlos Olarte", "authors": "Timo Lang and Carlos Olarte and Elaine Pimentel and Christian\n  Fermuller", "title": "A Game Model for Proofs with Costs", "comments": "To appear in TABLEAUX'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look at substructural calculi from a game semantic point of view, guided\nby certain intuitions about resource conscious and, more specifically, cost\nconscious reasoning. To this aim, we start with a game, where player I defends\na claim corresponding to a (single-conclusion) sequent, while player II tries\nto refute that claim. Branching rules for additive connectives are modeled by\nchoices of II, while branching for multiplicative connectives leads to\nsplitting the game into parallel subgames, all of which have to be won by\nplayer I to succeed. The game comes into full swing by adding cost labels to\nassumptions, and a corresponding budget. Different proofs of the same\nend-sequent are interpreted as more or less expensive strategies for I to\ndefend the corresponding claim. This leads to a new kind of labelled calculus,\nwhich can be seen as a fragment of SELL (subexponential linear logic). Finally,\nwe generalize the concept of costs in proofs by using a semiring structure,\nillustrate our interpretation by examples and investigate some\nproof-theoretical properties.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:42:13 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lang", "Timo", ""], ["Olarte", "Carlos", ""], ["Pimentel", "Elaine", ""], ["Fermuller", "Christian", ""]]}, {"id": "1906.11752", "submitter": "Alexander Koller", "authors": "Antoine Venant and Alexander Koller", "title": "Semantic expressive capacity with bounded memory", "comments": "Accepted at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the capacity of mechanisms for compositional semantic parsing\nto describe relations between sentences and semantic representations.\n  We prove that in order to represent certain relations, mechanisms which are\nsyntactically projective must be able to remember an unbounded number of\nlocations in the semantic representations, where nonprojective mechanisms need\nnot.\n  This is the first result of this kind, and has consequences both for\ngrammar-based and for neural systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:54:57 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Venant", "Antoine", ""], ["Koller", "Alexander", ""]]}, {"id": "1906.12220", "submitter": "Martin Ziegler", "authors": "Arno Pauly and Dongseong Seon and Martin Ziegler", "title": "Computing Haar Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Haar's Theorem, every compact group $G$ admits a unique\n(regular, right and) left-invariant Borel probability measure $\\mu_G$. Let the\nHaar integral (of $G$) denote the functional $\\int_G:\\mathcal{C}(G)\\ni f\\mapsto\n\\int f\\,d\\mu_G$ integrating any continuous function $f:G\\to\\mathbb{R}$ with\nrespect to $\\mu_G$. This generalizes, and recovers for the additive group\n$G=[0;1)\\mod 1$, the usual Riemann integral: computable (cmp. Weihrauch 2000,\nTheorem 6.4.1), and of computational cost characterizing complexity class\n#P$_1$ (cmp. Ko 1991, Theorem 5.32). We establish that in fact every computably\ncompact computable metric group renders the Haar integral computable: once\nasserting computability using an elegant synthetic argument, exploiting\nuniqueness in a computably compact space of probability measures; and once\npresenting and analyzing an explicit, imperative algorithm based on 'maximum\npackings' with rigorous error bounds and guaranteed convergence. Regarding\ncomputational complexity, for the groups $\\mathcal{SO}(3)$ and\n$\\mathcal{SU}(2)$ we reduce the Haar integral to and from Euclidean/Riemann\nintegration. In particular both also characterize #P$_1$. Implementation and\nempirical evaluation using the iRRAM C++ library for exact real computation\nconfirms the (thus necessary) exponential runtime.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:48:32 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 05:20:31 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Pauly", "Arno", ""], ["Seon", "Dongseong", ""], ["Ziegler", "Martin", ""]]}]