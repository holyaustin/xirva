[{"id": "1605.00371", "submitter": "Lorenzo Clemente Lorenzo Clemente", "authors": "Lorenzo Clemente and Pawe{\\l} Parys and Sylvain Salvati and Igor\n  Walukiewicz", "title": "The Diagonal Problem for Higher-Order Recursion Schemes is Decidable", "comments": "technical report; to appear in LICS'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A non-deterministic recursion scheme recognizes a language of finite trees.\nThis very expressive model can simulate, among others, higher-order pushdown\nautomata with collapse. We show decidability of the diagonal problem for\nschemes. This result has several interesting consequences. In particular, it\ngives an algorithm that computes the downward closure of languages of words\nrecognized by schemes. In turn, this has immediate application to separability\nproblems and reachability analysis of concurrent systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 07:15:16 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Clemente", "Lorenzo", ""], ["Parys", "Pawe\u0142", ""], ["Salvati", "Sylvain", ""], ["Walukiewicz", "Igor", ""]]}, {"id": "1605.00381", "submitter": "Wataru Hino", "authors": "Wataru Hino, Hiroki Kobayashi, Ichiro Hasuo and Bart Jacobs", "title": "Healthiness from Duality", "comments": "13 pages, Extended version with appendices of a paper accepted to\n  LICS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthiness is a good old question in program logics that dates back to\nDijkstra. It asks for an intrinsic characterization of those predicate\ntransformers which arise as the (backward) interpretation of a certain class of\nprograms. There are several results known for healthiness conditions: for\ndeterministic programs, nondeterministic ones, probabilistic ones, etc.\nBuilding upon our previous works on so-called state-and-effect triangles, we\ncontribute a unified categorical framework for investigating healthiness\nconditions. We find the framework to be centered around a dual adjunction\ninduced by a dualizing object, together with our notion of relative\nEilenberg-Moore algebra playing fundamental roles too. The latter notion seems\ninteresting in its own right in the context of monads, Lawvere theories and\nenriched categories.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 07:58:04 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Hino", "Wataru", ""], ["Kobayashi", "Hiroki", ""], ["Hasuo", "Ichiro", ""], ["Jacobs", "Bart", ""]]}, {"id": "1605.00565", "submitter": "Marcin Kozik", "authors": "Marcin Kozik", "title": "Weaker consistency notions for all the CSPs of bounded width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterization of all the Constraint Satisfaction Problems of bounded\nwidth, proposed by Feder and Vardi [SICOMP'98], was confirmed in [Bulatov'09]\nand independently in [FOCS'09, JACM'14]. Both proofs are based on the\n(2,3)-consistency (using Prague consistency in [FOCS'09], directly in\n[Bulatov'09]) which is costly to verify.\n  We introduce a new consistency notion, Singleton Linear Arc Consistency\n(SLAC), and show that it solves the same family of problems. SLAC is weaker\nthan Singleton Arc Consistency (SAC) and thus the result answers the question\nfrom [JLC'13] by showing that SAC solves all the problems of bounded width. At\nthe same time the problem of verifying weaker consistency (even SAC) offers\nsignificant computational advantages over the problem of verifying\n(2,3)-consistency which improves the algorithms solving the CSPs of bounded\nwidth.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 16:52:45 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 20:53:55 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Kozik", "Marcin", ""]]}, {"id": "1605.00604", "submitter": "Stefan Mitsch", "authors": "Stefan Mitsch, Khalil Ghorbal, David Vogelbacher, Andr\\'e Platzer", "title": "Formal Verification of Obstacle Avoidance and Navigation of Ground\n  Robots", "comments": null, "journal-ref": "International Journal of Robotics Research. 36(12), pp. 1312-1340,\n  2017", "doi": "10.1177/0278364917733549", "report-no": null, "categories": "cs.SY cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The safety of mobile robots in dynamic environments is predicated on making\nsure that they do not collide with obstacles. In support of such safety\narguments, we analyze and formally verify a series of increasingly powerful\nsafety properties of controllers for avoiding both stationary and moving\nobstacles: (i) static safety, which ensures that no collisions can happen with\nstationary obstacles, (ii) passive safety, which ensures that no collisions can\nhappen with stationary or moving obstacles while the robot moves, (iii) the\nstronger passive friendly safety in which the robot further maintains\nsufficient maneuvering distance for obstacles to avoid collision as well, and\n(iv) passive orientation safety, which allows for imperfect sensor coverage of\nthe robot, i. e., the robot is aware that not everything in its environment\nwill be visible. We complement these provably correct safety properties with\nliveness properties: we prove that provably safe motion is flexible enough to\nlet the robot still navigate waypoints and pass intersections. We use hybrid\nsystem models and theorem proving techniques that describe and formally verify\nthe robot's discrete control decisions along with its continuous, physical\nmotion. Moreover, we formally prove that safety can still be guaranteed despite\nsensor uncertainty and actuator perturbation, and when control choices for more\naggressive maneuvers are introduced. Our verification results are generic in\nthe sense that they are not limited to the particular choices of one specific\ncontrol algorithm but identify conditions that make them simultaneously apply\nto a broad class of control algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 18:26:58 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 18:03:11 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Mitsch", "Stefan", ""], ["Ghorbal", "Khalil", ""], ["Vogelbacher", "David", ""], ["Platzer", "Andr\u00e9", ""]]}, {"id": "1605.00723", "submitter": "Marijn Heule", "authors": "Marijn J. H. Heule, Oliver Kullmann, and Victor W. Marek", "title": "Solving and Verifying the boolean Pythagorean Triples problem via\n  Cube-and-Conquer", "comments": null, "journal-ref": "SAT 2016, LNCS 9710, pages 228-245", "doi": "10.1007/978-3-319-40970-2_15", "report-no": null, "categories": "cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boolean Pythagorean Triples problem has been a longstanding open problem\nin Ramsey Theory: Can the set N = $\\{1, 2, ...\\}$ of natural numbers be divided\ninto two parts, such that no part contains a triple $(a,b,c)$ with $a^2 + b^2 =\nc^2$ ? A prize for the solution was offered by Ronald Graham over two decades\nago.\n  We solve this problem, proving in fact the impossibility, by using the\nCube-and-Conquer paradigm, a hybrid SAT method for hard problems, employing\nboth look-ahead and CDCL solvers. An important role is played by dedicated\nlook-ahead heuristics, which indeed allowed to solve the problem on a cluster\nwith 800 cores in about 2 days.\n  Due to the general interest in this mathematical problem, our result requires\na formal proof. Exploiting recent progress in unsatisfiability proofs of SAT\nsolvers, we produced and verified a proof in the DRAT format, which is almost\n200 terabytes in size. From this we extracted and made available a compressed\ncertificate of 68 gigabytes, that allows anyone to reconstruct the DRAT proof\nfor checking.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 01:32:34 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Heule", "Marijn J. H.", ""], ["Kullmann", "Oliver", ""], ["Marek", "Victor W.", ""]]}, {"id": "1605.00950", "submitter": "Stefan Kiefer", "authors": "Christel Baier, Stefan Kiefer, Joachim Klein, Sascha Kl\\\"uppelholz,\n  David M\\\"uller, James Worrell", "title": "Markov Chains and Unambiguous Automata", "comments": "35 pages, draft journal article. The previous version (v1), 50 pages,\n  is the full version of a paper accepted at CAV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unambiguous automata are nondeterministic automata in which every word has at\nmost one accepting run. In this paper we give a polynomial-time algorithm for\nmodel checking discrete-time Markov chains against $\\omega$-regular\nspecifications represented as unambiguous automata. We furthermore show that\nthe complexity of this model checking problem lies in NC: the subclass of P\ncomprising those problems solvable in poly-logarithmic parallel time. These\ncomplexity bounds match the known bounds for model checking Markov chains\nagainst specifications given as deterministic automata, notwithstanding the\nfact that unambiguous automata can be exponentially more succinct than\ndeterministic automata. We report on an implementation of our procedure,\nincluding an experiment in which the implementation is used to model check LTL\nformulas on Markov chains.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 15:33:51 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 14:33:05 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Baier", "Christel", ""], ["Kiefer", "Stefan", ""], ["Klein", "Joachim", ""], ["Kl\u00fcppelholz", "Sascha", ""], ["M\u00fcller", "David", ""], ["Worrell", "James", ""]]}, {"id": "1605.01003", "submitter": "Samuel J. van Gool", "authors": "Silvio Ghilardi and Samuel J. van Gool", "title": "Monadic second order logic as the model companion of temporal logic", "comments": "22 pp. (10 pp. + 12 pp. appendix). LICS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main focus of this paper is on bisimulation-invariant MSO, and more\nparticularly on giving a novel model-theoretic approach to it. In model theory,\na model companion of a theory is a first-order description of the class of\nmodels in which all potentially solvable systems of equations and non-equations\nhave solutions. We show that bisimulation-invariant MSO on trees gives the\nmodel companion for a new temporal logic, \"fair CTL\", an enrichment of CTL with\nlocal fairness constraints. To achieve this, we give a completeness proof for\nthe logic fair CTL which combines tableaux and Stone duality, and a fair CTL\nencoding of the automata for the modal {\\mu}-calculus. Moreover, we also show\nthat MSO on binary trees is the model companion of binary deterministic fair\nCTL.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:06:51 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Ghilardi", "Silvio", ""], ["van Gool", "Samuel J.", ""]]}, {"id": "1605.01004", "submitter": "Antonis Achilleos", "authors": "Antonis Achilleos", "title": "The Completeness Problem for Modal Logic", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.2373.6727", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the completeness problem for Modal Logic and examine its\ncomplexity. For a definition of completeness for formulas, given a formula of a\nmodal logic, the completeness problem asks whether the formula is complete for\nthat logic. We discover that completeness and validity have the same complexity\n--- with certain exceptions for which there are, in general, no complete\nformulas. To prove upper bounds, we present a non-deterministic polynomial-time\nprocedure with an oracle from PSPACE that combines tableaux and a test for\nbisimulation, and determines whether a formula is complete.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:10:15 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 14:59:07 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Achilleos", "Antonis", ""]]}, {"id": "1605.01083", "submitter": "J\\\"urgen Koslowski", "authors": "Harley Eades III and Aaron Stump and Ryan McCleeary", "title": "Dualized Simple Type Theory", "comments": "47 pages, 10 figures", "journal-ref": "Logical Methods in Computer Science, Volume 12, Issue 3 (April 27,\n  2017) lmcs:2005", "doi": "10.2168/LMCS-12(3:2)2016", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new bi-intuitionistic type theory called Dualized Type Theory\n(DTT). It is a simple type theory with perfect intuitionistic duality, and\ncorresponds to a single-sided polarized sequent calculus. We prove DTT strongly\nnormalizing, and prove type preservation. DTT is based on a new propositional\nbi-intuitionistic logic called Dualized Intuitionistic Logic (DIL) that builds\non Pinto and Uustalu's logic L. DIL is a simplification of L by removing\nseveral admissible inference rules while maintaining consistency and\ncompleteness. Furthermore, DIL is defined using a dualized syntax by labeling\nformulas and logical connectives with polarities thus reducing the number of\ninference rules needed to define the logic. We give a direct proof of\nconsistency, but prove completeness by reduction to L.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 20:44:24 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 19:55:44 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Eades", "Harley", "III"], ["Stump", "Aaron", ""], ["McCleeary", "Ryan", ""]]}, {"id": "1605.01153", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Yassine Hamza, Harald Ruess", "title": "Structural Synthesis for GXW Specifications", "comments": "The long (including appendix) version being reviewed by CAV'16\n  program committee. Compared to the submitted version, one author (out of her\n  wish) is moved to the Acknowledgement. (v2) Corrected typos. (v3) Add an\n  additional remark over environment assumption and easy corner cases", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the GXW fragment of linear temporal logic (LTL) as the basis for\nsynthesizing embedded control software for safety-critical applications. Since\nGXW includes the use of a weak-until operator we are able to specify a number\nof diverse programmable logic control (PLC) problems, which we have compiled\nfrom industrial training sets. For GXW controller specifications, we develop a\nnovel approach for synthesizing a set of synchronously communicating\nactor-based controllers. This synthesis algorithm proceeds by means of\nrecursing over the structure of GXW specifications, and generates a set of\ndedicated and synchronously communicating sub-controllers according to the\nformula structure. In a subsequent step, 2QBF constraint solving identifies and\ntries to resolve potential conflicts between individual GXW specifications.\nThis structural approach to GXW synthesis supports traceability between\nrequirements and the generated control code as mandated by certification\nregimes for safety-critical software. Synthesis for GXW specifications is in\nPSPACE compared to 2EXPTIME-completeness of full-fledged LTL synthesis. Indeed\nour experimental results suggest that GXW synthesis scales well to\nindustrial-sized control synthesis problems with 20 input and output ports and\nbeyond.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 06:18:13 GMT"}, {"version": "v2", "created": "Sat, 7 May 2016 23:48:24 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 10:15:17 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["Hamza", "Yassine", ""], ["Ruess", "Harald", ""]]}, {"id": "1605.01198", "submitter": "Kord Eickmeyer", "authors": "Kord Eickmeyer and Ken-ichi Kawarabayashi", "title": "Successor-Invariant First-Order Logic on Graphs with Excluded\n  Topological Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the model-checking problem for successor-invariant first-order\nlogic is fixed-parameter tractable on graphs with excluded topological\nsubgraphs when parameterised by both the size of the input formula and the size\nof the exluded topological subgraph. Furthermore, we show that model-checking\nfor order-invariant first-order logic is tractable on coloured posets of\nbounded width, parameterised by both the size of the input formula and the\nwidth of the poset.\n  Our result for successor-invariant FO extends previous results for this logic\non planar graphs (Engelmann et al., LICS 2012) and graphs with excluded minors\n(Eickmeyer et al., LICS 2013), further narrowing the gap between what is known\nfor FO and what is known for successor-invariant FO. The proof uses Grohe and\nMarx's structure theorem for graphs with excluded topological subgraphs. For\norder-invariant FO we show that Gajarsk\\'y et al.'s recent result for FO\ncarries over to order-invariant FO.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 09:44:03 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Eickmeyer", "Kord", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1605.01199", "submitter": "Albert Atserias", "authors": "Albert Atserias and Szymon Toru\\'nczyk", "title": "Non-Homogenizable Classes of Finite Structures", "comments": "Very minor revision of the introduction section and the bibliography\n  of the earlier version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homogenization is a powerful way of taming a class of finite structures with\nseveral interesting applications in different areas, from Ramsey theory in\ncombinatorics to constraint satisfaction problems (CSPs) in computer science,\nthrough (finite) model theory. A few sufficient conditions for a class of\nfinite structures to allow homogenization are known, and here we provide a\nnecessary condition. This lets us show that certain natural classes are not\nhomogenizable: 1) the class of locally consistent systems of linear equations\nover the two-element field or any finite Abelian group, and 2) the class of\nfinite structures that forbid homomorphisms from a specific MSO-definable class\nof structures of treewidth two. In combination with known results, the first\nexample shows that, up to pp-interpretability, the CSPs that are solvable by\nlocal consistency methods are distinguished from the rest by the fact that\ntheir classes of locally consistent instances are homogenizable. The second\nexample shows that, for MSO-definable classes of forbidden patterns, treewidth\none versus two is the dividing line to homogenizability.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 09:47:00 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 16:28:45 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Atserias", "Albert", ""], ["Toru\u0144czyk", "Szymon", ""]]}, {"id": "1605.01511", "submitter": "Felix Klein", "authors": "Bernd Finkbeiner and Felix Klein", "title": "Bounded Cycle Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for the synthesis of Mealy machines from\nspecifications in linear-time temporal logic (LTL), where the number of cycles\nin the state graph of the implementation is limited by a given bound. Bounding\nthe number of cycles leads to implementations that are structurally simpler and\neasier to understand. We solve the synthesis problem via an extension of\nSAT-based bounded synthesis, where we additionally construct a witness\nstructure that limits the number of cycles. We also establish a\ntriple-exponential upper and lower bound for the potential blow-up between the\nlength of the LTL formula and the number of cycles in the state graph.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 07:21:02 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Finkbeiner", "Bernd", ""], ["Klein", "Felix", ""]]}, {"id": "1605.01596", "submitter": "Gianluca Caterina", "authors": "Vittorio Cafagna, Gianluca Caterina", "title": "Notes on a model for fuzzy computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In these notes we propose a setting for fuzzy computing in a framework\nsimilar to that of well-established theories of computation: boolean, and\nquantum computing. Our efforts have been directed towards stressing the formal\nsimilarities: there is a common pattern underlying these three theories. We\ntried to conform our approach, as much as possible, to this pattern. This work\nwas part of a project jointly with Professor Vittorio Cafagna. Professor\nCafagna passed away unexpectedly in 2007. His intellectual breadth and\ninspiring passion for mathematics is still very well alive.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 16:16:10 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Cafagna", "Vittorio", ""], ["Caterina", "Gianluca", ""]]}, {"id": "1605.01622", "submitter": "Jingchao Chen", "authors": "Jingchao Chen", "title": "Improving abcdSAT by At-Least-One Recently Used Clause Management\n  Strategy", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve further the 2015 version of abcdSAT by various heuristics such as\nat-least-one recently used strategy, learnt clause database approximation\nreduction etc. Based on the requirement of different tracks at the SAT\nCompetition 2016, we develop three versions of abcdSAT: drup, inc and lim,\nwhich participate in the competition of main (agile), incremental library and\nno-limit track, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 15:20:33 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Chen", "Jingchao", ""]]}, {"id": "1605.01686", "submitter": "Raine Ronnholm", "authors": "Raine R\\\"onnholm", "title": "The Expressive Power of k-ary Exclusion Logic", "comments": "Preprint of a paper in the special issue of WoLLIC2016 in Annals of\n  Pure and Applied Logic, 170(9):1070-1099, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the expressive power of k-ary exclusion logic, EXC[k],\nthat is obtained by extending first order logic with k-ary exclusion atoms. It\nis known that without arity bounds exclusion logic is equivalent with\ndependence logic. By observing the translations, we see that the expressive\npower of EXC[k] lies in between k-ary and (k+1)-ary dependence logics. We will\nshow that, at least in the case of k=1, the both of these inclusions are\nproper.\n  In a recent work by the author it was shown that k-ary inclusion-exclusion\nlogic is equivalent with k-ary existential second order logic, ESO[k]. We will\nshow that, on the level of sentences, it is possible to simulate inclusion\natoms with exclusion atoms, and this way express ESO[k]-sentences by using only\nk-ary exclusion atoms. For this translation we also need to introduce a novel\nmethod for \"unifying\" the values of certain variables in a team. As a\nconsequence, EXC[k] captures ESO[k] on the level of sentences, and we get a\nstrict arity hierarchy for exclusion logic. It also follows that k-ary\ninclusion logic is strictly weaker than EXC[k].\n  Finally we will use similar techniques to formulate a translation from ESO[k]\nto k-ary inclusion logic with strict semantics. Consequently, for any arity\nfragment of inclusion logic, strict semantics is more expressive than lax\nsemantics.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 18:37:32 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 17:10:57 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 05:09:57 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["R\u00f6nnholm", "Raine", ""]]}, {"id": "1605.01846", "submitter": "Pieter Van Hertum", "authors": "Pieter Van Hertum, Ingmar Dasseville, Gerda Janssens, Marc Denecker", "title": "The KB paradigm and its application to interactive configuration", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": "10.1017/S1471068416000156", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge base paradigm aims to express domain knowledge in a rich formal\nlanguage, and to use this domain knowledge as a knowledge base to solve various\nproblems and tasks that arise in the domain by applying multiple forms of\ninference. As such, the paradigm applies a strict separation of concerns\nbetween information and problem solving. In this paper, we analyze the\nprinciples and feasibility of the knowledge base paradigm in the context of an\nimportant class of applications: interactive configuration problems. In\ninteractive configuration problems, a configuration of interrelated objects\nunder constraints is searched, where the system assists the user in reaching an\nintended configuration. It is widely recognized in industry that good software\nsolutions for these problems are very difficult to develop. We investigate such\nproblems from the perspective of the KB paradigm. We show that multiple\nfunctionalities in this domain can be achieved by applying different forms of\nlogical inferences on a formal specification of the configuration domain. We\nreport on a proof of concept of this approach in a real-life application with a\nbanking company. To appear in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 07:39:19 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Van Hertum", "Pieter", ""], ["Dasseville", "Ingmar", ""], ["Janssens", "Gerda", ""], ["Denecker", "Marc", ""]]}, {"id": "1605.01886", "submitter": "Fritz M\\\"uller", "authors": "Fritz M\\\"uller", "title": "From Sazonov's Non-Dcpo Natural Domains to Closed Directed-Lub Partial\n  Orders", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normann proved that the domains of the game model of PCF (the domains of\nsequential functionals) need not be dcpos. Sazonov has defined natural domains\nfor a theory of such incomplete domains. This paper further develops that\ntheory. It defines lub-rules that infer natural lubs from existing natural\nlubs, and lub-rule classes that describe axiom systems like that of natural\ndomains. There is a canonical proper subcategory of the natural domains, the\nclosed directed lub partial orders (cdlubpo), that corresponds to the complete\nlub-rule class of all valid lub-rules. Cdlubpos can be completed to restricted\ndcpos, which are dcpos that retain the data of the incomplete cdlubpo as a\nsubset.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 10:50:35 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["M\u00fcller", "Fritz", ""]]}, {"id": "1605.01995", "submitter": "Yanjing Wang", "authors": "Yanjing Wang", "title": "Beyond knowing that: a new generation of epistemic logics", "comments": "36 pages, to appear in Jaakko Hintikka on knowledge and game\n  theoretical semantics, Springer's Outstanding Contributions to Logic Series\n  (some references are updated in this version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epistemic logic has become a major field of philosophical logic ever since\nthe groundbreaking work by Hintikka (1962). Despite its various successful\napplications in theoretical computer science, AI, and game theory, the\ntechnical development of the field has been mainly focusing on the\npropositional part, i.e., the propositional modal logics of \"knowing that\".\nHowever, knowledge is expressed in everyday life by using various other\nlocutions such as \"knowing whether\", \"knowing what\", \"knowing how\" and so on\n(knowing-wh hereafter). Such knowledge expressions are better captured in\nquantified epistemic logic, as was already discussed by Hintikka (1962) and his\nsequel works at length. This paper aims to draw the attention back again to\nsuch a fascinating but largely neglected topic. We first survey what Hintikka\nand others did in the literature of quantified epistemic logic, and then\nadvocate a new quantifier-free approach to study the epistemic logics of\nknowing-wh, which we believe can balance expressivity and complexity, and\ncapture the essential reasoning patterns about knowing-wh. We survey our recent\nline of work on the epistemic logics of \"knowing whether\", \"knowing what\" and\n\"knowing how\" to demonstrate the use of this new approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 16:24:03 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 02:23:48 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 08:54:01 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Wang", "Yanjing", ""]]}, {"id": "1605.02185", "submitter": "Carl Leonardsson", "authors": "Parosh Aziz Abdulla, Mohamed Faouzi Atig, Bengt Jonsson, Carl\n  Leonardsson", "title": "Stateless Model Checking for POWER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first framework for efficient application of stateless model\nchecking (SMC) to programs running under the relaxed memory model of POWER. The\nframework combines several contributions. The first contribution is that we\ndevelop a scheme for systematically deriving operational execution models from\nexisting axiomatic ones. The scheme is such that the derived execution models\nare well suited for efficient SMC. We apply our scheme to an axiomatic model of\nPOWER. Our main contribution is a technique for efficient SMC, called Relaxed\nStateless Model Checking (RSMC), which systematically explores the possible\ninequivalent executions of a program. RSMC is suitable for execution models\nobtained using our scheme. We prove that RSMC is sound and optimal for the\nPOWER memory model, in the sense that each complete program behavior is\nexplored exactly once. We show the feasibility of our technique by providing an\nimplementation for programs written in C/pthreads.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 12:36:19 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Abdulla", "Parosh Aziz", ""], ["Atig", "Mohamed Faouzi", ""], ["Jonsson", "Bengt", ""], ["Leonardsson", "Carl", ""]]}, {"id": "1605.02206", "submitter": "Henning Basold", "authors": "Henning Basold, Herman Geuvers", "title": "Type Theory based on Dependent Inductive and Coinductive Types", "comments": "10 pages as to appear with small changes at LICS 2016 + 5 pages of\n  examples and proofs", "journal-ref": null, "doi": "10.1145/2933575.2934514", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a dependent type theory that is based purely on inductive and\ncoinductive types, and the corresponding recursion and corecursion principles.\nThis results in a type theory with a small set of rules, while still being\nfairly expressive. For example, all well-known basic types and type formers\nthat are needed for using this type theory as a logic are definable:\npropositional connectives, like falsity, conjunction, disjunction, and function\nspace, dependent function space, existential quantification, equality, natural\nnumbers, vectors etc. The reduction relation on terms consists solely of a rule\nfor recursion and a rule for corecursion. The reduction relations for\nwell-known types arise from that. To further support the introduction of this\nnew type theory, we also prove fundamental properties of its term calculus.\nMost importantly, we prove subject reduction and strong normalisation of the\nreduction relation, which gives computational meaning to the terms.\n  The presented type theory is based on ideas from categorical logic that have\nbeen investigated before by the first author, and it extends Hagino's\ncategorical data types to a dependently typed setting. By basing the type\ntheory on concepts from category theory we maintain the duality between\ninductive and coinductive types, and it allows us to describe, for example, the\nfunction space as a coinductive type.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 15:52:14 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Basold", "Henning", ""], ["Geuvers", "Herman", ""]]}, {"id": "1605.02210", "submitter": "Adrian Onet", "authors": "Adrian Onet", "title": "Inference-based semantics in Data Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Exchange is an old problem that was firstly studied from a theoretical\npoint of view only in 2003. Since then many approaches were considered when it\ncame to the language describing the relationship between the source and the\ntarget schema. These approaches focus on what it makes a target instance a\n\"good\" solution for data-exchange. In this paper we propose the inference-based\nsemantics that solves many certain-answer anomalies existing in current\ndata-exchange semantics. To this we introduce a new mapping language between\nthe source and the target schema based on annotated bidirectional dependencies\n(abd) and, consequently define the semantics for this new language. It is shown\nthat the ABD-semantics can properly represent the inference-based semantics,\nfor any source-to-target mappings. We discovered three dichotomy results under\nthe new semantics for solution-existence, solution-check and UCQ evaluation\nproblems. These results rely on two factors describing the annotation used in\nthe mappings (density and cardinality). Finally we also investigate the\ncertain-answers evaluation problem under ABD-semantics and discover many\ntractable classes for non-UCQ queries even for a subclass of CQ with negation.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 16:28:40 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Onet", "Adrian", ""]]}, {"id": "1605.02311", "submitter": "J\\\"urgen Koslowski", "authors": "Andrzej S. Murawski (University of Warwick) and Nikos Tzevelekos\n  (Queen Mary University of London)", "title": "Block structure vs scope extrusion: between innocence and omniscience", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 12, Issue 3 (April 27,\n  2017) lmcs:2007", "doi": "10.2168/LMCS-12(3:3)2016", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the semantic meaning of block structure using game semantics. To\nthat end, we introduce the notion of block-innocent strategies and characterise\ncall-by-value computation with block-allocated storage through soundness,\nfinite definability and universality results. This puts us in a good position\nto conduct a comparative study of purely functional computation, computation\nwith block storage as well as that with dynamic memory allocation. For example,\nwe can show that dynamic variable allocation can be replaced with\nblock-allocated variables exactly when the term involved (open or closed) is of\nbase type and that block-allocated storage can be replaced with purely\nfunctional computation when types of order two are involved. To illustrate the\nrestrictive nature of block structure further, we prove a decidability result\nfor a finitary fragment of call-by-value Idealized Algol for which it is known\nthat allowing for dynamic memory allocation leads to undecidability.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 11:50:43 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 19:19:27 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Murawski", "Andrzej S.", "", "University of Warwick"], ["Tzevelekos", "Nikos", "", "Queen Mary University of London"]]}, {"id": "1605.02350", "submitter": "Zachary Kincaid", "authors": "Azadeh Farzan, Zachary Kincaid, Andreas Podelski", "title": "Proving Liveness of Parameterized Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctness of multi-threaded programs typically requires that they satisfy\nliveness properties. For example, a program may require that no thread is\nstarved of a shared resource, or that all threads eventually agree on a single\nvalue. This paper presents a method for proving that such liveness properties\nhold. Two particular challenges addressed in this work are that (1) the\ncorrectness argument may rely on global behaviour of the system (e.g., the\ncorrectness argument may require that all threads collectively progress towards\n\"the good thing\" rather than one thread progressing while the others do not\ninterfere), and (2) such programs are often designed to be executed by any\nnumber of threads, and the desired liveness properties must hold regardless of\nthe number of threads that are active in the program.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 19:10:11 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Farzan", "Azadeh", ""], ["Kincaid", "Zachary", ""], ["Podelski", "Andreas", ""]]}, {"id": "1605.03045", "submitter": "Micha{\\l} Pilipczuk", "authors": "Miko{\\l}aj Boja\\'nczyk and Micha{\\l} Pilipczuk", "title": "Definability equals recognizability for graphs of bounded treewidth", "comments": "21 pages, an extended abstract will appear in the proceedings of LICS\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DM cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a conjecture of Courcelle, which states that a graph property is\ndefinable in MSO with modular counting predicates on graphs of constant\ntreewidth if, and only if it is recognizable in the following sense:\nconstant-width tree decompositions of graphs satisfying the property can be\nrecognized by tree automata. While the forward implication is a classic fact\nknown as Courcelle's theorem, the converse direction remained open\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 15:03:07 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Boja\u0144czyk", "Miko\u0142aj", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1605.03227", "submitter": "Peter LeFanu Lumsdaine", "authors": "Kuen-Bang Hou (Favonia), Eric Finster, Dan Licata, Peter LeFanu\n  Lumsdaine", "title": "A mechanization of the Blakers-Massey connectivity theorem in Homotopy\n  Type Theory", "comments": "To appear in LICS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues investigations in \"synthetic homotopy theory\": the use\nof homotopy type theory to give machine-checked proofs of constructions from\nhomotopy theory\n  We present a mechanized proof of the Blakers-Massey connectivity theorem, a\nresult relating the higher-dimensional homotopy groups of a pushout type\n(roughly, a space constructed by gluing two spaces along a shared subspace) to\nthose of the components of the pushout. This theorem gives important\ninformation about the pushout type, and has a number of useful corollaries,\nincluding the Freudenthal suspension theorem, which has been studied in\nprevious formalizations.\n  The new proof is more elementary than existing ones in abstract\nhomotopy-theoretic settings, and the mechanization is concise and high-level,\nthanks to novel combinations of ideas from homotopy theory and type theory.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 22:13:17 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Hou", "Kuen-Bang", "", "Favonia"], ["Finster", "Eric", ""], ["Licata", "Dan", ""], ["Lumsdaine", "Peter LeFanu", ""]]}, {"id": "1605.03354", "submitter": "Vasco Brattka", "authors": "Vasco Brattka, Guido Gherardi, Rupert H\\\"olzl and Arno Pauly", "title": "The Vitali Covering Theorem in the Weihrauch Lattice", "comments": "13 pages", "journal-ref": "in: A. Day et al. (Eds.) Computability and Complexity: Essays\n  Dedicated to Rodney G. Downey on the Occasion of His 60th Birthday, Springer,\n  2017, LNCS vol. 10010, pp. 188-200", "doi": "10.1007/978-3-319-50062-1_14", "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the uniform computational content of the Vitali Covering Theorem for\nintervals using the tool of Weihrauch reducibility. We show that a more\ndetailed picture emerges than what a related study by Giusto, Brown, and\nSimpson has revealed in the setting of reverse mathematics. In particular,\ndifferent formulations of the Vitali Covering Theorem turn out to have\ndifferent uniform computational content. These versions are either computable\nor closely related to uniform variants of Weak Weak K\\H{o}nig's Lemma.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 09:46:02 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 16:25:50 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Brattka", "Vasco", ""], ["Gherardi", "Guido", ""], ["H\u00f6lzl", "Rupert", ""], ["Pauly", "Arno", ""]]}, {"id": "1605.03480", "submitter": "Sandra Kiefer", "authors": "Sandra Kiefer, Pascal Schweitzer", "title": "Upper Bounds on the Quantifier Depth for Graph Differentiation in\n  First-Order Logic", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (May 30,\n  2019) lmcs:5522", "doi": "10.23638/LMCS-15(2:19)2019", "report-no": null, "categories": "cs.LO cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that on graphs with n vertices, the 2-dimensional Weisfeiler-Leman\nalgorithm requires at most O(n^2/log(n)) iterations to reach stabilization.\nThis in particular shows that the previously best, trivial upper bound of\nO(n^2) is asymptotically not tight. In the logic setting, this translates to\nthe statement that if two graphs of size n can be distinguished by a formula in\nfirst-order logic with counting with 3 variables (i.e., in C3), then they can\nalso be distinguished by a C3-formula that has quantifier depth at most\nO(n^2/log(n)).\n  To prove the result we define a game between two players that enables us to\ndecouple the causal dependencies between the processes happening simultaneously\nover several iterations of the algorithm. This allows us to treat large color\nclasses and small color classes separately. As part of our proof we show that\nfor graphs with bounded color class size, the number of iterations until\nstabilization is at most linear in the number of vertices. This also yields a\ncorresponding statement in first-order logic with counting.\n  Similar results can be obtained for the respective logic without counting\nquantifiers, i.e., for the logic L3.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:28:46 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 00:30:00 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 20:49:04 GMT"}, {"version": "v4", "created": "Sun, 26 May 2019 15:25:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kiefer", "Sandra", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1605.03811", "submitter": "Romain Brenguier", "authors": "Romain Brenguier and Vojt\\v{e}ch Forejt", "title": "Decidability Results for Multi-objective Stochastic Games", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stochastic two-player turn-based games in which the objective of one\nplayer is to ensure several infinite-horizon total reward objectives, while the\nother player attempts to spoil at least one of the objectives. The games have\npreviously been shown not to be determined, and an approximation algorithm for\ncomputing a Pareto curve has been given. The major drawback of the existing\nalgorithm is that it needs to compute Pareto curves for finite horizon\nobjectives (for increasing length of the horizon), and the size of these Pareto\ncurves can grow unboundedly, even when the infinite-horizon Pareto curve is\nsmall. By adapting existing results, we first give an algorithm that computes\nthe Pareto curve for determined games. Then, as the main result of the paper,\nwe show that for the natural class of stopping games and when there are two\nreward objectives, the problem of deciding whether a player can ensure\nsatisfaction of the objectives with given thresholds is decidable. The result\nrelies on intricate and novel proof which shows that the Pareto curves contain\nonly finitely many points. As a consequence, we get that the two-objective\ndiscounted-reward problem for unrestricted class of stochastic games is\ndecidable.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 13:44:57 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Brenguier", "Romain", ""], ["Forejt", "Vojt\u011bch", ""]]}, {"id": "1605.03892", "submitter": "Pierre Fraigniaud", "authors": "Alkida Balliu and Gianlorenzo D'Angelo and Pierre Fraigniaud and\n  Dennis Olivetti", "title": "Local Distributed Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of distributed network computing, it is known that, for\nevery network predicate, each network configuration that satisfies this\npredicate can be proved using distributed certificates which can be verified\nlocally. However, this requires to leak information about the identities of the\nnodes in the certificates, which might not be applicable in a context in which\nprivacy is desirable. Unfortunately, it is known that if one insists on\ncertificates independent of the node identities, then not all network\npredicates can be proved using distributed certificates that can be verified\nlocally. In this paper, we prove that, for every network predicate, there is a\ndistributed protocol satisfying the following two properties: (1) for every\nnetwork configuration that is legal w.r.t. the predicate, and for any attempt\nby an adversary to prove the illegality of that configuration using distributed\ncertificates, there is a locally verifiable proof that the adversary is wrong,\nalso using distributed certificates; (2) for every network configuration that\nis illegal w.r.t. the predicate, there is a proof of that illegality, using\ndistributed certificates, such that no matter the way an adversary assigns its\nown set of distributed certificates in an attempt to prove the legality of the\nconfiguration, the actual illegality of the configuration will be locally\ndetected. In both cases, the certificates are independent of the identities of\nthe nodes. These results are achieved by investigating the so-called local\nhierarchy of complexity classes in which the certificates do not exploit the\nnode identities. Indeed, we give a characterization of such a hierarchy, which\nis of its own interest\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 16:51:54 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Balliu", "Alkida", ""], ["D'Angelo", "Gianlorenzo", ""], ["Fraigniaud", "Pierre", ""], ["Olivetti", "Dennis", ""]]}, {"id": "1605.04013", "submitter": "EPTCS", "authors": "Stefano Gogioso (University of Oxford)", "title": "A Corpus-based Toy Model for DisCoCat", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 20-28", "doi": "10.4204/EPTCS.221.3", "report-no": null, "categories": "cs.CL cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical compositional distributional (DisCoCat) model of meaning\nrigorously connects distributional semantics and pregroup grammars, and has\nfound a variety of applications in computational linguistics. From a more\nabstract standpoint, the DisCoCat paradigm predicates the construction of a\nmapping from syntax to categorical semantics. In this work we present a\nconcrete construction of one such mapping, from a toy model of syntax for\ncorpora annotated with constituent structure trees, to categorical semantics\ntaking place in a category of free R-semimodules over an involutive commutative\nsemiring R.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 00:32:01 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 00:40:51 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Gogioso", "Stefano", "", "University of Oxford"]]}, {"id": "1605.04136", "submitter": "J\\\"urgen Koslowski", "authors": "Joachim Parrow and Tjark Weber", "title": "The Largest Respectful Function", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 12, Issue 2 (June 29,\n  2016) lmcs:1643", "doi": "10.2168/LMCS-12(2:11)2016", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respectful functions were introduced by Sangiorgi as a compositional tool to\nformulate short and clear bisimulation proofs. Usually, the larger the\nrespectful function, the easier the bisimulation proof. In particular the\nlargest respectful function, defined as the pointwise union of all respectful\nfunctions, has been shown to be very useful. We here provide an explicit and\nconstructive characterization of it.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 11:25:26 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 09:34:50 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 19:46:47 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Parrow", "Joachim", ""], ["Weber", "Tjark", ""]]}, {"id": "1605.04271", "submitter": "Mar\\'ia Emilia Descotte", "authors": "Sergio Abriola, Mar\\'ia Emilia Descotte, Raul Fervari, Santiago\n  Figueira", "title": "Axiomatizations for downward XPath on Data Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give sound and complete axiomatizations for XPath with data tests by\n\"equality\" or \"inequality\", and containing the single \"child\" axis. This\ndata-aware logic predicts over data trees, which are tree-like structures whose\nevery node contains a label from a finite alphabet and a data value from an\ninfinite domain. The language allows us to compare data values of two nodes but\ncannot access the data values themselves (i.e. there is no comparison by\nconstants). Our axioms are in the style of equational logic, extending the\naxiomatization of data-oblivious XPath, by B. ten Cate, T. Litak and M. Marx.\nWe axiomatize the full logic with tests by \"equality\" and \"inequality\", and\nalso a simpler fragment with \"equality\" tests only. Our axiomatizations apply\nboth to node expressions and path expressions. The proof of completeness relies\non a novel normal form theorem for XPath with data tests.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 17:49:22 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 12:30:00 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Abriola", "Sergio", ""], ["Descotte", "Mar\u00eda Emilia", ""], ["Fervari", "Raul", ""], ["Figueira", "Santiago", ""]]}, {"id": "1605.04343", "submitter": "Adam Yedidia", "authors": "Adam Yedidia and Scott Aaronson", "title": "A Relatively Small Turing Machine Whose Behavior Is Independent of Set\n  Theory", "comments": "31 pages, 6 figures, with the last 10 pages devoted to bibliography\n  and appendices. Submitted to Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the definition of the Busy Beaver function by Rado in 1962, an\ninteresting open question has been the smallest value of n for which BB(n) is\nindependent of ZFC set theory. Is this n approximately 10, or closer to\n1,000,000, or is it even larger? In this paper, we show that it is at most\n7,910 by presenting an explicit description of a 7,910-state Turing machine Z\nwith 1 tape and a 2-symbol alphabet that cannot be proved to run forever in ZFC\n(even though it presumably does), assuming ZFC is consistent. The machine is\nbased on the work of Harvey Friedman on independent statements involving\norder-invariant graphs. In doing so, we give the first known upper bound on the\nhighest provable Busy Beaver number in ZFC. To create Z, we develop and use a\nhigher-level language, Laconic, which is much more convenient than direct state\nmanipulation. We also use Laconic to design two Turing machines, G and R, that\nhalt if and only if there are counterexamples to Goldbach's Conjecture and the\nRiemann Hypothesis, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 22:05:58 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Yedidia", "Adam", ""], ["Aaronson", "Scott", ""]]}, {"id": "1605.04400", "submitter": "Andrea Turrini", "authors": "Yong Li and Wanwei Liu and Andrea Turrini and Ernst Moritz Hahn and\n  Lijun Zhang", "title": "An Efficient Synthesis Algorithm for Parametric Markov Chains Against\n  Linear Time Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient algorithm for the parameter synthesis\nof PLTL formulas with respect to parametric Markov chains. The PLTL formula is\ntranslated to an almost fully partitioned B\\\"uchi automaton which is then\ncomposed with the parametric Markov chain. We then reduce the problem to\nsolving an optimisation problem, allowing to decide the satisfaction of the\nformula using an SMT solver. The algorithm works also for interval Markov\nchains. The complexity is linear in the size of the Markov chain, and\nexponential in the size of the formula. We provide a prototype and show the\nefficiency of our approach on a number of benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 09:35:05 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Li", "Yong", ""], ["Liu", "Wanwei", ""], ["Turrini", "Andrea", ""], ["Hahn", "Ernst Moritz", ""], ["Zhang", "Lijun", ""]]}, {"id": "1605.04744", "submitter": "Ashish Darbari", "authors": "Ashish Darbari, Iain Singleton, Michael Butler, John Colley", "title": "Formal Modelling, Testing and Verification of HSA Memory Models using\n  Event-B", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HSA Foundation has produced the HSA Platform System Architecture\nSpecification that goes a long way towards addressing the need for a clear and\nconsistent method for specifying weakly consistent memory. HSA is specified in\na natural language which makes it open to multiple ambiguous interpretations\nand could render bugs in implementations of it in hardware and software. In\nthis paper we present a formal model of HSA which can be used in the\ndevelopment and verification of both concurrent software applications as well\nas in the development and verification of the HSA-compliant platform itself. We\nuse the Event-B language to build a provably correct hierarchy of models from\nthe most abstract to a detailed refinement of HSA close to implementation\nlevel. Our memory models are general in that they represent an arbitrary number\nof masters, programs and instruction interleavings. We reason about such\ngeneral models using refinements. Using Rodin tool we are able to model and\nverify an entire hierarchy of models using proofs to establish that each\nrefinement is correct. We define an automated validation method that allows us\nto test baseline compliance of the model against a suite of published HSA\nlitmus tests. Once we complete model validation we develop a coverage driven\nmethod to extract a richer set of tests from the Event-B model and a user\nspecified coverage model. These tests are used for extensive regression testing\nof hardware and software systems. Our method of refinement based formal\nmodelling, baseline compliance testing of the model and coverage driven test\nextraction using the single language of Event-B is a new way to address a key\nchallenge facing the design and verification of multi-core systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 12:20:15 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Darbari", "Ashish", ""], ["Singleton", "Iain", ""], ["Butler", "Michael", ""], ["Colley", "John", ""]]}, {"id": "1605.05079", "submitter": "Ichiro Hasuo", "authors": "Ichiro Hasuo, Naohiko Hoshino", "title": "Semantics of Higher-Order Quantum Computation via Geometry of\n  Interaction", "comments": "Extended version of [Hasuo & Hoshino, LICS 2011]; to appear in Annals\n  of Pure and Applied Logic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much of the current study on quantum computation employs low-level\nformalisms such as quantum circuits, several high-level languages/calculi have\nbeen recently proposed aiming at structured quantum programming. The current\nwork contributes to the semantical study of such languages by providing\ninteraction-based semantics of a functional quantum programming language; the\nlatter is, much like Selinger and Valiron's, based on linear lambda calculus\nand equipped with features like the ! modality and recursion. The proposed\ndenotational model is the first one that supports the full features of a\nquantum functional programming language; we prove adequacy of our semantics.\nThe construction of our model is by a series of existing techniques taken from\nthe semantics of classical computation as well as from process theory. The most\nnotable among them is Girard's Geometry of Interaction (GoI), categorically\nformulated by Abramsky, Haghverdi and Scott. The mathematical genericity of\nthese techniques---largely due to their categorical formulation---is exploited\nfor our move from classical to quantum.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 09:50:26 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Hasuo", "Ichiro", ""], ["Hoshino", "Naohiko", ""]]}, {"id": "1605.05104", "submitter": "Damiano Zanardini", "authors": "Isabella Mastroeni and Damiano Zanardini", "title": "Abstract Program Slicing: an Abstract Interpretation-based approach to\n  Program Slicing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we formally define the notion of abstract program\nslicing, a general form of program slicing where properties of data are\nconsidered instead of their exact value. This approach is applied to a language\nwith numeric and reference values, and relies on the notion of abstract\ndependencies between program components (statements).\n  The different forms of (backward) abstract slicing are added to an existing\nformal framework where traditional, non-abstract forms of slicing could be\ncompared. The extended framework allows us to appreciate that abstract slicing\nis a generalization of traditional slicing, since traditional slicing (dealing\nwith syntactic dependencies) is generalized by (semantic) non-abstract forms of\nslicing, which are actually equivalent to an abstract form where the identity\nabstraction is performed on data.\n  Sound algorithms for computing abstract dependencies and a systematic\ncharacterization of program slices are provided, which rely on the notion of\nagreement between program states.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 10:48:45 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Mastroeni", "Isabella", ""], ["Zanardini", "Damiano", ""]]}, {"id": "1605.05551", "submitter": "Ernst Moritz Hahn", "authors": "Ernst Moritz Hahn and Arnd Hartmanns", "title": "Efficient Algorithms for Time- and Cost-Bounded Probabilistic Model\n  Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the design of probabilistic timed systems, bounded requirements concerning\nbehaviour that occurs within a given time, energy, or more generally cost\nbudget are of central importance. Traditionally, such requirements have been\nmodel-checked via a reduction to the unbounded case by unfolding the model\naccording to the cost bound. This exacerbates the state space explosion problem\nand significantly increases runtime. In this paper, we present three new\nalgorithms to model-check time- and cost-bounded properties for Markov decision\nprocesses and probabilistic timed automata that avoid unfolding. They are based\non a modified value iteration process, on an enumeration of schedulers, and on\nstate elimination techniques. We can now obtain results for any cost bound on a\nsingle state space no larger than for the corresponding unbounded or\nexpected-value property. In particular, we can naturally compute the cumulative\ndistribution function at no overhead. We evaluate the applicability and compare\nthe performance of our new algorithms and their implementation on a number of\ncase studies from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 12:34:35 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Hahn", "Ernst Moritz", ""], ["Hartmanns", "Arnd", ""]]}, {"id": "1605.05836", "submitter": "Radu Iosif", "authors": "Radu Iosif, Arnaud Sangnier", "title": "How hard is it to verify flat affine counter systems with the finite\n  monoid property ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study several decision problems for counter systems with guards defined by\nconvex polyhedra and updates defined by affine transformations. In general, the\nreachability problem is undecidable for such systems. Decidability can be\nachieved by imposing two restrictions: (i) the control structure of the counter\nsystem is flat, meaning that nested loops are forbidden, and (ii) the set of\nmatrix powers is finite, for any affine update matrix in the system. We provide\ntight complexity bounds for several decision problems of such systems, by\nproving that reachability and model checking for Past Linear Temporal Logic are\ncomplete for the second level of the polynomial hierarchy $\\Sigma^P_2$, while\nmodel checking for First Order Logic is PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 07:34:40 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Iosif", "Radu", ""], ["Sangnier", "Arnaud", ""]]}, {"id": "1605.05858", "submitter": "Moez AbdelGawad", "authors": "Robert Cartwright, Rebecca Parsons, Moez AbdelGawad", "title": "Domain Theory: An Introduction", "comments": "90 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph is an ongoing revision of \"Lectures On A Mathematical Theory\nof Computation\" by Dana Scott. Scott's monograph uses a formulation of domains\ncalled neighborhood systems in which finite elements are selected subsets of a\nmaster set of objects called \"tokens\". Since tokens have little intuitive\nsignificance, Scott has discarded neighborhood systems in favor of an\nequivalent formulation of domains called information systems. Unfortunately, he\nhas not rewritten his monograph to reflect this change.\n  We have rewritten Scott's monograph in terms of finitary bases instead of\ninformation systems. A finitary basis is an information system that is closed\nunder least upper bounds on finite consistent subsets. This convention ensures\nthat every finite answer is represented by a single basis object instead of a\nset of objects.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 09:06:01 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 12:21:17 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 14:28:34 GMT"}, {"version": "v4", "created": "Tue, 14 Jun 2016 06:36:36 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Cartwright", "Robert", ""], ["Parsons", "Rebecca", ""], ["AbdelGawad", "Moez", ""]]}, {"id": "1605.06233", "submitter": "Mohamed Mejri", "authors": "Mohamed Mejri and Hamdi Yahyaoui", "title": "Formal Specification and Integration of Distributed Security Policies", "comments": "Some major errors in the paper", "journal-ref": "Computer Languages, Systems & and Structures. 49 (2017) 1-35", "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper the Security Policy Language (SePL), which is a\nformal language for capturing and integrating distributed security policies.\nThe syntax of SePL includes several operators for the integration of policies\nand it is endowed with a denotational semantics that is a generic semantics,\ni.e., which is independent of any evaluation environment. We prove the\ncompleteness of SePL with respect to sets theory. Furthermore, we provide a\nformalization of a subset of the eXtensible Access Control Markup Language\n(XACML), which is the well-known standard informal specification language of\nWeb security policies. We provide also a semantics for XACML policy combining\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 07:41:43 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 05:35:52 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Mejri", "Mohamed", ""], ["Yahyaoui", "Hamdi", ""]]}, {"id": "1605.06719", "submitter": "EPTCS", "authors": "Dusko Pavlovic (University of Hawaii), Peter-Michael Seidel\n  (University of Hawaii)", "title": "(Modular) Effect Algebras are Equivalent to (Frobenius) Antispecial\n  Algebras", "comments": "In Proceedings QPL 2016, arXiv:1701.00242", "journal-ref": "EPTCS 236, 2017, pp. 145-160", "doi": "10.4204/EPTCS.236.10", "report-no": null, "categories": "quant-ph cs.LO math-ph math.CT math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effect algebras are one of the generalizations of Boolean algebras proposed\nin the quest for a quantum logic. Frobenius algebras are a tool of categorical\nquantum mechanics, used to present various families of observables in abstract,\noften nonstandard frameworks. Both effect algebras and Frobenius algebras\ncapture their respective fragments of quantum mechanics by elegant and succinct\naxioms; and both come with their conceptual mysteries. A particularly elegant\nand mysterious constraint, imposed on Frobenius algebras to characterize a\nclass of tripartite entangled states, is the antispecial law. A particularly\ncontentious issue on the quantum logic side is the modularity law, proposed by\nvon Neumann to mitigate the failure of distributivity of quantum logical\nconnectives. We show that, if quantum logic and categorical quantum mechanics\nare formalized in the same framework, then the antispecial law of categorical\nquantum mechanics corresponds to the natural requirement of effect algebras\nthat the units are each other's unique complements; and that the modularity law\ncorresponds to the Frobenius condition. These correspondences lead to the\nequivalence announced in the title. Aligning the two formalisms, at the very\nleast, sheds new light on the concepts that are more clearly displayed on one\nside than on the other (such as e.g. the orthogonality). Beyond that, it may\nalso open up new approaches to deep and important problems of quantum mechanics\n(such as the classification of complementary observables).\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 00:59:41 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 09:11:13 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2017 11:17:13 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Pavlovic", "Dusko", "", "University of Hawaii"], ["Seidel", "Peter-Michael", "", "University of Hawaii"]]}, {"id": "1605.06868", "submitter": "Anthony Widjaja Lin", "authors": "Matthew Hague and Anthony Widjaja Lin", "title": "Decidable models of integer-manipulating programs with recursive\n  parallelism (technical report)", "comments": "Full version of conference submission, 18 pages inc. appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study safety verification for multithreaded programs with recursive\nparallelism (i.e. unbounded thread creation and recursion) as well as unbounded\ninteger variables. Since the threads in each program configuration are\nstructured in a hierarchical fashion, our model is state-extended ground-tree\nrewrite systems equipped with shared unbounded integer counters that can be\nincremented, decremented, and compared against an integer constant. Since the\nmodel is Turing-complete, we propose a decidable underapproximation. First,\nusing a restriction similar to context-bounding, we underapproximate the global\ncontrol by a weak global control (i.e. DAGs possibly with self-loops), thereby\nlimiting the number of synchronisations between different threads. Second, we\nbound the number of reversals between non-decrementing and non-incrementing\nmodes of the counters. Under this restriction, we show that reachability\nbecomes NP-complete. In fact, it is poly-time reducible to satisfaction over\nexistential Presburger formulas, which allows one to tap into highly optimised\nSMT solvers. Our decidable approximation strictly generalises known decidable\nmodels including (i) weakly-synchronised ground-tree rewrite systems, and (ii)\nsynchronisation/reversal-bounded concurrent pushdown systems systems with\ncounters. Finally, we show that, when equipped with reversal-bounded counters,\nrelaxing the weak control restriction by the notion of senescence results in\nundecidability.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 01:17:38 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Hague", "Matthew", ""], ["Lin", "Anthony Widjaja", ""]]}, {"id": "1605.06938", "submitter": "Matija Pretnar", "authors": "Ohad Kammar and Matija Pretnar", "title": "No value restriction is needed for algebraic effects and handlers", "comments": null, "journal-ref": "Journal of Functional Programming, 27, 2017", "doi": "10.1017/S0956796816000320.", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a straightforward, sound Hindley-Milner polymorphic type system\nfor algebraic effects and handlers in a call-by-value calculus, which allows\ntype variable generalisation of arbitrary computations, not just values. This\nresult is surprising. On the one hand, the soundness of unrestricted\ncall-by-value Hindley-Milner polymorphism is known to fail in the presence of\ncomputational effects such as reference cells and continuations. On the other\nhand, many programming examples can be recast to use effect handlers instead of\nthese effects. Analysing the expressive power of effect handlers with respect\nto state effects, we claim handlers cannot express reference cells, and show\nthey can simulate dynamically scoped state.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 08:51:36 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Kammar", "Ohad", ""], ["Pretnar", "Matija", ""]]}, {"id": "1605.06996", "submitter": "Josef Urban", "authors": "Chad Brown and Josef Urban", "title": "Extracting Higher-Order Goals from the Mizar Mathematical Library", "comments": "Accepted to CICM 2016. The final publication will be available at\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain constructs allowed in Mizar articles cannot be represented in\nfirst-order logic but can be represented in higher-order logic. We describe a\nway to obtain higher-order theorem proving problems from Mizar articles that\nmake use of these constructs. In particular, higher-order logic is used to\nrepresent schemes, a global choice construct and set level binders. The\nhigher-order automated theorem provers Satallax and LEO-II have been run on\ncollections of these problems and the results are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 12:37:54 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Brown", "Chad", ""], ["Urban", "Josef", ""]]}, {"id": "1605.07068", "submitter": "William Farmer", "authors": "William M. Farmer", "title": "Incorporating Quotation and Evaluation into Church's Type Theory: Syntax\n  and Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ${\\rm \\small CTT}_{\\rm qe}$ is a version of Church's type theory that\nincludes quotation and evaluation operators that are similar to quote and eval\nin the Lisp programming language. With quotation and evaluation it is possible\nto reason in ${\\rm \\small CTT}_{\\rm qe}$ about the interplay of the syntax and\nsemantics of expressions and, as a result, to formalize syntax-based\nmathematical algorithms. We present the syntax and semantics of ${\\rm \\small\nCTT}_{\\rm qe}$ and give several examples that illustrate the usefulness of\nhaving quotation and evaluation in ${\\rm \\small CTT}_{\\rm qe}$. We do not give\na proof system for ${\\rm \\small CTT}_{\\rm qe}$, but we do sketch what a proof\nsystem could look like.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:04:32 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 14:03:17 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Farmer", "William M.", ""]]}, {"id": "1605.07503", "submitter": "Carlos Barron-Romero Prof.", "authors": "Carlos Barr\\'on-Romero", "title": "A novel algorithm for solving the Decision Boolean Satisfiability\n  Problem without algebra", "comments": "arXiv admin note: text overlap with arXiv:1602.06867 Published in\n  COMTEL 2016 (http://www.comtel.pe/memoriacomtel/COMTEL2016.pdf) See in\n  http://academicos.azc.uam.mx/cbr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper depicts an algorithm for solving the Decision Boolean\nSatisfiability Problem using the binary numerical properties of a Special\nDecision Satisfiability Problem, parallel execution, object oriented, and short\ntermination. The two operations: expansion and simplification are used to\nexplains why using algebra grows the resolution steps. It is proved that its\ncomplexity has an upper bound of $2^{n-1}$ where $n$ is the number of logical\nvariables of the given problem.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 02:06:46 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 20:46:29 GMT"}, {"version": "v3", "created": "Sat, 28 May 2016 09:03:36 GMT"}, {"version": "v4", "created": "Tue, 31 May 2016 00:18:18 GMT"}, {"version": "v5", "created": "Mon, 16 Jan 2017 02:13:50 GMT"}, {"version": "v6", "created": "Sun, 26 Nov 2017 21:36:45 GMT"}, {"version": "v7", "created": "Sat, 14 Apr 2018 08:58:50 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Barr\u00f3n-Romero", "Carlos", ""]]}, {"id": "1605.07577", "submitter": "Bohua Zhan", "authors": "Bohua Zhan", "title": "AUTO2, a saturation-based heuristic prover for higher-order logic", "comments": "16 pages, accepted for ITP 2016", "journal-ref": "ITP 2016, LNCS 9807, pp. 441--456, 2016", "doi": "10.1007/978-3-319-43144-4_27", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new theorem prover for classical higher-order logic named\nauto2. The prover is designed to make use of human-specified heuristics when\nsearching for proofs. The core algorithm is a best-first search through the\nspace of propositions derivable from the initial assumptions, where new\npropositions are added by user-defined functions called proof steps. We\nimplemented the prover in Isabelle/HOL, and applied it to several formalization\nprojects in mathematics and computer science, demonstrating the high level of\nautomation it can provide in a variety of possible proof tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 18:34:01 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Zhan", "Bohua", ""]]}, {"id": "1605.07753", "submitter": "Edon Kelmendi", "authors": "Edon Kelmendi (LaBRI), Hugo Gimbert (LaBRI)", "title": "Deciding Maxmin Reachability in Half-Blind Stochastic Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-player, turn-based, stochastic games with reachability conditions are\nconsidered, where the maximizer has no information (he is blind) and is\nrestricted to deterministic strategies whereas the minimizer is perfectly\ninformed. We ask the question of whether the game has maxmin 1, in other words\nwe ask whether for all $\\epsilon>0$ there exists a deterministic strategy for\nthe (blind) maximizer such that against all the strategies of the minimizer, it\nis possible to reach the set of final states with probability larger than\n$1-\\epsilon$. This problem is undecidable in general, but we define a class of\ngames, called leaktight half-blind games where the problem becomes decidable.\nWe also show that mixed strategies in general are stronger for both players and\nthat optimal strategies for the minimizer might require infinite-memory.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 06:53:44 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kelmendi", "Edon", "", "LaBRI"], ["Gimbert", "Hugo", "", "LaBRI"]]}, {"id": "1605.07808", "submitter": "Carlos Lombardi", "authors": "Carlos Lombardi, Alejandro R\\'ios, Roel de Vrijer", "title": "Projections for infinitary rewriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof terms in term rewriting are a representation means for reduction\nsequences, and more in general for contraction activity, allowing to\ndistinguish e.g simultaneous from sequential reduction. Proof terms for\nfinitary, first-order, left-linear term rewriting are described in the Terese\nbook, chapter 8. In a previous work, we defined an extension of the finitary\nproof-term formalism, that allows to describe contractions in infinitary\nfirst-order term rewriting, and gave a characterisation of permutation\nequivalence.\n  In this work, we discuss how projections of possibly infinite rewrite\nsequences can be modeled using proof terms. Again, the foundation is a\ncharacterisation of projections for finitary rewriting described in Terese,\nSection 8.7. We extend this characterisation to infinitary rewriting and also\nrefine it, by describing precisely the role that structural equivalence plays\nin the development of the notion of projection. The characterisation we propose\nyields a definite expression, i.e. a proof term, that describes the projection\nof an infinitary reduction over another.\n  To illustrate the working of projections, we show how a common reduct of a\n(possibly infinite) reduction and a single step that makes part of it can be\nobtained via their respective projections. We show, by means of several\nexamples, that the proposed definition yields the expected behavior also in\ncases beyond those covered by this result. Finally, we discuss how the notion\nof limit is used in our definition of projection for infinite reduction.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 10:17:30 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 20:28:01 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Lombardi", "Carlos", ""], ["R\u00edos", "Alejandro", ""], ["de Vrijer", "Roel", ""]]}, {"id": "1605.08096", "submitter": "EPTCS", "authors": "Luca Aceto (ICE-TCS, School of Computer Science, Reykjavik\n  University), Adrian Francalanza (Computer Science, Faculty of Information &\n  Communications Technology, University of Malta), Anna Ingolfsdottir (ICE-TCS,\n  School of Computer Science, Reykjavik University)", "title": "Proceedings First Workshop on Pre- and Post-Deployment Verification\n  Techniques", "comments": null, "journal-ref": "EPTCS 208, 2016", "doi": "10.4204/EPTCS.208", "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PrePost (Pre- and Post-Deployment Verification Techniques) workshop aimed\nat bringing together researchers working in the field of computer-aided\nvalidation and verification to discuss the connections and interplay between\npre- and post-deployment verification techniques. Examples of the topics\ncovered by the workshop are the relationships between classic model checking\nand testing on the one hand and runtime verification and statistical model\nchecking on the other, and between type systems that may be checked either\nstatically or dynamically through techniques such as runtime monitoring.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 22:42:38 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Aceto", "Luca", "", "ICE-TCS, School of Computer Science, Reykjavik\n  University"], ["Francalanza", "Adrian", "", "Computer Science, Faculty of Information &\n  Communications Technology, University of Malta"], ["Ingolfsdottir", "Anna", "", "ICE-TCS,\n  School of Computer Science, Reykjavik University"]]}, {"id": "1605.08106", "submitter": "G. A. Kavvos", "authors": "G. A. Kavvos", "title": "The Many Worlds of Modal {\\lambda}-calculi: I. Curry-Howard for\n  Necessity, Possibility and Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a survey of {\\lambda}-calculi that, through the Curry-Howard\nisomorphism, correspond to constructive modal logics. We cover the prehistory\nof the subject and then concentrate on the developments that took place in the\n1990s and early 2000s. We discuss logical aspects, modal {\\lambda}-calculi, and\ntheir categorical semantics. The logics underlying the calculi surveyed are\nconstructive versions of K, K4, S4, and LTL.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 00:09:16 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Kavvos", "G. A.", ""]]}, {"id": "1605.08187", "submitter": "Martin Mladenov", "authors": "Martin Mladenov and Vaishak Belle and Kristian Kersting", "title": "The Symbolic Interior Point Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend in probabilistic inference emphasizes the codification of\nmodels in a formal syntax, with suitable high-level features such as\nindividuals, relations, and connectives, enabling descriptive clarity,\nsuccinctness and circumventing the need for the modeler to engineer a custom\nsolver. Unfortunately, bringing these linguistic and pragmatic benefits to\nnumerical optimization has proven surprisingly challenging. In this paper, we\nturn to these challenges: we introduce a rich modeling language, for which an\ninterior-point method computes approximate solutions in a generic way. While\nlogical features easily complicates the underlying model, often yielding\nintricate dependencies, we exploit and cache local structure using algebraic\ndecision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently\nrealizable in ADDs, but we argue and show that well-known optimization methods\nare not ideal for ADDs. Our engine, therefore, invokes a sophisticated\nmatrix-free approach. We demonstrate the flexibility of the resulting\nsymbolic-numeric optimizer on decision making and compressed sensing tasks with\nmillions of non-zero entries.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 08:26:34 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 17:11:30 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 18:29:14 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Mladenov", "Martin", ""], ["Belle", "Vaishak", ""], ["Kersting", "Kristian", ""]]}, {"id": "1605.08367", "submitter": "Rodrigo De Salvo Braz", "authors": "Rodrigo de Salvo Braz, Ciaran O'Reilly, Vibhav Gogate, Rina Dechter", "title": "Probabilistic Inference Modulo Theories", "comments": "Submitted to StarAI-16 workshop as closely revised version of\n  IJCAI-16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SGDPLL(T), an algorithm that solves (among many other problems)\nprobabilistic inference modulo theories, that is, inference problems over\nprobabilistic models defined via a logic theory provided as a parameter\n(currently, propositional, equalities on discrete sorts, and inequalities, more\nspecifically difference arithmetic, on bounded integers). While many solutions\nto probabilistic inference over logic representations have been proposed,\nSGDPLL(T) is simultaneously (1) lifted, (2) exact and (3) modulo theories, that\nis, parameterized by a background logic theory. This offers a foundation for\nextending it to rich logic languages such as data structures and relational\ndata. By lifted, we mean algorithms with constant complexity in the domain size\n(the number of values that variables can take). We also detail a solver for\nsummations with difference arithmetic and show experimental results from a\nscenario in which SGDPLL(T) is much faster than a state-of-the-art\nprobabilistic solver.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 17:10:10 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 02:29:20 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Braz", "Rodrigo de Salvo", ""], ["O'Reilly", "Ciaran", ""], ["Gogate", "Vibhav", ""], ["Dechter", "Rina", ""]]}, {"id": "1605.08563", "submitter": "Vivek Nigam", "authors": "Vivek Nigam, Carolyn Talcott, Abra\\~ao Aires Urquiza", "title": "Towards the Automated Verification of Cyber-Physical Security Protocols:\n  Bounding the Number of Timed Intruders", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timed Intruder Models have been proposed for the verification of\nCyber-Physical Security Protocols (CPSP) amending the traditional Dolev-Yao\nintruder to obey the physical restrictions of the environment. Since to learn a\nmessage, a Timed Intruder needs to wait for a message to arrive, mounting an\nattack may depend on where Timed Intruders are. It may well be the case that in\nthe presence of a great number of intruders there is no attack, but there is an\nattack in the presence of a small number of well placed intruders. Therefore, a\nmajor challenge for the automated verification of CPSP is to determine how many\nTimed Intruders to use and where should they be placed. This paper answers this\nquestion by showing it is enough to use the same number of Timed Intruders as\nthe number of participants. We also report on some preliminary experimental\nresults in discovering attacks in CPSP.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 09:59:27 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Nigam", "Vivek", ""], ["Talcott", "Carolyn", ""], ["Urquiza", "Abra\u00e3o Aires", ""]]}, {"id": "1605.08949", "submitter": "Kohei Kishida", "authors": "Kohei Kishida", "title": "Logic of Local Inference for Contextuality in Quantum Physics and Beyond", "comments": "Forty-Third International Colloquium on Automata, Languages, and\n  Programming (ICALP 2016)", "journal-ref": "Forty-Third International Colloquium on Automata, Languages, and\n  Programming (ICALP 2016), 113:1--113:14, 2016", "doi": "10.4230/LIPIcs.ICALP.2016.113", "report-no": null, "categories": "cs.LO math.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextuality in quantum physics provides a key resource for quantum\ninformation and computation. The topological approach in [Abramsky and\nBrandenburger, New J. Phys., 2011, Abramsky et al., CSL 2015, 2015]\ncharacterizes contextuality as \"global inconsistency\" coupled with \"local\nconsistency\", revealing it to be a phenomenon also found in many other fields.\nThis has yielded a logical method of detecting and proving the \"global\ninconsistency\" part of contextuality. Our goal is to capture the other, \"local\nconsistency\" part, which requires a novel approach to logic that is sensitive\nto the topology of contexts. To achieve this, we formulate a logic of local\ninference by using context-sensitive theories and models in regular categories.\nThis provides a uniform framework for local consistency, and lays a foundation\nfor high-level methods of detecting, proving, and moreover using contextuality\nas computational resource.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 23:44:03 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kishida", "Kohei", ""]]}, {"id": "1605.09177", "submitter": "Danko Ilik", "authors": "Danko Ilik", "title": "Perspectives for proof unwinding by programming languages techniques", "comments": null, "journal-ref": "IfColog Journal of Logics and their Applications (FLAP), College\n  Publications, 2017, 4 (10), pp.3487-3508", "doi": null, "report-no": null, "categories": "math.LO cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we propose some future directions of work, potentially\nbeneficial to Mathematics and its foundations, based on the recent import of\nmethodology from the theory of programming languages into proof theory. This\nscientific essay, written for the audience of proof theorists as well as the\nworking mathematician, is not a survey of the field, but rather a personal view\nof the author who hopes that it may inspire future and fellow researchers.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 11:18:59 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Ilik", "Danko", ""]]}, {"id": "1605.09293", "submitter": "Michael F\\\"arber", "authors": "Michael F\\\"arber and Chad Brown", "title": "Internal Guidance for Satallax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new internal guidance method for automated theorem provers based\non the given-clause algorithm. Our method influences the choice of unprocessed\nclauses using positive and negative examples from previous proofs. To this end,\nwe present an efficient scheme for Naive Bayesian classification by\ngeneralising label occurrences to types with monoid structure. This makes it\npossible to extend existing fast classifiers, which consider only positive\nexamples, with negative ones. We implement the method in the higher-order logic\nprover Satallax, where we modify the delay with which propositions are\nprocessed. We evaluated our method on a simply-typed higher-order logic version\nof the Flyspeck project, where it solves 26% more problems than Satallax\nwithout internal guidance.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 16:01:51 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["F\u00e4rber", "Michael", ""], ["Brown", "Chad", ""]]}, {"id": "1605.09442", "submitter": "Murphy Berzish", "authors": "Vijay Ganesh, Murphy Berzish", "title": "Undecidability of a Theory of Strings, Linear Arithmetic over Length,\n  and String-Number Conversion", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been considerable interest in theories over string\nequations, length function, and string-number conversion predicate within the\nformal verification, software engineering, and security communities. SMT\nsolvers for these theories, such as Z3str2, CVC4, and S3, are of immense\npractical value in exposing security vulnerabilities in string-intensive\nprograms. Additionally, there are many open decidability and\ncomplexity-theoretic questions in the context of theories over strings that are\nof great interest to mathematicians. Motivated by the above-mentioned\napplications and open questions, we study a first-order, many-sorted,\nquantifier-free theory $T_{s,n}$ of string equations, linear arithmetic over\nstring length, and string-number conversion predicate and prove three theorems.\nFirst, we prove that the satisfiability problem for the theory $T_{s,n}$ is\nundecidable via a reduction from a theory of linear arithmetic over natural\nnumbers with power predicate, we call power arithmetic. Second, we show that\nthe string-numeric conversion predicate is expressible in terms of the power\npredicate, string equations, and length function. This second theorem, in\nconjunction with the reduction we propose for the undecidability theorem,\nsuggests that the power predicate is expressible in terms of word equations and\nlength function if and only if the string-numeric conversion predicate is also\nexpressible in the same fragment. Such results are very useful tools in\ncomparing the expressive power of different theories, and for establishing\ndecidability and complexity results. Third, we provide a consistent\naxiomatization ${\\Gamma}$ for the functions and predicates of $T_{s,n}$.\nAdditionally, we prove that the theory $T_{\\Gamma}$ , obtained via logical\nclosure of ${\\Gamma}$, is not a complete theory.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 23:02:49 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 22:37:08 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 23:14:17 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Ganesh", "Vijay", ""], ["Berzish", "Murphy", ""]]}, {"id": "1605.09446", "submitter": "Murphy Berzish", "authors": "Sanu Subramanian, Murphy Berzish, Yunhui Zheng, Omer Tripp, Vijay\n  Ganesh", "title": "A Solver for a Theory of Strings and Bit-vectors", "comments": "22 pages, 4 figures, submitted to FM2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a solver for a many-sorted first-order quantifier-free theory\n$T_{w,bv}$ of string equations, string length represented as bit-vectors, and\nbit-vector arithmetic aimed at formal verification, automated testing, and\nsecurity analysis of C/C++ applications. Our key motivation for building such a\nsolver is the observation that existing string solvers are not efficient at\nmodeling the string/bit-vector combination. Current approaches either reduce\nstrings to bit-vectors and use a bit-vector solver as a backend, or model\nbit-vectors as natural numbers and use a solver for the combined theory of\nstrings and natural numbers. Both these approaches are inefficient for\ndifferent reasons. Modeling strings as bit-vectors destroys structure inherent\nin string equations thus missing opportunities for efficiently deciding such\nformulas, and modeling bit-vectors as natural numbers is known to be\ninefficient. Hence, there is a clear need for a solver that models strings and\nbit-vectors natively.\n  Our solver Z3strBV is a decision procedure for the theory $T_{w,bv}$\ncombining solvers for bit-vector and string equations. We demonstrate\nexperimentally that Z3strBV is significantly more efficient than reduction of\nstring/bit-vector constraints to strings/natural numbers. Additionally, we\nprove decidability for the theory $T_{w,bv}$. We also propose two optimizations\nwhich can be adapted to other contexts. The first accelerates convergence on a\nconsistent assignment of string lengths, and the second, dubbed library-aware\nSMT solving, fixes summaries for built-in string functions (e.g., {\\tt strlen}\nin C/C++), which Z3strBV uses directly instead of analyzing the functions from\nscratch each time. Finally, we demonstrate experimentally that Z3strBV is able\nto detect nontrivial overflows in real-world system-level code, as confirmed\nagainst 7 security vulnerabilities from CVE and Mozilla database.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 23:21:06 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Subramanian", "Sanu", ""], ["Berzish", "Murphy", ""], ["Zheng", "Yunhui", ""], ["Tripp", "Omer", ""], ["Ganesh", "Vijay", ""]]}]