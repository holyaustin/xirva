[{"id": "1511.00346", "submitter": "Ichiro Hasuo", "authors": "Ichiro Hasuo, Shunsuke Shimizu, Corina Cirstea", "title": "Lattice-Theoretic Progress Measures and Coalgebraic Model Checking (with\n  Appendices)", "comments": "24 pages, Extended version with appendices of a paper accepted to\n  POPL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of formal verification in general and model checking in\nparticular, parity games serve as a mighty vehicle: many problems are encoded\nas parity games, which are then solved by the seminal algorithm by Jurdzinski.\nIn this paper we identify the essence of this workflow to be the notion of\nprogress measure, and formalize it in general, possibly infinitary,\nlattice-theoretic terms. Our view on progress measures is that they are to\nnested/alternating fixed points what invariants are to safety/greatest fixed\npoints, and what ranking functions are to liveness/least fixed points. That is,\nprogress measures are combination of the latter two notions (invariant and\nranking function) that have been extensively studied in the context of\n(program) verification.\n  We then apply our theory of progress measures to a general model-checking\nframework, where systems are categorically presented as coalgebras. The\nframework's theoretical robustness is witnessed by a smooth transfer from the\nbranching-time setting to the linear-time one. Although the framework can be\nused to derive some decision procedures for finite settings, we also expect the\nproposed framework to form a basis for sound proof methods for some\nundecidable/infinitary problems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 00:53:41 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 09:22:36 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2016 08:11:59 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Hasuo", "Ichiro", ""], ["Shimizu", "Shunsuke", ""], ["Cirstea", "Corina", ""]]}, {"id": "1511.00384", "submitter": "Arthur Ryman", "authors": "Arthur Ryman", "title": "Z Specification for the W3C Editor's Draft Core SHACL Semantics", "comments": "57 pages, Invited Expert contribution to the W3C RDF Data Shapes\n  Working Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a formalization of the W3C Draft Core SHACL Semantics\nspecification using Z notation. This formalization exercise has identified a\nnumber of quality issues in the draft. It has also established that the\nrecursive definitions in the draft are well-founded. Further formal validation\nof the draft will require the use of an executable specification technology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 05:31:42 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ryman", "Arthur", ""]]}, {"id": "1511.00523", "submitter": "Guillermo P\\'erez", "authors": "Paul Hunter, Guillermo A. P\\'erez, and Jean-Fran\\c{c}ois Raskin", "title": "Minimizing Regret in Discounted-Sum Games", "comments": "arXiv admin note: text overlap with arXiv:1504.01708; some typos have\n  been removed in the proof of simple strategies being sufficient to minimize\n  regret against any adversary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of minimizing regret in discounted-sum\ngames played on weighted game graphs. We give algorithms for the general\nproblem of computing the minimal regret of the controller (Eve) as well as\nseveral variants depending on which strategies the environment (Adam) is\npermitted to use. We also consider the problem of synthesizing regret-free\nstrategies for Eve in each of these scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 14:37:45 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 11:49:50 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 14:08:28 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Hunter", "Paul", ""], ["P\u00e9rez", "Guillermo A.", ""], ["Raskin", "Jean-Fran\u00e7ois", ""]]}, {"id": "1511.00754", "submitter": "Ond\\v{r}ej Leng\\'al", "authors": "Yu-Fang Chen, Chiao Hsieh, Ond\\v{r}ej Leng\\'al, Tsung-Ju Lii,\n  Ming-Hsien Tsai, Bow-Yaw Wang, and Farn Wang", "title": "PAC Learning-Based Verification and Model Synthesis", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel technique for verification and model synthesis of\nsequential programs. Our technique is based on learning a regular model of the\nset of feasible paths in a program, and testing whether this model contains an\nincorrect behavior. Exact learning algorithms require checking equivalence\nbetween the model and the program, which is a difficult problem, in general\nundecidable. Our learning procedure is therefore based on the framework of\nprobably approximately correct (PAC) learning, which uses sampling instead and\nprovides correctness guarantees expressed using the terms error probability and\nconfidence. Besides the verification result, our procedure also outputs the\nmodel with the said correctness guarantees. Obtained preliminary experiments\nshow encouraging results, in some cases even outperforming mature software\nverifiers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 01:44:03 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Chen", "Yu-Fang", ""], ["Hsieh", "Chiao", ""], ["Leng\u00e1l", "Ond\u0159ej", ""], ["Lii", "Tsung-Ju", ""], ["Tsai", "Ming-Hsien", ""], ["Wang", "Bow-Yaw", ""], ["Wang", "Farn", ""]]}, {"id": "1511.01123", "submitter": "Maximilian Jaroschek", "authors": "Maximilian Jaroschek, Pablo Federico Dobal, Pascal Fontaine", "title": "Adapting Real Quantifier Elimination Methods for Conflict Set\n  Computation", "comments": null, "journal-ref": "Frontiers of Combining Systems, 151--166, isbn 978-3-319-24245-3,\n  2015", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The satisfiability problem in real closed fields is decidable. In the context\nof satisfiability modulo theories, the problem restricted to conjunctive sets\nof literals, that is, sets of polynomial constraints, is of particular\nimportance. One of the central problems is the computation of good explanations\nof the unsatisfiability of such sets, i.e.\\ obtaining a small subset of the\ninput constraints whose conjunction is already unsatisfiable. We adapt two\ncommonly used real quantifier elimination methods, cylindrical algebraic\ndecomposition and virtual substitution, to provide such conflict sets and\ndemonstrate the performance of our method in practice.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 21:28:30 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Jaroschek", "Maximilian", ""], ["Dobal", "Pablo Federico", ""], ["Fontaine", "Pascal", ""]]}, {"id": "1511.01272", "submitter": "Thomas Ehrhard", "authors": "Thomas Ehrhard (PPS), Michele Pagani (PPS), Christine Tasson (PPS)", "title": "Full abstraction for probabilistic PCF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic version of PCF, a well-known simply typed\nuniversal functional language. The type hierarchy is based on a single ground\ntype of natural numbers. Even if the language is globally call-by-name, we\nallow a call-by-value evaluation for ground type arguments in order to provide\nthe language with a suitable algorithmic expressiveness. We describe a\ndenotational semantics based on probabilistic coherence spaces, a model of\nclassical Linear Logic developed in previous works. We prove an adequacy and an\nequational full abstraction theorem showing that equality in the model\ncoincides with a natural notion of observational equivalence.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:32:32 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Ehrhard", "Thomas", "", "PPS"], ["Pagani", "Michele", "", "PPS"], ["Tasson", "Christine", "", "PPS"]]}, {"id": "1511.01368", "submitter": "Eugene Goldberg", "authors": "Eugene Goldberg", "title": "Equivalence Checking By Logic Relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for Equivalence Checking (EC) of Boolean\ncircuits based on a general technique called Logic Relaxation (LoR). The\nessence of LoR is to relax the formula to be solved and compute a superset S of\nthe set of new behaviors. Namely, S contains all new satisfying assignments\nthat appeared due to relaxation and does not contain assignments satisfying the\noriginal formula. Set S is generated by a procedure called partial quantifier\nelimination. If all possible bad behaviors are in S, the original formula\ncannot have them and so the property described by this formula holds. The\nappeal of EC by LoR is twofold. First, it facilitates generation of powerful\ninductive proofs. Second, proving inequivalence comes down to checking the\npresence of some bad behaviors in the relaxed formula i.e. in a simpler version\nof the original formula. We give some experimental evidence that supports our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 15:22:13 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2016 12:11:50 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2016 08:26:29 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Goldberg", "Eugene", ""]]}, {"id": "1511.01566", "submitter": "EPTCS", "authors": "Samson Abramsky (University of Oxford), Dominic Horsman (University of\n  Oxford)", "title": "DEMONIC programming: a computational language for single-particle\n  equilibrium thermodynamics, and its formal semantics", "comments": "In Proceedings QPL 2015, arXiv:1511.01181. Dominic Horsman published\n  previously as Clare Horsman", "journal-ref": "EPTCS 195, 2015, pp. 1-16", "doi": "10.4204/EPTCS.195.1", "report-no": null, "categories": "cs.LO cond-mat.stat-mech cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maxwell's Demon, 'a being whose faculties are so sharpened that he can follow\nevery molecule in its course', has been the centre of much debate about its\nabilities to violate the second law of thermodynamics. Landauer's hypothesis,\nthat the Demon must erase its memory and incur a thermodynamic cost, has become\nthe standard response to Maxwell's dilemma, and its implications for the\nthermodynamics of computation reach into many areas of quantum and classical\ncomputing. It remains, however, still a hypothesis. Debate has often centred\naround simple toy models of a single particle in a box. Despite their\nsimplicity, the ability of these systems to accurately represent thermodynamics\n(specifically to satisfy the second law) and whether or not they display\nLandauer Erasure, has been a matter of ongoing argument. The recent\nNorton-Ladyman controversy is one such example.\n  In this paper we introduce a programming language to describe these simple\nthermodynamic processes, and give a formal operational semantics and program\nlogic as a basis for formal reasoning about thermodynamic systems. We formalise\nthe basic single-particle operations as statements in the language, and then\nshow that the second law must be satisfied by any composition of these basic\noperations. This is done by finding a computational invariant of the system. We\nshow, furthermore, that this invariant requires an erasure cost to exist within\nthe system, equal to kTln2 for a bit of information: Landauer Erasure becomes a\ntheorem of the formal system. The Norton-Ladyman controversy can therefore be\nresolved in a rigorous fashion, and moreover the formalism we introduce gives a\nset of reasoning tools for further analysis of Landauer erasure, which are\nprovably consistent with the second law of thermodynamics.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:41:13 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Abramsky", "Samson", "", "University of Oxford"], ["Horsman", "Dominic", "", "University of\n  Oxford"]]}, {"id": "1511.01568", "submitter": "EPTCS", "authors": "Jaap Boender (Middlesex University), Florian Kamm\\\"uller (Middlesex\n  University), Rajagopal Nagarajan (Middlesex University)", "title": "Formalization of Quantum Protocols using Coq", "comments": "In Proceedings QPL 2015, arXiv:1511.01181", "journal-ref": "EPTCS 195, 2015, pp. 71-83", "doi": "10.4204/EPTCS.195.6", "report-no": null, "categories": "cs.LO cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum Information Processing, which is an exciting area of research at the\nintersection of physics and computer science, has great potential for\ninfluencing the future development of information processing systems. The\nbuilding of practical, general purpose Quantum Computers may be some years into\nthe future. However, Quantum Communication and Quantum Cryptography are well\ndeveloped. Commercial Quantum Key Distribution systems are easily available and\nseveral QKD networks have been built in various parts of the world. The\nsecurity of the protocols used in these implementations rely on\ninformation-theoretic proofs, which may or may not reflect actual system\nbehaviour. Moreover, testing of implementations cannot guarantee the absence of\nbugs and errors. This paper presents a novel framework for modelling and\nverifying quantum protocols and their implementations using the proof assistant\nCoq. We provide a Coq library for quantum bits (qubits), quantum gates, and\nquantum measurement. As a step towards verifying practical quantum\ncommunication and security protocols such as Quantum Key Distribution, we\nsupport multiple qubits, communication and entanglement. We illustrate these\nconcepts by modelling the Quantum Teleportation Protocol, which communicates\nthe state of an unknown quantum bit using only a classical channel.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:42:01 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Boender", "Jaap", "", "Middlesex University"], ["Kamm\u00fcller", "Florian", "", "Middlesex\n  University"], ["Nagarajan", "Rajagopal", "", "Middlesex University"]]}, {"id": "1511.01569", "submitter": "EPTCS", "authors": "Kenta Cho (Radboud University, Nijmegen)", "title": "Total and Partial Computation in Categorical Quantum Foundations", "comments": "In Proceedings QPL 2015, arXiv:1511.01181", "journal-ref": "EPTCS 195, 2015, pp. 116-135", "doi": "10.4204/EPTCS.195.9", "report-no": null, "categories": "cs.LO math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uncovers the fundamental relationship between total and partial\ncomputation in the form of an equivalence of certain categories. This\nequivalence involves on the one hand effectuses, which are categories for total\ncomputation, introduced by Jacobs for the study of quantum/effect logic. On the\nother hand, it involves what we call FinPACs with effects; they are finitely\npartially additive categories equipped with effect algebra structures, serving\nas categories for partial computation. It turns out that the Kleisli category\nof the lift monad (-)+1 on an effectus is always a FinPAC with effects, and\nthis construction gives rise to the equivalence. Additionally, state-and-effect\ntriangles over FinPACs with effects are presented.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:42:30 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Cho", "Kenta", "", "Radboud University, Nijmegen"]]}, {"id": "1511.01570", "submitter": "EPTCS", "authors": "Kenta Cho, Bart Jacobs, Bas Westerbaan, Bram Westerbaan", "title": "Quotient-Comprehension Chains", "comments": "In Proceedings QPL 2015, arXiv:1511.01181", "journal-ref": "EPTCS 195, 2015, pp. 136-147", "doi": "10.4204/EPTCS.195.10", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quotients and comprehension are fundamental mathematical constructions that\ncan be described via adjunctions in categorical logic. This paper reveals that\nquotients and comprehension are related to measurement, not only in quantum\nlogic, but also in probabilistic and classical logic. This relation is\npresented by a long series of examples, some of them easy, and some also highly\nnon-trivial (esp. for von Neumann algebras). We have not yet identified a\nunifying theory. Nevertheless, the paper contributes towards such a theory by\nintroducing the new quotient-and-comprehension perspective on measurement\ninstruments, and by describing the examples on which such a theory should be\nbuilt.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:42:39 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Cho", "Kenta", ""], ["Jacobs", "Bart", ""], ["Westerbaan", "Bas", ""], ["Westerbaan", "Bram", ""]]}, {"id": "1511.01573", "submitter": "EPTCS", "authors": "David Quick", "title": "Encoding !-tensors as !-graphs with neighbourhood orders", "comments": "In Proceedings QPL 2015, arXiv:1511.01181", "journal-ref": "EPTCS 195, 2015, pp. 307-320", "doi": "10.4204/EPTCS.195.23", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagrammatic reasoning using string diagrams provides an intuitive language\nfor reasoning about morphisms in a symmetric monoidal category. To allow\nworking with infinite families of string diagrams, !-graphs were introduced as\na method to mark repeated structure inside a diagram. This led to !-graphs\nbeing implemented in the diagrammatic proof assistant Quantomatic. Having a\npartially automated program for rewriting diagrams has proven very useful, but\nbeing based on !-graphs, only commutative theories are allowed. An enriched\nabstract tensor notation, called !-tensors, has been used to formalise the\nnotion of !-boxes in non-commutative structures. This work-in-progress paper\npresents a method to encode !-tensors as !-graphs with some additional\nstructure. This will allow us to leverage the existing code from Quantomatic\nand quickly provide various tools for non-commutative diagrammatic reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:44:40 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Quick", "David", ""]]}, {"id": "1511.01633", "submitter": "Anthony Widjaja Lin", "authors": "Anthony W. Lin, Pablo Barcelo", "title": "String Solving with Word Equations and Transducers: Towards a Logic for\n  Analysing Mutation XSS (Full Version)", "comments": "Full version of POPL'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental issue of decidability of satisfiability over string\nlogics with concatenations and finite-state transducers as atomic operations.\nAlthough restricting to one type of operations yields decidability, little is\nknown about the decidability of their combined theory, which is especially\nrelevant when analysing security vulnerabilities of dynamic web pages in a more\nrealistic browser model. On the one hand, word equations (string logic with\nconcatenations) cannot precisely capture sanitisation functions (e.g.\nhtmlescape) and implicit browser transductions (e.g. innerHTML mutations). On\nthe other hand, transducers suffer from the reverse problem of being able to\nmodel sanitisation functions and browser transductions, but not string\nconcatenations. Naively combining word equations and transducers easily leads\nto an undecidable logic. Our main contribution is to show that the\n\"straight-line fragment\" of the logic is decidable (complexity ranges from\nPSPACE to EXPSPACE). The fragment can express the program logics of\nstraight-line string-manipulating programs with concatenations and\ntransductions as atomic operations, which arise when performing bounded model\nchecking or dynamic symbolic executions. We demonstrate that the logic can\nnaturally express constraints required for analysing mutation XSS in web\napplications. Finally, the logic remains decidable in the presence of length,\nletter-counting, regular, indexOf, and disequality constraints.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 07:21:46 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Lin", "Anthony W.", ""], ["Barcelo", "Pablo", ""]]}, {"id": "1511.01807", "submitter": "Thorsten Wissmann", "authors": "Prateek Karandikar and Philippe Schnoebelen", "title": "The height of piecewise-testable languages and the complexity of the\n  logic of subwords", "comments": "This article is a full version of \"The height of piecewise-testable\n  languages with applications in logical complexity\", in Proc. CSL 2016, LIPiCS\n  62:37", "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (April 30,\n  2019) lmcs:5409", "doi": "10.23638/LMCS-15(2:6)2019", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The height of a piecewise-testable language $L$ is the maximum length of the\nwords needed to define $L$ by excluding and requiring given subwords. The\nheight of $L$ is an important descriptive complexity measure that has not yet\nbeen investigated in a systematic way. This article develops a series of new\ntechniques for bounding the height of finite languages and of languages\nobtained by taking closures by subwords, superwords and related operations.\n  As an application of these results, we show that\n$\\mathsf{FO}^2(A^*,\\sqsubseteq)$, the two-variable fragment of the first-order\nlogic of sequences with the subword ordering, can only express\npiecewise-testable properties and has elementary complexity.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 16:45:19 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 09:54:48 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 18:09:58 GMT"}, {"version": "v4", "created": "Sun, 28 Apr 2019 13:39:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Karandikar", "Prateek", ""], ["Schnoebelen", "Philippe", ""]]}, {"id": "1511.02175", "submitter": "Argimiro Arratia", "authors": "Argimiro Arratia and Carlos E. Ortiz", "title": "Methods of Class Field Theory to Separate Logics over Finite Residue\n  Classes and Circuit Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separations among the first order logic ${\\cal R}ing(0,+,*)$ of finite\nresidue class rings, its extensions with generalized quantifiers, and in the\npresence of a built-in order are shown, using algebraic methods from class\nfield theory. These methods include classification of spectra of sentences over\nfinite residue classes as systems of congruences, and the study of their\n$h$-densities over the set of all prime numbers, for various functions $h$ on\nthe natural numbers. Over ordered structures the logic of finite residue class\nrings and extensions are known to capture DLOGTIME-uniform circuit complexity\nclasses ranging from $AC^0$ to $TC^0$. Separating these circuit complexity\nclasses is directly related to classifying the $h$-density of spectra of\nsentences in the corresponding logics of finite residue classes. We further\ngive general conditions under which a logic over the finite residue class rings\nhas a sentence whose spectrum has no $h$-density. One application of this\nresult is that in ${\\cal R}ing(0,+,*,<) + M$, the logic of finite residue class\nrings with built-in order and extended with the majority quantifier $M$, there\nare sentences whose spectrum have no exponential density.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 17:52:08 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Arratia", "Argimiro", ""], ["Ortiz", "Carlos E.", ""]]}, {"id": "1511.02528", "submitter": "EPTCS", "authors": "Rob van Glabbeek, Jan Friso Groote, Peter H\\\"ofner", "title": "Proceedings Workshop on Models for Formal Analysis of Real Systems", "comments": null, "journal-ref": "EPTCS 196, 2015", "doi": "10.4204/EPTCS.196", "report-no": null, "categories": "cs.LO cs.CR cs.OS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of MARS 2015, the first workshop on\nModels for Formal Analysis of Real Systems, held on November 23, 2015 in Suva,\nFiji, as an affiliated workshop of LPAR 2015, the 20th International Conference\non Logic for Programming, Artificial Intelligence and Reasoning.\n  The workshop emphasises modelling over verification. It aims at discussing\nthe lessons learned from making formal methods for the verification and\nanalysis of realistic systems. Examples are:\n  (1) Which formalism is chosen, and why?\n  (2) Which abstractions have to be made and why?\n  (3) How are important characteristics of the system modelled?\n  (4) Were there any complications while modelling the system?\n  (5) Which measures were taken to guarantee the accuracy of the model?\n  We invited papers that present full models of real systems, which may lay the\nbasis for future comparison and analysis. An aim of the workshop is to present\ndifferent modelling approaches and discuss pros and cons for each of them.\nAlternative formal descriptions of the systems presented at this workshop are\nencouraged, which should foster the development of improved specification\nformalisms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 21:12:17 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["van Glabbeek", "Rob", ""], ["Groote", "Jan Friso", ""], ["H\u00f6fner", "Peter", ""]]}, {"id": "1511.02529", "submitter": "Iliano Cervesato", "authors": "Iliano Cervesato (Carnegie Mellon University), Carsten Sch\\\"urmann (IT\n  University of Copenhagen)", "title": "Proceedings First International Workshop on Focusing", "comments": null, "journal-ref": "EPTCS 197, 2015", "doi": "10.4204/EPTCS.197", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the proceedings of WoF'15, the First International\nWorkshop on Focusing, held on November 23rd, 2015 in Suva, Fiji. The workshop\nwas a half-day satellite event of LPAR-20, the 20th International Conferences\non Logic for Programming, Artificial Intelligence and Reasoning.\n  The program committee selected four papers for presentation at WoF'15, and\ninclusion in this volume. In addition, the program included an invited talk by\nElaine Pimentel.\n  Focusing is a proof search strategy that alternates two phases: an inversion\nphase where invertible sequent rules are applied exhaustively and a chaining\nphase where it selects a formula and decomposes it maximally using\nnon-invertible rules. Focusing is one of the most exciting recent developments\nin computational logic: it is complete for many logics of interest and provides\na foundation for their use as programming languages and rewriting calculi.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 21:15:13 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 05:49:10 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Cervesato", "Iliano", "", "Carnegie Mellon University"], ["Sch\u00fcrmann", "Carsten", "", "IT\n  University of Copenhagen"]]}, {"id": "1511.02615", "submitter": "Przemys{\\AA}aw Daca", "authors": "Przemys{\\l}aw Daca, Ashutosh Gupta, Thomas A. Henzinger", "title": "Abstraction-driven Concolic Testing", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concolic testing is a promising method for generating test suites for large\nprograms. However, it suffers from the path-explosion problem and often fails\nto find tests that cover difficult-to-reach parts of programs. In contrast,\nmodel checkers based on counterexample-guided abstraction refinement explore\nprograms exhaustively, while failing to scale on large programs with precision.\nIn this paper, we present a novel method that iteratively combines concolic\ntesting and model checking to find a test suite for a given coverage criterion.\nIf concolic testing fails to cover some test goals, then the model checker\nrefines its program abstraction to prove more paths infeasible, which reduces\nthe search space for concolic testing. We have implemented our method on top of\nthe concolic-testing tool CREST and the model checker CpaChecker. We evaluated\nour tool on a collection of programs and a category of SvComp benchmarks. In\nour experiments, we observed an improvement in branch coverage compared to\nCREST from 48% to 63% in the best case, and from 66% to 71% on average.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 09:50:44 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 18:06:37 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Daca", "Przemys\u0142aw", ""], ["Gupta", "Ashutosh", ""], ["Henzinger", "Thomas A.", ""]]}, {"id": "1511.02629", "submitter": "Luke Ong", "authors": "C.-H. Luke Ong", "title": "Normalisation by Traversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method of computing the beta-normal eta-long form of a\nsimply-typed lambda-term by constructing traversals over a variant abstract\nsyntax tree of the term. In contrast to beta-reduction, which changes the term\nby substitution, this method of normalisation by traversals leaves the original\nterm intact. We prove the correctness of the normalisation procedure by game\nsemantics. As an application, we establish a path-traversal correspondence\ntheorem which is the basis of a key decidability result in higher-order model\nchecking.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 10:51:06 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Ong", "C. -H. Luke", ""]]}, {"id": "1511.02920", "submitter": "Rory Lucyshyn-Wright", "authors": "Rory B. B. Lucyshyn-Wright", "title": "Enriched algebraic theories and monads for a system of arities", "comments": "Minor changes to reflect journal version (published January 31, 2016)", "journal-ref": "Theory and Applications of Categories, Vol. 31, No. 5, 2016, pp.\n  101-137. Published 2016-01-31", "doi": null, "report-no": null, "categories": "math.CT cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under a minimum of assumptions, we develop in generality the basic theory of\nuniversal algebra in a symmetric monoidal closed category $\\mathcal{V}$ with\nrespect to a specified system of arities $j:\\mathcal{J} \\hookrightarrow\n\\mathcal{V}$. Lawvere's notion of algebraic theory generalizes to this context,\nresulting in the notion of single-sorted $\\mathcal{V}$-enriched\n$\\mathcal{J}$-cotensor theory, or $\\mathcal{J}$-theory for short. For suitable\nchoices of $\\mathcal{V}$ and $\\mathcal{J}$, such $\\mathcal{J}$-theories include\nthe enriched algebraic theories of Borceux and Day, the enriched Lawvere\ntheories of Power, the equational theories of Linton's 1965 work, and the\n$\\mathcal{V}$-theories of Dubuc, which are recovered by taking $\\mathcal{J} =\n\\mathcal{V}$ and correspond to arbitrary $\\mathcal{V}$-monads on $\\mathcal{V}$.\nWe identify a modest condition on $j$ that entails that the\n$\\mathcal{V}$-category of $\\mathcal{T}$-algebras exists and is monadic over\n$\\mathcal{V}$ for every $\\mathcal{J}$-theory $\\mathcal{T}$, even when\n$\\mathcal{T}$ is not small and $\\mathcal{V}$ is neither complete nor\ncocomplete. We show that $j$ satisfies this condition if and only if $j$\npresents $\\mathcal{V}$ as a free cocompletion of $\\mathcal{J}$ with respect to\nthe weights for left Kan extensions along $j$, and so we call such systems of\narities eleutheric. We show that $\\mathcal{J}$-theories for an eleutheric\nsystem may be equivalently described as (i) monads in a certain one-object\nbicategory of profunctors on $\\mathcal{J}$, and (ii) $\\mathcal{V}$-monads on\n$\\mathcal{V}$ satisfying a certain condition. We prove a characterization\ntheorem for the categories of algebras of $\\mathcal{J}$-theories, considered as\n$\\mathcal{V}$-categories $\\mathcal{A}$ equipped with a specified\n$\\mathcal{V}$-functor $\\mathcal{A} \\rightarrow \\mathcal{V}$.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:46:05 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 16:15:10 GMT"}, {"version": "v3", "created": "Tue, 26 Apr 2016 20:31:02 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Lucyshyn-Wright", "Rory B. B.", ""]]}, {"id": "1511.03003", "submitter": "Ron van der Meyden", "authors": "Ron van der Meyden and Manas K. Patra", "title": "Undecidable Cases of Model Checking Probabilistic Temporal-Epistemic\n  Logic", "comments": "This is an extended version, with full proofs, of a paper that\n  appeared in TARK 2015. It corrects an error in the TARK 2015 pre-proceedings\n  version, in the definition of mixed-time polynomial atomic probability\n  formulas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the decidability of model-checking logics of time, knowledge\nand probability, with respect to two epistemic semantics: the clock and\nsynchronous perfect recall semantics in partially observed discrete-time Markov\nchains. Decidability results are known for certain restricted logics with\nrespect to these semantics, subject to a variety of restrictions that are\neither unexplained or involve a longstanding unsolved mathematical problem. We\nshow that mild generalizations of the known decidable cases suffice to render\nthe model checking problem definitively undecidable. In particular, for a\nsynchronous perfect recall, a generalization from temporal operators with\nfinite reach to operators with infinite reach renders model checking\nundecidable. The case of the clock semantics is closely related to a monadic\nsecond order logic of time and probability that is known to be decidable,\nexcept on a set of measure zero. We show that two distinct extensions of this\nlogic make model checking undecidable. One of these involves polynomial\ncombinations of probability terms, the other involves monadic second order\nquantification into the scope of probability operators. These results explain\nsome of the restrictions in previous work.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 06:51:33 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["van der Meyden", "Ron", ""], ["Patra", "Manas K.", ""]]}, {"id": "1511.03693", "submitter": "Thorsten Wissmann", "authors": "Hugo Nobrega, Arno Pauly", "title": "Game characterizations and lower cones in the Weihrauch degrees", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 3 (August 6,\n  2019) lmcs:5670", "doi": "10.23638/LMCS-15(3:11)2019", "report-no": null, "categories": "math.LO cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a parametrized version of the Wadge game for functions and show\nthat each lower cone in the Weihrauch degrees is characterized by such a game.\nThese parametrized Wadge games subsume the original Wadge game, the eraser and\nbacktrack games as well as Semmes's tree games. In particular, we propose that\nthe lower cones in the Weihrauch degrees are the answer to Andretta's question\non which classes of functions admit game characterizations.\n  We then discuss some applications of such parametrized Wadge games. Using\nmachinery from Weihrauch reducibility theory, we introduce games characterizing\nevery (transfinite) level of the Baire hierarchy via an iteration of a pruning\nderivative on countably branching trees.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:35:12 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 20:01:41 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 18:55:04 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 15:58:36 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Nobrega", "Hugo", ""], ["Pauly", "Arno", ""]]}, {"id": "1511.03749", "submitter": "M\\'onica Mar\\'ia Mart\\'inez Amarante Mmma", "authors": "Monica Martinez and Edelweis Rohrer and Paula Severi", "title": "Complexity of the Description Logic ALCM", "comments": "Long version of a submitted paper, 43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the problem of checking consistency of a knowledge\nbase in the Description Logic ALCM is ExpTime-complete. The M stands for\nmeta-modelling as defined by Motz, Rohrer and Severi. To show our main result,\nwe define an ExpTime Tableau algorithm as an extension of an algorithm for\nchecking consistency of a knowledge base in ALC by Nguyen and Szalas.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 01:31:38 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Martinez", "Monica", ""], ["Rohrer", "Edelweis", ""], ["Severi", "Paula", ""]]}, {"id": "1511.04169", "submitter": "EPTCS", "authors": "Sidney Amani, Toby Murray (NICTA and University of New South Wales,\n  Australia)", "title": "Specifying a Realistic File System", "comments": "In Proceedings MARS 2015, arXiv:1511.02528", "journal-ref": "EPTCS 196, 2015, pp. 1-9", "doi": "10.4204/EPTCS.196.1", "report-no": null, "categories": "cs.LO cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the most interesting elements of the correctness specification of\nBilbyFs, a performant Linux flash file system. The BilbyFs specification\nsupports asynchronous writes, a feature that has been overlooked by several\nfile system verification projects, and has been used to verify the correctness\nof BilbyFs's fsync() C implementation. It makes use of nondeterminism to be\nconcise and is shallowly-embedded in higher-order logic.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:27:39 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Amani", "Sidney", "", "NICTA and University of New South Wales,\n  Australia"], ["Murray", "Toby", "", "NICTA and University of New South Wales,\n  Australia"]]}, {"id": "1511.04170", "submitter": "EPTCS", "authors": "June Andronick (NICTA and UNSW), Corey Lewis (NICTA), Carroll Morgan\n  (NICTA and UNSW)", "title": "Controlled Owicki-Gries Concurrency: Reasoning about the Preemptible\n  eChronos Embedded Operating System", "comments": "In Proceedings MARS 2015, arXiv:1511.02528", "journal-ref": "EPTCS 196, 2015, pp. 10-24", "doi": "10.4204/EPTCS.196.2", "report-no": null, "categories": "cs.LO cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a controlled concurrency framework, derived from the\nOwicki-Gries method, for describing a hardware interface in detail sufficient\nto support the modelling and verification of small, embedded operating systems\n(OS's) whose run-time responsiveness is paramount. Such real-time systems run\nwith interrupts mostly enabled, including during scheduling. That differs from\nmany other successfully modelled and verified OS's that typically reduce the\ncomplexity of concurrency by running on uniprocessor platforms and by switching\ninterrupts off as much as possible. Our framework builds on the traditional\nOwicki-Gries method, for its fine-grained concurrency is needed for\nhigh-performance system code. We adapt it to support explicit concurrency\ncontrol, by providing a simple, faithful representation of the hardware\ninterface that allows software to control the degree of interleaving between\nuser code, OS code, interrupt handlers and a scheduler that controls context\nswitching. We then apply this framework to model the interleaving behavior of\nthe eChronos OS, a preemptible real-time OS for embedded micro-controllers. We\ndiscuss the accuracy and usability of our approach when instantiated to model\nthe eChronos OS. Both our framework and the eChronos model are formalised in\nthe Isabelle/HOL theorem prover, taking advantage of the high level of\nautomation in modern reasoning tools.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:27:48 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Andronick", "June", "", "NICTA and UNSW"], ["Lewis", "Corey", "", "NICTA"], ["Morgan", "Carroll", "", "NICTA and UNSW"]]}, {"id": "1511.04172", "submitter": "EPTCS", "authors": "Franck Cassez (Macquarie University, Sydney, Australia), Pablo\n  Gonz\\'alez de Aledo Marug\\'an (University of Cantabria, Santander, Spain)", "title": "Timed Automata for Modelling Caches and Pipelines", "comments": "In Proceedings MARS 2015, arXiv:1511.02528", "journal-ref": "EPTCS 196, 2015, pp. 37-45", "doi": "10.4204/EPTCS.196.4", "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on modelling the timing aspects of binary programs\nrunning on architectures featuring caches and pipelines. The objective is to\nobtain a timed automaton model to compute tight bounds for the worst-case\nexecution time (WCET) of the programs using model-checking techniques.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:28:08 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Cassez", "Franck", "", "Macquarie University, Sydney, Australia"], ["Marug\u00e1n", "Pablo Gonz\u00e1lez de Aledo", "", "University of Cantabria, Santander, Spain"]]}, {"id": "1511.04173", "submitter": "EPTCS", "authors": "Kaylash Chaudhary (University of the South Pacific), Ansgar Fehnker\n  (University of the South Pacific), Jaco van de Pol (University of Twente),\n  Marielle Stoelinga (University of Twente)", "title": "Modeling and Verification of the Bitcoin Protocol", "comments": "In Proceedings MARS 2015, arXiv:1511.02528", "journal-ref": "EPTCS 196, 2015, pp. 46-60", "doi": "10.4204/EPTCS.196.5", "report-no": null, "categories": "cs.LO cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is a popular digital currency for online payments, realized as a\ndecentralized peer-to-peer electronic cash system. Bitcoin keeps a ledger of\nall transactions; the majority of the participants decides on the correct\nledger. Since there is no trusted third party to guard against double spending,\nand inspired by its popularity, we would like to investigate the correctness of\nthe Bitcoin protocol. Double spending is an important threat to electronic\npayment systems. Double spending would happen if one user could force a\nmajority to believe that a ledger without his previous payment is the correct\none. We are interested in the probability of success of such a double spending\nattack, which is linked to the computational power of the attacker. This paper\nexamines the Bitcoin protocol and provides its formalization as an UPPAAL\nmodel. The model will be used to show how double spending can be done if the\nparties in the Bitcoin protocol behave maliciously, and with what probability\ndouble spending occurs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:28:19 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Chaudhary", "Kaylash", "", "University of the South Pacific"], ["Fehnker", "Ansgar", "", "University of the South Pacific"], ["van de Pol", "Jaco", "", "University of Twente"], ["Stoelinga", "Marielle", "", "University of Twente"]]}, {"id": "1511.04174", "submitter": "EPTCS", "authors": "Wendelin Serwe (Inria)", "title": "Formal Specification and Verification of Fully Asynchronous\n  Implementations of the Data Encryption Standard", "comments": "In Proceedings MARS 2015, arXiv:1511.02528", "journal-ref": "EPTCS 196, 2015, pp. 61-147", "doi": "10.4204/EPTCS.196.6", "report-no": null, "categories": "cs.LO cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two formal models of the Data Encryption Standard (DES),\na first using the international standard LOTOS, and a second using the more\nrecent process calculus LNT. Both models encode the DES in the style of\nasynchronous circuits, i.e., the data-flow blocks of the DES algorithm are\nrepresented by processes communicating via rendezvous. To ensure correctness of\nthe models, several techniques have been applied, including model checking,\nequivalence checking, and comparing the results produced by a prototype\nautomatically generated from the formal model with those of existing\nimplementations of the DES. The complete code of the models is provided as\nappendices and also available on the website of the CADP verification toolbox.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:28:28 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Serwe", "Wendelin", "", "Inria"]]}, {"id": "1511.04177", "submitter": "EPTCS", "authors": "Vivek Nigam, Giselle Reis, Leonardo Lima", "title": "Towards the Automated Generation of Focused Proof Systems", "comments": "In Proceedings WoF'15, arXiv:1511.02529", "journal-ref": "EPTCS 197, 2015, pp. 1-6", "doi": "10.4204/EPTCS.197.1", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of formulating and proving the completeness of\nfocused-like proof systems in an automated fashion. Focusing is a discipline on\nproofs which structures them into phases in order to reduce proof search\nnon-determinism. We demonstrate that it is possible to construct a complete\nfocused proof system from a given un-focused proof system if it satisfies some\nconditions. Our key idea is to generalize the completeness proof based on\npermutation lemmas given by Miller and Saurin for the focused linear logic\nproof system. This is done by building a graph from the rule permutation\nrelation of a proof system, called permutation graph. We then show that from\nthe permutation graph of a given proof system, it is possible to construct a\ncomplete focused proof system, and additionally infer for which formulas\ncontraction is admissible. An implementation for building the permutation graph\nof a system is provided. We apply our technique to generate the focused proof\nsystems MALLF, LJF and LKF for linear, intuitionistic and classical logics,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:34:12 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Nigam", "Vivek", ""], ["Reis", "Giselle", ""], ["Lima", "Leonardo", ""]]}, {"id": "1511.04178", "submitter": "EPTCS", "authors": "Roberto Blanco (Inria and LIX), Dale Miller (Inria and LIX)", "title": "Proof Outlines as Proof Certificates: A System Description", "comments": "In Proceedings WoF'15, arXiv:1511.02529", "journal-ref": "EPTCS 197, 2015, pp. 7-14", "doi": "10.4204/EPTCS.197.2", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the foundational proof certificate (FPC) framework to the problem of\ndesigning high-level outlines of proofs. The FPC framework provides a means to\nformally define and check a wide range of proof evidence. A focused proof\nsystem is central to this framework and such a proof system provides an\ninteresting approach to proof reconstruction during the process of proof\nchecking (relying on an underlying logic programming implementation). Here, we\nillustrate how the FPC framework can be used to design proof outlines and then\nto exploit proof checkers as a means for expanding outlines into fully detailed\nproofs. In order to validate this approach to proof outlines, we have built the\nACheck system that allows us to take a sequence of theorems and apply the proof\noutline \"do the obvious induction and close the proof using previously proved\nlemmas\".\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:34:22 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Blanco", "Roberto", "", "Inria and LIX"], ["Miller", "Dale", "", "Inria and LIX"]]}, {"id": "1511.04179", "submitter": "EPTCS", "authors": "St\\'ephane Graham-Lengrand (CNRS - Ecole Polytechnique - INRIA - SRI\n  International)", "title": "Realisability semantics of abstract focussing, formalised", "comments": "In Proceedings WoF'15, arXiv:1511.02529", "journal-ref": "EPTCS 197, 2015, pp. 15-28", "doi": "10.4204/EPTCS.197.3", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sequent calculus for abstract focussing, equipped with\nproof-terms: in the tradition of Zeilberger's work, logical connectives and\ntheir introduction rules are left as a parameter of the system, which collapses\nthe synchronous and asynchronous phases of focussing as macro rules. We go\nfurther by leaving as a parameter the operation that extends a context of\nhypotheses with new ones, which allows us to capture both classical and\nintuitionistic focussed sequent calculi. We then define the realisability\nsemantics of (the proofs of) the system, on the basis of Munch-Maccagnoni's\northogonality models for the classical focussed sequent calculus, but now\noperating at the higher level of abstraction mentioned above. We prove, at that\nlevel, the Adequacy Lemma, namely that if a term is of type A, then in the\nmodel its denotation is in the (set-theoretic) interpretation of A. This\nexhibits the fact that the universal quantification involved when taking the\northogonal of a set, reflects in the semantics Zeilberger's universal\nquantification in the macro rule for the asynchronous phase. The system and its\nsemantics are all formalised in Coq.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:34:31 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Graham-Lengrand", "St\u00e9phane", "", "CNRS - Ecole Polytechnique - INRIA - SRI\n  International"]]}, {"id": "1511.04180", "submitter": "EPTCS", "authors": "Glyn Morrill (Universitat Polit\\`ecnica de Catalunya), Oriol\n  Valent\\'in (Universitat Polit\\`ecnica de Catalunya)", "title": "Multiplicative-Additive Focusing for Parsing as Deduction", "comments": "In Proceedings WoF'15, arXiv:1511.02529", "journal-ref": "EPTCS 197, 2015, pp. 29-54", "doi": "10.4204/EPTCS.197.4", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spurious ambiguity is the phenomenon whereby distinct derivations in grammar\nmay assign the same structural reading, resulting in redundancy in the parse\nsearch space and inefficiency in parsing. Understanding the problem depends on\nidentifying the essential mathematical structure of derivations. This is\ntrivial in the case of context free grammar, where the parse structures are\nordered trees; in the case of categorial grammar, the parse structures are\nproof nets. However, with respect to multiplicatives intrinsic proof nets have\nnot yet been given for displacement calculus, and proof nets for additives,\nwhich have applications to polymorphism, are involved. Here we approach\nmultiplicative-additive spurious ambiguity by means of the proof-theoretic\ntechnique of focalisation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:34:41 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Morrill", "Glyn", "", "Universitat Polit\u00e8cnica de Catalunya"], ["Valent\u00edn", "Oriol", "", "Universitat Polit\u00e8cnica de Catalunya"]]}, {"id": "1511.04271", "submitter": "Alessandra Palmigiano", "authors": "Willem Conradie, Alessandra Palmigiano, Sumit Sourabh, Zhiguang Zhao", "title": "Canonicity and Relativized Canonicity via Pseudo-Correspondence: an\n  Application of ALBA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize Venema's result on the canonicity of the additivity of positive\nterms, from classical modal logic to a vast class of logics the algebraic\nsemantics of which is given by varieties of normal distributive lattice\nexpansions (normal DLEs), aka `distributive lattices with operators'. We\nprovide two contrasting proofs for this result: the first is along the lines of\nVenema's pseudo-correspondence argument but using the insights and tools of\nunified correspondence theory, and in particular the algorithm ALBA; the second\ncloser to the style of J\\'onsson. Using insights gleaned from the second proof,\nwe define a suitable enhancement of the algorithm ALBA, which we use prove the\ncanonicity of certain syntactically defined classes of DLE-inequalities (called\nthe meta-inductive inequalities), relative to the structures in which the\nformulas asserting the additivity of some given terms are valid.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 13:15:35 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 15:03:42 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Conradie", "Willem", ""], ["Palmigiano", "Alessandra", ""], ["Sourabh", "Sumit", ""], ["Zhao", "Zhiguang", ""]]}, {"id": "1511.05334", "submitter": "Pierre Lescanne", "authors": "Katarzyna Grygiel (TCS), Pierre Lescanne (TCS, LIP)", "title": "Counting and Generating Terms in the Binary Lambda Calculus (Extended\n  version)", "comments": "extended version of arXiv:1401.0379", "journal-ref": "J. Funct. Prog. 25 (2015) e24", "doi": "10.1017/S0956796815000271", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a paper entitled Binary lambda calculus and combinatory logic, John Tromp\npresents a simple way of encoding lambda calculus terms as binary sequences. In\nwhat follows, we study the numbers of binary strings of a given size that\nrepresent lambda terms and derive results from their generating functions,\nespecially that the number of terms of size n grows roughly like 1.963447954.\n.. n. In a second part we use this approach to generate random lambda terms\nusing Boltzmann samplers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 10:20:01 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Grygiel", "Katarzyna", "", "TCS"], ["Lescanne", "Pierre", "", "TCS, LIP"]]}, {"id": "1511.05750", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert (LACL), Ioana Cristescu (PPS)", "title": "Contextual equivalences in configuration structures and reversibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual equivalence equate terms that have the same observable behaviour\nin any context. A standard contextual equivalence for CCS is the strong barbed\ncongruence. Configuration structures are a denotational semantics for processes\nin which one define equivalences that are more discriminating, i.e. that\ndistinguish the denotation of terms equated by barbed congruence. Hereditary\nhistory preserving bisimulation (HHPB) is such a relation. We define a strong\nback-and-forth barbed congruence on RCCS, a reversible variant of CCS. We show\nthat the relation induced by the back-and-forth congruence on configuration\nstructures is equivalent to HHPB, thus providing a contextual characterization\nof HHPB.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 12:10:02 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "LACL"], ["Cristescu", "Ioana", "", "PPS"]]}, {"id": "1511.05866", "submitter": "Diego Latella", "authors": "Diego Latella (CNR-ISTI), Mieke Massink (CNR-ISTI), Erik P De Vink\n  (Eindhoven University of Technology - Department of Mathematics and Computer\n  Science)", "title": "Bisimulation of Labelled State-to-Function Transition Systems\n  Coalgebraically", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 4 (December\n  22, 2015) lmcs:1617", "doi": "10.2168/LMCS-11(4:16)2015", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled state-to-function transition systems, FuTS for short, are\ncharacterized by transitions which relate states to functions of states over\ngeneral semirings, equipped with a rich set of higher-order operators. As such,\nFuTS constitute a convenient modeling instrument to deal with process languages\nand their quantitative extensions in particular. In this paper, the notion of\nbisimulation induced by a FuTS is addressed from a coalgebraic point of view. A\ncorrespondence result is established stating that FuTS-bisimilarity coincides\nwith behavioural equivalence of the associated functor. As generic examples,\nthe equivalences underlying substantial fragments of major examples of\nquantitative process algebras are related to the bisimilarity of specific FuTS.\nThe examples range from a stochastic process language, PEPA, to a language for\nInteractive Markov Chains, IML, a (discrete) timed process language, TPC, and a\nlanguage for Markov Automata, MAL. The equivalences underlying these languages\nare related to the bisimilarity of their specific FuTS. By the correspondence\nresult coalgebraic justification of the equivalences of these calculi is\nobtained. The specific selection of languages, besides covering a large variety\nof process interaction models and modelling choices involving quantities,\nallows us to show different classes of FuTS, namely so-called simple FuTS,\ncombined FuTS, nested FuTS, and general FuTS.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 16:30:38 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 07:45:15 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Latella", "Diego", "", "CNR-ISTI"], ["Massink", "Mieke", "", "CNR-ISTI"], ["De Vink", "Erik P", "", "Eindhoven University of Technology - Department of Mathematics and Computer\n  Science"]]}, {"id": "1511.05888", "submitter": "Nicole Schweikardt", "authors": "Frederik Harwath (Goethe-Universit\\~At Frankfurt am Main), Lucas\n  Heimberg (Humboldt-Universit\\~At zu Berlin), Nicole Schweikardt\n  (Humboldt-Universit\\~At zu Berlin)", "title": "Preservation and decomposition theorems for bounded degree structures", "comments": "42 pages and 3 figures. This is the full version of: Frederik\n  Harwath, Lucas Heimberg, and Nicole Schweikardt. Preservation and\n  decomposition theorems for bounded degree structures. In Joint Meeting of the\n  23rd EACSL Annual Conference on Computer Science Logic (CSL) and the 29th\n  Annual ACM/IEEE Symposium on Logic in Computer Science (LICS), CSL-LICS'14,\n  pages 49:1-49:10. ACM, 2014", "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 4 (December\n  29, 2015) lmcs:1618", "doi": "10.2168/LMCS-11(4:17)2015", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide elementary algorithms for two preservation theorems for\nfirst-order sentences (FO) on the class \\^ad of all finite structures of degree\nat most d: For each FO-sentence that is preserved under extensions\n(homomorphisms) on \\^ad, a \\^ad-equivalent existential (existential-positive)\nFO-sentence can be constructed in 5-fold (4-fold) exponential time. This is\ncomplemented by lower bounds showing that a 3-fold exponential blow-up of the\ncomputed existential (existential-positive) sentence is unavoidable. Both\nalgorithms can be extended (while maintaining the upper and lower bounds on\ntheir time complexity) to input first-order sentences with modulo m counting\nquantifiers (FO+MODm). Furthermore, we show that for an input FO-formula, a\n\\^ad-equivalent Feferman-Vaught decomposition can be computed in 3-fold\nexponential time. We also provide a matching lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 17:25:28 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 19:38:36 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Harwath", "Frederik", "", "Goethe-Universit\u00c3t Frankfurt am Main"], ["Heimberg", "Lucas", "", "Humboldt-Universit\u00c3t zu Berlin"], ["Schweikardt", "Nicole", "", "Humboldt-Universit\u00c3t zu Berlin"]]}, {"id": "1511.06260", "submitter": "Federico Aschieri", "authors": "Federico Aschieri", "title": "Game Semantics and the Geometry of Backtracking: a New Complexity\n  Analysis of Interaction", "comments": "Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present abstract complexity results about Coquand and Hyland-Ong game\nsemantics, that will lead to new bounds on the length of first-order\ncut-elimination, normalization, interaction between expansion trees and any\nother dialogical process game semantics can model and apply to. In particular,\nwe provide a novel method to bound the length of interactions between visible\nstrategies and to measure precisely the tower of exponentials defining the\nworst-case complexity. Our study improves the old estimates on average by\nseveral exponentials.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:03:20 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 10:35:34 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2016 09:49:43 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Aschieri", "Federico", ""]]}, {"id": "1511.06668", "submitter": "Bishoksan Kafle", "authors": "Bishoksan Kafle", "title": "Solving non-linear Horn clauses using a linear solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing an efficient non-linear Horn clause solver is a challenging task\nsince the solver has to reason about the tree structures rather than the linear\nones as in a linear solver. In this paper we propose an incremental approach to\nsolving a set of non-linear Horn clauses using a linear Horn clause solver. We\nachieve this by interleaving a program transformation and a linear solver. The\nprogram transformation is based on the notion of tree dimension, which we apply\nto trees corresponding to Horn clause derivations. The dimension of a tree is a\nmeasure of its non-linearity -- for example a linear tree (whose nodes have at\nmost one child) has dimension zero while a complete binary tree has dimension\nequal to its height.\n  A given set of Horn clauses $P$ can be transformed into a new set of clauses\n$P^k$ (whose derivation trees are the subset of $P$'s derivation trees with\ndimension at most $k$). We start by generating $P^k$ with $k=0$, which is\nlinear by definition, then pass it to a linear solver. If $P^k$ has a solution\n$M$, and is a solution to $P$ then $P$ has a solution $M$. If $M$ is not a\nsolution of $P$, we plugged $M$ to $P^{(k+1)}$ which again becomes linear and\npass it to the solver and continue successively for increasing value of $k$\nuntil we find a solution to $P$ or resources are exhausted. Experiment on some\nHorn clause verification benchmarks indicates that this is a promising approach\nfor solving a set of non-linear Horn clauses using a linear solver. It\nindicates that many times a solution obtained for some under-approximation\n$P^k$ of $P$ becomes a solution for $P$ for a fairly small value of $k$.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:22:02 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Kafle", "Bishoksan", ""]]}, {"id": "1511.07319", "submitter": "Alessandro Provetti", "authors": "Alessandro Provetti, Andrea Zucchellini", "title": "A Note on Flagg and Friedman's Epistemic and Intuitionistic Formal\n  Systems", "comments": "Under evaluation by Annals of Pure and Applied Logic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report our findings on the properties of Flagg and Friedman's translation\nfrom Epistemic into Intuitionistic logic, which was proposed as the basis of a\ncomprehensive proof method for the faithfulness of the Goodel translation. We\nfocus on the propositional case and raise the issue of the admissibility of the\ntranslated necessitation rule. Then, we contribute to Flagg and Friedman's\nprogram by giving an explicit proof of the soundness of their translation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 20:38:50 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Provetti", "Alessandro", ""], ["Zucchellini", "Andrea", ""]]}, {"id": "1511.07536", "submitter": "Shayak Sen Shayak Sen", "authors": "Anupam Datta, Joseph Y. Halpern, John C. Mitchell, Arnab Roy, Shayak\n  Sen", "title": "A Symbolic Logic with Concrete Bounds for Cryptographic Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal logic for quantitative reasoning about security\nproperties of network protocols. The system allows us to derive concrete\nsecurity bounds that can be used to choose key lengths and other security\nparameters. We provide axioms for reasoning about digital signatures and random\nnonces, with security properties based on the concrete security of signature\nschemes and pseudorandom number generators (PRG). The formal logic supports\nfirst-order reasoning and reasoning about protocol invariants, taking concrete\nsecurity bounds into account. Proofs constructed in our logic also provide\nconventional asymptotic security guarantees because of the way that concrete\nbounds accumulate in proofs. As an illustrative example, we use the formal\nlogic to prove an authentication property with concrete bounds of a\nsignature-based challenge-response protocol.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 01:52:04 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Datta", "Anupam", ""], ["Halpern", "Joseph Y.", ""], ["Mitchell", "John C.", ""], ["Roy", "Arnab", ""], ["Sen", "Shayak", ""]]}, {"id": "1511.07663", "submitter": "Kuldeep Meel", "authors": "Supratik Chakraborty, Kuldeep S. Meel, Rakesh Mistry, Moshe Y. Vardi", "title": "Approximate Probabilistic Inference via Word-Level Counting", "comments": "Full version of AAAI 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing-based model counting has emerged as a promising approach for\nlarge-scale probabilistic inference on graphical models. A key component of\nthese techniques is the use of xor-based 2-universal hash functions that\noperate over Boolean domains. Many counting problems arising in probabilistic\ninference are, however, naturally encoded over finite discrete domains.\nTechniques based on bit-level (or Boolean) hash functions require these\nproblems to be propositionalized, making it impossible to leverage the\nremarkable progress made in SMT (Satisfiability Modulo Theory) solvers that can\nreason directly over words (or bit-vectors). In this work, we present the first\napproximate model counter that uses word-level hashing functions, and can\ndirectly leverage the power of sophisticated SMT solvers. Empirical evaluation\nover an extensive suite of benchmarks demonstrates the promise of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 11:52:28 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 15:36:35 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 05:36:28 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Meel", "Kuldeep S.", ""], ["Mistry", "Rakesh", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1511.07865", "submitter": "Ekaterina Komendantskaya Dr", "authors": "Ekaterina Komendantskaya, Patricia Johann and Martin Schmidt", "title": "Structural Resolution: a Framework for Coinductive Proof Search and\n  Proof Construction in Horn Clause Logic", "comments": "A working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programming (LP) is a programming language based on first-order Horn\nclause logic that uses SLD-resolution as a semi-decision procedure. Finite\nSLD-computations are inductively sound and complete with respect to least\nHerbrand models of logic programs. Dually, the corecursive approach to\nSLD-resolution views infinite SLD-computations as successively approximating\ninfinite terms contained in programs' greatest complete Herbrand models.\nState-of-the-art algorithms implementing corecursion in LP are based on loop\ndetection. However, such algorithms support inference of logical entailment\nonly for rational terms, and they do not account for the important property of\nproductivity in infinite SLD-computations. Loop detection thus lags behind\ncoinductive methods in interactive theorem proving (ITP) and term-rewriting\nsystems (TRS).\n  Structural resolution is a newly proposed alternative to SLD-resolution that\nmakes it possible to define and semi-decide a notion of productivity\nappropriate to LP. In this paper, we prove soundness of structural resolution\nrelative to Herbrand model semantics for productive inductive, coinductive, and\nmixed inductive-coinductive logic programs.\n  We introduce two algorithms that support coinductive proof search for\ninfinite productive terms. One algorithm combines the method of loop detection\nwith productive structural resolution, thus guaranteeing productivity of\ncoinductive proofs for infinite rational terms. The other allows to make lazy\nsound observations of fragments of infinite irrational productive terms. This\nputs coinductive methods in LP on par with productivity-based observational\napproaches to coinduction in ITP and TRS.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 20:53:04 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 07:54:35 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Komendantskaya", "Ekaterina", ""], ["Johann", "Patricia", ""], ["Schmidt", "Martin", ""]]}, {"id": "1511.08049", "submitter": "Sarmen Keshishzadeh", "authors": "Sarmen Keshishzadeh and Arjan J. Mooij and Jozef Hooman", "title": "Industrial Experiences with a Formal DSL Semantics to Check Correctness\n  of DSL Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A domain specific language (DSL) abstracts from implementation details and is\naligned with the way domain experts reason about a software component. The\ndevelopment of DSLs is usually centered around a grammar and transformations\nthat generate implementation code or analysis models. The semantics of the\nlanguage is often defined implicitly and in terms of a transformation to\nimplementation code. In the presence of multiple transformations from the DSL,\nthe consistency of the generated artifacts with respect to the semantics of the\nDSL is a relevant issue. We show that a formal semantics is essential for\nchecking the consistency between the generated artifacts. We exploit the formal\nsemantics in an industrial project and use formal techniques based on\nequivalence checking and model-based testing for consistency checking. We\nreport about our experience with this approach in an industrial development\nproject.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 13:09:20 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Keshishzadeh", "Sarmen", ""], ["Mooij", "Arjan J.", ""], ["Hooman", "Jozef", ""]]}, {"id": "1511.08447", "submitter": "Brijesh Dongol", "authors": "Brijesh Dongol, Robert M. Hierons", "title": "Decidability and Complexity for Quiescent Consistency and its Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quiescent consistency is a notion of correctness for a concurrent object that\ngives meaning to the object's behaviours in quiescent states, i.e., states in\nwhich none of the object's operations are being executed. Correctness of an\nimplementation object is defined in terms of a corresponding abstract\nspecification. This gives rise to two important verification questions:\nmembership (checking whether a behaviour of the implementation is allowed by\nthe specification) and correctness (checking whether all behaviours of the\nimplementation are allowed by the specification). In this paper, we show that\nthe membership problem for quiescent consistency is NP-complete and that the\ncorrectness problem is decidable, but coNP-hard and in EXPSPACE. For both\nproblems, we consider restricted versions of quiescent consistency by assuming\nan upper limit on the number of events between two quiescent points. Here, we\nshow that the membership problem is in PTIME, whereas correctness is in PSPACE.\n  Quiescent consistency does not guarantee sequential consistency, i.e., it\nallows operation calls by the same process to be reordered when mapping to an\nabstract specification. Therefore, we also consider quiescent sequential\nconsistency, which strengthens quiescent consistency with an additional\nsequential consistency condition. We show that the unrestricted versions of\nmembership and correctness are NP-complete and undecidable, respectively. When\nby placing a limit on the number of events between two quiescent points,\nmembership is in PTIME, while correctness is in PSPACE. Finally, we consider a\nversion of quiescent sequential consistency that places an upper limit on the\nnumber of processes for every run of the implementation, and show that the\nmembership problem for quiescent sequential consistency with this restriction\nis in PTIME.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 16:46:30 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Dongol", "Brijesh", ""], ["Hierons", "Robert M.", ""]]}, {"id": "1511.08605", "submitter": "Bruno Courcelle", "authors": "Bruno Courcelle (LaBRI)", "title": "Fly-automata for checking MSO 2 graph properties", "comments": "Submitted for publication in December 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A more descriptive but too long title would be : Constructing fly-automata to\ncheck properties of graphs of bounded tree-width expressed by monadic\nsecond-order formulas written with edge quantifications. Such properties are\ncalled MSO2 in short. Fly-automata (FA) run bottom-up on terms denoting graphs\nand compute \"on the fly\" the necessary states and transitions instead of\nlooking into huge, actually unimplementable tables. In previous works, we have\nconstructed FA that process terms denoting graphs of bounded clique-width, in\norder to check their monadic second-order (MSO) properties (expressed by\nformulas without edge quan-tifications). Here, we adapt these FA to incidence\ngraphs, so that they can check MSO2 properties of graphs of bounded tree-width.\nThis is possible because: (1) an MSO2 property of a graph is nothing but an MSO\nproperty of its incidence graph and (2) the clique-width of the incidence graph\nof a graph is linearly bounded in terms of its tree-width. Our constructions\nare actually implementable and usable. We detail concrete constructions of\nautomata in this perspective.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 10:19:43 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 09:13:45 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Courcelle", "Bruno", "", "LaBRI"]]}, {"id": "1511.08678", "submitter": "Jeroen Meijer", "authors": "Jeroen Meijer and Jaco van de Pol", "title": "Bandwidth and Wavefront Reduction for Static Variable Ordering in\n  Symbolic Model Checking", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the applicability of bandwidth and wavefront reduction\nalgorithms to static variable ordering. In symbolic model checking event\nlocality plays a major role in time and memory usage. For example, in Petri\nnets event locality can be captured by dependency matrices, where nonzero\nentries indicate whether a transition modifies a place. The quality of event\nlocality has been expressed as a metric called (weighted) event span. The\nbandwidth of a matrix is a metric indicating the distance of nonzero elements\nto the diagonal. Wavefront is a metric indicating the degree of nonzeros on one\nend of the diagonal of the matrix. Bandwidth and wavefront are well studied\nmetrics used in sparse matrix solvers.\n  In this work we prove that span is limited by twice the bandwidth of a\nmatrix. This observation makes bandwidth reduction algorithms useful for\nobtaining good variable orders. One major issue we address is that the\nreduction algorithms can only be applied on symmetric matrices, while the\ndependency matrices are asymmetric. We show that the Sloan algorithm executed\non the total graph of the adjacency graph gives the best variable orders.\nPractically, we demonstrate that our work allows to call standard sparse matrix\noperations in Boost and ViennaCL, computing very good static variable orders in\nmilliseconds. Future work is promising, because a whole new spectrum of more\noff-the-shelf algorithms, including metaheuristic ones, become available for\nvariable ordering.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 14:09:27 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Meijer", "Jeroen", ""], ["van de Pol", "Jaco", ""]]}, {"id": "1511.08723", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli and Pierre Bourhis and Pierre Senellart", "title": "Provenance Circuits for Trees and Treelike Instances (Extended Version)", "comments": "48 pages. Presented at ICALP'15", "journal-ref": null, "doi": "10.1007/978-3-662-47666-6_5", "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Query evaluation in monadic second-order logic (MSO) is tractable on trees\nand treelike instances, even though it is hard for arbitrary instances. This\ntractability result has been extended to several tasks related to query\nevaluation, such as counting query results [3] or performing query evaluation\non probabilistic trees [10]. These are two examples of the more general problem\nof computing augmented query output, that is referred to as provenance. This\narticle presents a provenance framework for trees and treelike instances, by\ndescribing a linear-time construction of a circuit provenance representation\nfor MSO queries. We show how this provenance can be connected to the usual\ndefinitions of semiring provenance on relational instances [20], even though we\ncompute it in an unusual way, using tree automata; we do so via intrinsic\ndefinitions of provenance for general semirings, independent of the operational\ndetails of query evaluation. We show applications of this provenance to capture\nexisting counting and probabilistic results on trees and treelike instances,\nand give novel consequences for probability evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 16:11:56 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Senellart", "Pierre", ""]]}, {"id": "1511.08851", "submitter": "Makoto Hamana", "authors": "Makoto Hamana, Kazutaka Matsuda and Kazuyuki Asada", "title": "The Algebra of Recursive Graph Transformation Language UnCAL: Complete\n  Axiomatisation and Iteration Categorical Semantics", "comments": "53 pages, to appear in MSCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide mathematical foundations of a graph\ntransformation language, called UnCAL, using categorical semantics of type\ntheory and fixed points. About twenty years ago, Buneman et al. developed a\ngraph database query language UnQL on the top of a functional meta-language\nUnCAL for describing and manipulating graphs. Recently, the functional\nprogramming community has shown renewed interest in UnCAL, because it provides\nan efficient graph transformation language which is useful for various\napplications, such as bidirectional computation.\n  In order to make UnCAL more flexible and fruitful for further extensions and\napplications, in this paper, we give a more conceptual understanding of UnCAL\nusing categorical semantics. Our general interest of this paper is to clarify\nwhat is the algebra of UnCAL. Thus, we give an equational axiomatisation and\ncategorical semantics of UnCAL, both of which are new. We show that the\naxiomatisation is complete for the original bisimulation semantics of UnCAL.\nMoreover, we provide a clean characterisation of the computation mechanism of\nUnCAL called \"structural recursion on graphs\" using our categorical semantics.\nWe show a concrete model of UnCAL given by the lambdaG-calculus, which shows an\ninteresting connection to lazy functional programming.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 23:26:05 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 21:24:18 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hamana", "Makoto", ""], ["Matsuda", "Kazutaka", ""], ["Asada", "Kazuyuki", ""]]}, {"id": "1511.08999", "submitter": "Marco Voigt", "authors": "Thomas Sturm, Marco Voigt and Christoph Weidenbach", "title": "Deciding First-Order Satisfiability when Universal and Existential\n  Variables are Separated", "comments": "Extended version of our LICS 2016 conference paper, 23 pages", "journal-ref": null, "doi": "10.1145/2933575.2934532", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new decidable fragment of first-order logic with equality,\nwhich strictly generalizes two already well-known ones -- the\nBernays-Sch\\\"onfinkel-Ramsey (BSR) Fragment and the Monadic Fragment. The\ndefining principle is the syntactic separation of universally quantified\nvariables from existentially quantified ones at the level of atoms. Thus, our\nclassification neither rests on restrictions on quantifier prefixes (as in the\nBSR case) nor on restrictions on the arity of predicate symbols (as in the\nmonadic case). We demonstrate that the new fragment exhibits the finite model\nproperty and derive a non-elementary upper bound on the computing time required\nfor deciding satisfiability in the new fragment. For the subfragment of prenex\nsentences with the quantifier prefix $\\exists^* \\forall^* \\exists^*$ the\nsatisfiability problem is shown to be complete for NEXPTIME. Finally, we\ndiscuss how automated reasoning procedures can take advantage of our results.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 12:05:12 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 19:36:44 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 16:17:34 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Sturm", "Thomas", ""], ["Voigt", "Marco", ""], ["Weidenbach", "Christoph", ""]]}, {"id": "1511.09186", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "An Algebraic Approach for Approximity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison to traditionally accurate computing, approximate computing focuses\non the rapidity of the satisfactory solution, but not the unnecessary accuracy\nof the solution. Approximate bisimularity is the approximate one corresponding\nto traditionally accurate bisimilarity. Based on the work of distances between\nbasic processes, we propose an algebraic approach for distances between\nprocesses to support a whole process calculus CCS, which contains prefix, sum,\ncomposition, restriction, relabeling and recursion.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 07:46:43 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1511.09230", "submitter": "Robin Adams", "authors": "Robin Adams and Bart Jacobs", "title": "A Type Theory for Probabilistic and Bayesian Reasoning", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel type theory and logic for probabilistic\nreasoning. Its logic is quantitative, with fuzzy predicates. It includes\nnormalisation and conditioning of states. This conditioning uses a key aspect\nthat distinguishes our probabilistic type theory from quantum type theory,\nnamely the bijective correspondence between predicates and side-effect free\nactions (called instrument, or assert, maps). The paper shows how suitable\ncomputation rules can be derived from this predicate-action correspondence, and\nuses these rules for calculating conditional probabilities in two well-known\nexamples of Bayesian reasoning in (graphical) models. Our type theory may thus\nform the basis for a mechanisation of Bayesian inference.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 10:28:55 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Adams", "Robin", ""], ["Jacobs", "Bart", ""]]}, {"id": "1511.09324", "submitter": "Alejandro D\\'iaz-Caro", "authors": "Alejandro D\\'iaz-Caro and Pablo E. Mart\\'inez L\\'opez", "title": "Isomorphisms considered as equalities: Projecting functions and\n  enhancing partial application through and implementation of lambda+", "comments": "A prototype writen in Haskell can be found at\n  http://diaz-caro.web.unq.edu.ar/IsoAsEq-v1.0.tar.gz", "journal-ref": "ACM Proceedings of IFL'15(9), 2015", "doi": "10.1145/2897336.2897346", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an implementation of lambda+, a recently introduced simply typed\nlambda-calculus with pairs where isomorphic types are made equal. The rewrite\nsystem of lambda+ is a rewrite system modulo an equivalence relation, which\nmakes its implementation non-trivial. We also extend lambda+ with natural\nnumbers and general recursion and use Beki\\'c's theorem to split mutual\nrecursions. This splitting, together with the features of lambda+, allows for a\nnovel way of program transformation by reduction, by projecting a function\nbefore it is applied in order to simplify it. Also, currying together with the\nassociativity and commutativity of pairs gives an enhanced form of partial\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 14:30:29 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 16:33:07 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["D\u00edaz-Caro", "Alejandro", ""], ["L\u00f3pez", "Pablo E. Mart\u00ednez", ""]]}, {"id": "1511.09394", "submitter": "Ekaterina Komendantskaya Dr", "authors": "Peng Fu and Ekaterina Komendantskaya and Tom Schrijvers and Andrew\n  Pond", "title": "Proof Relevant Corecursive Resolution", "comments": "23 pages, with appendices in FLOPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolution lies at the foundation of both logic programming and type class\ncontext reduction in functional languages. Terminating derivations by\nresolution have well-defined inductive meaning, whereas some non-terminating\nderivations can be understood coinductively. Cycle detection is a popular\nmethod to capture a small subset of such derivations. We show that in fact\ncycle detection is a restricted form of coinductive proof, in which the atomic\nformula forming the cycle plays the role of coinductive hypothesis.\n  This paper introduces a heuristic method for obtaining richer coinductive\nhypotheses in the form of Horn formulas. Our approach subsumes cycle detection\nand gives coinductive meaning to a larger class of derivations. For this\npurpose we extend resolution with Horn formula resolvents and corecursive\nevidence generation. We illustrate our method on non-terminating type class\nresolution problems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 17:17:27 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Fu", "Peng", ""], ["Komendantskaya", "Ekaterina", ""], ["Schrijvers", "Tom", ""], ["Pond", "Andrew", ""]]}, {"id": "1511.09423", "submitter": "Zoltan Esik", "authors": "Arnaud Carayol, Zoltan Esik", "title": "An analysis of the equational properties of the well-founded fixed point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-founded fixed points have been used in several areas of knowledge\nrepresentation and reasoning and to give semantics to logic programs involving\nnegation. They are an important ingredient of approximation fixed point theory.\nWe study the logical properties of the (parametric) well-founded fixed point\noperation. We show that the operation satisfies several, but not all of the\nequational properties of fixed point operations described by the axioms of\niteration theories.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 18:37:25 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 07:45:13 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Carayol", "Arnaud", ""], ["Esik", "Zoltan", ""]]}, {"id": "1511.09450", "submitter": "EPTCS", "authors": "Leander Tentrup, Alexander Weinert, Martin Zimmermann", "title": "Approximating Optimal Bounds in Prompt-LTL Realizability in\n  Doubly-exponential Time", "comments": "In Proceedings GandALF 2016, arXiv:1609.03648", "journal-ref": "EPTCS 226, 2016, pp. 302-315", "doi": "10.4204/EPTCS.226.21", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization variant of the realizability problem for Prompt\nLinear Temporal Logic, an extension of Linear Temporal Logic (LTL) by the\nprompt eventually operator whose scope is bounded by some parameter. In the\nrealizability optimization problem, one is interested in computing the minimal\nsuch bound that allows to realize a given specification. It is known that this\nproblem is solvable in triply-exponential time, but not whether it can be done\nin doubly-exponential time, i.e., whether it is just as hard as solving LTL\nrealizability.\n  We take a step towards resolving this problem by showing that the optimum can\nbe approximated within a factor of two in doubly-exponential time. Also, we\nreport on a proof-of-concept implementation of the algorithm based on bounded\nLTL synthesis, which computes the smallest implementation of a given\nspecification. In our experiments, we observe a tradeoff between the size of\nthe implementation and the bound it realizes. We investigate this tradeoff in\nthe general case and prove upper bounds, which reduce the search space for the\nalgorithm, and matching lower bounds.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 20:07:18 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 14:04:50 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 01:00:46 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Tentrup", "Leander", ""], ["Weinert", "Alexander", ""], ["Zimmermann", "Martin", ""]]}]