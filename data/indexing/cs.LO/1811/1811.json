[{"id": "1811.00192", "submitter": "Umang Mathur", "authors": "Umang Mathur, P. Madhusudan, Mahesh Viswanathan", "title": "Decidable Verification of Uninterpreted Programs", "comments": null, "journal-ref": null, "doi": "10.1145/3290359", "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of completely automatically verifying uninterpreted\nprograms---programs that work over arbitrary data models that provide an\ninterpretation for the constants, functions and relations the program uses. The\nverification problem asks whether a given program satisfies a postcondition\nwritten using quantifier-free formulas with equality on the final state, with\nno loop invariants, contracts, etc. being provided. We show that this problem\nis undecidable in general. The main contribution of this paper is a subclass of\nprograms, called coherent programs that admits decidable verification, and can\nbe decided in PSPACE. We then extend this class of programs to classes of\nprograms that are $k$-coherent, where $k \\in \\mathbb{N}$, obtained by\n(automatically) adding $k$ ghost variables and assignments that make them\ncoherent. We also extend the decidability result to programs with recursive\nfunction calls and prove several undecidability results that show why our\nrestrictions to obtain decidability seem necessary.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:37:27 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 16:26:35 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 20:19:00 GMT"}, {"version": "v4", "created": "Wed, 26 Aug 2020 17:31:13 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Mathur", "Umang", ""], ["Madhusudan", "P.", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1811.00699", "submitter": "Zhilin Wu", "authors": "Chong Gao, Taolue Chen, Zhilin Wu", "title": "Separation Logic with Linearly Compositional Inductive Predicates and\n  Set Data Constraints", "comments": "31 pages, 2 figures, SOFSEM 2019, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We identify difference-bound set constraints (DBS), an analogy of\ndifference-bound arithmetic constraints for sets. DBS can express not only set\nconstraints but also arithmetic constraints over set elements. We integrate DBS\ninto separation logic with linearly compositional inductive predicates,\nobtaining a logic thereof where set data constraints of linear data structures\ncan be specified. We show that the satisfiability of this logic is decidable. A\ncrucial step of the decision procedure is to compute the transitive closure of\nDBS-definable set relations, to capture which we propose an extension of\nquantified set constraints with Presburger Arithmetic (RQSPA). The\nsatisfiability of RQSPA is then shown to be decidable by harnessing advanced\nautomata-theoretic techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 01:25:26 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Gao", "Chong", ""], ["Chen", "Taolue", ""], ["Wu", "Zhilin", ""]]}, {"id": "1811.00756", "submitter": "Uri Abraham", "authors": "Uri Abraham", "title": "On the Lazy Set object", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to employ the Lazy Set algorithm as an example for\na mathematical framework for proving the linearizability of distributed\nsystems. The proof in this approach is divided into two stages of lower and\nhigher abstraction level. At the higher level a list of \"axioms\" is formulated\nand a proof is given that any model theoretic structure that satisfies these\naxioms is linearizable. At this level the algorithm is not mentioned. At the\nlower level, a Simpler Lazy Set algorithm is described, and it is shown that\nany execution of this simpler algorithm generates a model of these axioms (and\nis therefore linearizable). Finally the linearization of the Lazy Set algorithm\nis obtained by proving that any of its executions has a {\\em reduct} that is an\nexecution of the Simpler algorithm. So the reduct executions are linearizable\nand this entails immediately linearizability of the Lazy Set algorithm itself.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 06:58:44 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Abraham", "Uri", ""]]}, {"id": "1811.00796", "submitter": "Mitsuru Kusumoto", "authors": "Mitsuru Kusumoto, Keisuke Yahata, Masahiro Sakai", "title": "Automated Theorem Proving in Intuitionistic Propositional Logic by Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem-solving in automated theorem proving (ATP) can be interpreted as\na search problem where the prover constructs a proof tree step by step. In this\npaper, we propose a deep reinforcement learning algorithm for proof search in\nintuitionistic propositional logic. The most significant challenge in the\napplication of deep learning to the ATP is the absence of large, public theorem\ndatabase. We, however, overcame this issue by applying a novel data\naugmentation procedure at each iteration of the reinforcement learning. We also\nimprove the efficiency of the algorithm by representing the syntactic structure\nof formulas by a novel compact graph representation. Using the large volume of\naugmented data, we train highly accurate graph neural networks that approximate\nthe value function for the set of the syntactic structures of formulas. Our\nmethod is also cost-efficient in terms of computational time. We will show that\nour prover outperforms Coq's $\\texttt{tauto}$ tactic, a prover based on\nhuman-engineered heuristics. Within the specified time limit, our prover solved\n84% of the theorems in a benchmark library, while $\\texttt{tauto}$ was able to\nsolve only 52%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 09:49:18 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Kusumoto", "Mitsuru", ""], ["Yahata", "Keisuke", ""], ["Sakai", "Masahiro", ""]]}, {"id": "1811.00970", "submitter": "Jakub Opr\\v{s}al", "authors": "Libor Barto, Jakub Bul\\'in, Andrei Krokhin, Jakub Opr\\v{s}al", "title": "Algebraic approach to promise constraint satisfaction", "comments": "Extended version (73 pages). Preliminary versions of parts of this\n  paper were published in the proceedings of STOC 2019 and LICS 2019", "journal-ref": "J. ACM 68, 4, Article 28 (July 2021), 66 pages", "doi": "10.1145/3457606", "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity and approximability of the constraint satisfaction problem\n(CSP) has been actively studied over the last 20 years. A new version of the\nCSP, the promise CSP (PCSP) has recently been proposed, motivated by open\nquestions about the approximability of variants of satisfiability and graph\ncolouring. The PCSP significantly extends the standard decision CSP. The\ncomplexity of CSPs with a fixed constraint language on a finite domain has\nrecently been fully classified, greatly guided by the algebraic approach, which\nuses polymorphisms --- high-dimensional symmetries of solution spaces --- to\nanalyse the complexity of problems. The corresponding classification for PCSPs\nis wide open and includes some long-standing open questions, such as the\ncomplexity of approximate graph colouring, as special cases.\n  The basic algebraic approach to PCSP was initiated by Brakensiek and\nGuruswami, and in this paper we significantly extend it and lift it from\nconcrete properties of polymorphisms to their abstract properties. We introduce\na new class of problems that can be viewed as algebraic versions of the (Gap)\nLabel Cover problem, and show that every PCSP with a fixed constraint language\nis equivalent to a problem of this form. This allows us to identify a \"measure\nof symmetry\" that is well suited for comparing and relating the complexity of\ndifferent PCSPs via the algebraic approach. We demonstrate how our theory can\nbe applied by improving the state-of-the-art in approximate graph colouring: we\nshow that, for any $k\\geq 3$, it is NP-hard to find a $(2k-1)$-colouring of a\ngiven $k$-colourable graph.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:35:36 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 14:38:43 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 11:42:46 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Barto", "Libor", ""], ["Bul\u00edn", "Jakub", ""], ["Krokhin", "Andrei", ""], ["Opr\u0161al", "Jakub", ""]]}, {"id": "1811.01014", "submitter": "Abhisekh Sankaran", "authors": "Abhisekh Sankaran", "title": "A Generalization of the {\\L}o\\'s-Tarski Preservation Theorem -\n  Dissertation Summary", "comments": "26 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a summary of the author's Ph.D. dissertation\n(arXiv:1609.06297). In addition to an overview of notions and results, it also\nprovides sketches of various proofs and simplified presentations of certain\nabstract results of the dissertation, that concern tree representations of\nstructures. Further, some extensions of the dissertation results are presented.\nThese include the connections of the model-theoretic notions introduced in the\nthesis with fixed parameter tractability and notions in the structure theory of\nsparse graph classes. The constructive aspects of the proofs of the\nmodel-theoretic results of the dissertation are used to obtain (algorithmic)\nmeta-kernels for various dense graphs such as graphs of bounded clique-width\nand subclasses of these like $m$-partite cographs and graph classes of bounded\nshrub-depth. Finally, the article presents updated definitions and results\nconcerning the notion of logical fractals which is a generalization of the\nEquivalent Bounded Substructure Property from the dissertation. In particular,\nour results show that (natural finitary adaptations of) both the upward and\ndownward versions of the L\\\"owenheim-Skolem theorem from classical model theory\ncan be recovered in a variety of algorithmically interesting settings, and\nfurther in most cases, in effective form and even for logics beyond first order\nlogic.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:58:14 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Sankaran", "Abhisekh", ""]]}, {"id": "1811.01070", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "Truly Concurrent Process Algebra to Unifying Quantum and Classical\n  Computing", "comments": "141 pages, 23 figures, 57 tables. arXiv admin note: substantial text\n  overlap with arXiv:1611.09035, arXiv:1610.02500, arXiv:1810.00868,\n  arXiv:1311.2960, arXiv:1501.05260, arXiv:1404.0665, arXiv:1507.03344", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truly concurrent process algebras are generalizations to the traditional\nprocess algebras for true concurrency, CTC to CCS, APTC to ACP, $\\pi_{tc}$ to\n$\\pi$ calculus, APPTC to probabilistic process algebra. Now, it is the time to\nutilize truly concurrent process algebras APTC and APPTC to model quantum\ncomputing and unify quantum and classical computing. In this book, we introduce\nthe preliminaries, the utilization of APTC to unify quantum and classical\ncomputing and its usage in verification of quantum communication protocols, the\nutilization of APPTC to unifying quantum and classical computing and its usage\nin verification of quantum communication protocols.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 20:07:55 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 15:40:16 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 01:07:28 GMT"}, {"version": "v4", "created": "Wed, 21 Jul 2021 13:19:53 GMT"}, {"version": "v5", "created": "Wed, 28 Jul 2021 06:17:10 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1811.01144", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "The IFF Approach to the Lattice of Theories", "comments": "April 30, 2003, The Information Flow Framework is reachable at\n  http://web.archive.org/web/20121008145548, http://suo.ieee.org/IFF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IFF approach for the notion of \"lattice of theories\" uses the idea of a\nconcept lattice from Formal Concept Analysis (Ganter and Wille) and the idea of\nthe truth classification from Information Flow (Barwise and Seligman). The IFF\napproach is concentrated in the joining of these two important ideas. The\nresult is called the truth concept lattice, the concept lattice of the truth\nclassification. The IFF provides a principled (versus ad hoc) approach for John\nSowa's \"lattice of theories\" framework. The \"lattice of theories\" is\nrepresented by the truth concept lattice, each theory in the lattice is\nrepresented by a formal concept in the truth concept lattice.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 02:35:03 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1811.01318", "submitter": "Aaron Stump", "authors": "Aaron Stump", "title": "Syntax and Typing for Cedille Core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document specifies a core version of the type theory implemented in the\nCedille tool. Cedille is a language for dependently typed programming and\ncomputer-checked proof. Cedille can elaborate source programs down to Cedille\nCore, which can be checked in a straightforward way by a small checker (a\nreference implementation included with Cedille is under 1000 lines of Haskell).\nOther tools could also target Cedille Core as an expressive backend type\ntheory. The document describes syntax and typing rules for Cedille Core.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 03:49:51 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Stump", "Aaron", ""]]}, {"id": "1811.01351", "submitter": "Albert Atserias", "authors": "Albert Atserias and Tuomas Hakoniemi", "title": "Size-Degree Trade-Offs for Sums-of-Squares and Positivstellensatz Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that if a system of degree-$k$ polynomial constraints on~$n$ Boolean\nvariables has a Sums-of-Squares (SOS) proof of unsatisfiability with at\nmost~$s$ many monomials, then it also has one whose degree is of the order of\nthe square root of~$n \\log s$ plus~$k$. A similar statement holds for the more\ngeneral Positivstellensatz (PS) proofs. This establishes size-degree trade-offs\nfor SOS and PS that match their analogues for weaker proof systems such as\nResolution, Polynomial Calculus, and the proof systems for the LP and SDP\nhierarchies of Lov\\'asz and Schrijver. As a corollary to this, and to the known\ndegree lower bounds, we get optimal integrality gaps for exponential size SOS\nproofs for sparse random instances of the standard NP-hard constraint\noptimization problems. We also get exponential size SOS lower bounds for\nTseitin and Knapsack formulas. The proof of our main result relies on a\nzero-gap duality theorem for pre-ordered vector spaces that admit an order\nunit, whose specialization to PS and SOS may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 11:45:21 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 14:49:15 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Atserias", "Albert", ""], ["Hakoniemi", "Tuomas", ""]]}, {"id": "1811.01590", "submitter": "Andreas Achen", "authors": "Andreas Achen", "title": "Putting the Agents back in the Domain: A Two-Sorted Term-Modal Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MA math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper syntax and semantics will be presented for an expansion\nof ordinary n-agent QML with constant domain, non-rigid constants, rigid\nvariables and including both functions, relations, and equality. Further, the\nnumber of agents will be specified axiomatically thus ensuring maximal\nflexibility wrt. the cardinality of the set of agents. Domain, variables, and\nconstants will be partitioned in an agent-part and an object-part and the\nsyntax will be expanded to include strings in which indexes of modal operators\nare quantified over as wff's of the language. This will enhance expressiveness\nregarding the epistemic status of agents. Such a term-modal version of the\nlogic K is shown to be sound and complete wrt. the class of (appropriate)\nframes, and a term-version of S4 is shown to be sound and complete wrt. the\nclass of (appropriate) frames in which the relations are transitive. It should\nbe noted that completeness is shown via the framework of canonical models and\nthus allows for non-complicated generalizations to other logics than the\nterm-versions of K and S4.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:29:28 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Achen", "Andreas", ""]]}, {"id": "1811.01692", "submitter": "Carmine Dodaro", "authors": "Carmine Dodaro and Francesco Ricca", "title": "The External Interface for Extending WASP", "comments": "22 pages, 1 figure, Under consideration in Theory and Practice of\n  Logic Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 20 (2020) 225-248", "doi": "10.1017/S1471068418000558", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer set programming (ASP) is a successful declarative formalism for\nknowledge representation and reasoning. The evaluation of ASP programs is\nnowadays based on the Conflict-Driven Clause Learning (CDCL) backtracking\nsearch algorithm. Recent work suggested that the performance of CDCL-based\nimplementations can be considerably improved on specific benchmarks by\nextending their solving capabilities with custom heuristics and propagators.\nHowever, embedding such algorithms into existing systems requires expert\nknowledge of the internals of ASP implementations. The development of effective\nsolver extensions can be made easier by providing suitable programming\ninterfaces. In this paper, we present the interface for extending the\nCDCL-based ASP solver WASP. The interface is both general, i.e. it can be used\nfor providing either new branching heuristics and propagators, and external,\ni.e. the implementation of new algorithms requires no internal modifications of\nWASP. Moreover, we review the applications of the interface witnessing it can\nbe successfully used to extend WASP for solving effectively hard instances of\nboth real-world and synthetic problems. Under consideration in Theory and\nPractice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:04:48 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 09:58:31 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Dodaro", "Carmine", ""], ["Ricca", "Francesco", ""]]}, {"id": "1811.01712", "submitter": "Marcel Jackson G", "authors": "Marcel Jackson and Szabolcs Mikulas", "title": "Domain and range for angelic and demonic compositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give finite axiomatizations for the varieties generated by representable\ndomain--range algebras when the semigroup operation is interpreted as angelic\nor demonic composition, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 21:27:17 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jackson", "Marcel", ""], ["Mikulas", "Szabolcs", ""]]}, {"id": "1811.01942", "submitter": "Yehia Abd Alrahman", "authors": "Yehia Abd Alrahman and Hugo Torres Vieira", "title": "Operation Control Protocols in Power Distribution Grids", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future power distribution grids will comprise a large number of components,\neach potentially able to carry out operations autonomously. Clearly, in order\nto ensure safe operation of the grid, individual operations must be coordinated\namong the different components. Since operation safety is a global property,\nmodelling component coordination typically involves reasoning about systems at\na global level. In this paper, we propose a language for specifying grid\noperation control protocols from a global point of view. We show how such\nglobal specifications can be used to automatically generate local controllers\nof individual components, and that the distributed implementation yielded by\nsuch controllers operationally corresponds to the global specification. We\nshowcase our development by modelling a fault management scenario in power\ngrids.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 21:02:12 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Alrahman", "Yehia Abd", ""], ["Vieira", "Hugo Torres", ""]]}, {"id": "1811.02005", "submitter": "EPTCS", "authors": "Rob Sumners (Centaur Technology)", "title": "A Toolbox For Property Checking From Simulation Using Incremental SAT\n  (Extended Abstract)", "comments": "In Proceedings ACL2 2018, arXiv:1810.03762", "journal-ref": "EPTCS 280, 2018, pp. 95-97", "doi": "10.4204/EPTCS.280.7", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tool that primarily supports the ability to check bounded\nproperties starting from a sequence of states in a run. The target design is\ncompiled into an AIGNET which is then selectively and iteratively translated\ninto an incremental SAT instance in which clauses are added for new terms and\nsimplified by the assignment of existing literals. Additional applications of\nthe tool can be derived by the user providing alternative attachments of\nconstrained functions which guide the iterations and SAT checks performed. Some\nVerilog RTL examples are included for reference.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 00:36:40 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Sumners", "Rob", "", "Centaur Technology"]]}, {"id": "1811.02041", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "Conceptua: Institutions in a Topos", "comments": "32 pages, 8 figures, preprint 2005", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tarski's semantic definition of truth is the composition of its extensional\nand intensional aspects. Abstract satisfaction, the core of the semantic\ndefinition of truth, is the basis for the theory of institutions (Goguen and\nBurstall). The satisfaction relation for first order languages (the truth\nclassification), and the preservation of truth by first order interpretations\n(the truth infomorphism), form a key motivating example in the theory of\nInformation Flow (IF) (Barwise and Seligman). The concept lattice notion, which\nis the central structure studied by the theory of Formal Concept Analysis (FCA)\n(Ganter and Wille), is constructed by the polar factorization of derivation.\nThe study of classification structures (IF) and the study of conceptual\nstructures (FCA) provide a principled foundation for the logical theory of\nknowledge representation and organization. In an effort to unify these two\nareas, the paper \"Distributed Conceptual Structures\" (Kent arXiv:1810.04774)\nabstracted the basic theorem of FCA in order to established three levels of\ncategorical equivalence between classification structures and conceptual\nstructures. In this paper, we refine this approach by resolving the equivalence\nas the category-theoretic factorization of the Galois connection of derivation.\nThe equivalence between classification and conceptual structures is mediated by\nthe opposite motions of factorization and composition. Abstract truth factors\nthrough the concept lattice of theories in terms of its extensional and\nintensional aspects.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 21:35:00 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1811.02133", "submitter": "Thorsten Wissmann", "authors": "Naoki Kobayashi, Ugo Dal Lago, Charles Grellois", "title": "On the Termination Problem for Probabilistic Higher-Order Recursive\n  Programs", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (October\n  2, 2020) lmcs:6817", "doi": "10.23638/LMCS-16(4:2)2020", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last two decades, there has been much progress on model checking of\nboth probabilistic systems and higher-order programs. In spite of the emergence\nof higher-order probabilistic programming languages, not much has been done to\ncombine those two approaches. In this paper, we initiate a study on the\nprobabilistic higher-order model checking problem, by giving some first\ntheoretical and experimental results. As a first step towards our goal, we\nintroduce PHORS, a probabilistic extension of higher-order recursion schemes\n(HORS), as a model of probabilistic higher-order programs. The model of PHORS\nmay alternatively be viewed as a higher-order extension of recursive Markov\nchains. We then investigate the probabilistic termination problem -- or,\nequivalently, the probabilistic reachability problem. We prove that almost sure\ntermination of order-2 PHORS is undecidable. We also provide a fixpoint\ncharacterization of the termination probability of PHORS, and develop a sound\n(but possibly incomplete) procedure for approximately computing the termination\nprobability. We have implemented the procedure for order-2 PHORSs, and\nconfirmed that the procedure works well through preliminary experiments that\nare reported at the end of the article.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 02:40:29 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 06:06:36 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 05:14:43 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2020 17:01:36 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kobayashi", "Naoki", ""], ["Lago", "Ugo Dal", ""], ["Grellois", "Charles", ""]]}, {"id": "1811.02209", "submitter": "Marco Peressotti", "authors": "Wen Kokke, Fabrizio Montesi, Marco Peressotti", "title": "Better Late Than Never: A Fully Abstract Semantics for Classical\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Hypersequent Classical Processes (HCP), a revised interpretation\nof the \"Proofs as Processes\" correspondence between linear logic and the\n{\\pi}-calculus initially proposed by Abramsky [1994], and later developed by\nBellin and Scott [1994], Caires and Pfenning [2010], and Wadler [2014], among\nothers. HCP mends the discrepancies between linear logic and the syntax and\nobservable semantics of parallel composition in the {\\pi}-calculus, by\nconservatively extending linear logic to hyperenvironments (collections of\nenvironments, inspired by the hypersequents by Avron [1991]). Separation of\nenvironments in hyperenvironments is internalised by $\\otimes$ and corresponds\nto parallel process behaviour. Thanks to this property, for the first time we\nare able to extract a labelled transition system (lts) semantics from proof\nrewritings. Leveraging the information on parallelism at the level of types, we\nobtain a logical reconstruction of the delayed actions that Merro and Sangiorgi\n[2004] formulated to model non-blocking I/O in the {\\pi}-calculus. We define a\ndenotational semantics for processes based on Brzozowski derivatives, and\nuncover that non-interference in HCP corresponds to Fubini's theorem of double\nantiderivation. Having an lts allows us to validate HCP using the standard\ntoolbox of behavioural theory. We instantiate bisimilarity and barbed\ncongruence for HCP, and obtain a full abstraction result: bisimilarity,\ndenotational equivalence, and barbed congruence coincide.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 07:56:06 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Kokke", "Wen", ""], ["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""]]}, {"id": "1811.02446", "submitter": "Pavel Naumov", "authors": "Pavel Naumov and Jia Tao", "title": "Knowledge and Blameworthiness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blameworthiness of an agent or a coalition of agents is often defined in\nterms of the principle of alternative possibilities: for the coalition to be\nresponsible for an outcome, the outcome must take place and the coalition\nshould have had a strategy to prevent it. In this article we argue that in the\nsettings with imperfect information, not only should the coalition have had a\nstrategy, but it also should have known that it had a strategy, and it should\nhave known what the strategy was. The main technical result of the article is a\nsound and complete bimodal logic that describes the interplay between knowledge\nand blameworthiness in strategic games with imperfect information.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:02:31 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 15:32:25 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 13:15:15 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Naumov", "Pavel", ""], ["Tao", "Jia", ""]]}, {"id": "1811.02478", "submitter": "Ren\\'e Vestergaard", "authors": "Ren\\'e Vestergaard and Emmanuel Pietriga", "title": "Proofs of life: molecular-biology reasoning simulates cell behaviors\n  from first principles", "comments": "37 pages, including 9 figures, plus 244 pages of supplementary\n  information", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We axiomatize the molecular-biology reasoning style, show compliance of the\nstandard reference: Ptashne, A Genetic Switch, and present proof-theory-induced\ntechnologies to help infer phenotypes and to predict life cycles from\ngenotypes. The key is to note that `reductionist discipline' entails\nconstructive reasoning: any proof of a compound property can be decomposed to\nproofs of constituent properties. Proof theory makes explicit the inner\nstructure of the axiomatized reasoning style and allows the permissible\ndynamics to be presented as a mode of computation that can be executed and\nanalyzed. Constructivity and execution guarantee simulation when working over\ndomain-specific languages. Here, we exhibit phenotype properties for genotype\nreasons: a molecular-biology argument is an open-system concurrent computation\nthat results in compartment changes and is performed among processes of\nphysiology change as determined from the molecular programming of given DNA.\nLife cycles are the possible sequentializations of the processes. A main\nimplication of our construction is that formal correctness provides a\ncomplementary perspective on science that is as fundamental there as for pure\nmathematics. The bulk of the presented work has been verified formally correct\nby computer.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 11:29:45 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 12:46:36 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 09:00:35 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Vestergaard", "Ren\u00e9", ""], ["Pietriga", "Emmanuel", ""]]}, {"id": "1811.02536", "submitter": "Ross Horne", "authors": "Ross Horne", "title": "A Bisimilarity Congruence for the Applied pi-Calculus Sufficiently\n  Coarse to Verify Privacy Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is the first thorough investigation into the coarsest notion of\nbisimilarity for the applied pi-calculus that is a congruence relation: open\nbarbed bisimilarity. An open variant of labelled bisimilarity (quasi-open\nbisimilarity), better suited to constructing bisimulations, is proven to\ncoincide with open barbed bisimilarity. These bisimilary congruences are shown\nto be characterised by an intuitionistic modal logic that can be used, for\nexample, to describe an attack on privacy whenever a privacy property is\nviolated. Open barbed bisimilarity provides a compositional approach to\nverifying cryptographic protocols, since properties proven can be reused in any\ncontext, including under input prefix. Furthermore, open barbed bisimilarity is\nsufficiently coarse for reasoning about security and privacy properties of\ncryptographic protocols; in constrast to the finer bisimilarity congruence,\nopen bisimilarity, which cannot verify certain privacy properties.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:12:02 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Horne", "Ross", ""]]}, {"id": "1811.02710", "submitter": "Richard Garner", "authors": "Richard Garner", "title": "Hypernormalisation, linear exponential monads and the Giry tricocycloid", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new categorical perspectives on Jacobs' notion of\nhypernormalisation of sub-probability distributions. In particular, we show\nthat a suitable general framework for notions of hypernormalisation is that of\na symmetric monoidal category endowed with a linear exponential monad---a\nnotion arising in the categorical semantics of Girard's linear logic.\n  We show that Jacobs' original notion of hypernormalisation arises in this way\nfrom the finitely supported probability measure monad on the category of sets,\nwhich can be seen as a linear exponential monad with respect to a monoidal\nstructure on sets arising from a quantum-algebraic object which we term the\nGiry tricocycloid. We give many other examples of hypernormalisation arising\nfrom other linear exponential monads.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 00:50:49 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Garner", "Richard", ""]]}, {"id": "1811.02835", "submitter": "Andrei Arusoaie", "authors": "Andrei Arusoaie and Dorel Lucanu", "title": "Unification in Matching Logic - Extended Version", "comments": "22 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching Logic is a framework for specifying programming language semantics\nand reasoning about programs. Its formulas are called patterns and are built\nwith variables, symbols, connectives and quantifiers. A pattern is a\ncombination of structural components (term patterns), which must be matched,\nand constraints (predicate patterns), which must be satisfied. Dealing with\nmore than one structural component in a pattern could be cumbersome because it\ninvolves multiple matching operations. A source for getting patterns with many\nstructural components is the conjunction of patterns. Here, we propose a method\nthat uses a syntactic unification algorithm to transform conjunctions of\nstructural patterns into equivalent patterns having only one structural\ncomponent and some additional constraints. We prove the soundness of our\napproach, we discuss why the approach is not complete and we provide sound\nstrategies to generate certificates for the equivalences, validated using Coq.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 11:12:44 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 05:53:09 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 13:24:28 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Arusoaie", "Andrei", ""], ["Lucanu", "Dorel", ""]]}, {"id": "1811.03167", "submitter": "Matthew Hague", "authors": "Taolue Chen and Matthew Hague and Anthony W. Lin and Philipp R\\\"ummer\n  and Zhilin Wu", "title": "Decision Procedures for Path Feasibility of String-Manipulating Programs\n  with Complex Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design and implementation of decision procedures for checking path\nfeasibility in string-manipulating programs is an important problem, whose\napplications include symbolic execution and automated detection of cross-site\nscripting (XSS) vulnerabilities. A (symbolic) path is a finite sequence of\nassignments and assertions (i.e. without loops), and checking its feasibility\namounts to determining the existence of inputs that yield a successful\nexecution.\n  We give two general semantic conditions which together ensure the\ndecidability of path feasibility: (1) each assertion admits regular monadic\ndecomposition, and (2) each assignment uses a (possibly nondeterministic)\nfunction whose inverse relation preserves regularity. We show these conditions\nare expressive since they are satisfied by a multitude of string operations.\nThey also strictly subsume existing decidable string theories, and most\nexisting benchmarks (e.g. most of Kaluza's, and all of SLOG's, Stranger's, and\nSLOTH's). We give a simple decision procedure and an extensible architecture of\na string solver in that a user may easily incorporate his/her own string\nfunctions. We show the general fragment has a tight, but high complexity. To\naddress this, we propose to allow only partial string functions (i.e., prohibit\nnondeterminism) in condition (2). When nondeterministic functions are needed,\nwe also provide a syntactic fragment that provides a support of\nnondeterministic functions but can be reduced to an existing solver SLOTH.\n  We provide an efficient implementation of our decision procedure for\ndeterministic partial string functions in a new string solver OSTRICH. It\nprovides built-in support for concatenation, reverse, functional transducers,\nand replaceall and provides a framework for extensibility to support further\nstring functions. We demonstrate the efficacy of our new solver against other\ncompetitive solvers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 22:13:04 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Chen", "Taolue", ""], ["Hague", "Matthew", ""], ["Lin", "Anthony W.", ""], ["R\u00fcmmer", "Philipp", ""], ["Wu", "Zhilin", ""]]}, {"id": "1811.03176", "submitter": "Jianwen Li", "authors": "Jianwen Li, Kristin Y. Rozier, Geguang Pu, Yueling Zhang, Moshe Y.\n  Vardi", "title": "SAT-based Explicit LTLf Satisfiability Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a SAT-based framework for LTLf (Linear Temporal Logic on\nFinite Traces) satisfiability checking. We use propositional SAT-solving\ntechniques to construct a transition system for the input LTLf formula;\nsatisfiability checking is then reduced to a path-search problem over this\ntransition system. Furthermore, we introduce CDLSC (Conflict-Driven LTLf\nSatisfiability Checking), a novel algorithm that leverages information produced\nby propositional SAT solvers from both satisfiability and unsatisfiability\nresults. Experimental evaluations show that CDLSC outperforms all other\nexisting approaches for LTLf satisfiability checking, by demonstrating an\napproximate four-fold speedup compared to the second-best solver.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 22:58:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Li", "Jianwen", ""], ["Rozier", "Kristin Y.", ""], ["Pu", "Geguang", ""], ["Zhang", "Yueling", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1811.03276", "submitter": "EPTCS", "authors": "Gijs Wijnholds (Queen Mary University of London), Mehrnoosh Sadrzadeh\n  (Queen Mary University of London)", "title": "Classical Copying versus Quantum Entanglement in Natural Language: The\n  Case of VP-ellipsis", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 103-119", "doi": "10.4204/EPTCS.283.8", "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares classical copying and quantum entanglement in natural\nlanguage by considering the case of verb phrase (VP) ellipsis. VP ellipsis is a\nnon-linear linguistic phenomenon that requires the reuse of resources, making\nit the ideal test case for a comparative study of different copying behaviours\nin compositional models of natural language. Following the line of research in\ncompositional distributional semantics set out by (Coecke et al., 2010) we\ndevelop an extension of the Lambek calculus which admits a controlled form of\ncontraction to deal with the copying of linguistic resources. We then develop\ntwo different compositional models of distributional meaning for this calculus.\nIn the first model, we follow the categorical approach of (Coecke et al., 2013)\nin which a functorial passage sends the proofs of the grammar to linear maps on\nvector spaces and we use Frobenius algebras to allow for copying. In the second\ncase, we follow the more traditional approach that one finds in categorial\ngrammars, whereby an intermediate step interprets proofs as non-linear lambda\nterms, using multiple variable occurrences that model classical copying. As a\ncase study, we apply the models to derive different readings of ambiguous\nelliptical phrases and compare the analyses that each model provides.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:12:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Wijnholds", "Gijs", "", "Queen Mary University of London"], ["Sadrzadeh", "Mehrnoosh", "", "Queen Mary University of London"]]}, {"id": "1811.03606", "submitter": "Jurriaan Rot", "authors": "Filippo Bonchi, Tom van Bussel, Matias David Lee, Jurriaan Rot", "title": "Bisimilarity of Open Terms in Stream GSOS", "comments": null, "journal-ref": "Science of Computer Programming, Volume 172, pages 1-26, 2019,\n  Elsevier", "doi": "10.1016/j.scico.2018.10.007", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream GSOS is a specification format for operations and calculi on infinite\nsequences. The notion of bisimilarity provides a canonical proof technique for\nequivalence of closed terms in such specifications. In this paper, we focus on\nopen terms, which may contain variables, and which are equivalent whenever they\ndenote the same stream for every possible instantiation of the variables. Our\nmain contribution is to capture equivalence of open terms as bisimilarity on\ncertain Mealy machines, providing a concrete proof technique. Moreover, we\nintroduce an enhancement of this technique, called bisimulation up-to\nsubstitutions, and show how to combine it with other up-to techniques to obtain\na powerful method for proving equivalence of open terms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:40:39 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 15:13:09 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Bonchi", "Filippo", ""], ["van Bussel", "Tom", ""], ["Lee", "Matias David", ""], ["Rot", "Jurriaan", ""]]}, {"id": "1811.03678", "submitter": "Jacques Carette", "authors": "Jacques Carette, Roshan P. James, Amr Sabry", "title": "Embracing the Laws of Physics: Three Reversible Models of Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main models of computation (the Turing Machine and the RAM) make\nfundamental assumptions about which primitive operations are realizable. The\nconsensus is that these include logical operations like conjunction,\ndisjunction and negation, as well as reading and writing to memory locations.\nThis perspective conforms to a macro-level view of physics and indeed these\noperations are realizable using macro-level devices involving thousands of\nelectrons. This point of view is however incompatible with quantum mechanics,\nor even elementary thermodynamics, as both imply that information is a\nconserved quantity of physical processes, and hence of primitive computational\noperations.\n  Our aim is to re-develop foundational computational models that embraces the\nprinciple of conservation of information. We first define what conservation of\ninformation means in a computational setting. We emphasize that computations\nmust be reversible transformations on data. One can think of data as modeled\nusing topological spaces and programs as modeled by reversible deformations. We\nillustrate this idea using three notions of data. The first assumes\nunstructured finite data, i.e., discrete topological spaces. The corresponding\nnotion of reversible computation is that of permutations. We then consider a\nstructured notion of data based on the Curry-Howard correspondence; here\nreversible deformations, as a programming language for witnessing type\nisomorphisms, comes from proof terms for commutative semirings. We then \"move\nup a level\" to treat programs as data. The corresponding notion of reversible\nprograms equivalences comes from the \"higher dimensional\" analog to commutative\nsemirings: symmetric rig groupoids. The coherence laws for these are exactly\nthe program equivalences we seek.\n  We conclude with some generalizations inspired by homotopy type theory and\nsurvey directions for further research.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 21:03:18 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:22:25 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Carette", "Jacques", ""], ["James", "Roshan P.", ""], ["Sabry", "Amr", ""]]}, {"id": "1811.04196", "submitter": "Ohad Kammar", "authors": "Matthijs V\\'ak\\'ar, Ohad Kammar, and Sam Staton", "title": "A Domain Theory for Statistical Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We give an adequate denotational semantics for languages with recursive\nhigher-order types, continuous probability distributions, and soft constraints.\nThese are expressive languages for building Bayesian models of the kinds used\nin computational statistics and machine learning. Among them are untyped\nlanguages, similar to Church and WebPPL, because our semantics allows recursive\nmixed-variance datatypes. Our semantics justifies important program\nequivalences including commutativity.\n  Our new semantic model is based on `quasi-Borel predomains'. These are a\nmixture of chain-complete partial orders (cpos) and quasi-Borel spaces.\nQuasi-Borel spaces are a recent model of probability theory that focuses on\nsets of admissible random elements. Probability is traditionally treated in cpo\nmodels using probabilistic powerdomains, but these are not known to be\ncommutative on any class of cpos with higher order functions. By contrast,\nquasi-Borel predomains do support both a commutative probabilistic powerdomain\nand higher-order functions. As we show, quasi-Borel predomains form both a\nmodel of Fiore's axiomatic domain theory and a model of Kock's synthetic\nmeasure theory.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 05:09:54 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 17:09:32 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["V\u00e1k\u00e1r", "Matthijs", ""], ["Kammar", "Ohad", ""], ["Staton", "Sam", ""]]}, {"id": "1811.04313", "submitter": "Iddo Tzameret", "authors": "Iddo Tzameret, Stephen A. Cook", "title": "Uniform, Integral and Feasible Proofs for the Determinant Identities", "comments": "76 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to provide weak as possible axiomatic assumptions in which one can\ndevelop basic linear algebra, we give a uniform and integral version of the\nshort propositional proofs for the determinant identities demonstrated over\n$GF(2)$ in Hrubes-Tzameret [SICOMP'15]. Specifically, we show that the\nmultiplicativity of the determinant function and the Cayley-Hamilton theorem\nover the integers are provable in the bounded arithmetic theory\n$\\mathbf{VNC}^2$; the latter is a first-order theory corresponding to the\ncomplexity class $\\mathbf{NC}^2$ consisting of problems solvable by uniform\nfamilies of polynomial-size circuits and $O(\\log ^2 n)$-depth. This also\nestablishes the existence of uniform polynomial-size $\\mathbf{NC}^2$-Frege\nproofs of the basic determinant identities over the integers (previous\npropositional proofs hold only over the two element field).\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 20:49:38 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Tzameret", "Iddo", ""], ["Cook", "Stephen A.", ""]]}, {"id": "1811.04566", "submitter": "Manoj Raut", "authors": "Manoj K. Raut", "title": "An Algorithm for Computing Prime Implicates in Modal Logic Using\n  Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have proposed an algorithm for computing prime implicates of\na modal formula in $\\mathbf{K}$ using resolution method suggested in\n\\cite{Enjalbert}. The algorithm suggested in this paper takes polynomial times\nexponential time ,i.e, $O(n^{2k}\\times 2^{n})$ to compute prime implicates\nwhereas Binevenu's algorithm \\cite{Bienvenu} takes doubly exponential time to\ncompute prime implicates. We have also proved its correctness.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 05:49:09 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 10:51:41 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Raut", "Manoj K.", ""]]}, {"id": "1811.04800", "submitter": "Michael Morak", "authors": "Wolfgang Faber, Michael Morak, Stefan Woltran", "title": "Strong Equivalence for Epistemic Logic Programs Made Easy (Extended\n  Version)", "comments": "Long version of paper published at AAAI'19, extended with full proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Epistemic Logic Programs (ELPs), that is, Answer Set Programming (ASP)\nextended with epistemic operators, have received renewed interest in recent\nyears, which led to a flurry of new research, as well as efficient solvers. An\nimportant question is under which conditions a sub-program can be replaced by\nanother one without changing the meaning, in any context. This problem is known\nas strong equivalence, and is well-studied for ASP. For ELPs, this question has\nbeen approached by embedding them into epistemic extensions of equilibrium\nlogics. In this paper, we consider a simpler, more direct characterization that\nis directly applicable to the language used in state-of-the-art ELP solvers.\nThis also allows us to give tight complexity bounds, showing that strong\nequivalence for ELPs remains coNP-complete, as for ASP. We further use our\nresults to provide syntactic characterizations for tautological rules and rule\nsubsumption for ELPs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:36:39 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Faber", "Wolfgang", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1811.04801", "submitter": "Oleg Verbitsky", "authors": "V. Arvind, Frank Fuhlbr\\\"uck, Johannes K\\\"obler, Oleg Verbitsky", "title": "On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph\n  Properties", "comments": "The results on fractional graph parameters are excluded from this\n  version and will appear as a separate paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) is a fruitful\napproach to the Graph Isomorphism problem. 2-WL corresponds to the original\nalgorithm suggested by Weisfeiler and Leman over 50 years ago. 1-WL is the\nclassical color refinement routine. Indistinguishability by $k$-WL is an\nequivalence relation on graphs that is of fundamental importance for\nisomorphism testing, descriptive complexity theory, and graph similarity\ntesting which is also of some relevance in artificial intelligence. Focusing on\ndimensions $k=1,2$, we investigate subgraph patterns whose counts are $k$-WL\ninvariant, and whose occurrence is $k$-WL invariant. We achieve a complete\ndescription of all such patterns for dimension $k=1$ and considerably extend\nthe previous results known for $k=2$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:44:43 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 07:45:21 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 12:47:29 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Arvind", "V.", ""], ["Fuhlbr\u00fcck", "Frank", ""], ["K\u00f6bler", "Johannes", ""], ["Verbitsky", "Oleg", ""]]}, {"id": "1811.04826", "submitter": "Tajana Ban Kirigin", "authors": "Max Kanovich, Tajana Ban Kirigin, Vivek Nigam, Andre Scedrov and\n  Carolyn Talcott", "title": "Compliance in Real Time Multiset Rewriting Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of compliance in Multiset Rewriting Models (MSR) has been\nintroduced for untimed models and for models with discrete time. In this paper\nwe revisit the notion of compliance and adapt it to fit with additional\nnondeterminism specific for dense time domains. Existing MSR with dense time\nare extended with critical configurations and non-critical traces, that is,\ntraces involving no critical configurations. Complexity of related {\\em\nnon-critical reachability problem} is investigated. Although this problem is\nundecidable in general, we prove that for balanced MSR with dense time the\nnon-critical reachability problem is PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 16:11:45 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kanovich", "Max", ""], ["Kirigin", "Tajana Ban", ""], ["Nigam", "Vivek", ""], ["Scedrov", "Andre", ""], ["Talcott", "Carolyn", ""]]}, {"id": "1811.05056", "submitter": "Matthew Moore", "authors": "Matthew Moore", "title": "Finite degree clones are undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clone of functions on a finite domain determines and is determined by its\nsystem of invariant relations (=predicates). When a clone is determined by a\nfinite number of relations, we say that the clone is of finite degree. For each\nMinsky Machine $\\mathcal{M}$ we associate a finitely generated clone\n$\\mathcal{C}$ such that $\\mathcal{C}$ has finite degree if and only if\n$\\mathcal{M}$ halts, thus proving that deciding whether a given clone has\nfinite degree is impossible.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 00:30:26 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 17:26:32 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 00:58:42 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 22:41:25 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Moore", "Matthew", ""]]}, {"id": "1811.05074", "submitter": "Dazhu Li", "authors": "Dazhu Li", "title": "Losing Connection:the Modal Logic of Definable Link Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we start with a two-player game that models communication\nunder adverse circumstances in everyday life and study it from the perspective\nof a modal logic of graphs, where links can be deleted locally according to\ndefinitions available to the adversarial player. We first introduce a new\nlanguage, semantics, and some typical validities. We then formulate a new type\nof first-order translation for this modal logic and prove its correctness.\nThen, a novel notion of bisimulation is proposed which leads to a\ncharacterization theorem for the logic as a fragment of first-order logic, and\na further investigation is made of its expressive power against hybrid modal\nlanguages. Next, we discuss how to axiomatize this logic of link deletion,\nusing dynamic-epistemic logics as a contrast. Finally, we show that our new\nmodal logic lacks both the tree model property and the finite model property,\nand that its satisfiability problem is undecidable.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:45:59 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 15:47:35 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Li", "Dazhu", ""]]}, {"id": "1811.05094", "submitter": "Curtis Bright", "authors": "Curtis Bright, Dragomir Z. Djokovic, Ilias Kotsireas, Vijay Ganesh", "title": "A SAT+CAS Approach to Finding Good Matrices: New Examples and\n  Counterexamples", "comments": null, "journal-ref": null, "doi": "10.1609/aaai.v33i01.33011435", "report-no": null, "categories": "cs.LO cs.SC math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We enumerate all circulant good matrices with odd orders divisible by 3 up to\norder 70. As a consequence of this we find a previously overlooked set of good\nmatrices of order 27 and a new set of good matrices of order 57. We also find\nthat circulant good matrices do not exist in the orders 51, 63, and 69, thereby\nfinding three new counterexamples to the conjecture that such matrices exist in\nall odd orders. Additionally, we prove a new relationship between the entries\nof good matrices and exploit this relationship in our enumeration algorithm.\nOur method applies the SAT+CAS paradigm of combining computer algebra\nfunctionality with modern SAT solvers to efficiently search large spaces which\nare specified by both algebraic and logical constraints.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 04:07:56 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bright", "Curtis", ""], ["Djokovic", "Dragomir Z.", ""], ["Kotsireas", "Ilias", ""], ["Ganesh", "Vijay", ""]]}, {"id": "1811.05116", "submitter": "Garry Pantelis", "authors": "Garry Pantelis", "title": "Programs as the Language of Science", "comments": "arXiv admin note: substantial text overlap with arXiv:1510.04469", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently it is widely accepted that the language of science is mathematics.\nThis book explores an alternative idea where the future of science is based on\nthe language of algorithms and programs. How such a language can actually be\nimplemented in the sciences is outlined in some detail. We start by\nconstructing a simple formal system where statements are represented as\nprograms and inference is based on computability as opposed to the classical\nnotion of truth value assignments.\n  The focus is on theories where the intrinsic properties and dynamic state of\nreal world objects can be defined in terms of information and subject to laws\nbased on simple deterministic rules and finite state arithmetic. Such models,\nit is argued, not only offer alternative simulation tools, as opposed to those\nbased on discrete approximations of conventional continuum theories, but in\nthemselves can be regarded as a language that describes the physical laws at a\nfundamental level. This book does not examine any specific application in\ndetail but rather attempts to lay down a foundation for the validation of such\ntheories by employing the inference scheme based on computability logic.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 05:40:45 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 08:52:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Pantelis", "Garry", ""]]}, {"id": "1811.05305", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "Relation of Web Service Orchestration, Abstract Process, Web Service and\n  Choreography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We refine the relation of Web service orchestration, abstract process, Web\nservice, and Web service choreography in Web service composition, under the\nsituation of cross-organizational corporation. We also introduce the formal\nverification process of this relation through an example.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 03:35:18 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1811.05548", "submitter": "Luciano Putruele", "authors": "Pablo F. Castro, Pedro R. D'Argenio, Ramiro Demasi, and Luciano\n  Putruele", "title": "Measuring Masking Fault-Tolerance", "comments": "25 pages including an appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a notion of fault-tolerance distance between\nlabeled transition systems. Intuitively, this notion of distance measures the\ndegree of fault-tolerance exhibited by a candidate system. In practice, there\nare different kinds of fault-tolerance, here we restrict ourselves to the\nanalysis of masking fault-tolerance because it is often a highly desirable goal\nfor critical systems. Roughly speaking, a system is masking fault-tolerant when\nit is able to completely mask the faults, not allowing these faults to have any\nobservable consequences for the users. We capture masking fault-tolerance via a\nsimulation relation, which is accompanied by a corresponding game\ncharacterization. We enrich the resulting games with quantitative objectives to\ndefine the notion of masking fault-tolerance distance. Furthermore, we\ninvestigate the basic properties of this notion of masking distance, and we\nprove that it is a directed pseudo metric. We have implemented our approach in\na prototype tool that automatically compute the masking distance between a\nnominal system and a fault-tolerant version of it. We have used this tool to\nmeasure the masking tolerance of multiple instances of several case studies\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:15:46 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 21:14:11 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Castro", "Pablo F.", ""], ["D'Argenio", "Pedro R.", ""], ["Demasi", "Ramiro", ""], ["Putruele", "Luciano", ""]]}, {"id": "1811.05602", "submitter": "Ajay Kumar Eeralla", "authors": "Ajay Kumar Eeralla, Christopher Lynch", "title": "Bounded ACh Unification", "comments": null, "journal-ref": "Math. Struct. Comp. Sci. 30 (2020) 664-682", "doi": "10.1017/S0960129520000183", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the unification modulo an equational theory ACh,\nwhich consists of a function symbol $h$ that is homomorphic over an\nassociative-commutative operator $+$. Since the unification modulo ACh theory\nis undecidable, we define a variant of the problem called \\textit{bounded ACh\nunification}. In this bounded version of ACh unification, we essentially bound\nthe number of times $h$ can be applied to a term recursively, and only allow\nsolutions that satisfy this bound. There is no bound on the number of\noccurrences of $h$ in a term, and the $+$ symbol can be applied an unlimited\nnumber of times. We give inference rules for solving the bounded version of the\nproblem and prove that the rules are sound, complete, and terminating. We have\nimplemented the algorithm in Maude and give experimental results. We argue that\nthis algorithm is useful in cryptographic protocol analysis.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 02:31:22 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 23:52:14 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Eeralla", "Ajay Kumar", ""], ["Lynch", "Christopher", ""]]}, {"id": "1811.05677", "submitter": "Vincenzo Ciancia", "authors": "Gina Belmonte, Vincenzo Ciancia, Diego Latella, Mieke Massink", "title": "VoxLogicA: a Spatial Model Checker for Declarative Image Analysis\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and spatio-temporal model checking techniques have a wide range of\napplication domains, among which large scale distributed systems and signal and\nimage analysis. We explore a new domain, namely (semi-)automatic contouring in\nMedical Imaging, introducing the tool VoxLogicA which merges the\nstate-of-the-art library of computational imaging algorithms ITK with the\nunique combination of declarative specification and optimised execution\nprovided by spatial logic model checking. The result is a rapid, logic based\nanalysis development methodology. The analysis of an existing benchmark of\nmedical images for segmentation of brain tumours shows that simple VoxLogicA\nanalysis can reach state-of-the-art accuracy, competing with best-in-class\nalgorithms, with the advantage of explainability and replicability.\nFurthermore, due to a two-orders-of-magnitude speedup compared to the existing\ngeneral-purpose spatio-temporal model checker topochecker, VoxLogicA enables\ninteractive development of analysis of 3D medical images, which can greatly\nfacilitate the work of professionals in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 08:10:15 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Belmonte", "Gina", ""], ["Ciancia", "Vincenzo", ""], ["Latella", "Diego", ""], ["Massink", "Mieke", ""]]}, {"id": "1811.05751", "submitter": "Evan Piermont", "authors": "Joseph Y. Halpern, Evan Piermont", "title": "Partial Awareness", "comments": "Appears in AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a modal logic to capture partial awareness. The logic has three\nbuilding blocks: objects, properties, and concepts. Properties are unary\npredicates on objects; concepts are Boolean combinations of properties. We take\nan agent to be partially aware of a concept if she is aware of the concept\nwithout being aware of the properties that define it. The logic allows for\nquantification over objects and properties, so that the agent can reason about\nher own unawareness. We then apply the logic to contracts, which we view as\nsyntactic objects that dictate outcomes based on the truth of formulas. We show\nthat when agents are unaware of some relevant properties, referencing concepts\nthat agents are only partially aware of can improve welfare.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 12:40:13 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Piermont", "Evan", ""]]}, {"id": "1811.05815", "submitter": "Johannes Oetsch", "authors": "Johannes Oetsch and Juan-Carlos Nieves", "title": "A Knowledge Representation Perspective on Activity Theory", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent technologies, in particular systems to promote health and\nwell-being, are inherently centered around the human being, and they need to\ninterrelate with human activities at their core. While social sciences provide\nangles to study such activities, e.g., within the framework of\ncultural-historical activity theory, there is no formal approach to give an\naccount of complex human activities from a Knowledge Representation and\nReasoning (KR) perspective. Our goal is to develop a logic-based framework to\nspecify complex activities that is directly informed by activity theory. There,\ncomplex activity refers to the process that mediates the relation between a\nsubject and some motivating object which in turn generates a hierarchy of goals\nthat direct actions. We introduce a new temporal logic to formalise key\nconcepts from activity theory and study various inference problems in our\nframework. We furthermore discuss how to use Answer-Set Programming as a KR\nshell for activity reasoning that allows to solve various reasoning tasks in a\nuniform way.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:43:57 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Oetsch", "Johannes", ""], ["Nieves", "Juan-Carlos", ""]]}, {"id": "1811.05879", "submitter": "Denis Efremov", "authors": "Grigoriy Volkov, Mikhail Mandrykin, Denis Efremov", "title": "Lemma Functions for Frama-C: C Programs as Proofs", "comments": "8 pages, 2 tables, 7 listings. To appear in the \"ISPRAS Open 2018\"\n  conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development of an auto-active verification technique\nin the Frama-C framework. We outline the lemma functions method and present the\ncorresponding ACSL extension, its implementation in Frama-C, and evaluation on\na set of string-manipulating functions from the Linux kernel. We illustrate the\nbenefits our approach can bring concerning the effort required to prove lemmas,\ncompared to the approach based on interactive provers such as Coq. Current\nlimitations of the method and its implementation are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 16:14:45 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Volkov", "Grigoriy", ""], ["Mandrykin", "Mikhail", ""], ["Efremov", "Denis", ""]]}, {"id": "1811.06065", "submitter": "Vincenzo Ciancia", "authors": "Fabrizio Banci Buonamici, Gina Belmonte, Vincenzo Ciancia, Diego\n  Latella, Mieke Massink", "title": "Spatial Logics and Model Checking for Medical Imaging (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on spatial and spatio-temporal model checking provides novel\nimage analysis methodologies, rooted in logical methods for topological spaces.\nMedical Imaging (MI) is a field where such methods show potential for\nground-breaking innovation. Our starting point is SLCS, the Spatial Logic for\nClosure Spaces -- Closure Spaces being a generalisation of topological spaces,\ncovering also discrete space structures -- and topochecker, a model-checker for\nSLCS (and extensions thereof). We introduce the logical language ImgQL (\"Image\nQuery Language\"). ImgQL extends SLCS with logical operators describing distance\nand region similarity. The spatio-temporal model checker topochecker is\ncorrespondingly enhanced with state-of-the-art algorithms, borrowed from\ncomputational image processing, for efficient implementation of distancebased\noperators, namely distance transforms. Similarity between regions is defined by\nmeans of a statistical similarity operator, based on notions from statistical\ntexture analysis. We illustrate our approach by means of two examples of\nanalysis of Magnetic Resonance images: segmentation of glioblastoma and its\noedema, and segmentation of rectal carcinoma.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:02:31 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Buonamici", "Fabrizio Banci", ""], ["Belmonte", "Gina", ""], ["Ciancia", "Vincenzo", ""], ["Latella", "Diego", ""], ["Massink", "Mieke", ""]]}, {"id": "1811.06099", "submitter": "Ron van der Meyden", "authors": "Ron van der Meyden", "title": "On the specification and verification of atomic swap smart contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain systems and smart contracts provide ways to securely implement\nmulti-party transactions without the use of trusted intermediaries, which\ncurrently underpin many commercial transactions. However, they do so by\ntransferring trust to computer systems, raising the question of whether code\ncan be trusted. Experience with high value losses resulting from incorrect code\nhas already shown that formal verification of smart contracts is likely to be\nbeneficial. This note investigates the specification and verification of a\nsimple form of multi-party transaction, atomic swaps. It is argued that logics\nwith the ability to express properties of strategies of players in a\nmulti-agent setting are conceptually useful for this purpose, although\nultimately, for our specific examples, the less expressive setting of temporal\nlogic suffices for verification of concrete implementations. This is\nillustrated through a number of examples of the use of a model checker to\nverify atomic swap smart contracts in on-chain and cross-chain settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:27:13 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["van der Meyden", "Ron", ""]]}, {"id": "1811.06235", "submitter": "James Wallbridge", "authors": "James Wallbridge", "title": "Jets and differential linear logic", "comments": "V2, 30 pages", "journal-ref": "Math. Struct. Comp. Sci. 30 (2020) 865-891", "doi": "10.1017/S0960129520000249", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the category of vector bundles over a fixed smooth manifold and\nits corresponding category of convenient modules are models for intuitionistic\ndifferential linear logic. The exponential modality is modelled by composing\nthe jet comonad, whose Kleisli category has linear differential operators as\nmorphisms, with the more familiar distributional comonad, whose Kleisli\ncategory has smooth maps as morphisms. Combining the two comonads gives a new\ninterpretation of the semantics of differential linear logic where the Kleisli\nmorphisms are smooth local functionals, or equivalently, smooth partial\ndifferential operators, and the codereliction map induces the functional\nderivative. This points towards a logic and hence computational theory of\nnon-linear partial differential equations and their solutions based on\nvariational calculus.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:40:30 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 00:27:02 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wallbridge", "James", ""]]}, {"id": "1811.06259", "submitter": "Alexander Shen", "authors": "Alexander Shen (ESCAPE)", "title": "Axiomatic approach to the theory of algorithms and relativized\n  computability", "comments": "Traduction en anglais 2018", "journal-ref": "Vestnik Moskovskogo Universiteta, Ser. 1, Mathematics, mechanics,\n  1980, pp.27-29", "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that many theorems in recursion theory can be \"relativized\".\nThis means that they remain true if partial recursive functions are replaced by\nfunctions that are partial recursive relative to some fixed oracle set.\nUspensky formulates three \"axioms\" called \"axiom of computation records\",\n\"axiom of programs'\" and \"arithmeticity axiom\". Then, using these axioms (more\nprecisely, two first ones) he proves basic results of the recursion theory.\nThese two axioms are true also for the class of functions that are partial\nrecursive relative to some fixed oracle set. Also this class is closed under\nsubstitution, primitive recursion and minimization ($\\mu$-operator); these\n(intuitively obvious) closure properties are also used in the proofs. This\nobservation made by Uspensky explains why many theorems of recursion theory can\nbe relativized. It turns out that the reverse statement is also true: all\nrelativizable results follow from the first two axioms and closure properties.\nIndeed, \\emph{every class of partial functions that is closed under\nsubstitution, primitive recursion and minimization that satisfies the first two\naxioms is the class of functions that are partial recursive relative to some\noracle set $A$}. This is the main result of the present article.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 09:56:23 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Shen", "Alexander", "", "ESCAPE"]]}, {"id": "1811.06459", "submitter": "Abhisekh Sankaran", "authors": "Abhisekh Sankaran", "title": "Revisiting the generalized {\\L}o\\'s-Tarski theorem", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new proof of the generalized {\\L}o\\'s-Tarski theorem\n($\\mathsf{GLT}(k)$) introduced in [1], over arbitrary structures. Instead of\nusing $\\lambda$-saturation as in [1], we construct just the \"required\nsaturation\" directly using ascending chains of structures. We also strengthen\nthe failure of $\\mathsf{GLT}(k)$ in the finite shown in [2], by strengthening\nthe failure of the {\\L}o\\'s-Tarski theorem in this context. In particular, we\nprove that not just universal sentences, but for each fixed $k$, even\n$\\Sigma^0_2$ sentences containing $k$ existential quantifiers fail to capture\nhereditariness in the finite. We conclude with two problems as future\ndirections, concerning the {\\L}o\\'s-Tarski theorem and $\\mathsf{GLT}(k)$, both\nin the context of all finite structures.\n  [1] 10.1016/j.apal.2015.11.001 ; [2] 10.1007/978-3-642-32621-9\\_22\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 16:43:43 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Sankaran", "Abhisekh", ""]]}, {"id": "1811.06460", "submitter": "Maaike Zwart", "authors": "Maaike Zwart, Dan Marsden", "title": "Don't Try This at Home: No-Go Theorems for Distributive Laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beck's distributive laws provide sufficient conditions under which two monads\ncan be composed, and monads arising from distributive laws have many desirable\ntheoretical properties. Unfortunately, finding and verifying distributive laws,\nor establishing if one even exists, can be extremely difficult and error-prone.\n  We develop general-purpose techniques for showing when there can be no\ndistributive law between two monads. Two approaches are presented. The first\nwidely generalizes ideas from a counterexample attributed to Plotkin, yielding\ngeneral-purpose theorems that recover the previously known situations in which\nno distributive law can exist. Our second approach is entirely novel,\nencompassing new practical situations beyond our generalization of Plotkin's\napproach. It negatively resolves the open question of whether the list monad\ndistributes over itself.\n  Our approach adopts an algebraic perspective throughout, exploiting a\nsyntactic characterization of distributive laws. This approach is key to\ngeneralizing beyond what has been achieved by direct calculations in previous\nwork. We show via examples many situations in which our theorems can be\napplied. This includes a detailed analysis of distributive laws for members of\nan extension of the Boom type hierarchy, well known to functional programmers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 16:44:28 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 16:11:47 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Zwart", "Maaike", ""], ["Marsden", "Dan", ""]]}, {"id": "1811.06502", "submitter": "Stefan Mitsch", "authors": "Stefan Mitsch and Andr\\'e Platzer", "title": "Verified Runtime Validation for Partially Observable Hybrid Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal verification provides strong safety guarantees but only for models of\ncyber-physical systems. Hybrid system models describe the required interplay of\ncomputation and physical dynamics, which is crucial to guarantee what\ncomputations lead to safe physical behavior (e.g., cars should not collide).\nControl computations that affect physical dynamics must act in advance to avoid\npossibly unsafe future circumstances. Formal verification then ensures that the\ncontrollers correctly identify and provably avoid unsafe future situations\nunder a certain model of physics. But any model of physics necessarily deviates\nfrom reality and, moreover, any observation with real sensors and manipulation\nwith real actuators is subject to uncertainty. This makes runtime validation a\ncrucial step to monitor whether the model assumptions hold for the real system\nimplementation.\n  The key question is what property needs to be runtime-monitored and what a\nsatisfied runtime monitor entails about the safety of the system: the\nobservations of a runtime monitor only relate back to the safety of the system\nif they are themselves accompanied by a proof of correctness! For an unbroken\nchain of correctness guarantees, we, thus, synthesize runtime monitors in a\nprovably correct way from provably safe hybrid system models. This paper\naddresses the inevitable challenge of making the synthesized monitoring\nconditions robust to partial observability of sensor uncertainty and partial\ncontrollability due to actuator disturbance. We show that the monitoring\nconditions result in provable safety guarantees with fallback controllers that\nreact to monitor violation at runtime.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:59:13 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 20:38:44 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Mitsch", "Stefan", ""], ["Platzer", "Andr\u00e9", ""]]}, {"id": "1811.06560", "submitter": "Mani A", "authors": "A. Mani", "title": "High Granular Operator Spaces, and Less-Contaminated General Rough\n  Mereologies", "comments": "Research paper: Preprint: Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular operator spaces and variants had been introduced and used in\ntheoretical investigations on the foundations of general rough sets by the\npresent author over the last few years. In this research, higher order versions\nof these are presented uniformly as partial algebraic systems. They are also\nadapted for practical applications when the data is representable by data\ntable-like structures according to a minimalist schema for avoiding\ncontamination. Issues relating to valuations used in information systems or\ntables are also addressed. The concept of contamination introduced and studied\nby the present author across a number of her papers, concerns mixing up of\ninformation across semantic domains (or domains of discourse). Rough inclusion\nfunctions (\\textsf{RIF}s), variants, and numeric functions often have a direct\nor indirect role in contaminating algorithms. Some solutions that seek to\nreplace or avoid them have been proposed and investigated by the present author\nin some of her earlier papers. Because multiple kinds of solution are of\ninterest to the contamination problem, granular generalizations of RIFs are\nproposed, and investigated. Interesting representation results are proved and a\ncore algebraic strategy for generalizing Skowron-Polkowski style of rough\nmereology (though for a very different purpose) is formulated. A number of\nexamples have been added to illustrate key parts of the proposal in higher\norder variants of granular operator spaces. Further algorithms grounded in\nmereological nearness, suited for decision-making in human-machine interaction\ncontexts, are proposed by the present author. Applications of granular\n\\textsf{RIF}s to partial/soft solutions of the inverse problem are also\ninvented in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 19:03:38 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 22:27:50 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 01:20:36 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 04:59:05 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Mani", "A.", ""]]}, {"id": "1811.06771", "submitter": "Bishoksan Kafle", "authors": "Bishoksan Kafle, Graeme Gange, Peter Schachte, Harald Sondergaard,\n  Peter J. Stuckey", "title": "Precondition Inference via Partitioning of Initial States", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precondition inference is a non-trivial task with several applications in\nprogram analysis and verification. We present a novel iterative method for\nautomatically deriving sufficient preconditions for safety and unsafety of\nprograms which introduces a new dimension of modularity. Each iteration\nmaintains over-approximations of the set of \\emph{safe} and \\emph{unsafe}\n\\emph{initial} states. Then we repeatedly use the current abstractions to\npartition the program's \\emph{initial} states into those known to be safe,\nknown to be unsafe and unknown, and construct a revised program focusing on\nthose initial states that are not yet known to be safe or unsafe. An\nexperimental evaluation of the method on a set of software verification\nbenchmarks shows that it can solve problems which are not solvable using\nprevious methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 11:55:51 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Kafle", "Bishoksan", ""], ["Gange", "Graeme", ""], ["Schachte", "Peter", ""], ["Sondergaard", "Harald", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "1811.06779", "submitter": "Satoshi Kura", "authors": "Satoshi Kura, Natsuki Urabe, Ichiro Hasuo", "title": "Tail Probabilities for Randomized Program Runtimes via Martingales for\n  Higher Moments", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs with randomization constructs is an active research topic,\nespecially after the recent introduction of martingale-based analysis methods\nfor their termination and runtimes. Unlike most of the existing works that\nfocus on proving almost-sure termination or estimating the expected runtime, in\nthis work we study the tail probabilities of runtimes-such as \"the execution\ntakes more than 100 steps with probability at most 1%.\" To this goal, we devise\na theory of supermartingales that overapproximate higher moments of runtime.\nThese higher moments, combined with a suitable concentration inequality, yield\nuseful upper bounds of tail probabilities. Moreover, our vector-valued\nformulation enables automated template-based synthesis of those\nsupermartingales. Our experiments suggest the method's practical use.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 12:24:35 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 07:53:55 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Kura", "Satoshi", ""], ["Urabe", "Natsuki", ""], ["Hasuo", "Ichiro", ""]]}, {"id": "1811.06787", "submitter": "Thomas Seiller", "authors": "Luc Pellissier and Thomas Seiller", "title": "PRAMs over integers do not compute maxflow efficiently", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new semantic method for proving lower bounds in\ncomputational complexity. We use it to prove that $\\mathbf{maxflow}$, a\n$\\mathbf{Ptime}$-complete problem, is not computable in polylogarithmic time on\nparallel random access machines (PRAMs) working with integers, showing that\n$\\mathbf{NC}_{\\mathbb{Z}}\\neq\\mathbf{Ptime}$, where $\\mathbf{NC}_{\\mathbb{Z}}$\nis the complexity class defined by such machines, and $\\mathbf{Ptime}$ is the\nstandard class of polynomial time computable problems (on, say, a Turing\nmachine). On top of showing this new separation result, we show our method\ncaptures previous lower bounds results from the literature: Steele and Yao's\nlower bounds for algebraic decision trees, Ben-Or's lower bounds for algebraic\ncomputation trees, Cucker's proof that $\\mathbf{NC}_{\\mathbb{R}}$ is not equal\nto $\\mathbf{Ptime}_{\\mathbb{R}}$, and Mulmuley's lower bounds for \"PRAMs\nwithout bit operations\".\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 12:55:40 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 15:54:15 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pellissier", "Luc", ""], ["Seiller", "Thomas", ""]]}, {"id": "1811.06799", "submitter": "Sebastian Siebertz", "authors": "Grzegorz Fabia\\'nski and Micha{\\l} Pilipczuk and Sebastian Siebertz\n  and Szymon Toru\\'nczyk", "title": "Progressive Algorithms for Domination and Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM math.CO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generic algorithmic paradigm that we call progressive\nexploration, which can be used to develop simple and efficient parameterized\ngraph algorithms. We identify two model-theoretic properties that lead to\nefficient progressive algorithms, namely variants of the Helly property and\nstability. We demonstrate our approach by giving linear-time fixed-parameter\nalgorithms for the distance-r dominating set problem (parameterized by the\nsolution size) in a wide variety of restricted graph classes, such as powers of\nnowhere dense classes, map graphs, and (for $r=1$) biclique-free graphs.\nSimilarly, for the distance-r independent set problem the technique can be used\nto give a linear-time fixed-parameter algorithm on any nowhere dense class.\nDespite the simplicity of the method, in several cases our results extend known\nboundaries of tractability for the considered problems and improve the best\nknown running times.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 13:33:39 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Fabia\u0144ski", "Grzegorz", ""], ["Pilipczuk", "Micha\u0142", ""], ["Siebertz", "Sebastian", ""], ["Toru\u0144czyk", "Szymon", ""]]}, {"id": "1811.06936", "submitter": "Adrien Koutsos", "authors": "Adrien Koutsos", "title": "Deciding Indistinguishability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational indistinguishability is a key property in cryptography and\nverification of security protocols. Current tools for proving it rely on\ncryptographic game transformations.\n  We follow Bana and Comon's approach, axiomatizing what an adversary cannot\ndistinguish. We prove the decidability of a set of first-order axioms that are\nboth computationally sound and expressive enough. This can be viewed as the\ndecidability of a family of cryptographic game transformations. Our proof\nrelies on term rewriting and automated deduction techniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:39:14 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 15:16:43 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Koutsos", "Adrien", ""]]}, {"id": "1811.06961", "submitter": "Philipp J. Meyer", "authors": "Philipp J. Meyer, Javier Esparza, Philip Offtermatt", "title": "Computing the Expected Execution Time of Probabilistic Workflow Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free-Choice Workflow Petri nets, also known as Workflow Graphs, are a popular\nmodel in Business Process Modeling.\n  In this paper we introduce Timed Probabilistic Workflow Nets (TPWNs), and\ngive them a Markov Decision Process (MDP) semantics. Since the time needed to\nexecute two parallel tasks is the maximum of the times, and not their sum, the\nexpected time cannot be directly computed using the theory of MDPs with\nrewards. In our first contribution, we overcome this obstacle with the help of\n\"earliest-first\" schedulers, and give a single exponential-time algorithm for\ncomputing the expected time.\n  In our second contribution, we show that computing the expected time is\n#P-hard, and so polynomial algorithms are very unlikely to exist. Further,\n#P-hardness holds even for workflows with a very simple structure in which all\ntransitions times are 1 or 0, and all probabilities are 1 or 0.5.\n  Our third and final contribution is an experimental investigation of the\nruntime of our algorithm on a set of industrial benchmarks. Despite the\nnegative theoretical results, the results are very encouraging. In particular,\nthe expected time of every workflow in a popular benchmark suite with 642\nworkflow nets can be computed in milliseconds.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 18:35:37 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 13:55:38 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Meyer", "Philipp J.", ""], ["Esparza", "Javier", ""], ["Offtermatt", "Philip", ""]]}, {"id": "1811.07145", "submitter": "Gethin Norman", "authors": "Marta Kwiatkowska, Gethin Norman, David Parker, Gabriel Santos", "title": "Equilibria-based Probabilistic Model Checking for Concurrent Stochastic\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic model checking for stochastic games enables formal verification\nof systems that comprise competing or collaborating entities operating in a\nstochastic environment. Despite good progress in the area, existing approaches\nfocus on zero-sum goals and cannot reason about scenarios where entities are\nendowed with different objectives. In this paper, we propose probabilistic\nmodel checking techniques for concurrent stochastic games based on Nash\nequilibria. We extend the temporal logic rPATL (probabilistic alternating-time\ntemporal logic with rewards) to allow reasoning about players with distinct\nquantitative goals, which capture either the probability of an event occurring\nor a reward measure. We present algorithms to synthesise strategies that are\nsubgame perfect social welfare optimal Nash equilibria, i.e., where there is no\nincentive for any players to unilaterally change their strategy in any state of\nthe game, whilst the combined probabilities or rewards are maximised. We\nimplement our techniques in the PRISM-games tool and apply them to several case\nstudies, including network protocols and robot navigation, showing the benefits\ncompared to existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 11:22:42 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 10:21:46 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 10:59:30 GMT"}, {"version": "v4", "created": "Wed, 3 Jul 2019 08:16:49 GMT"}, {"version": "v5", "created": "Mon, 8 Jul 2019 10:37:54 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Kwiatkowska", "Marta", ""], ["Norman", "Gethin", ""], ["Parker", "David", ""], ["Santos", "Gabriel", ""]]}, {"id": "1811.07146", "submitter": "Guillermo P\\'erez", "authors": "Micha\\\"el Cadilhac, Guillermo A. P\\'erez, and Marie van den Bogaard", "title": "The Impatient May Use Limited Optimism to Minimize Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discounted-sum games provide a formal model for the study of reinforcement\nlearning, where the agent is enticed to get rewards early since later rewards\nare discounted. When the agent interacts with the environment, she may regret\nher actions, realizing that a previous choice was suboptimal given the behavior\nof the environment. The main contribution of this paper is a PSPACE algorithm\nfor computing the minimum possible regret of a given game. To this end, several\nresults of independent interest are shown. (1) We identify a class of\nregret-minimizing and admissible strategies that first assume that the\nenvironment is collaborating, then assume it is adversarial---the precise\ntiming of the switch is key here. (2) Disregarding the computational cost of\nnumerical analysis, we provide an NP algorithm that checks that the regret\nentailed by a given time-switching strategy exceeds a given value. (3) We show\nthat determining whether a strategy minimizes regret is decidable in PSPACE.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 11:23:58 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Cadilhac", "Micha\u00ebl", ""], ["P\u00e9rez", "Guillermo A.", ""], ["Bogaard", "Marie van den", ""]]}, {"id": "1811.07149", "submitter": "Giuseppe Greco", "authors": "Giuseppe Greco and Peter Jipsen and Krishna Manoorkar and Alessandra\n  Palmigiano and Apostolos Tzimoulis", "title": "Logics for Rough Concept Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking an algebraic perspective on the basic structures of Rough Concept\nAnalysis as the starting point, in this paper we introduce some varieties of\nlattices expanded with normal modal operators which can be regarded as the\nnatural rough algebra counterparts of certain subclasses of rough formal\ncontexts, and introduce proper display calculi for the logics associated with\nthese varieties which are sound, complete, conservative and with uniform cut\nelimination and subformula property. These calculi modularly extend the\nmulti-type calculi for rough algebras to a `nondistributive' (i.e. general\nlattice-based) setting.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 12:03:02 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Greco", "Giuseppe", ""], ["Jipsen", "Peter", ""], ["Manoorkar", "Krishna", ""], ["Palmigiano", "Alessandra", ""], ["Tzimoulis", "Apostolos", ""]]}, {"id": "1811.07644", "submitter": "Henning Basold", "authors": "Henning Basold and Ekaterina Komendantskaya and Yue Li", "title": "Coinduction in Uniform: Foundations for Corecursive Proof Search with\n  Horn Clauses", "comments": null, "journal-ref": "LNCS 11423 (2019) 783-813", "doi": "10.1007/978-3-030-17184-1_28", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish proof-theoretic, constructive and coalgebraic foundations for\nproof search in coinductive Horn clause theories. Operational semantics of\ncoinductive Horn clause resolution is cast in terms of coinductive uniform\nproofs; its constructive content is exposed via soundness relative to an\nintuitionistic first-order logic with recursion controlled by the later\nmodality; and soundness of both proof systems is proven relative to a novel\ncoalgebraic description of complete Herbrand models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 12:30:17 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 10:55:13 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Basold", "Henning", ""], ["Komendantskaya", "Ekaterina", ""], ["Li", "Yue", ""]]}, {"id": "1811.08061", "submitter": "Chunhui Guo", "authors": "Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha", "title": "Model and Integrate Medical Resource Available Times and Relationships\n  in Verifiably Correct Executable Medical Best Practice Guideline Models\n  (Extended Version)", "comments": "full version, 12 pages", "journal-ref": "ACM/IEEE 9th International Conference on Cyber-Physical Systems\n  (ICCPS), 2018", "doi": "10.1109/ICCPS.2018.00032", "report-no": null, "categories": "cs.SE cs.FL cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving patient care safety is an ultimate objective for medical\ncyber-physical systems. A recent study shows that the patients' death rate is\nsignificantly reduced by computerizing medical best practice guidelines. Recent\ndata also show that some morbidity and mortality in emergency care are directly\ncaused by delayed or interrupted treatment due to lack of medical resources.\nHowever, medical guidelines usually do not provide guidance on medical resource\ndemands and how to manage potential unexpected delays in resource availability.\nIf medical resources are temporarily unavailable, safety properties in existing\nexecutable medical guideline models may fail which may cause increased risk to\npatients under care. The paper presents a separately model and jointly verify\n(SMJV) architecture to separately model medical resource available times and\nrelationships and jointly verify safety properties of existing medical best\npractice guideline models with resource models being integrated in. The SMJV\narchitecture allows medical staff to effectively manage medical resource\ndemands and unexpected resource availability delays during emergency care. The\nseparated modeling approach also allows different domain professionals to make\nindependent model modifications, facilitates the management of frequent\nresource availability changes, and enables resource statechart reuse in\nmultiple medical guideline models. A simplified stroke scenario is used as a\ncase study to investigate the effectiveness and validity of the SMJV\narchitecture. The case study indicates that the SMJV architecture is able to\nidentify unsafe properties caused by unexpected resource delays.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:01:54 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Guo", "Chunhui", ""], ["Fu", "Zhicheng", ""], ["Zhang", "Zhenyu", ""], ["Ren", "Shangping", ""], ["Sha", "Lui", ""]]}, {"id": "1811.08064", "submitter": "Chunhui Guo", "authors": "Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha", "title": "Model and Integrate Medical Resource Availability into Verifiably\n  Correct Executable Medical Guidelines - Technical Report", "comments": "full version, 8 pages. arXiv admin note: substantial text overlap\n  with arXiv:1811.08061", "journal-ref": "IEEE/ACM 36th International Conference on Computer-Aided Design\n  (ICCAD), 2017", "doi": "10.1109/ICCAD.2017.8203885", "report-no": null, "categories": "cs.SE cs.FL cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving effectiveness and safety of patient care is an ultimate objective\nfor medical cyber-physical systems. A recent study shows that the patients'\ndeath rate can be reduced by computerizing medical guidelines. Most existing\nmedical guideline models are validated and/or verified based on the assumption\nthat all necessary medical resources needed for a patient care are always\navailable. However, the reality is that some medical resources, such as special\nmedical equipment or medical specialists, can be temporarily unavailable for an\nindividual patient. In such cases, safety properties validated and/or verified\nin existing medical guideline models without considering medical resource\navailability may not hold any more. The paper argues that considering medical\nresource availability is essential in building verifiably correct executable\nmedical guidelines. We present an approach to explicitly and separately model\nmedical resource availability and automatically integrate resource availability\nmodels into an existing statechart-based computerized medical guideline model.\nThis approach requires minimal change in existing medical guideline models to\ntake into consideration of medical resource availability in validating and\nverifying medical guideline models. A simplified stroke scenario is used as a\ncase study to investigate the effectiveness and validity of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:02:41 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Guo", "Chunhui", ""], ["Fu", "Zhicheng", ""], ["Zhang", "Zhenyu", ""], ["Ren", "Shangping", ""], ["Sha", "Lui", ""]]}, {"id": "1811.08131", "submitter": "Claude Marche", "authors": "Sylvain Conchon (LRI), Amit Goel, Sava Krstic, Rupak Majumdar\n  (MPI-SWS), Mattias Roux (LRI)", "title": "FAR-Cubicle - A new reachability algorithm for Cubicle", "comments": null, "journal-ref": "2017 Formal Methods in Computer-Aided Design (FMCAD), Oct 2017,\n  Vienna, France. IEEE", "doi": "10.23919/FMCAD.2017.8102256", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic algorithm for verifying safety properties of\nparameterized software systems. This algorithm is based on both IC3 and Lazy\nAnnotation. We implemented it in Cubicle, a model checker for verifying safety\nproperties of array-based systems. Cache-coherence protocols and mutual\nexclusion algorithms are known examples of such systems. Our algorithm\niteratively builds an abstract reachability graph refining the set of reachable\nstates from counterexamples. Refining is made through counterexample\napproximation. We show the effectiveness and limitations of this algorithm and\ntradeoffs that results from it.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 09:01:42 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Conchon", "Sylvain", "", "LRI"], ["Goel", "Amit", "", "MPI-SWS"], ["Krstic", "Sava", "", "MPI-SWS"], ["Majumdar", "Rupak", "", "MPI-SWS"], ["Roux", "Mattias", "", "LRI"]]}, {"id": "1811.08197", "submitter": "Fabian Reiter", "authors": "Benedikt Bollig, Patricia Bouyer, and Fabian Reiter", "title": "Identifiers in Registers - Describing Network Algorithms with Logic", "comments": "17 pages (+ 17 pages of appendices), 1 figure (+ 1 figure in the\n  appendix)", "journal-ref": null, "doi": "10.1007/978-3-030-17127-8_7", "report-no": null, "categories": "cs.FL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formal model of distributed computing based on register automata\nthat captures a broad class of synchronous network algorithms. The local memory\nof each process is represented by a finite-state controller and a fixed number\nof registers, each of which can store the unique identifier of some process in\nthe network. To underline the naturalness of our model, we show that it has the\nsame expressive power as a certain extension of first-order logic on graphs\nwhose nodes are equipped with a total order. Said extension lets us define new\nfunctions on the set of nodes by means of a so-called partial fixpoint\noperator. In spirit, our result bears close resemblance to a classical theorem\nof descriptive complexity theory that characterizes the complexity class PSPACE\nin terms of partial fixpoint logic (a proper superclass of the logic we\nconsider here).\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:56:53 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Bollig", "Benedikt", ""], ["Bouyer", "Patricia", ""], ["Reiter", "Fabian", ""]]}, {"id": "1811.08338", "submitter": "Aleks Kissinger", "authors": "Bart Jacobs and Aleks Kissinger and Fabio Zanasi", "title": "Causal Inference by String Diagram Surgery", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting causal relationships from observed correlations is a growing area\nin probabilistic reasoning, originating with the seminal work of Pearl and\nothers from the early 1990s. This paper develops a new, categorically oriented\nview based on a clear distinction between syntax (string diagrams) and\nsemantics (stochastic matrices), connected via interpretations as\nstructure-preserving functors. A key notion in the identification of causal\neffects is that of an intervention, whereby a variable is forcefully set to a\nparticular value independent of any prior propensities. We represent the effect\nof such an intervention as an endofunctor which performs `string diagram\nsurgery' within the syntactic category of string diagrams. This diagram surgery\nin turn yields a new, interventional distribution via the interpretation\nfunctor. While in general there is no way to compute interventional\ndistributions purely from observed data, we show that this is possible in\ncertain special cases using a calculational tool called comb disintegration. We\ndemonstrate the use of this technique on a well-known toy example, where we\npredict the causal effect of smoking on cancer in the presence of a confounding\ncommon cause. After developing this specific example, we show this technique\nprovides simple sufficient conditions for computing interventions which apply\nto a wide variety of situations considered in the causal inference literature.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:05:25 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 11:16:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Jacobs", "Bart", ""], ["Kissinger", "Aleks", ""], ["Zanasi", "Fabio", ""]]}, {"id": "1811.08846", "submitter": "Zhe Xu", "authors": "Zhe Xu, Melkior Ornik, A. Agung Julius and Ufuk Topcu", "title": "Information-Guided Temporal Logic Inference with Prior Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of inferring knowledge from data so that\nthe inferred knowledge is interpretable and informative to humans who have\nprior knowledge. Given a dataset as a collection of system trajectories, we\ninfer parametric linear temporal logic (pLTL) formulas that are informative and\nsatisfied by the trajectories in the dataset with high probability. The\ninformativeness of the inferred formula is measured by the information gain\nwith respect to given prior knowledge represented by a prior probability\ndistribution. We first present two algorithms to compute the information gain\nwith a focus on two types of prior probability distributions: stationary\nprobability distributions and probability distributions expressed by discrete\ntime Markov chains. Then we provide a method to solve the inference problem for\na subset of pLTL formulas with polynomial time complexity with respect to the\nnumber of Boolean connectives in the formula. We provide implementations of the\nproposed approach on explaining anomalous patterns, patterns changes and\nexplaining the policies of Markov decision processes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:48:59 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Xu", "Zhe", ""], ["Ornik", "Melkior", ""], ["Julius", "A. Agung", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1811.09014", "submitter": "EPTCS", "authors": "Paolo Masci (HASLab/INESC TEC and Universidade do Minho, Portugal.),\n  Rosemary Monahan (Maynooth University, Ireland), Virgile Prevosto (Software\n  Safety and Security Lab, France)", "title": "Proceedings 4th Workshop on Formal Integrated Development Environment", "comments": null, "journal-ref": "EPTCS 284, 2018", "doi": "10.4204/EPTCS.284", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of F-IDE 2018, the fourth international\nworkshop on Formal Integrated Development Environment, which was held as a FLoC\n2018 satellite event, on July 14, 2018, in Oxford, England.\n  High levels of safety, security and also privacy standards require the use of\nformal methods to specify and develop compliant software (sub)systems. Any\nstandard comes with an assessment process, which requires a complete\ndocumentation of the application in order to ease the justification of design\nchoices and the review of code and proofs. Thus tools are needed for handling\nspecifications, program constructs and verification artifacts. The aim of the\nF-IDE workshop is to provide a forum for presenting and discussing research\nefforts as well as experience returns on design, development and usage of\nformal IDE aiming at making formal methods \"easier\" for both specialists and\nnon-specialists.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:00:38 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Masci", "Paolo", "", "HASLab/INESC TEC and Universidade do Minho, Portugal."], ["Monahan", "Rosemary", "", "Maynooth University, Ireland"], ["Prevosto", "Virgile", "", "Software\n  Safety and Security Lab, France"]]}, {"id": "1811.09071", "submitter": "Manuel Schneckenreither", "authors": "Georg Moser, Manuel Schneckenreither", "title": "Automated Amortised Resource Analysis for Term Rewrite Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish an automated amortised resource analysis for term\nrewrite systems. The method is presented in an annotated type system and gives\nrise to polynomial bounds on the innermost runtime complexity of the analysed\nterm rewrite system. Our analysis does not restrict the input rewrite system in\nany way so that rewrite systems may serve as abstractions of first-order,\neagerly evaluated functional programs over user-defined inductive data-types.\nThis facilitates integration in a general framework for resource analysis of\nprograms. In particular, we have implemented the method and integrated it into\nour analysis tool TcT. Furthermore, we have coupled the established analysis\nwith a complexity reflecting transformation from pure OCaml programs. This\nextends the provided analysis to a fully automated resource analysis of\nhigher-order functional programs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 09:41:03 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Moser", "Georg", ""], ["Schneckenreither", "Manuel", ""]]}, {"id": "1811.09435", "submitter": "Petr Savick\\'y", "authors": "Petr Ku\\v{c}era, Petr Savick\\'y", "title": "Backdoor Decomposable Monotone Circuits and their Propagation Complete\n  Encodings", "comments": "The paper was significantly rewritten to improve readability, it is\n  now an extended version of the paper accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a compilation language of backdoor decomposable monotone circuits\n(BDMCs) which generalizes several concepts appearing in the literature, e.g.\nDNNFs and backdoor trees. A $\\mathcal{C}$-BDMC sentence is a monotone circuit\nwhich satisfies decomposability property (such as in DNNF) in which the inputs\n(or leaves) are associated with CNF encodings from a given base class\n$\\mathcal{C}$. We consider the class of propagation complete (PC) encodings as\na base class and we show that PC-BDMCs are polynomially equivalent to PC\nencodings. Additionally, we use this to determine the properties of PC-BDMCs\nand PC encodings with respect to the knowledge compilation map including the\nlist of efficient operations on the languages.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 11:31:29 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 13:18:12 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 22:11:43 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 22:42:13 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ku\u010dera", "Petr", ""], ["Savick\u00fd", "Petr", ""]]}, {"id": "1811.09438", "submitter": "Lucas Carvalho Cordeiro", "authors": "Alessandro Trindade and Lucas Cordeiro", "title": "Automated Verification of Stand-alone Solar Photovoltaic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With declining costs and increasing performance, the deployment of renewable\nenergy systems is growing faster. Particular attention is given to stand-alone\nsolar photovoltaic systems in rural areas or where grid extension is\nunfeasible. Tools to evaluate electrification projects are available, but they\nare based on simulations that do not cover all aspects of the design space.\nAutomated verification using model checking has proven to be an effective\ntechnique to program verification. This paper marks the first application of\nsoftware model checking to formally verify the design of a stand-alone solar\nphotovoltaic system including solar panel, charge controller, battery,\ninverter, and electric load. Case studies, from real photovoltaic systems\ndeployed in five different sites, ranging from 700W to 1,200W, were used to\nevaluate this proposed approach and to compare that with specialized simulation\ntools. Data from practical applications show the effectiveness of our approach,\nwhere specific conditions that lead to failures in a photovoltaic solar system\nare only detected by our automated verification method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 11:44:57 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Trindade", "Alessandro", ""], ["Cordeiro", "Lucas", ""]]}, {"id": "1811.09454", "submitter": "Anantha Padmanabha", "authors": "Anantha Padmanabha and R Ramanujam", "title": "Propositional modal logic with implicit modal quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propositional term modal logic is interpreted over Kripke structures with\nunboundedly many accessibility relations and hence the syntax admits variables\nindexing modalities and quantification over them. This logic is undecidable,\nand we consider a variable-free propositional bi-modal logic with implicit\nquantification. Thus $[\\forall] \\alpha$ asserts necessity over all\naccessibility relations and $[\\exists] \\alpha$ is classical necessity over some\naccessibility relation. The logic is associated with a natural bisimulation\nrelation over models and we show that the logic is exactly the bisimulation\ninvariant fragment of a two sorted first order logic. The logic is easily seen\nto be decidable and admits a complete axiomatization of valid formulas.\nMoreover the decision procedure extends naturally to the `bundled fragment' of\nfull term modal logic.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 12:42:16 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 11:10:49 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Padmanabha", "Anantha", ""], ["Ramanujam", "R", ""]]}, {"id": "1811.09813", "submitter": "Aditya Grover", "authors": "Aditya Grover, Tudor Achim, Stefano Ermon", "title": "Streamlining Variational Inference for Constraint Satisfaction Problems", "comments": "NeurIPS 2018", "journal-ref": null, "doi": "10.1088/1742-5468/ab371f", "report-no": null, "categories": "cs.AI cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms for solving constraint satisfaction problems are based on\nsurvey propagation, a variational inference scheme used to obtain approximate\nmarginal probability estimates for variable assignments. These marginals\ncorrespond to how frequently each variable is set to true among satisfying\nassignments, and are used to inform branching decisions during search; however,\nmarginal estimates obtained via survey propagation are approximate and can be\nself-contradictory. We introduce a more general branching strategy based on\nstreamlining constraints, which sidestep hard assignments to variables. We show\nthat streamlined solvers consistently outperform decimation-based solvers on\nrandom k-SAT instances for several problem sizes, shrinking the gap between\nempirical performance and theoretical limits of satisfiability by 16.3% on\naverage for k=3,4,5,6.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 11:08:14 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Grover", "Aditya", ""], ["Achim", "Tudor", ""], ["Ermon", "Stefano", ""]]}, {"id": "1811.10400", "submitter": "Eric Rothstein-Morris", "authors": "Eric Rothstein-Morris and Sun Jun", "title": "Quantifying Attacker Capability Via Model Checking Multiple Properties\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to solve a practical problem, i.e., how to quantify the risk\nbrought upon a system by different attackers. The answer is useful for\noptimising resource allocation for system defence. Given a set of safety\nrequirements, we quantify the attacker capability in terms of the set of safety\nrequirements an attacker can compromise. Given a system (in the presence of an\nattacker), model checking it against each safety requirement one by one is\nexpensive and wasteful since the same state space is explored many times. We\nthus propose model checking multiple properties efficiently by means of\ncoalgebraic model checking using enhanced coinduction techniques. We apply the\nproposed technique to a real-world water treatment system and the results show\nthat our approach can effectively reduce the effort required for model\nchecking.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:19:09 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Rothstein-Morris", "Eric", ""], ["Jun", "Sun", ""]]}, {"id": "1811.10401", "submitter": "Tobias Kapp\\'e", "authors": "Tobias Kapp\\'e and Paul Brunet and Jurriaan Rot and Alexandra Silva\n  and Jana Wagemaker and Fabio Zanasi", "title": "Kleene Algebra with Observations", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.CONCUR.2019.41", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kleene algebra with tests (KAT) is an algebraic framework for reasoning about\nthe control flow of sequential programs. Generalising KAT to reason about\nconcurrent programs is not straightforward, because axioms native to KAT in\nconjunction with expected axioms for concurrency lead to an anomalous equation.\nIn this paper, we propose Kleene algebra with observations (KAO), a variant of\nKAT, as an alternative foundation for extending KAT to a concurrent setting. We\ncharacterise the free model of KAO, and establish a decision procedure w.r.t.\nits equational theory.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 16:56:43 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 08:05:27 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 09:50:06 GMT"}, {"version": "v4", "created": "Wed, 21 Aug 2019 09:45:00 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Kapp\u00e9", "Tobias", ""], ["Brunet", "Paul", ""], ["Rot", "Jurriaan", ""], ["Silva", "Alexandra", ""], ["Wagemaker", "Jana", ""], ["Zanasi", "Fabio", ""]]}, {"id": "1811.10814", "submitter": "EPTCS", "authors": "Sylvain Dailler (Inria, Universit\\'e Paris-Saclay, F-91120 Palaiseau),\n  Claude March\\'e (Inria, Universit\\'e Paris-Saclay, F-91120 Palaiseau),\n  Yannick Moy (AdaCore, F-75009 Paris)", "title": "Lightweight Interactive Proving inside an Automatic Program Verifier", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 1-15", "doi": "10.4204/EPTCS.284.1", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among formal methods, the deductive verification approach allows establishing\nthe strongest possible formal guarantees on critical software. The downside is\nthe cost in terms of human effort required to design adequate formal\nspecifications and to successfully discharge the required proof obligations. To\npopularize deductive verification in an industrial software development\nenvironment, it is essential to provide means to progressively transition from\nsimple and automated approaches to deductive verification. The SPARK\nenvironment, for development of critical software written in Ada, goes towards\nthis goal by providing automated tools for formally proving that some code\nfulfills the requirements expressed in Ada contracts. In a program verifier\nthat makes use of automatic provers to discharge the proof obligations, a need\nfor some additional user interaction with proof tasks shows up: either to help\nanalyzing the reason of a proof failure or, ultimately, to discharge the\nverification conditions that are out-of-reach of state-of-the-art automatic\nprovers. Adding interactive proof features in SPARK appears to be complicated\nby the fact that the proof toolchain makes use of the independent, intermediate\nverification tool Why3, which is generic enough to accept multiple front-ends\nfor different input languages. This paper reports on our approach to extend\nWhy3 with interactive proof features and also with a generic client-server\ninfrastructure allowing integration of proof interaction into an external,\nfront-end graphical user interface such as the one of SPARK.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:00:07 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Dailler", "Sylvain", "", "Inria, Universit\u00e9 Paris-Saclay, F-91120 Palaiseau"], ["March\u00e9", "Claude", "", "Inria, Universit\u00e9 Paris-Saclay, F-91120 Palaiseau"], ["Moy", "Yannick", "", "AdaCore, F-75009 Paris"]]}, {"id": "1811.10815", "submitter": "EPTCS", "authors": "Jan Bessai (Technical University of Dortmund), Anna Vasileva\n  (Technical University of Dortmund)", "title": "User Support for the Combinator Logic Synthesizer Framework", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 16-25", "doi": "10.4204/EPTCS.284.2", "report-no": null, "categories": "cs.LO cs.FL cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usability is crucial for the adoption of software development technologies.\nThis is especially true in development stages, where build processes fail,\nbecause software is not yet complete or was incompletely modified. We present\nearly work that aims to improve usability of the Combinatory Logic Synthesizer\n(CL)S framework, especially in these stages. (CL)S is a publicly available\ntype-based development tool for the automatic composition of software\ncomponents from a user-specified repository. It provides an implementation of a\ntype inhabitation algorithm for Combinatory Logic with intersection types,\nwhich is fully integrated into the Scala programming language. Here, we\nspecifically focus on building a web-based IDE to make potentially incomplete\nor erroneous input specifications for and decisions of the algorithm\nunderstandable for non-experts. A main aspect of this is providing graphical\nrepresentations illustrating the step-wise search process of the algorithm. We\nalso provide a detailed discussion of possible future work to further improve\nthe understandability of these representations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:00:29 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Bessai", "Jan", "", "Technical University of Dortmund"], ["Vasileva", "Anna", "", "Technical University of Dortmund"]]}, {"id": "1811.10816", "submitter": "EPTCS", "authors": "Paolo Arcaini (National Institute of Informatics), Riccardo Melioli\n  (Dipartimento di Informatica, Universit\\`a degli Studi di Milano), Elvinia\n  Riccobene (Dipartimento di Informatica, Universit\\`a degli Studi di Milano)", "title": "AsmetaF: A Flattener for the ASMETA Framework", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014. The first two authors\n  are supported by ERATO HASUO Metamathematics for Systems Design Project (No.\n  JPMJER1603), JST. Funding Reference number: 10.13039/501100009024 ERATO", "journal-ref": "EPTCS 284, 2018, pp. 26-36", "doi": "10.4204/EPTCS.284.3", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract State Machines (ASMs) have shown to be a suitable high-level\nspecification method for complex, even industrial, systems; the ASMETA\nframework, supporting several validation and verification activities on ASM\nmodels, is an example of a formal integrated development environment. Although\nASMs allow modeling complex systems in a rather concise way -and this is\nadvantageous for specification purposes-, such concise notation is in general a\nproblem for verification activities as model checking and theorem proving that\nrely on tools accepting simpler notations.\n  In this paper, we propose a flattener tool integrated in the ASMETA framework\nthat transforms a general ASM model in a flattened model constituted only of\nupdate, parallel, and conditional rules; such model is easier to map to\nnotations of verification tools. Experiments show the effect of applying the\ntool to some representative case studies of the ASMETA repository.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:00:47 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Arcaini", "Paolo", "", "National Institute of Informatics"], ["Melioli", "Riccardo", "", "Dipartimento di Informatica, Universit\u00e0 degli Studi di Milano"], ["Riccobene", "Elvinia", "", "Dipartimento di Informatica, Universit\u00e0 degli Studi di Milano"]]}, {"id": "1811.10818", "submitter": "EPTCS", "authors": "Alexander Kn\\\"uppel, Thomas Th\\\"um, Carsten Pardylla, Ina Schaefer", "title": "Experience Report on Formally Verifying Parts of OpenJDK's API with KeY", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 53-70", "doi": "10.4204/EPTCS.284.5", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deductive verification of software has not yet found its way into industry,\nas complexity and scalability issues require highly specialized experts. The\nlong-term perspective is, however, to develop verification tools aiding\nindustrial software developers to find bugs or bottlenecks in software systems\nfaster and more easily. The KeY project constitutes a framework for specifying\nand verifying software systems, aiming at making formal verification tools\napplicable for mainstream software development. To help the developers of KeY,\nits users, and the deductive verification community, we summarize our\nexperiences with KeY 2.6.1 in specifying and verifying real-world Java code\nfrom a users perspective. To this end, we concentrate on parts of the\nCollections-API of OpenJDK 6, where an informal specification exists. While we\ndescribe how we bridged informal and formal specification, we also exhibit\naccompanied challenges that we encountered. Our experiences are that (a) in\nprinciple, deductive verification for API-like code bases is feasible, but\nrequires high expertise, (b) developing formal specifications for existing code\nbases is still notoriously hard, and (c) the under-specification of certain\nlanguage constructs in Java is challenging for tool builders. Our initial\neffort in specifying parts of OpenJDK 6 constitutes a stepping stone towards a\ncase study for future research.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:01:21 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kn\u00fcppel", "Alexander", ""], ["Th\u00fcm", "Thomas", ""], ["Pardylla", "Carsten", ""], ["Schaefer", "Ina", ""]]}, {"id": "1811.11093", "submitter": "Wenda Li", "authors": "Wenda Li and Lawrence C. Paulson", "title": "Counting Polynomial Roots in Isabelle/HOL: A Formal Proof of the\n  Budan-Fourier Theorem", "comments": "12 pages. Published at CPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in computer algebra and numerical analysis can be reduced to\ncounting or approximating the real roots of a polynomial within an interval.\nExisting verified root-counting procedures in major proof assistants are mainly\nbased on the classical Sturm theorem, which only counts distinct roots.\n  In this paper, we have strengthened the root-counting ability in Isabelle/HOL\nby first formally proving the Budan-Fourier theorem. Subsequently, based on\nDescartes' rule of signs and Taylor shift, we have provided a verified\nprocedure to efficiently over-approximate the number of real roots within an\ninterval, counting multiplicity. For counting multiple roots exactly, we have\nextended our previous formalisation of Sturm's theorem. Finally, we combine\nverified components in the developments above to improve our previous certified\ncomplex-root-counting procedures based on Cauchy indices. We believe those\nverified routines will be crucial for certifying programs and building tactics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:48:52 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Li", "Wenda", ""], ["Paulson", "Lawrence C.", ""]]}, {"id": "1811.11123", "submitter": "Claudio Menghi", "authors": "Claudio Menghi, Alessandro Maria Rizzi, Anna Bernasconi", "title": "Integrating Topological Proofs with Model Checking to Instrument\n  Iterative Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System development is not a linear, one-shot process. It proceeds through\nrefinements and revisions. To support assurance that the system satisfies its\nrequirements, it is desirable that continuous verification can be performed\nafter each refinement or revision step. To achieve practical adoption, formal\nsystem modeling and verification must accommodate continuous verification\nefficiently and effectively. Our proposal to address this problem is TOrPEDO, a\nverification approach where models are given via Partial Kripke Structures\n(PKSs) and requirements are specified as Linear-time Temporal Logic (LTL)\nproperties. PKSs support refinement, by deliberately indicating unspecified\nparts of the model that are later completed. We support verification in two\ncomplementary forms: via model checking and proofs. Model checking is useful to\nprovide counterexamples, i.e., pinpoint model behaviors that violate\nrequirements. Proofs are instead useful since they can explain why requirements\nare satisfied. In our work, we introduce a specific concept of proof, called\ntopological proof (TP). A TP produces a slice of the original PKS which\njustifies the property satisfaction. Because models can be incomplete, TOrPEDO\nsupports reasoning on requirements satisfaction, violation, and possible\nsatisfaction (in the case where the satisfaction depends on unknown parts).\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:27:23 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Menghi", "Claudio", ""], ["Rizzi", "Alessandro Maria", ""], ["Bernasconi", "Anna", ""]]}, {"id": "1811.11129", "submitter": "James  Peters Ph.D.", "authors": "M.Z. Ahmad, J.F. Peters", "title": "Descriptive Unions. A Fibre Bundle Characterization of the Union of\n  Descriptively Near Sets", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extension of descriptive intersection and provides a\nframework for descriptive unions of nonempty sets. Fibre bundles provide\nstructures that characterize spatially near as well as descriptively near sets,\ntheir descriptive intersection and their unions. The properties of four\ndifferent forms of descriptive unions are given. A main result given in this\npaper is the equivalence between ordinary set intersection and a descriptive\nunion. Applications of descriptive unions are given with respect to Jeffs-Novik\nconvex unions and descriptive unions in digital images.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:01:57 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Ahmad", "M. Z.", ""], ["Peters", "J. F.", ""]]}, {"id": "1811.11501", "submitter": "Markus Hecher", "authors": "Johannes K. Fichte, Markus Hecher, Arne Meier", "title": "Counting Complexity for Reasoning in Abstract Argumentation", "comments": "Extended version of a paper published at AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider counting and projected model counting of\nextensions in abstract argumentation for various semantics. When asking for\nprojected counts we are interested in counting the number of extensions of a\ngiven argumentation framework while multiple extensions that are identical when\nrestricted to the projected arguments count as only one projected extension. We\nestablish classical complexity results and parameterized complexity results\nwhen the problems are parameterized by treewidth of the undirected\nargumentation graph. To obtain upper bounds for counting projected extensions,\nwe introduce novel algorithms that exploit small treewidth of the undirected\nargumentation graph of the input instance by dynamic programming (DP). Our\nalgorithms run in time double or triple exponential in the treewidth depending\non the considered semantics. Finally, we take the exponential time hypothesis\n(ETH) into account and establish lower bounds of bounded treewidth algorithms\nfor counting extensions and projected extension.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 11:21:43 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Meier", "Arne", ""]]}, {"id": "1811.11961", "submitter": "Denis Firsov", "authors": "Denis Firsov, Larry Diehl, Christopher Jenkins, Aaron Stump", "title": "Course-of-Value Induction in Cedille", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the categorical setting, histomorphisms model a course-of-value recursion\nscheme that allows functions to be defined using arbitrary previously computed\nvalues. In this paper, we use the Calculus of Dependent Lambda Eliminations\n(CDLE) to derive a lambda-encoding of inductive datatypes that admits\ncourse-of-value induction. Similar to course-of-value recursion,\ncourse-of-value induction gives access to inductive hypotheses at arbitrary\ndepth of the inductive arguments of a function. We show that the derived\ncourse-of-value datatypes are well-behaved by proving Lambek's lemma and\ncharacterizing the computational behavior of the induction principle. Our work\nis formalized in the Cedille programming language and also includes several\nexamples of course-of-value functions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 04:43:22 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 18:18:05 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Firsov", "Denis", ""], ["Diehl", "Larry", ""], ["Jenkins", "Christopher", ""], ["Stump", "Aaron", ""]]}, {"id": "1811.12127", "submitter": "Anton Fuxjaeger", "authors": "Anton Fuxjaeger and Vaishak Belle", "title": "Scaling up Probabilistic Inference in Linear and Non-Linear Hybrid\n  Domains by Leveraging Knowledge Compilation", "comments": "In proceedings of ICAART, 2020. A version also appears in AAAI\n  Workshop: Statistical Relational Artificial Intelligence (StarAI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted model integration (WMI) extends weighted model counting (WMC) in\nproviding a computational abstraction for probabilistic inference in mixed\ndiscrete-continuous domains. WMC has emerged as an assembly language for\nstate-of-the-art reasoning in Bayesian networks, factor graphs, probabilistic\nprograms and probabilistic databases. In this regard, WMI shows immense promise\nto be much more widely applicable, especially as many real-world applications\ninvolve attribute and feature spaces that are continuous and mixed.\nNonetheless, state-of-the-art tools for WMI are limited and less mature than\ntheir propositional counterparts. In this work, we propose a new implementation\nregime that leverages propositional knowledge compilation for scaling up\ninference. In particular, we use sentential decision diagrams, a tractable\nrepresentation of Boolean functions, as the underlying model counting and model\nenumeration scheme. Our regime performs competitively to state-of-the-art WMI\nsystems but is also shown to handle a specific class of non-linear constraints\nover non-linear potentials.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:24:23 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 15:15:37 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Fuxjaeger", "Anton", ""], ["Belle", "Vaishak", ""]]}, {"id": "1811.12377", "submitter": "Lo\\\"ic Paulev\\'e", "authors": "Stefan Haar, Juraj Kol\\v{c}\\'ak, Lo\\\"ic Paulev\\'e", "title": "Combining Refinement of Parametric Models with Goal-Oriented Reduction\n  of Dynamics", "comments": "Proceedings paper of VMCAI 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric models abstract part of the specification of dynamical models by\nintegral parameters. They are for example used in computational systems\nbiology, notably with parametric regulatory networks, which specify the global\narchitecture (interactions) of the networks, while parameterising the precise\nrules for drawing the possible temporal evolutions of the states of the\ncomponents. A key challenge is then to identify the discrete parameters\ncorresponding to concrete models with desired dynamical properties. This paper\naddresses the restriction of the abstract execution of parametric regulatory\n(discrete) networks by the means of static analysis of reachability properties\n(goal states). Initially defined at the level of concrete parameterised models,\nthe goal-oriented reduction of dynamics is lifted to parametric networks, and\nis proven to preserve all the minimal traces to the specified goal states. It\nresults that one can jointly perform the refinement of parametric networks\n(restriction of domain of parameters) while reducing the necessary transitions\nto explore and preserving reachability properties of interest.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:44:28 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Haar", "Stefan", ""], ["Kol\u010d\u00e1k", "Juraj", ""], ["Paulev\u00e9", "Lo\u00efc", ""]]}, {"id": "1811.12576", "submitter": "\\'Etienne Andr\\'e", "authors": "\\'Etienne Andr\\'e, Beno\\^it Delahaye, Paulin Fournier, Didier Lime", "title": "Parametric Timed Broadcast Protocols", "comments": "This is the author (and extended) version of the manuscript of the\n  same name published in the proceedings of the 20th International Conference\n  on Verification, Model Checking, and Abstract Interpretation (VMCAI 2019).\n  This version contains additional examples and all proofs", "journal-ref": null, "doi": "10.1007/978-3-030-11245-5_23", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider state reachability in networks composed of many\nidentical processes running a parametric timed broadcast protocol (PTBP). PTBP\nare a new model extending both broadcast protocols and parametric timed\nautomata. This work is, up to our knowledge, the first to consider the\ncombination of both a parametric network size and timing parameters in clock\nguard constraints. Since the communication topology is of utmost importance in\nbroadcast protocols, we investigate reachability problems in both clique\nsemantics where every message reaches every processes, and in reconfigurable\nsemantics where the set of receivers is chosen non-deterministically. In\naddition, we investigate the decidability status depending on whether the\ntiming parameters in guards appear only as upper bounds in guards, as lower\nbounds or when the set of parameters is partitioned in lower-bound and\nupper-bound parameters.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 01:55:28 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 02:18:25 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Andr\u00e9", "\u00c9tienne", ""], ["Delahaye", "Beno\u00eet", ""], ["Fournier", "Paulin", ""], ["Lime", "Didier", ""]]}]