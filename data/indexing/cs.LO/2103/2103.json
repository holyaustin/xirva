[{"id": "2103.00223", "submitter": "Andr\\'as Kov\\'acs", "authors": "Andr\\'as Kov\\'acs", "title": "Generalized Universe Hierarchies and First-Class Universe Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In type theories, universe hierarchies are commonly used to increase the\nexpressive power of the theory while avoiding inconsistencies arising from size\nissues. There are numerous ways to specify universe hierarchies, and theories\nmay differ in details of cumulativity, choice of universe levels, specification\nof type formers and eliminators, and available internal operations on levels.\nIn the current work, we aim to provide a framework which covers a large part of\nthe design space. First, we develop syntax and semantics for cumulative\nuniverse hierarchies, where levels may come from any set equipped with a\ntransitive well-founded ordering. In the semantics, we show that\ninduction-recursion can be used to model transfinite hierarchies, and also\nsupport lifting operations on type codes which strictly preserve type formers.\nThen, we consider a setup where universe levels are first-class types and\nsubject to arbitrary internal reasoning. This generalizes the bounded\npolymorphism features of Coq and at the same time the internal level\ncomputations in Agda.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 14:07:24 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 13:49:13 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 10:46:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kov\u00e1cs", "Andr\u00e1s", ""]]}, {"id": "2103.00386", "submitter": "Z\\\"umr\\\"ut Ak\\c{c}am", "authors": "Z\\\"umr\\\"ut Ak\\c{c}am, Daniel S. Hono II, Paliath Narendran and Andrew\n  Pulver", "title": "On Problems Dual to Unification: The String-Rewriting Case", "comments": "28 pages, 6 figures, will be submitted for LMCS journal. arXiv admin\n  note: substantial text overlap with arXiv:1706.05607", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate problems which are dual to the unification\nproblem, namely the Fixed Point (FP) problem, Common Term (CT) problem and the\nCommon Equation (CE) problem for string rewriting systems. Our main motivation\nis computing fixed points in systems, such as loop invariants in programming\nlanguages. We show that the fixed point (FP) problem is reducible to the common\nterm problem. Our new results are: (i) the fixed point problem is undecidable\nfor finite convergent string rewriting systems whereas it is decidable in\npolynomial time for finite, convergent and dwindling string rewriting systems,\n(ii) the common term problem is undecidable for the class of dwindling string\nrewriting systems, and (iii) for the class of finite, monadic and convergent\nsystems, the common equation problem is decidable in polynomial time but for\nthe class of dwindling string rewriting systems, common equation problem is\nundecidable.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 03:34:02 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ak\u00e7am", "Z\u00fcmr\u00fct", ""], ["Hono", "Daniel S.", "II"], ["Narendran", "Paliath", ""], ["Pulver", "Andrew", ""]]}, {"id": "2103.00418", "submitter": "Francois Luus", "authors": "Francois Luus, Prithviraj Sen, Pavan Kapanipathi, Ryan Riegel,\n  Ndivhuwo Makondo, Thabang Lebese, Alexander Gray", "title": "Logic Embeddings for Complex Query Answering", "comments": "IBM Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering logical queries over incomplete knowledge bases is challenging\nbecause: 1) it calls for implicit link prediction, and 2) brute force answering\nof existential first-order logic queries is exponential in the number of\nexistential variables. Recent work of query embeddings provides fast querying,\nbut most approaches model set logic with closed regions, so lack negation.\nQuery embeddings that do support negation use densities that suffer drawbacks:\n1) only improvise logic, 2) use expensive distributions, and 3) poorly model\nanswer uncertainty. In this paper, we propose Logic Embeddings, a new approach\nto embedding complex queries that uses Skolemisation to eliminate existential\nvariables for efficient querying. It supports negation, but improves on density\napproaches: 1) integrates well-studied t-norm logic and directly evaluates\nsatisfiability, 2) simplifies modeling with truth values, and 3) models\nuncertainty with truth bounds. Logic Embeddings are competitively fast and\naccurate in query answering over large, incomplete knowledge graphs, outperform\non negation queries, and in particular, provide improved modeling of answer\nuncertainty as evidenced by a superior correlation between answer set size and\nembedding entropy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 07:52:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Luus", "Francois", ""], ["Sen", "Prithviraj", ""], ["Kapanipathi", "Pavan", ""], ["Riegel", "Ryan", ""], ["Makondo", "Ndivhuwo", ""], ["Lebese", "Thabang", ""], ["Gray", "Alexander", ""]]}, {"id": "2103.00729", "submitter": "Rob van Glabbeek", "authors": "Rob van Glabbeek, Ursula Goltz, Jens-Wolfhard Schicke", "title": "On Causal Semantics of Petri Nets", "comments": "This paper continues and completes the work of arXiv:1103.5916.\n  Together, these papers show that a structural conflict net is conflict-free\n  iff it has exactly one maximal run---equivalently formalised as maximal\n  BD-run, maximal BD-process or maximal GR-process up to swapping equivalence.\n  The \"only if\" direction, together with a taxonomy of suitable notions of\n  maximality, is contributed here", "journal-ref": null, "doi": null, "report-no": "Informatik Bericht Nr. 2011-06, TU Braunschweig", "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider approaches for causal semantics of Petri nets, explicitly\nrepresenting dependencies between transition occurrences. For one-safe nets or\ncondition/event-systems, the notion of process as defined by Carl Adam Petri\nprovides a notion of a run of a system where causal dependencies are reflected\nin terms of a partial order. A well-known problem is how to generalise this\nnotion for nets where places may carry several tokens. Goltz and Reisig have\ndefined such a generalisation by distinguishing tokens according to their\ncausal history. However, this so-called individual token interpretation is\noften considered too detailed. A number of approaches have tackled the problem\nof defining a more abstract notion of process, thereby obtaining a so-called\ncollective token interpretation. Here we give a short overview on these\nattempts and then identify a subclass of Petri nets, called structural conflict\nnets, where the interplay between conflict and concurrency due to token\nmultiplicity does not occur. For this subclass, we define abstract processes as\nequivalence classes of Goltz-Reisig processes. We justify this approach by\nshowing that we obtain exactly one maximal abstract process if and only if the\nunderlying net is conflict-free with respect to a canonical notion of conflict.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 03:24:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["van Glabbeek", "Rob", ""], ["Goltz", "Ursula", ""], ["Schicke", "Jens-Wolfhard", ""]]}, {"id": "2103.01046", "submitter": "Anil Shukla", "authors": "Anish Mallick, Anil Shukla", "title": "Extending Prolog for Quantified Boolean Horn Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prolog is a well known declarative programming language based on\npropositional Horn formulas. It is useful in various areas, including\nartificial intelligence, automated theorem proving, mathematical logic and so\non. An active research area for many years is to extend Prolog to larger\nclasses of logic. Some important extensions of it includes the constraint logic\nprogramming, and the object oriented logic programming. However, it cannot\nsolve problems having arbitrary quantified Horn formulas.\n  To be precise, the facts, rules and queries in Prolog are not allowed to have\narbitrary quantified variables. The paper overcomes this major limitations of\nProlog by extending it for the quantified Boolean Horn formulas. We achieved\nthis by extending the SLD-resolution proof system for quantified Boolean Horn\nformulas, followed by proposing an efficient model for implementation. The\npaper shows that the proposed implementation also supports the first-order\npredicate Horn logic with arbitrary quantified variables.\n  The paper also introduces for the first time, a declarative programming for\nthe quantified Boolean Horn formulas.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:39:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mallick", "Anish", ""], ["Shukla", "Anil", ""]]}, {"id": "2103.01199", "submitter": "Tikhon Pshenitsyn", "authors": "Tikhon Pshenitsyn", "title": "Introduction to a Hypergraph Logic Unifying Different Variants of the\n  Lambek Calculus", "comments": "Submitted to International Colloquium on Automata, Languages and\n  Programming 2021. arXiv admin note: text overlap with arXiv:2010.00819", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper hypergraph Lambek calculus ($\\mathrm{HL}$) is presented. This\nformalism aims to generalize the Lambek calculus ($\\mathrm{L}$) to hypergraphs\nas hyperedge replacement grammars extend context-free grammars. In contrast to\nthe Lambek calculus, $\\mathrm{HL}$ deals with hypergraph types and sequents;\nits axioms and rules naturally generalize those of $\\mathrm{L}$. Consequently,\ncertain properties (e.g. the cut elimination) can be lifted from $\\mathrm{L}$\nto $\\mathrm{HL}$. It is shown that $\\mathrm{L}$ can be naturally embedded in\n$\\mathrm{HL}$; moreover, a number of its variants ($\\mathrm{LP}$,\n$\\mathrm{NL}$, $\\mathrm{NLP}$, $\\mathrm{L}$ with modalities,\n$\\mathrm{L}^\\ast(\\mathbf{1})$, $\\mathrm{L}^{\\mathrm{R}}$) can also be embedded\nin $\\mathrm{HL}$ via different graph constructions. We also establish a\nconnection between $\\mathrm{HL}$ and Datalog with embedded implications. It is\nproved that the parsing problem for $\\mathrm{HL}$ is NP-complete.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:46:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Pshenitsyn", "Tikhon", ""]]}, {"id": "2103.01395", "submitter": "Peter Baumgartner", "authors": "Peter Baumgartner", "title": "The Fusemate Logic Programming System (System Description)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusemate is a logic programming system that implements the possible model\nsemantics for disjunctive logic programs. Its input language is centered around\na weak notion of stratification with comprehension and aggregation operators on\ntop of it. Fusemate is implemented as a shallow embedding in the Scala\nprogramming language. This enables using Scala data types natively as terms, a\ntight interface with external systems, and it makes model computation available\nas an ordinary container data structure constructor. The paper describes the\nabove features and demonstrates them with a non-trivial use-case, the embedding\nof the description logic $\\cal ALCIF$ into Fusemate's input language\n  This version of the paper corrects an error in the published version, which\nused an unsuitable version of \"blocking\" in the $\\cal ALCIF$ embedding.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:15:56 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 10:31:51 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 05:25:54 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Baumgartner", "Peter", ""]]}, {"id": "2103.01490", "submitter": "Rob van Glabbeek", "authors": "Rob van Glabbeek, Ursula Goltz and Jens-Wolfhard Schicke-Uffmann", "title": "Abstract Processes and Conflicts in Place/Transition Systems", "comments": "The results of this paper appeared before in arXiv:2103.00729.\n  However, there they were formulated differently, as we didn't have the\n  current concept of a largest abstract process. Our proofs are conceptually\n  much simpler than the ones in arXiv:2103.00729, as they are carried out\n  directly on abstract processes, rather than via the auxiliary concepts of\n  BD-runs and FS-runs. arXiv admin note: text overlap with arXiv:1103.5916", "journal-ref": null, "doi": "10.1016/j.ic.2021.104706", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For one-safe Petri nets or condition/event-systems, a process as defined by\nCarl Adam Petri provides a notion of a run of a system where causal\ndependencies are reflected in terms of a partial order. Goltz and Reisig have\ngeneralised this concept for nets where places carry multiple tokens, by\ndistinguishing tokens according to their causal history. However, this\nso-called individual token interpretation is often considered too detailed.\nHere we identify a subclass of Petri nets, called structural conflict nets,\nwhere no interplay between conflict and concurrency due to token multiplicity\noccurs. For this subclass, we define abstract processes as equivalence classes\nof Goltz-Reisig processes. We justify this approach by showing that there is a\nlargest abstract process if and only if the underlying net is conflict-free\nwith respect to a canonical notion of conflict.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:23:45 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["van Glabbeek", "Rob", ""], ["Goltz", "Ursula", ""], ["Schicke-Uffmann", "Jens-Wolfhard", ""]]}, {"id": "2103.01671", "submitter": "Johannes Marti", "authors": "Johannes Marti and Yde Venema", "title": "Focus-style proof systems and interpolation for the alternation-free\n  $\\mu$-calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a cut-free sequent calculus for the\nalternation-free fragment of the modal $\\mu$-calculus. This system allows for\ncyclic proofs and uses a simple focus mechanism to control the unravelling of\nfixpoints along infinite branches. We show that the proof system is sound and\ncomplete and apply it to prove that the alternation-free fragment has the Craig\ninterpolation property.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:09:50 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 18:08:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Marti", "Johannes", ""], ["Venema", "Yde", ""]]}, {"id": "2103.01734", "submitter": "Cosimo Perini Brogi", "authors": "Cosimo Perini Brogi", "title": "An analytic calculus for intuitionistic belief", "comments": "This is a very rough draft that is intended as the second part of\n  work-in-progress started with [13]. For sure, many expository refinements are\n  required to the present paper: it is basically a collection of rough results\n  and reflections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intuitionistic belief has been axiomatized by Artemov and Protopopescu as an\nextension of intuitionistic propositional logic by means of the distributivity\nscheme K, and of co-reflection $A\\rightarrow\\Box A$. This way, belief is\ninterpreted as a result of verification, and it fits an extended\nBrouwer-Heyting-Kolmogorov interpretation for intuitionistic propositional\nlogic with an epistemic modality. In the present paper, structural properties\nof a natural deduction system $\\mathsf{IEL}^{-}$ for intuitionistic belief are\ninvestigated. The focus is on the analyticity of the calculus, so that the\nnormalization theorem and the subformula property are proven firstly. From\nthese, decidability and consistency of the logic follow as corollaries.\nFinally, disjunction properties, $\\Box$-primality, and admissibility of\nreflection rule are established by using purely proof-theoretic methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:10:43 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 15:17:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Brogi", "Cosimo Perini", ""]]}, {"id": "2103.01911", "submitter": "W{\\l}odzimierz Drabent", "authors": "W{\\l}odzimierz Drabent", "title": "SLD-resolution without occur-check, an example", "comments": "9 pages. This version: small corrections, mainly p.7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the occur-check is not needed for a certain definite clause\nlogic program, independently from the selection rule. First we prove that the\nprogram is occur-check free. Then we consider a more general class of queries,\nunder which the program is not occur-check free; however we show that it will\nbe correctly executed under Prolog without occur-check.\n  The main result of this report states that the occur-check may be skipped for\nthe cases in which a single run of a standard nondeterministic unification\nalgorithm does not fail due to the occur-check. The usual approaches are based\non the notion of NSTO (not subject to occur-check), which considers all the\nruns. To formulate the result, it was necessary to introduce an abstraction of\na \"unification\" algorithm without the occur-check.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:09:09 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 17:27:10 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Drabent", "W\u0142odzimierz", ""]]}, {"id": "2103.02024", "submitter": "Jason Z.S. Hu", "authors": "Jason Z. S. Hu", "title": "Internal Category with Families in Presheaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we review a construction of category with families (CwF) in a\npresheaf category. When the base category of a presheaf category is a CwF, we\ninternalize this CwF structure in the CwF of the presheaf category. This note\nassumes working knowledge on category theory.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:05:28 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hu", "Jason Z. S.", ""]]}, {"id": "2103.02073", "submitter": "Alexandre Cl\\'ement", "authors": "Cyril Branciard, Alexandre Cl\\'ement, Mehdi Mhalla, and Simon Perdrix", "title": "Coherent control and distinguishability of quantum channels via\n  PBS-diagrams", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a graphical language for coherent control of general quantum\nchannels inspired by practical quantum optical setups involving polarising beam\nsplitters (PBS). As standard completely positive trace preserving maps are\nknown not to be appropriate to represent coherently controlled quantum\nchannels, we propose to instead use purified channels, an extension of\nStinespring's dilation. We characterise the observational equivalence of\npurified channels in various coherent-control contexts, paving the way towards\na faithful representation of quantum channels under coherent control.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 22:56:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Branciard", "Cyril", ""], ["Cl\u00e9ment", "Alexandre", ""], ["Mhalla", "Mehdi", ""], ["Perdrix", "Simon", ""]]}, {"id": "2103.02191", "submitter": "Gelin Zhang", "authors": "Gelin Zhang, Zhe Hou, Yanhong Huang, Jianqi Shi, Hadrien Bride, Jin\n  Song Dong, Yongsheng Gao", "title": "Extracting Optimal Explanations for Ensemble Trees via Logical Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensemble trees are a popular machine learning model which often yields high\nprediction performance when analysing structured data. Although individual\nsmall decision trees are deemed explainable by nature, an ensemble of large\ntrees is often difficult to understand. In this work, we propose an approach\ncalled optimised explanation (OptExplain) that faithfully extracts global\nexplanations of ensemble trees using a combination of logical reasoning,\nsampling and optimisation. Building on top of this, we propose a method called\nthe profile of equivalent classes (ProClass), which uses MAX-SAT to simplify\nthe explanation even further. Our experimental study on several datasets shows\nthat our approach can provide high-quality explanations to large ensemble trees\nmodels, and it betters recent top-performers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:35:13 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhang", "Gelin", ""], ["Hou", "Zhe", ""], ["Huang", "Yanhong", ""], ["Shi", "Jianqi", ""], ["Bride", "Hadrien", ""], ["Dong", "Jin Song", ""], ["Gao", "Yongsheng", ""]]}, {"id": "2103.02302", "submitter": "Nicholas Houghton-Larsen", "authors": "Nicholas Gauguin Houghton-Larsen", "title": "A Mathematical Framework for Causally Structured Dilations and its\n  Relation to Quantum Self-Testing", "comments": "PhD thesis submitted to the University of Copenhagen (ISBN\n  978-87-7125-039-8). Advised by prof. Matthias Christandl, submitted 1st of\n  December 2020, defended 11th of February 2021. Keywords: dilations, applied\n  category theory, quantum foundations, causal structure, quantum self-testing.\n  242 pages, 1 figure. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation for this thesis was to recast quantum self-testing [MY98,MY04]\nin operational terms. The result is a category-theoretic framework for\ndiscussing the following general question: How do different implementations of\nthe same input-output process compare to each other? In the proposed framework,\nan input-output process is modelled by a causally structured channel in some\nfixed theory, and its implementations are modelled by causally structured\ndilations formalising hidden side-computations. These dilations compare through\na pre-order formalising relative strength of side-computations. Chapter 1\nreviews a mathematical model for physical theories as semicartesian symmetric\nmonoidal categories. Many concrete examples are discussed, in particular\nquantum and classical information theory. The key feature is that the model\nfacilitates the notion of dilations. Chapter 2 is devoted to the study of\ndilations. It introduces a handful of simple yet potent axioms about dilations,\none of which (resembling the Purification Postulate [CDP10]) entails a duality\ntheorem encompassing a large number of classic no-go results for quantum\ntheory. Chapter 3 considers metric structure on physical theories, introducing\nin particular a new metric for quantum channels, the purified diamond distance,\nwhich generalises the purified distance [TCR10,Tom12] and relates to the Bures\ndistance [KSW08a]. Chapter 4 presents a category-theoretic formalism for\ncausality in terms of '(constructible) causal channels' and 'contractions'. It\nsimplifies aspects of the formalisms [CDP09,KU17] and relates to traces in\nmonoidal categories [JSV96]. The formalism allows for the definition of 'causal\ndilations' and the establishment of a non-trivial theory of such dilations.\nChapter 5 realises quantum self-testing from the perspective of chapter 4, thus\npointing towards the first known operational foundation for self-testing.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:32:34 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Houghton-Larsen", "Nicholas Gauguin", ""]]}, {"id": "2103.02343", "submitter": "Alexander Gheorghiu", "authors": "Alexander Gheorghiu, Simon Docherty, and David Pym", "title": "Provability in BI's Sequent Calculus is Decidable", "comments": "Submitted to CADE-28", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SC math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The logic of Bunched Implications (BI) combines both additive and\nmultiplicative connectives, which include two primitive intuitionistic\nimplications. As a consequence, contexts in the sequent presentation are not\nlists, nor multisets, but rather tree-like structures called bunches. This\nadditional complexity notwithstanding, the logic has a well-behaved metatheory\nadmitting all the familiar forms of semantics and proof systems. However, the\npresentation of an effective proof-search procedure has been elusive since the\nlogic's debut. We show that one can reduce the proof-search space for any given\nsequent to a primitive recursive set, the argument generalizing Gentzen's\ndecidability argument for classical propositional logic and combining key\nfeatures of Dyckhoff's contraction-elimination argument for intuitionistic\nlogic. An effective proof-search procedure, and hence decidability of\nprovability, follows as a corollary.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:48:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gheorghiu", "Alexander", ""], ["Docherty", "Simon", ""], ["Pym", "David", ""]]}, {"id": "2103.02355", "submitter": "Mohammad Abdulaziz", "authors": "Mohammad Abdulaziz", "title": "Cost Optimal Planning as Satisfiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate upper bounds on the length of cost optimal plans that are\nvalid for problems with 0-cost actions. We employ these upper bounds as\nhorizons for a SAT-based encoding of planning with costs. Given an initial\nupper bound on the cost of the optimal plan, we experimentally show that this\nSAT-based approach is able to compute plans with better costs, and in many\ncases it can match the optimal cost. Also, in multiple instances, the approach\nis successful in proving that a certain cost is the optimal plan cost.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:18:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Abdulaziz", "Mohammad", ""]]}, {"id": "2103.02976", "submitter": "Nikita Zyuzin", "authors": "Nikita Zyuzin, Aleksandar Nanevski", "title": "Contextual Modal Types for Algebraic Effects and Handlers", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Programming languages with algebraic effects often rely on effect annotations\nto track algebraic operations in a computation. This is particularly important\nin the presence of effect handlers, where handling of an effect may remove it\nfrom the annotation in the computation's type.\n  In this paper we view the algebraic effect theory of a computation as a\nvariable context, and track the effects directly in computation's modal type\nwith ECMTT, a novel calculus for algebraic effects and handlers. Our calculus\nsupports computations with effects and derives from Contextual Modal Type\nTheory (CMTT), from which it inherits a number of important logical properties.\n  In contrast to type-and-effect systems, where effects are attached as\nannotations to a prior computational language, ECMMT tracks effects by a\ncontextualized variant of the modal $\\Box$ (necessity) operator of the\nintuitionistic modal logic of CMTT. In programming, the effect annotations are\nexplicitly managed by the dedicated term constructors corresponding to the\nlogical introduction and elimination forms for the modality. The modal\nfoundation leads to customary logical properties of local soundness and\ncompleteness, and determines the operational semantics of ECMTT directly by\n$\\beta$-reduction. In addition, effect handlers become a generalization of\nexplicit substitutions, which in CMTT serve to reach one context from another.\n  To the best of our knowledge, ECMTT is the first system to relate algebraic\neffects to modal types. We also see it as a step towards providing a\ncorrespondence in the style of Curry and Howard that may transfer a number of\nresults from the fields of modal logic and modal type theory to that of\nalgebraic effects.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:50:25 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zyuzin", "Nikita", ""], ["Nanevski", "Aleksandar", ""]]}, {"id": "2103.03032", "submitter": "Hans van Ditmarsch", "authors": "Hans van Ditmarsch", "title": "Wanted Dead or Alive : Epistemic logic for impure simplicial complexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a logic of knowledge for impure simplicial complexes. Impure\nsimplicial complexes represent distributed systems under uncertainty over which\nprocesses are still active (are alive) and which processes have failed or\ncrashed (are dead). Our work generalizes the logic of knowledge for pure\nsimplicial complexes, where all processes are alive, by Goubault et al. Our\nlogical semantics has a satisfaction relation defined simultaneously with a\ndefinability relation. The latter restricts which formulas are allowed to have\na truth value: dead processes cannot know or be ignorant of any proposition,\nand live processes cannot know or be ignorant of propositions involving\nprocesses they know to be dead. The logic satisfies some but not all axioms and\nrules of the modal logic S5. Impure simplicial complexes correspond to Kripke\nmodels where each agent's accessibility relation is an equivalence relation on\na subset of the domain only, and otherwise empty, and where each propositional\nvariable is known by an agent. We also propose a notion of bisimulation for\nimpure simplexes and show bisimulation correspondence on certain finitary\nsimplexes. % Dynamic aspects of our semantics, such as how to formalize\npossibly incomplete tasks and algorithms in distributed computing, is left for\nfuture research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:47:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["van Ditmarsch", "Hans", ""]]}, {"id": "2103.03482", "submitter": "William Wagner", "authors": "William Wagner, Anna \\'Zakowska, Clement Aladi, Joseph Santhosh", "title": "Pilot Investigation for a Comprehensive Taxonomy of Autonomous Entities", "comments": "15 pages, 4 figures, 7 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LO cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper documents an exploratory pilot study to define the term Autonomous\nEntity, and any characteristics that are required to identify or classify an\nAutonomous Entity. Our solution builds on previous work with regard to\nphilosophical and scientific classification methods but focuses on a novel\nDesign Science Research Methodology (DSRM) and model to help identify those\ncharacteristics which make any autonomous entity similar or different from\nothers. We have solved the problem of not having an existing term to define our\nlens by creating a new combinatorial term: \"Riskyishness\". We present a DSRM\nand instrument for initial investigation, as well as observational and\nstatistical descriptions of their use in the real world to solicit domain\nexpertise and statistical evidence. Further, we demonstrate a specific\napplication of the methodology by creating a second artifact - a tool to score\nexisting and future technologies based on Riskyishness. The first artifact also\nprovides a technique to disentangle miscellaneous existing technologies or add\ndimensions to the tools to capture future additions and paradigm shifts.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:51:40 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 07:33:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wagner", "William", ""], ["\u0179akowska", "Anna", ""], ["Aladi", "Clement", ""], ["Santhosh", "Joseph", ""]]}, {"id": "2103.03599", "submitter": "Laura Kovacs", "authors": "Andreas Humenberger and Laura Kovacs", "title": "Algebra-based Synthesis of Loops and their Invariants (Invited Paper)", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-67067-2_2", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Provably correct software is one of the key challenges in our softwaredriven\nsociety. While formal verification establishes the correctness of a given\nprogram, the result of program synthesis is a program which is correct by\nconstruction. In this paper we overview some of our results for both of these\nscenarios when analysing programs with loops. The class of loops we consider\ncan be modelled by a system of linear recurrence equations with constant\ncoefficients, called C-finite recurrences. We first describe an algorithmic\napproach for synthesising all polynomial equality invariants of such\nnon-deterministic numeric single-path loops. By reverse engineering invariant\nsynthesis, we then describe an automated method for synthesising program loops\nsatisfying a given set of polynomial loop invariants. Our results have\napplications towards proving partial correctness of programs, compiler\noptimisation and generating number sequences from algebraic relations.\n  This is a preprint that was invited for publication at VMCAI 2021.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:05:23 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Humenberger", "Andreas", ""], ["Kovacs", "Laura", ""]]}, {"id": "2103.03607", "submitter": "Laura Kovacs", "authors": "Laura Kovacs, Hanna Lachnitt, and Stefan Szeider", "title": "Formalizing Graph Trail Properties in Isabelle/HOL", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-53518-6_8", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a dataset expressing and proving properties of graph trails,\nusing Isabelle/HOL. We formalize the reasoning about strictly increasing and\ndecreasing trails, using weights over edges, and prove lower bounds over the\nlength of trails in weighted graphs. We do so by extending the graph theory\nlibrary of Isabelle/HOL with an algorithm computing the length of a longest\nstrictly decreasing graph trail starting from a vertex for a given weight\ndistribution, and prove that any decreasing trail is also an increasing one.\n  This preprint has been accepted for publication at CICM 2020.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:22:29 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kovacs", "Laura", ""], ["Lachnitt", "Hanna", ""], ["Szeider", "Stefan", ""]]}, {"id": "2103.03871", "submitter": "Francesco Gavazzo", "authors": "Ugo Dal Lago and Francesco Gavazzo", "title": "Modal Reasoning = Metric Reasoning, via Lawvere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graded modal types systems and coeffects are becoming a standard formalism to\ndeal with context-dependent computations where code usage plays a central role.\nThe theory of program equivalence for modal and coeffectful languages, however,\nis considerably underdeveloped if compared to the denotational and operational\nsemantics of such languages. This raises the question of how much of the theory\nof ordinary program equivalence can be given in a modal scenario. In this work,\nwe show that coinductive equivalences can be extended to a modal setting, and\nwe do so by generalising Abramsky's applicative bisimilarity to coeffectful\nbehaviours. To achieve this goal, we develop a general theory of ternary\nprogram relations based on the novel notion of a comonadic lax extension, on\ntop of which we define a modal extension of Abramsky's applicative bisimilarity\n(which we dub modal applicative bisimilarity). We prove such a relation to be a\ncongruence, this way obtaining a compositional technique for reasoning about\nmodal and coeffectful behaviours. But this is not the end of the story: we also\nestablish a correspondence between modal program relations and program\ndistances. This correspondence shows that modal applicative bisimilarity and (a\nproperly extended) applicative bisimilarity distance coincide, this way\nrevealing that modal program equivalences and program distances are just two\nsides of the same coin.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:57:18 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Gavazzo", "Francesco", ""]]}, {"id": "2103.03930", "submitter": "Yuri Gurevich", "authors": "Yuri Gurevich", "title": "Logical foundations: Personal perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.HO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an attempt to illustrate the glorious history of logical foundations\nand to discuss the uncertain future.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:26:33 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gurevich", "Yuri", ""]]}, {"id": "2103.03965", "submitter": "Christopher Porter", "authors": "Adam Case, Christopher P. Porter", "title": "The intersection of algorithmically random closed sets and effective\n  dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we study several aspects of the intersections of\nalgorithmically random closed sets. First, we answer a question of Cenzer and\nWeber, showing that the operation of intersecting relatively random closed sets\n(with respect to certain underlying measures induced by Bernoulli measures on\nthe space of codes of closed sets), which preserves randomness, can be\ninverted: a random closed set of the appropriate type can be obtained as the\nintersection of two relatively random closed sets. We then extend the\nCenzer/Weber analysis to the intersection of multiple random closed sets,\nidentifying the Bernoulli measures with respect to which the intersection of\nrelatively random closed sets can be non-empty. We lastly apply our analysis to\nprovide a characterization of the effective Hausdorff dimension of sequences in\nterms of the degree of intersectability of random closed sets that contain\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 22:36:43 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Case", "Adam", ""], ["Porter", "Christopher P.", ""]]}, {"id": "2103.04287", "submitter": "Thierry Coquand", "authors": "Thierry Coquand", "title": "Reduction Free Normalisation for a proof irrelevant type of propositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show normalisation and decidability of convertibility for a type theory\nwith a hierarchy of universes and a proof irrelevant type of propositions,\nclose to the type system used in the proof assistant Lean. Contrary to previous\narguments, the proof does not require explicitly to introduce a notion of\nneutral and normal forms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 07:39:52 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Coquand", "Thierry", ""]]}, {"id": "2103.04841", "submitter": "Alberto Termine Sig.", "authors": "Alberto Termine, Alessandro Antonucci, Alessandro Facchini, Giuseppe\n  Primiero", "title": "Robust Model Checking with Imprecise Markov Reward Models", "comments": "Forthcoming in the proceedings of ISIPTA 2021 (International\n  Symposium of Imprecise Probability: Theory and Applications)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years probabilistic model checking has become an important area of\nresearch because of the diffusion of computational systems of stochastic\nnature. Despite its great success, standard probabilistic model checking\nsuffers the limitation of requiring a sharp specification of the probabilities\ngoverning the model behaviour. The theory of imprecise probabilities offers a\nnatural approach to overcome such limitation by a sensitivity analysis with\nrespect to the values of these parameters. However, only extensions based on\ndiscrete-time imprecise Markov chains have been considered so far for such a\nrobust approach to model checking. We present a further extension based on\nimprecise Markov reward models. In particular, we derive efficient algorithms\nto compute lower and upper bounds of the expected cumulative reward and\nprobabilistic bounded rewards based on existing results for imprecise Markov\nchains. These ideas are tested on a real case study involving the spend-down\ncosts of geriatric medicine departments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:47:40 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 15:32:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Termine", "Alberto", ""], ["Antonucci", "Alessandro", ""], ["Facchini", "Alessandro", ""], ["Primiero", "Giuseppe", ""]]}, {"id": "2103.04961", "submitter": "Stephen Wolfram", "authors": "Stephen Wolfram", "title": "Multiway Turing Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiway Turing machines (also known as nondeterministic Turing machines or\nNDTMs) with explicit, simple rules are studied. Even very simple rules are\nfound to generate complex behavior, characterized by complex multiway graphs,\nthat can be visualized in multispace that combines \"tape\" and branchial space.\nThe threshold for complex behavior appears to be machines with just s = 1 head\nstates, k = 2 tape colors and p = 3 possible cases, and such machines may\npotentially be universal. Other characteristics of multiway Turing machines are\nalso studied, including causal invariance, cyclic tapes and generalized busy\nbeaver problems. Multiway Turing machines provide minimal examples of a variety\nof issues encountered in both concurrent computing and the theory of observers\nin quantum mechanics, especially in our recent models of physics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:32:21 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wolfram", "Stephen", ""]]}, {"id": "2103.05087", "submitter": "Alessio Mansutti", "authors": "Dmitry Chistikov, Christoph Haase, Alessio Mansutti", "title": "Presburger arithmetic with threshold counting quantifiers is easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a quantifier elimination procedures for the extension of Presburger\narithmetic with a unary threshold counting quantifier $\\exists^{\\ge c} y$ that\ndetermines whether the number of different $y$ satisfying some formula is at\nleast $c \\in \\mathbb N$, where $c$ is given in binary. Using a standard\nquantifier elimination procedure for Presburger arithmetic, the resulting\ntheory is easily seen to be decidable in 4ExpTime. Our main contribution is to\ndevelop a novel quantifier-elimination procedure for a more general counting\nquantifier that decides this theory in 3ExpTime, meaning that it is no harder\nto decide than standard Presburger arithmetic. As a side result, we obtain an\nimproved quantifier elimination procedure for Presburger arithmetic with\ncounting quantifiers as studied by Schweikardt [ACM Trans. Comput. Log., 6(3),\npp. 634-671, 2005], and a 3ExpTime quantifier-elimination procedure for\nPresburger arithmetic extended with a generalised modulo counting quantifier.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:36:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chistikov", "Dmitry", ""], ["Haase", "Christoph", ""], ["Mansutti", "Alessio", ""]]}, {"id": "2103.05117", "submitter": "Krzysztof Mierzewski", "authors": "Johan van Benthem, Krzysztof Mierzewski and Francesca Zaffora Blando", "title": "The Modal Logic of Stepwise Removal", "comments": null, "journal-ref": null, "doi": "10.1017/S1755020320000258", "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We investigate the modal logic of stepwise removal of objects, both for its\nintrinsic interest as a logic of quantification without replacement, and as a\npilot study to better understand the complexity jumps between dynamic epistemic\nlogics of model transformations and logics of freely chosen graph changes that\nget registered in a growing memory. After introducing this logic\n($\\textsf{MLSR}$) and its corresponding removal modality, we analyze its\nexpressive power and prove a bisimulation characterization theorem. We then\nprovide a complete Hilbert-style axiomatization for the logic of stepwise\nremoval in a hybrid language enriched with nominals and public announcement\noperators. Next, we show that model-checking for $\\textsf{MLSR}$ is\nPSPACE-complete, while its satisfiability problem is undecidable. Lastly, we\nconsider an issue of fine-structure: the expressive power gained by adding the\nstepwise removal modality to fragments of first-order logic.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:06:06 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["van Benthem", "Johan", ""], ["Mierzewski", "Krzysztof", ""], ["Blando", "Francesca Zaffora", ""]]}, {"id": "2103.05550", "submitter": "Sarah Winter", "authors": "Emmanuel Filiot and Christof L\\\"oding and Sarah Winter", "title": "Synthesis from Weighted Specifications with Partial Domains over Finite\n  Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.GT cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the synthesis problem of terminating reactive\nsystems from quantitative specifications. Such systems are modeled as finite\ntransducers whose executions are represented as finite words in $(I\\times\nO)^*$, where $I,O$ are finite sets of input and output symbols, respectively. A\nweighted specification $S$ assigns a rational value (or $-\\infty$) to words in\n$(I\\times O)^*$, and we consider three kinds of objectives for synthesis,\nnamely threshold objectives where the system's executions are required to be\nabove some given threshold, best-value and approximate objectives where the\nsystem is required to perform as best as it can by providing output symbols\nthat yield the best value and $\\varepsilon$-best value respectively w.r.t. $S$.\nWe establish a landscape of decidability results for these three objectives and\nweighted specifications with partial domain over finite words given by\ndeterministic weighted automata equipped with sum, discounted-sum and average\nmeasures. The resulting objectives are not regular in general and we develop an\ninfinite game framework to solve the corresponding synthesis problems, namely\nthe class of (weighted) critical prefix games.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:57:12 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Filiot", "Emmanuel", ""], ["L\u00f6ding", "Christof", ""], ["Winter", "Sarah", ""]]}, {"id": "2103.05581", "submitter": "William DeMeo", "authors": "William DeMeo", "title": "The Agda Universal Algebra Library, Part 1: Foundation", "comments": "32 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Agda Universal Algebra Library (UALib) is a library of types and programs\n(theorems and proofs) we developed to formalize the foundations of universal\nalgebra in dependent type theory using the Agda programming language and proof\nassistant. The UALib includes a substantial collection of definitions,\ntheorems, and proofs from general algebra and equational logic, including many\nexamples that exhibit the power of inductive and dependent types for\nrepresenting and reasoning about relations, algebraic structures, and\nequational theories. In this paper we discuss the logical foundations on which\nthe library is built, and describe the types defined in the first 13 modules of\nthe library. Special attention is given to aspects of the library that seem\nmost interesting or challenging from a type theory or mathematical foundations\nperspective.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:38:07 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 22:16:13 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 14:52:25 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["DeMeo", "William", ""]]}, {"id": "2103.05674", "submitter": "Sarah Winter", "authors": "Emmanuel Filiot and Sarah Winter", "title": "Continuous Uniformization of Rational Relations and Synthesis of\n  Computable Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A uniformizer of a binary relation is a function whose graph is contained in\nthe relation and which is defined on the same domain as the relation. It is\nknown that any rational relation of infinite words, i.e. a relation given as a\ntransducer, admits a rational uniformizer. Although rational, those\nuniformizers are not necessarily well-behaved, in the sense that the $i$th\nletter of the output word may depend on the whole infinite input word. In other\nwords, those uniformizers might not be continuous (for the Cantor topology).\nThis paper addresses the question of whether rational relations of infinite\nwords can be uniformized by continuous functions. On the negative side,\ncontinuous uniformizers might not exist in general and we prove that deciding\ntheir existence is algorithmically impossible. On the positive side, we exhibit\na large class of rational relations of infinite words, called weakly\ndeterministic rational relations, for which deciding whether a relation in this\nclass admits a continuous uniformizer is an ExpTime-c problem. This class\nincludes the known classes of deterministic rational relations and automatic\nrelations of infinite words.\n  As an application of the previous result, and by exploiting a connection\nbetween computability and continuity for rational functions of infinite words,\nwe show a result on the synthesis of computable functions from specifications\ngiven as weakly deterministic rational relations. In particular, we show that\ndeciding the existence of a computable uniformizer is ExpTime-c and if there is\none, it is possible to effectively synthesize a deterministic two-way\ntransducer computing it. This generalizes the classical setting of Church\nsynthesis to asynchronous implementations which can arbitrarily delay the\nproduction of their output signals.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:21:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Filiot", "Emmanuel", ""], ["Winter", "Sarah", ""]]}, {"id": "2103.05776", "submitter": "Hao Ren Ph.D.", "authors": "Hao Ren, Ratnesh Kumar, Matthew Clark", "title": "\"ReLIC: Reduced Logic Inference for Composition\" for Quantifier\n  Elimination based Compositional Reasoning and Verification", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents our research on quantifier elimination (QE) for\ncompositional reasoning and verification. For compositional reasoning, QE\nprovides the foundation of our approach, serving as the calculus for\ncomposition to derive the strongest system-property in a single step, from the\ngiven component atomic-properties and their interconnection relation. We first\ndeveloped this framework for time-independent properties, and later extended it\nto time-dependent property composition. The extension requires, in addition,\nshifting the given properties along time to span the time horizon of interest,\nhe least of which for the strongest system-property is no more than the total\ntime horizons of the component level atomic-properties. The\nsystem-initial-condition is also composed from atomic-initial-conditions of the\ncomponents the same way. It is used to verify a desired system-level property,\nalongside the derived strongest system-property, by way of induction. Our\ncomposition approach is uniform regardless of the composition types\n(cascade/parallel/feedback) for both time-dependent and time-independent\nproperties.\n  We developed a new prototype verifier named ReLIC (Reduced Logic Inference\nfor Composition) that implements our above approaches. We demonstrated it\nthrough several illustrative and practical examples. Further, we advanced the\n$k$-induction based model-checking with QE capabilities, by formulating its\nbase and inductive steps into QE problems where all the variables are\nuniversally quantified. Our integration of the QE solver Redlog with the\n$k$-induction based model-checking tool JKind, shows the successful solving of\na non-linear problem that the SMT capable JKind failed to resolve. Finally, we\nalso showcase the recent adoption of our approaches within an industrial V\\&V\ntool suite for augmented static analysis of Simulink models and Deep Neural\nNetworks (DNNs).\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 18:35:29 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ren", "Hao", ""], ["Kumar", "Ratnesh", ""], ["Clark", "Matthew", ""]]}, {"id": "2103.05895", "submitter": "Tianyu Wang", "authors": "Tianyu Wang, Nikolay Atanasov", "title": "Inverse Reinforcement Learning of Autonomous Behaviors Encoded as\n  Weighted Finite Automata", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method for learning logical task specifications and\ncost functions from demonstrations. Linear temporal logic (LTL) formulas are\nwidely used to express complex objectives and constraints for autonomous\nsystems. Yet, such specifications may be challenging to construct by hand.\nInstead, we consider demonstrated task executions, whose temporal logic\nstructure and transition costs need to be inferred by an autonomous agent. We\nemploy a spectral learning approach to extract a weighted finite automaton\n(WFA), approximating the unknown logic structure of the task. Thereafter, we\ndefine a product between the WFA for high-level task guidance and a Labeled\nMarkov decision process (L-MDP) for low-level control and optimize a cost\nfunction that matches the demonstrator's behavior. We demonstrate that our\nmethod is capable of generalizing the execution of the inferred task\nspecification to new environment configurations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 06:42:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wang", "Tianyu", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2103.06741", "submitter": "Francesco Santini", "authors": "Fabio Gadducci and Francesco Santini", "title": "Residuation for Soft Constraints: Lexicographic Orders and Approximation\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residuation theory concerns the study of partially ordered algebraic\nstructures, most often monoids, equipped with a weak inverse for the monoidal\noperator. One of its area of application has been constraint programming, whose\nkey requirement is the presence of an aggregator operator for combining\npreferences. Given a residuated monoid of preferences, the paper first shows\nhow to build a new residuated monoid of (possibly infinite) tuples, which is\nbased on the lexicographic order. Second, it introduces a variant of an\napproximation technique (known as Mini-bucket) that exploits the presence of\nthe weak inverse.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:39:48 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Gadducci", "Fabio", ""], ["Santini", "Francesco", ""]]}, {"id": "2103.06931", "submitter": "Stephen Wolfram", "authors": "Stephen Wolfram", "title": "After 100 Years, Can We Finally Crack Post's Problem of Tag? A Story of\n  Computational Irreducibility, and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Empirical, theoretical and historical aspects of Post's \"problem of tag\" from\n1921 are explored. Evidence of strong computational irreducibility is found.\nDespite their deterministic origin, the lengths of successive sequences\ngenerated seem to closely approximate random walks. All 10^25 smallest initial\nconditions are found to eventually halt, although sometimes in > 6*10^11 steps.\nImplications of the Principle of Computational Equivalence are discussed, along\nwith examples of identifiable computational capabilities of tag systems.\nVarious minimal examples of complex behavior are found, including a less-biased\nanalog of the 3n+1 Collatz problem. There is also discussion of the history of\nEmil Post and of tag systems in the context of ideas about the foundations of\nmathematics and computation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:02:25 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wolfram", "Stephen", ""]]}, {"id": "2103.07224", "submitter": "Fu Song", "authors": "Yedi Zhang and Zhe Zhao and Guangke Chen and Fu Song and Taolue Chen", "title": "BDD4BNN: A BDD-based Quantitative Analysis Framework for Binarized\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying and explaining the behavior of neural networks is becoming\nincreasingly important, especially when they are deployed in safety-critical\napplications. In this paper, we study verification problems for Binarized\nNeural Networks (BNNs), the 1-bit quantization of general real-numbered neural\nnetworks. Our approach is to encode BNNs into Binary Decision Diagrams (BDDs),\nwhich is done by exploiting the internal structure of the BNNs. In particular,\nwe translate the input-output relation of blocks in BNNs to cardinality\nconstraints which are then encoded by BDDs. Based on the encoding, we develop a\nquantitative verification framework for BNNs where precise and comprehensive\nanalysis of BNNs can be performed. We demonstrate the application of our\nframework by providing quantitative robustness analysis and interpretability\nfor BNNs. We implement a prototype tool BDD4BNN and carry out extensive\nexperiments which confirm the effectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:02:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhang", "Yedi", ""], ["Zhao", "Zhe", ""], ["Chen", "Guangke", ""], ["Song", "Fu", ""], ["Chen", "Taolue", ""]]}, {"id": "2103.07392", "submitter": "Vitor Machado", "authors": "Vitor Machado, Mario Benevides", "title": "Temporal Logic for Social Networks", "comments": "19 pages, 2 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a logic with a class of social network models that is\nbased on standard Linear Temporal Logic (LTL), leveraging the power of existing\nmodel checkers for the analysis of social networks. We provide a short\nliterature overview, and then define our logic and its axiomatization, present\nsome simple motivational examples of both models and formulas, and show its\nsoundness and completeness via a translation into propositional formulas.\nLastly, we briefly discuss model checking and time complexity analysis.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:36:19 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Machado", "Vitor", ""], ["Benevides", "Mario", ""]]}, {"id": "2103.07397", "submitter": "Anja Petkovi\\'c", "authors": "Andrej Bauer and Anja Petkovi\\'c", "title": "An extensible equality checking algorithm for dependent type theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general and user-extensible equality checking algorithm that is\napplicable to a large class of type theories. The algorithm has a type-directed\nphase for applying extensionality rules and a normalization phase based on\ncomputation rules, where both kinds of rules are defined using the\ntype-theoretic concept of object-invertible rules. We also give sufficient\nsyntactic criteria for recognizing such rules, as well as a simple\npattern-matching algorithm for applying them. A third component of the\nalgorithm is a suitable notion of principal arguments, which determines a\nnotion of normal form. By varying these, we obtain known notions, such as weak\nhead-normal and strong normal forms. We prove that our algorithm is sound. We\nimplemented it in the Andromeda~2 proof assistant, which supports\nuser-definable type theories. The user need only provide the equality rules\nthey wish to use, which the algorithm automatically classifies as computation\nor extensionality rules, and select appropriate principal arguments.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:42:09 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Bauer", "Andrej", ""], ["Petkovi\u0107", "Anja", ""]]}, {"id": "2103.07558", "submitter": "Uwe Egbert Wolter", "authors": "Uwe Wolter", "title": "First-Order Sketch Conditions and Constraints -- A Category Independent\n  Approach", "comments": "16 pages, submitted to ICGT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generalizing different variants of \"graph conditions and constraints\" as well\nas \"universal constraints\" and \"negative universal constraints\" in the Diagram\nPredicate Framework (DPF), we introduce for arbitrary categories $\\mathbf{Cxt}$\nand \"statement\" functors $\\mathtt{Stm}:\\mathbf{Cxt}\\to\\mathbf{Set}$ general\nfirst-order sketch conditions and constraints. Sketches are used in DPF to\nformalize different kinds of diagrammatic software models. We discuss the use\nof sketch constraints for describing the syntactic structure of sketches. We\noutline the use of sketch constraints to deduce knowledge implicitly given in a\nsketch as well as procedures to deduce sketch constraints from given sketch\nconstraints. We use the simple but paradigmatic modeling formalism \"Category\nTheory\" as running example.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:44:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wolter", "Uwe", ""]]}, {"id": "2103.07699", "submitter": "Georg Schmid", "authors": "Georg Schmid, Viktor Kun\\v{c}ak", "title": "Proving and Disproving Programs with Shared Mutable Data", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tool for verification of deterministic programs with shared\nmutable references against specifications such as assertions, preconditions,\npostconditions, and read/write effects. We implement our tool by encoding\nprograms with mutable references into annotated purely functional recursive\nprograms. We then rely on function unfolding and the SMT solver Z3 to prove or\ndisprove safety and to establish program termination. Our tool uses a new\ntranslation of programs where frame conditions are encoded using\nquantifier-free formulas in first-order logic (instead of relying on\nquantifiers or separation logic). This quantifier-free encoding enables SMT\nsolvers to prove safety or report counterexamples relative to the semantics of\nprocedure specifications. Our encoding is possible thanks to the expressive\npower of the extended array theory of the Z3 SMT solver. In addition to the\nability to report counterexamples, our tool retains efficiency of reasoning\nabout purely functional layers of data structures, providing expressiveness for\nmutable data but also a significant level of automation for purely functional\naspects of software. We illustrate our tool through examples manipulating\nmutable linked structures and arrays.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 12:00:22 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Schmid", "Georg", ""], ["Kun\u010dak", "Viktor", ""]]}, {"id": "2103.07863", "submitter": "Kees Middelburg", "authors": "C.A. Middelburg", "title": "Imperative process algebra with abstraction", "comments": "29 pages, revision of v2 with section added about relatively uncommon\n  choices made", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces an imperative process algebra based on ACP (Algebra of\nCommunicating Processes). Like other imperative process algebras, this process\nalgebra deals with processes of the kind that arises from the execution of\nimperative programs. It distinguishes itself from already existing imperative\nprocess algebras among other things by supporting abstraction from actions that\nare considered not to be visible. The support of abstraction opens interesting\napplication possibilities of the process algebra. This paper goes briefly into\nthe possibility of information-flow security analysis of the kind that is\nconcerned with the leakage of confidential data. For the presented\naxiomatization, soundness and semi-completeness results with respect to a\nnotion of branching bisimulation equivalence are established.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 07:52:48 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:11:30 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 11:23:48 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Middelburg", "C. A.", ""]]}, {"id": "2103.08046", "submitter": "Reijo Jaakkola", "authors": "Reijo Jaakkola", "title": "Ordered fragments of first-order logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a recently introduced algebraic framework for the classification of\nfragments of first-order logic, we study the complexity of the satisfiability\nproblem for several ordered fragments of first-order logic, which are obtained\nfrom the ordered logic and the fluted logic by modifying some of their\nsyntactical restrictions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 21:39:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Jaakkola", "Reijo", ""]]}, {"id": "2103.08117", "submitter": "Hammad Ahmad", "authors": "Hammad Ahmad and Jean-Baptiste Jeannin", "title": "A Program Logic to Verify Signal Temporal Logic Specifications of Hybrid\n  Systems: Extended Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Signal temporal logic (STL) was introduced for monitoring temporal properties\nof continuous-time signals for continuous and hybrid systems. Differential\ndynamic logic (dL) was introduced to reason about the end states of a hybrid\nprogram. Over the past decade, STL and its variants have significantly gained\nin popularity in the industry for monitoring purposes, while dL has gained in\npopularity for verification of hybrid systems. In this paper, we bridge the gap\nbetween the two different logics by introducing signal temporal dynamic logic\n(STdL) -- a dynamic logic that reasons about a subset of STL specifications\nover executions of hybrid systems. Our work demonstrates that STL can be used\nfor deductive verification of hybrid systems. STdL significantly augments the\nexpressiveness of dL by allowing reasoning about temporal properties in given\ntime intervals. We provide a semantics and a proof calculus for STdL, along\nwith a proof of soundness and relative completeness.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:27:46 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 00:34:56 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ahmad", "Hammad", ""], ["Jeannin", "Jean-Baptiste", ""]]}, {"id": "2103.08459", "submitter": "Noemi Passing", "authors": "Bernd Finkbeiner, Gideon Geier, Noemi Passing", "title": "Specification Decomposition for Reactive Synthesis (Full Version)", "comments": "Full version of the corresponding NFM 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive synthesis is the task of automatically deriving an implementation\nfrom a specification. It is a promising technique for the development of\nverified programs and hardware. Despite recent advances, reactive synthesis is\nstill not practical when the specified systems reach a certain bound in size\nand complexity. In this paper, we present a modular synthesis algorithm that\ndecomposes the specification into smaller subspecifications. For them,\nindependent synthesis tasks are performed, and the composition of the resulting\nimplementations is guaranteed to satisfy the full specification. Our algorithm\nis a preprocessing technique that can be applied to a wide range of synthesis\ntools. We evaluate our approach with state-of-the-art synthesis tools on\nestablished benchmarks and obtain encouraging results: The overall runtime\ndecreases significantly when synthesizing implementations modularly.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:30:51 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 12:43:29 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Finkbeiner", "Bernd", ""], ["Geier", "Gideon", ""], ["Passing", "Noemi", ""]]}, {"id": "2103.08521", "submitter": "Paul Downen", "authors": "Paul Downen and Zena M. Ariola", "title": "Classical (Co)Recursion: Mechanics", "comments": "54 pages, 9 figures. Submitted to Journal of Functional Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primitive recursion is a mature, well-understood topic in the theory and\npractice of programming. Yet its dual, primitive corecursion, is\nunderappreciated and still seen as exotic. We aim to put them both on equal\nfooting by giving a foundation for primitive corecursion based on computation,\ngiving a terminating calculus analogous to the original computational\nfoundation of recursion. We show how the implementation details in an abstract\nmachine strengthens their connection, syntactically deriving corecursion from\nrecursion via logical duality. We also observe the impact of evaluation\nstrategy on the computational complexity of primitive (co)recursive\ncombinators: call-by-name allows for more efficient recursion, but\ncall-by-value allows for more efficient corecursion.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:46:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Downen", "Paul", ""], ["Ariola", "Zena M.", ""]]}, {"id": "2103.08535", "submitter": "Mnacho Echenim Mr", "authors": "Mnacho Echenim and Mehdi Mhalla", "title": "Quantum projective measurements and the CHSH inequality in Isabelle/HOL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a formalization in Isabelle/HOL of quantum projective\nmeasurements, a class of measurements involving orthogonal projectors that is\nfrequently used in quantum computing. We also formalize the CHSH inequality, a\nresult that holds on arbitrary probability spaces, which can used to disprove\nthe existence of a local hidden-variable theory for quantum mechanics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:58:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Echenim", "Mnacho", ""], ["Mhalla", "Mehdi", ""]]}, {"id": "2103.09092", "submitter": "William DeMeo", "authors": "William DeMeo", "title": "The Agda Universal Algebra Library, Part 2: Structure", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Agda Universal Algebra Library (UALib) is a library of types and programs\n(theorems and proofs) we developed to formalize the foundations of universal\nalgebra in dependent type theory using the Agda programming language and proof\nassistant.\n  The UALib includes a substantial collection of definitions, theorems, and\nproofs from universal algebra, equational logic, and model theory, and as such\nprovides many examples that exhibit the power of inductive and dependent types\nfor representing and reasoning about mathematical structures and equational\ntheories. In this paper, we describe the the types and proofs of the UALib that\nconcern homomorphisms, terms, and subalgebras.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 14:14:31 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["DeMeo", "William", ""]]}, {"id": "2103.10267", "submitter": "Ofer Strichman", "authors": "Alexander Ivrii and Ofer Strichman", "title": "Exploiting Isomorphic Subgraphs in SAT (Long version)", "comments": "The short version was submitted to SAT'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While static symmetry breaking has been explored in the SAT community for\ndecades, only as of 2010 research has focused on exploiting the same discovered\nsymmetry dynamically, during the run of the SAT solver, by learning extra\nclauses. The two methods are distinct and not compatible. The former prunes\nsolutions, whereas the latter does not -- it only prunes areas of the search\nthat do not have solutions, like standard conflict clauses. Both approaches,\nhowever, require what we call \\emph{full symmetry}, namely a\npropositionally-consistent mapping $\\sigma$ between the literals, such that\n$\\sigma(\\varphi) \\equiv \\varphi$, where here $\\equiv$ means syntactic\nequivalence modulo clause ordering and literal ordering within the clauses. In\nthis article we show that such full symmetry is not a necessary condition for\nadding extra clauses: isomorphism between possibly-overlapping subgraphs of the\ncolored incidence graph is sufficient. While finding such subgraphs is a\ncomputationally hard problem, there are many cases in which they can be\ndetected a priory by analyzing the high-level structure of the problem from\nwhich the CNF was derived. We demonstrate this principle with several\nwell-known problems, including Van der Waerden numbers, bounded model checking\nand Boolean Pythagorean triples.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:04:51 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ivrii", "Alexander", ""], ["Strichman", "Ofer", ""]]}, {"id": "2103.10701", "submitter": "Pierpaolo Dondio", "authors": "Pierpaolo Dondio, Luca Longo", "title": "Weakly Complete Semantics Based on Undecidedness Blocking", "comments": "48 pages, 9 figures. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we introduce a novel family of semantics called weakly complete\nsemantics. Differently from Dung's complete semantics, weakly complete\nsemantics employs a mechanism called undecidedness blocking by which the label\nundecided of an attacking argument is not always propagated to an otherwise\naccepted attacked argument. The new semantics are conflict-free, non-admissible\nbut employing a weaker notion of admissibility; they allow reinstatement and\nthey retain the majority of properties of complete semantics. We show how both\nweakly complete and Dung's complete semantics can be generated by applying\ndifferent undecidedness blocking strategies, making undecidedness blocking a\nunifying mechanism underlying argumentation semantics. The semantics are also\nan example of ambiguity blocking Dunganian semantics and the first semantics to\ntackle the problem of self-defeating attacking arguments. In the last part of\nthe paper we compare weakly complete semantics with the recent work of Baumann\net al. on weakly admissible semantics. Since the two families of semantics do\nnot coincide, a principle-based analysis of the two approaches is provided. The\nanalysis shows how our semantics satisfy a number of principles satisfied by\nDung's complete semantics but not by Baumann et al. semantics, including\ndirectionality, abstention, SCC-decomposability and cardinality of extensions,\nmaking them a more faithful non-admissible version of Dung' semantics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:29:14 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Dondio", "Pierpaolo", ""], ["Longo", "Luca", ""]]}, {"id": "2103.10881", "submitter": "Marie Farrell", "authors": "Marie Farrell, Rosemary Monahan, James F. Power", "title": "Building Specifications in the Event-B Institution", "comments": "54 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a formal semantics for the Event-B specification\nlanguage using the theory of institutions. We define an institution for\nEvent-B, EVT, and prove that it meets the validity requirements for\nsatisfaction preservation and model amalgamation. We also present a series of\nfunctions that show how the constructs of the Event-B specification language\ncan be mapped into our institution. Our semantics sheds new light on the\nstructure of the Event-B language, allowing us to clearly delineate three\nconstituent sub-languages: the superstructure, infrastructure and mathematical\nlanguages. One of the principal goals of our semantics is to provide access to\nthe generic modularisation constructs available in institutions, including\nspecification-building operators for parameterisation and refinement. We\ndemonstrate how these features subsume and enhance the corresponding features\nalready present in Event-B through a detailed study of their use in a worked\nexample. We have implemented our approach via a parser and translator for\nEvent-B specifications, EBtoEVT, which also provides a gateway to the Hets\ntoolkit for heterogeneous specification.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:05:41 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Farrell", "Marie", ""], ["Monahan", "Rosemary", ""], ["Power", "James F.", ""]]}, {"id": "2103.11197", "submitter": "Liyong Lin", "authors": "Liyong Lin, Ruochen Tai, Yuting Zhu, Rong Su", "title": "Observation-Assisted Heuristic Synthesis of Covert Attackers Against\n  Unknown Supervisors", "comments": "This paper is under review for the journal of Discrete Event Dynamic\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.FL cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of synthesis of covert attackers in the\nsetup where the model of the plant is available, but the model of the\nsupervisor is unknown, to the adversary. To compensate the lack of knowledge on\nthe supervisor, we assume that the adversary has recorded a (prefix-closed)\nfinite set of observations of the runs of the closed-loop system, which can be\nused for assisting the synthesis. We present a heuristic algorithm for the\nsynthesis of covert damage-reachable attackers, based on the model of the plant\nand the (finite) set of observations, by a transformation into solving an\ninstance of the partial-observation supervisor synthesis problem. The heuristic\nalgorithm developed in this paper may allow the adversary to synthesize covert\nattackers without having to know the model of the supervisor, which could be\nhard to obtain in practice. For simplicity, we shall only consider covert\nattackers that are able to carry out sensor replacement attacks and actuator\ndisablement attacks. The effectiveness of our approach is illustrated on a\nwater tank example adapted from the literature.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 15:43:52 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 08:35:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Lin", "Liyong", ""], ["Tai", "Ruochen", ""], ["Zhu", "Yuting", ""], ["Su", "Rong", ""]]}, {"id": "2103.11389", "submitter": "Guillaume Dubach", "authors": "Guillaume Dubach, Fabian Muehlboeck", "title": "Formal verification of Zagier's one-sentence proof", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We comment on two formal proofs of Fermat's sum of two squares theorem,\nwritten using the Mathematical Components libraries of the Coq proof assistant.\nThe first one follows Zagier's celebrated one-sentence proof; the second\nfollows David Christopher's more recent proof relying on partition-theoretic\narguments. Both formal proofs rely on a general property of involutions of\nfinite sets, of independent interest. The proof technique consists for the most\npart of automating recurrent tasks (such as case distinctions and computations\non natural numbers) via ad hoc tactics. With the same method, we also provide a\nformal proof of another classical result on primes of the form $a^2 + 2 b^2$.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:26:36 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 13:27:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dubach", "Guillaume", ""], ["Muehlboeck", "Fabian", ""]]}, {"id": "2103.11709", "submitter": "Mnacho Echenim", "authors": "Rachid Echahed and Mnacho Echenim and Mehdi Mhalla and Nicolas Peltier", "title": "A Superposition-Based Calculus for Quantum Diagrammatic Reasoning and\n  Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a class of rooted graphs which allows one to encode various\nkinds of classical or quantum circuits. We then follow a set-theoretic approach\nto define rewrite systems over the considered graphs and propose a new complete\nSuperposition callculus which handles sets of formulas consisting of equations\nor disequations over these graphs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:25:50 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Echahed", "Rachid", ""], ["Echenim", "Mnacho", ""], ["Mhalla", "Mehdi", ""], ["Peltier", "Nicolas", ""]]}, {"id": "2103.11751", "submitter": "Hiromi Ishii", "authors": "Hiromi Ishii", "title": "Functional Pearl: Witness Me -- Constructive Arguments Must Be Guided\n  with Concrete Witness", "comments": "Submitted to Haskell'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beloved Curry--Howard correspondence tells that types are intuitionistic\npropositions, and in constructive math, a proof of proposition can be seen as\nsome kind of a construction, or witness, conveying the information of the\nproposition. We demonstrate how useful this point of view is as the guiding\nprinciple for developing dependently-typed programs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:08:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ishii", "Hiromi", ""]]}, {"id": "2103.12811", "submitter": "Stephen Wolfram", "authors": "Stephen Wolfram", "title": "Combinators: A Centennial View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We give a modern computational introduction to the S,K combinators invented\nby Moses Sch\\\"onfinkel in 1920, and present a variety of new results and ideas\nabout combinators. We explore the spectrum of behavior obtained with small\ncombinator expressions, showing a variety of approaches to analysis and\nvisualization. We discuss the implications of evaluation strategies, and of\nmultiway systems representing all possible strategies. We show how causal\ngraphs introduced in recent models of fundamental physics can be applied to\ncombinators, as well as describing how combinators introduce a new form of\ntreelike separation. We give a variety of new results on minimal combinator\nexpressions, as well as showing how empirical computation theory and\ncomputational complexity theory can be done with combinators. We also suggest\nthat when viewed in terms of ongoing computation, the S combinator alone may be\ncapable of universal computation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 19:48:36 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wolfram", "Stephen", ""]]}, {"id": "2103.12862", "submitter": "Melissa Antonelli", "authors": "Melissa Antonelli, Ugo Dal Lago, Paolo Pistone", "title": "On Counting Propositional Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study counting propositional logic as an extension of propositional logic\nwith counting quantifiers. We prove that the complexity of the underlying\ndecision problem perfectly matches the appropriate level of Wagner's counting\nhierarchy, but also that the resulting logic admits a satisfactory\nproof-theoretical treatment. From the latter, a type system for a probabilistic\nlambda-calculus is derived in the spirit of the Curry-Howard correspondence,\nshowing the potential of counting propositional logic as a useful tool in\nseveral fields of theoretical computer science.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:49:16 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 13:15:50 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Antonelli", "Melissa", ""], ["Lago", "Ugo Dal", ""], ["Pistone", "Paolo", ""]]}, {"id": "2103.12872", "submitter": "Stella Biderman", "authors": "Louis Castricato and Stella Biderman and Rogelio E. Cardona-Rivera and\n  David Thue", "title": "Towards a Formal Model of Narratives", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the beginnings of a formal framework for modeling\nnarrative \\textit{qua} narrative. Our framework affords the ability to discuss\nkey qualities of stories and their communication, including the flow of\ninformation from a Narrator to a Reader, the evolution of a Reader's story\nmodel over time, and Reader uncertainty. We demonstrate its applicability to\ncomputational narratology by giving explicit algorithms for measuring the\naccuracy with which information was conveyed to the Reader and two novel\nmeasurements of story coherence.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:33:23 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Castricato", "Louis", ""], ["Biderman", "Stella", ""], ["Cardona-Rivera", "Rogelio E.", ""], ["Thue", "David", ""]]}, {"id": "2103.13694", "submitter": "Ana Ozaki", "authors": "Ana Ozaki", "title": "On the Complexity of Learning Description Logic Ontologies", "comments": "Presented at the Reasoning Web Summer School 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontologies are a popular way of representing domain knowledge, in particular,\nknowledge in domains related to life sciences. (Semi-)automating the process of\nbuilding an ontology has attracted researchers from different communities into\na field called \"Ontology Learning\". We provide a formal specification of the\nexact and the probably approximately correct learning models from computational\nlearning theory. Then, we recall from the literature complexity results for\nlearning lightweight description logic (DL) ontologies in these models.\nFinally, we highlight other approaches proposed in the literature for learning\nDL ontologies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:18:12 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ozaki", "Ana", ""]]}, {"id": "2103.14013", "submitter": "Garvin Melles", "authors": "Garvin Melles", "title": "Set Turing Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define a generalization of the Turing machine that computes on general\nsets. Our main theorem states that the class of generalized Turing machine\ncomputable functions and the class of Set Recursive functions coincide.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:46:27 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Melles", "Garvin", ""]]}, {"id": "2103.14482", "submitter": "Benno van den Berg", "authors": "Benno van den Berg and Robert Passmann", "title": "Converse extensionality and apartness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we try to find a computational interpretation for a strong form\nof extensionality, which we call \"converse extensionality\". These converse\nextensionality principles, which arise as the Dialectica interpretation of the\naxiom of extensionality, were first studied by Howard. In order to give a\ncomputational interpretation to these principles, we reconsider Brouwer's\napartness relation, a strong constructive form of inequality. Formally, we\nprovide a categorical construction to endow every typed combinatory algebra\nwith an apartness relation. We then exploit certain continuity principles and\nthat functions reflect apartness, as opposed to preserving equality, to prove\nthat the resulting categories of assemblies model some converse extensionality\nprinciples.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:10:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Berg", "Benno van den", ""], ["Passmann", "Robert", ""]]}, {"id": "2103.14600", "submitter": "Alper Kamil Bozkurt", "authors": "Alper Kamil Bozkurt, Yu Wang, Miroslav Pajic", "title": "Model-Free Learning of Safe yet Effective Controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.FL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning safe control policies that\nare also effective -- i.e., maximizing the probability of satisfying the linear\ntemporal logic (LTL) specification of the task, and the discounted reward\ncapturing the (classic) control performance. We consider unknown environments\nthat can be modeled as Markov decision processes (MDPs). We propose a\nmodel-free reinforcement learning algorithm that learns a policy that first\nmaximizes the probability of ensuring the safety, then the probability of\nsatisfying the given LTL specification and lastly, the sum of discounted\nQuality of Control (QoC) rewards. Finally, we illustrate the applicability of\nour RL-based approach on a case study.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:05:12 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bozkurt", "Alper Kamil", ""], ["Wang", "Yu", ""], ["Pajic", "Miroslav", ""]]}, {"id": "2103.14831", "submitter": "Aman Goel", "authors": "Aman Goel, Karem A. Sakallah", "title": "On Symmetry and Quantification: A New Approach to Verify Distributed\n  Protocols", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-76384-8_9", "report-no": null, "categories": "cs.LO cs.DC cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proving that an unbounded distributed protocol satisfies a given safety\nproperty amounts to finding a quantified inductive invariant that implies the\nproperty for all possible instance sizes of the protocol. Existing methods for\nsolving this problem can be described as search procedures for an invariant\nwhose quantification prefix fits a particular template. We propose an\nalternative constructive approach that does not prescribe, a priori, a specific\nquantifier prefix. Instead, the required prefix is automatically inferred\nwithout any search by carefully analyzing the structural symmetries of the\nprotocol. The key insight underlying this approach is that symmetry and\nquantification are closely related concepts that express protocol invariance\nunder different re-arrangements of its components. We propose symmetric\nincremental induction, an extension of the finite-domain IC3/PDR algorithm,\nthat automatically derives the required quantified inductive invariant by\nexploiting the connection between symmetry and quantification. While various\nattempts have been made to exploit symmetry in verification applications, to\nour knowledge, this is the first demonstration of a direct link between\nsymmetry and quantification in the context of clause learning during\nincremental induction. We also describe a procedure to automatically find a\nminimal finite size, the cutoff, that yields a quantified invariant proving\nsafety for any size.\n  Our approach is implemented in IC3PO, a new verifier for distributed\nprotocols that significantly outperforms the state-of-the-art, scales orders of\nmagnitude faster, and robustly derives compact inductive invariants fully\nautomatically.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 07:36:39 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Goel", "Aman", ""], ["Sakallah", "Karem A.", ""]]}, {"id": "2103.14933", "submitter": "Maximiliano Cristia", "authors": "Maximiliano Cristi\\'a and Gianfranco Rossi", "title": "$\\{log\\}$: Applications to Software Specification, Prototyping and\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document shows how Z specifications can be translated into $\\{log\\}$\nand, later, on how $\\{log\\}$ can be used to run simulations and automated\nproofs. This can help users of other specification languages such as B and VDM\nto use $\\{log\\}$ along the same lines. The presentation is rather informal and\nuser-oriented. More technical and formal presentations can be found in the\npapers published by the authors. We also assume the reader has at least a basic\nknowledge of the Z notation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 15:58:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cristi\u00e1", "Maximiliano", ""], ["Rossi", "Gianfranco", ""]]}, {"id": "2103.14946", "submitter": "Alexandru Baltag", "authors": "Alexandru Baltag and Johan van Benthem", "title": "A Simple Logic of Functional Dependence", "comments": "56 pages. Journal of Philosophical Logic (2021)", "journal-ref": null, "doi": "10.1007/s10992-020-09588-z", "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a simple decidable logic of functional dependence LFD,\nbased on an extension of classical propositional logic with dependence atoms\nplus dependence quantifiers treated as modalities, within the setting of\ngeneralized assignment semantics for first order logic. The expressive\nstrength, complete proof calculus and meta-properties of LFD are explored.\nVarious language extensions are presented as well, up to undecidable\nmodal-style logics for independence and dynamic logics of changing dependence\nmodels. Finally, more concrete settings for dependence are discussed:\ncontinuous dependence in topological models, linear dependence in vector\nspaces, and temporal dependence in dynamical systems and games.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 17:03:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Baltag", "Alexandru", ""], ["van Benthem", "Johan", ""]]}, {"id": "2103.14992", "submitter": "Chunxiao Li", "authors": "Chunxiao Li, Jonathan Chung, Soham Mukherjee, Marc Vinyals, Noah\n  Fleming, Antonina Kolokolova, Alice Mu, Vijay Ganesh", "title": "On the Hierarchical Community Structure of Practical Boolean Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern CDCL SAT solvers easily solve industrial instances containing tens of\nmillions of variables and clauses, despite the theoretical intractability of\nthe SAT problem. This gap between practice and theory is a central problem in\nsolver research. It is believed that SAT solvers exploit structure inherent in\nindustrial instances, and hence there have been numerous attempts over the last\n25 years at characterizing this structure via parameters. These can be\nclassified as rigorous, i.e., they serve as a basis for complexity-theoretic\nupper bounds (e.g., backdoors), or correlative, i.e., they correlate well with\nsolver run time and are observed in industrial instances (e.g., community\nstructure). Unfortunately, no parameter proposed to date has been shown to be\nboth strongly correlative and rigorous over a large fraction of industrial\ninstances. Given the sheer difficulty of the problem, we aim for an\nintermediate goal of proposing a set of parameters that is strongly correlative\nand has good theoretical properties. Specifically, we propose parameters based\non a graph partitioning called Hierarchical Community Structure (HCS), which\ncaptures the recursive community structure of a graph of a Boolean formula. We\nshow that HCS parameters are strongly correlative with solver run time using an\nEmpirical Hardness Model, and further build a classifier based on HCS\nparameters that distinguishes between easy industrial and hard random/crafted\ninstances with very high accuracy. We further strengthen our hypotheses via\nscaling studies. On the theoretical side, we show that counterexamples which\nplagued community structure do not apply to HCS, and that there is a subset of\nHCS parameters such that restricting them limits the size of embeddable\nexpanders.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:48:25 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 18:28:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Chunxiao", ""], ["Chung", "Jonathan", ""], ["Mukherjee", "Soham", ""], ["Vinyals", "Marc", ""], ["Fleming", "Noah", ""], ["Kolokolova", "Antonina", ""], ["Mu", "Alice", ""], ["Ganesh", "Vijay", ""]]}, {"id": "2103.15140", "submitter": "Felix Weitk\\\"amper", "authors": "Felix Weitk\\\"amper", "title": "Scaling the weight parameters in Markov logic networks and relational\n  logistic regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider Markov logic networks and relational logistic regression as two\nfundamental representation formalisms in statistical relational artificial\nintelligence that use weighted formulas in their specification. However, Markov\nlogic networks are based on undirected graphs, while relational logistic\nregression is based on directed acyclic graphs. We show that when scaling the\nweight parameters with the domain size, the asymptotic behaviour of a\nrelational logistic regression model is transparently controlled by the\nparameters, and we supply an algorithm to compute asymptotic probabilities. We\nalso show using two examples that this is not true for Markov logic networks.\nWe also discuss using several examples, mainly from the literature, how the\napplication context can help the user to decide when such scaling is\nappropriate and when using the raw unscaled parameters might be preferable. We\nhighlight random sampling as a particularly promising area of application for\nscaled models and expound possible avenues for further research.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 14:24:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weitk\u00e4mper", "Felix", ""]]}, {"id": "2103.15193", "submitter": "Henry DeYoung", "authors": "Ankush Das, Henry DeYoung, Andreia Mordido, Frank Pfenning", "title": "Subtyping on Nested Polymorphic Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of subtyping to enable a wider range of well-typed programs is\nundeniable. However, the interaction between subtyping, recursion, and\npolymorphism is not completely understood yet. In this work, we explore\nsubtyping in a system of nested, recursive, and polymorphic types with a\ncoinductive interpretation, and we prove that this problem is undecidable. Our\nresults will be broadly applicable, but to keep our study grounded in a\nconcrete setting, we work with an extension of session types with explicit\npolymorphism, parametric type constructors, and nested types. We prove that\nsubtyping is undecidable even for the fragment with only internal choices and\nnested unary recursive type constructors. Despite this negative result, we\npresent a subtyping algorithm for our system and prove its soundness. We\nminimize the impact of the inescapable incompleteness by enabling the\nprogrammer to seed the algorithm with subtyping declarations (that are\nvalidated by the algorithm). We have implemented the proposed algorithm in Rast\nand it showed to be efficient in various example programs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 18:19:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Das", "Ankush", ""], ["DeYoung", "Henry", ""], ["Mordido", "Andreia", ""], ["Pfenning", "Frank", ""]]}, {"id": "2103.15223", "submitter": "Thomas Kahl", "authors": "Thomas Kahl", "title": "On symmetric higher-dimensional automata and bisimilarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that a higher-dimensional automaton is hhp-bisimilar to the free\nsymmetric HDA generated by it. Consequently, up to hereditary\nhistory-preserving bisimilarity, ordinary HDAs and symmetric HDAs are models of\nconcurrency with the same expressive power.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:20:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kahl", "Thomas", ""]]}, {"id": "2103.15292", "submitter": "Ian Hayes", "authors": "Ian J. Hayes and Larissa A. Meinicke and Patrick A. Meiring", "title": "Deriving Laws for Developing Concurrent Programs in a Rely-Guarantee\n  Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a theory for the refinement of shared-memory\nconcurrent algorithms from specifications. Our approach avoids restrictive\natomicity contraints. It provides a range of constructs for specifying\nconcurrent programs and laws for refining these to code. We augment pre and\npost condition specifications with Jones' rely and guarantee conditions, which\nwe encode as commands within a wide-spectrum language. Program components are\nspecified using either partial and total correctness versions of end-to-end\nspecifications. Operations on shared data structures and atomic machine\noperations (e.g. compare-and-swap) are specified using an atomic specification\ncommand. All the above constructs are defined in terms of a simple core\nlanguage, based on four primitive commands and a handful of operators, and for\nwhich we have developed an extensive algebraic theory in Isabelle/HOL. For\nshared memory programs, expression evaluation is subject to fine-grained\ninterference and we have avoided atomicity restrictions other than for read and\nwrite of primitive types (words). Expression evaluation and assignment commands\nare also defined in terms of our core language primitives, allowing laws for\nreasoning about them to be proven in the theory. Control structures such as\nconditionals, recursion and loops are all defined in terms of the core\nlanguage. In developing the laws for refining to such structures from\nspecifications we have taken care to develop laws that are as general as\npossible; our laws are typically more general than those found in the\nliterature. In developing our concurrent refinement theory we have taken care\nto focus on the algebraic properties of our commands and operators, which has\nallowed us to reuse algebraic theories, including well-known theories, such as\nlattices and boolean algebra, as well as programming-specific algebras, such as\nour synchronous algebra.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:56:06 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 02:20:28 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hayes", "Ian J.", ""], ["Meinicke", "Larissa A.", ""], ["Meiring", "Patrick A.", ""]]}, {"id": "2103.15453", "submitter": "Pierre Clairambault", "authors": "Simon Castellan (IRISA, CELTIQUE), Pierre Clairambault (LIP, PLUME)", "title": "Disentangling Parallelism and Interference in Game Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game semantics is a denotational semantics presenting compositionally the\ncomputational behaviour of various kinds of effectful programs. One of its\ncelebrated achievement is to have obtained full abstraction results for\nprogramming languages with a variety of computational effects, in a single\nframework. This is known as the semantic cube or Abramsky's cube, which for\nsequential deterministic programs establishes a correspondence between certain\nconditions on strategies (''innocence'', ''well-bracketing'', ''visibility'')\nand the absence of matching computational effects. Outside of the sequential\ndeterministic realm, there are still a wealth of game semantics-based full\nabstraction results; but they no longer fit in a unified canvas. In particular,\nGhica and Murawski's fully abstract model for shared state concurrency (IA)\ndoes not have a matching notion of pure parallel program-we say that\nparallelism and interference (i.e. state plus semaphores) are entangled. In\nthis paper we construct a causal version of Ghica and Murawski's model, also\nfully abstract for IA. We provide compositional conditions parallel innocence\nand sequentiality, respectively banning interference and parallelism, and\nleading to four full abstraction results. To our knowledge, this is the first\nextension of Abramsky's semantic cube programme beyond the sequential\ndeterministic world.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:37:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Castellan", "Simon", "", "IRISA, CELTIQUE"], ["Clairambault", "Pierre", "", "LIP, PLUME"]]}, {"id": "2103.15521", "submitter": "Manuel Gieseking", "authors": "Manuel Gieseking, Jesko Hecking-Harbusch, Ann Yanich", "title": "A Web Interface for Petri Nets with Transits and Petri Games", "comments": "8 pages, 2 figures, version corresponding to the TACAS'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing algorithms for distributed systems is an error-prone task. Formal\nmodels like Petri nets with transits and Petri games can prevent errors when\ndeveloping such algorithms. Petri nets with transits allow us to follow the\ndata flow between components in a distributed system. They can be model checked\nagainst specifications in LTL on both the local data flow and the global\nbehavior. Petri games allow the synthesis of local controllers for distributed\nsystems from safety specifications. Modeling problems in these formalisms\nrequires defining extended Petri nets which can be cumbersome when performed\ntextually.\n  In this paper, we present a web interface that allows an intuitive, visual\ndefinition of Petri nets with transits and Petri games. The corresponding model\nchecking and synthesis problems are solved directly on a server. In the\ninterface, implementations, counterexamples, and all intermediate steps can be\nanalyzed and simulated. Stepwise simulations and interactive state space\ngeneration support the user in detecting modeling errors.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:45:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gieseking", "Manuel", ""], ["Hecking-Harbusch", "Jesko", ""], ["Yanich", "Ann", ""]]}, {"id": "2103.15623", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Doriana Medi\\'c (Inria)", "title": "Enabling Replications and Contexts in Reversible Concurrent Calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing formalisms for the algebraic specification and representation of\nnetworks of reversible agents suffer some shortcomings. Despite multiple\nattempts, reversible declensions of the Calculus of Communicating Systems (CCS)\ndo not offer satisfactory adaptation of notions that are usual in\n''forward-only'' process algebras, such as replication or context. They also\nseem to fail to leverage possible new features stemming from reversibility,\nsuch as the capacity of distinguishing between multiple replications, based on\nhow they replicate the memory mechanism allowing to reverse the computation.\nExisting formalisms disallow the ''hot-plugging'' of processes during their\nexecution in contexts that also have a past. Finally, they assume the existence\nof ''eternally fresh'' keys or identifiers that, if implemented poorly, could\nresult in unnecessary bottlenecks and look-ups involving all the threads. In\nthis paper, we begin investigating those issues, by first designing a process\nalgebra endowed with a mechanism to generate identifiers without the need to\nconsult with the other threads. We use this calculus to recast the possible\nrepresentations of non-determinism in CCS, and as a by-product establish a\nsimple and straightforward definition of concurrency. Our reversible calculus\nis then proven to satisfy expected properties, and allows to lay out precisely\ndifferent representations of the replication of a process with a memory. We\nalso observe that none of the reversible bisimulations defined thus far are\ncongruences under our notion of ''reversible'' contexts.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:03:04 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "Inria"], ["Medi\u0107", "Doriana", "", "Inria"]]}, {"id": "2103.15710", "submitter": "Carlos Erneto Ramirez Ovalle cerO", "authors": "Miguel Andres Velasquez, Carlos Ernesto Ramirez", "title": "Representation of a vehicular traffic model using hybrid systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a great diversity of formal models to understand the dynamics of\ntransport and vehicular flow on a road. Many of these models are inspired by\nthe dynamics of flows governed by partial differential equations. However, it\nis possible to simplify these models to ordinary equations by considering\nconstant variations in some of the input variables in this type of models.\nHowever, given that these types of systems present discrete changes when the\nvehicle density is altered in some sections of the lane, it seems reasonable to\nmake use of hybrid systems to better understand the evolution of these\ndynamics. In this work we are interested in making use of dynamic differential\nlogic to formally verify one of these models proposed in ordinary equations.\nThis verification will be done through a proof assistant specially designed for\nhybrid systems called KeYmaera. Once we adapt the model to a hybrid system\nrepresentation we proceed to use KeYmaera to verify that the proposed model is\nformally correct.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:49:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Velasquez", "Miguel Andres", ""], ["Ramirez", "Carlos Ernesto", ""]]}, {"id": "2103.15758", "submitter": "Sebastian Weichwald", "authors": "Eigil F. Rischel, Sebastian Weichwald", "title": "Compositional Abstraction Error and a Category of Causal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interventional causal models describe joint distributions over some variables\nused to describe a system, one for each intervention setting. They provide a\nformal recipe for how to move between joint distributions and make predictions\nabout the variables upon intervening on the system. Yet, it is difficult to\nformalise how we may change the underlying variables used to describe the\nsystem, say from fine-grained to coarse-grained variables. Here, we argue that\ncompositionality is a desideratum for model transformations and the associated\nerrors. We develop a framework for model transformations and abstractions with\na notion of error that is compositional: when abstracting a reference model M\nmodularly, first obtaining M' and then further simplifying that to obtain M'',\nthen the composite transformation from M to M'' exists and its error can be\nbounded by the errors incurred by each individual transformation step. Category\ntheory, the study of mathematical objects via the compositional transformations\nbetween them, offers a natural language for developing our framework. We\nintroduce a category of finite interventional causal models and, leveraging\ntheory of enriched categories, prove that our framework enjoys the desired\ncompositionality properties.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:48:12 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Rischel", "Eigil F.", ""], ["Weichwald", "Sebastian", ""]]}, {"id": "2103.15776", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Matthijs V\\'ak\\'ar", "title": "CHAD: Combinatory Homomorphic Automatic Differentiation", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.05283", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Combinatory Homomorphic Automatic Differentiation (CHAD), a\nprincipled, pure, provably correct method for performing forward- and\nreverse-mode automatic differentiation (AD) on programming languages with\nexpressive features. It implements AD as a compositional, type-respecting\nsource-code transformation that generates purely functional code. This code\ntransformation is principled in the sense that it is the unique homomorphic\n(structure preserving) extension to expressive languages of the well-known and\nunambiguous definitions of automatic differentiation for a first-order\nfunctional language. Correctness of the method follows by a (compositional)\nlogical relations argument that shows that the semantics of the syntactic\nderivative is the usual calculus derivative of the semantics of the original\nprogram. In their most elegant formulation, the transformations generate code\nwith linear types. However, the transformations can be implemented in a\nstandard functional language without sacrificing correctness. This\nimplementation can be achieved by making use of abstract data types to\nrepresent the required linear types, e.g. through the use of a basic module\nsystem.\n  In this paper, we detail the method when applied to a simple higher-order\nlanguage for manipulating statically sized arrays. However, we explain how the\nmethodology applies, more generally, to functional languages with other\nexpressive features.\n  Finally, we discuss how the scope of CHAD extends beyond applications in\nautomatic differentiation to other dynamic program analyses that accumulate\ndata in a commutative monoid.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:10:22 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:23:45 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "2103.15820", "submitter": "Jonathan Gorard", "authors": "Jonathan Gorard, Manojna Namuduri, Xerxes D. Arsiwalla", "title": "ZX-Calculus and Extended Wolfram Model Systems II: Fast Diagrammatic\n  Reasoning with an Application to Quantum Circuit Simplification", "comments": "104 pages, 57 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a novel algorithmic methodology for performing\nautomated diagrammatic deductions over combinatorial structures, using a\ncombination of modified equational theorem-proving techniques and the extended\nWolfram model hypergraph rewriting formalism developed by the authors in\nprevious work. We focus especially upon the application of this new algorithm\nto the problem of automated circuit simplification in quantum information\ntheory, using Wolfram model multiway operator systems combined with the\nZX-calculus formalism for enacting fast diagrammatic reasoning over linear\ntransformations between qubits. We show how to construct a generalization of\nthe deductive inference rules for Knuth-Bendix completion in which equation\nmatches are selected on the basis of causal edge density in the associated\nmultiway system, before proceeding to demonstrate how to embed the higher-order\nlogic of the ZX-calculus rules within this first-order equational framework.\nAfter showing explicitly how the (hyper)graph rewritings of both Wolfram model\nsystems and the ZX-calculus can be effectively realized within this formalism,\nwe proceed to exhibit comparisons of time complexity vs. proof complexity for\nthis new algorithmic approach when simplifying randomly-generated Clifford\ncircuits down to pseudo-normal form, as well as when reducing the number of\nT-gates in randomly-generated non-Clifford circuits, with circuit sizes ranging\nup to 3000 gates, illustrating that the method performs favorably in comparison\nwith existing circuit simplification frameworks, and also exhibiting the\napproximately quadratic speedup obtained by employing the causal edge density\noptimization. Finally, we present a worked example of an automated proof of\ncorrectness for a simple quantum teleportation protocol, in order to\ndemonstrate more clearly the internal operations of the theorem-proving\nprocedure.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:23:06 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Gorard", "Jonathan", ""], ["Namuduri", "Manojna", ""], ["Arsiwalla", "Xerxes D.", ""]]}, {"id": "2103.16156", "submitter": "Eike Neumann", "authors": "Eike Neumann", "title": "Uniform Envelopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the author's PhD thesis (2019) universal envelopes were introduced as a\ntool for studying the continuously obtainable information on discontinuous\nfunctions.\n  To any function $f \\colon X \\to Y$ between $\\operatorname{qcb}_0$-spaces one\ncan assign a so-called universal envelope which, in a well-defined sense,\nencodes all continuously obtainable information on the function. A universal\nenvelope consists of two continuous functions $F \\colon X \\to L$ and $\\xi_L\n\\colon Y \\to L$ with values in a $\\Sigma$-split injective space $L$. Any\ncontinuous function with values in an injective space whose composition with\nthe original function is again continuous factors through the universal\nenvelope. However, it is not possible in general to uniformly compute this\nfactorisation.\n  In this paper we propose the notion of uniform envelopes. A uniform envelope\nis additionally endowed with a map $u_L \\colon L \\to \\mathcal{O}^2(Y)$ that is\ncompatible with the multiplication of the double powerspace monad\n$\\mathcal{O}^2$ in a certain sense. This yields for every continuous map with\nvalues in an injective space a choice of uniformly computable extension. Under\na suitable condition which we call uniform universality, this extension yields\na uniformly computable solution for the above factorisation problem.\n  Uniform envelopes can be endowed with a composition operation. We establish\ncriteria that ensure that the composition of two uniformly universal envelopes\nis again uniformly universal. These criteria admit a partial converse and we\nprovide evidence that they cannot be easily improved in general.\n  Not every function admits a uniformly universal uniform envelope. We can\nhowever assign to every function a canonical envelope that is in some sense as\nclose as possible to a uniform envelope. We obtain a composition theorem\nsimilar to the uniform case.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:30:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Neumann", "Eike", ""]]}, {"id": "2103.16294", "submitter": "Danny Vagnozzi", "authors": "Anuj Dawar, Danny Vagnozzi", "title": "On the relative power of algebraic approximations of graph isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We compare the capabilities of two approaches to approximating graph\nisomorphism using linear algebraic methods: the \\emph{invertible map tests}\n(introduced by Dawar and Holm) and proof systems with algebraic rules, namely\n\\emph{polynomial calculus}, \\emph{monomial calculus} and \\emph{Nullstellensatz\ncalculus}. In the case of fields of characteristic zero, these variants are all\nessentially equivalent to the the Weisfeiler-Leman algorithms. In positive\ncharacteristic we show that the invertible map method can simulate the monomial\ncalculus and identify a potential way to extend this to the monomial calculus.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:43:44 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Dawar", "Anuj", ""], ["Vagnozzi", "Danny", ""]]}, {"id": "2103.16443", "submitter": "Frances Cleary Ms", "authors": "Frances Cleary, David Henshall, Sasitharan Balasubramaniam", "title": "On-body Edge Computing through E-Textile Programmable Logic Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  E-textiles has received tremendous attention in recent years due to the\ncapability of integrating sensors into a garment to provide high precision\nsensing of the human body. Besides sensing, a number of solutions for e-textile\ngarments have also integrated wireless interfaces allowing these sensing data\nto be transmitted and also sensors that allow users to provide instructions\nthrough touching. While this has provided a new level of sensing that can\nresult in unprecedented applications, there has been little attention placed on\non-body computing for e-textiles. Facilitating computing on e-textiles can\nresult in a new form of On-body Edge Computing, where sensor information are\nprocessed very close to the body before being transmitted to an external device\nor wireless access point. This form of computing can provide new security and\ndata privacy capabilities and at the same time provide opportunities for new\nenergy harvesting mechanisms to process the data through the garment. This\npaper proposes this concept through embroidered Programmable Logic Array (PLA)\nintegrated into e-textiles. In the way that PLAs have programmable logic\ncircuits by interconnecting different AND, NOT and OR gates, we propose\ne-textile based gates that are sewn into a garment and connected through\nconductive thread stitching. Two designs are proposed and this includes Single\nand Multi-Layered PLA. Experimental validations have been conducted at the\nindividual gates as well as the entire PLA circuits to determine the voltage\nutilization as well as logic computing reliability. Our proposed approach can\nusher in a new form of On-Body Edge Computing for e-textile garments for future\nwearable technologies\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:42:54 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Cleary", "Frances", ""], ["Henshall", "David", ""], ["Balasubramaniam", "Sasitharan", ""]]}, {"id": "2103.16833", "submitter": "Tom Hirschowitz", "authors": "Tom Hirschowitz (LAMA), Ambroise Lafont (UNSW)", "title": "A categorical framework for congruence of applicative bisimilarity in\n  higher-order languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applicative bisimilarity is a coinductive characterisation of observational\nequivalence in call-by-name lambda-calculus, introduced by Abramsky in 1990.\nHowe (1989) gave a direct proof that it is a congruence. We propose a\ncategorical framework for specifying operational semantics, in which we prove\nthat (an abstract analogue of) applicative bisimilarity is automatically a\ncongruence. Example instances include standard applicative bisimilarity in\ncall-by-name and call-by-value $\\lambda$-calculus, as well as in a simple\nnon-deterministic variant.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:22:24 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hirschowitz", "Tom", "", "LAMA"], ["Lafont", "Ambroise", "", "UNSW"]]}]