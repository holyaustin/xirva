[{"id": "0709.0446", "submitter": "Riccardo Pucella", "authors": "Alessio Lomuscio and Wojciech Penczek", "title": "Logic Column 19: Symbolic Model Checking for Temporal-Epistemic Logics", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": null, "abstract": "  This article surveys some of the recent work in verification of temporal\nepistemic logic via symbolic model checking, focusing on OBDD-based and\nSAT-based approaches for epistemic logics built on discrete and real-time\nbranching time temporal logics.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2007 14:38:10 GMT"}], "update_date": "2007-09-05", "authors_parsed": [["Lomuscio", "Alessio", ""], ["Penczek", "Wojciech", ""]]}, {"id": "0709.1080", "submitter": "Cas Cremers", "authors": "Cas Cremers", "title": "On the Protocol Composition Logic PCL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": null, "abstract": "  A recent development in formal security protocol analysis is the Protocol\nComposition Logic (PCL). We identify a number of problems with this logic as\nwell as with extensions of the logic, as defined in\n[DDMP05,HSD+05,He05,Dat05,Der06,DDMR07]. The identified problems imply strong\nrestrictions on the scope of PCL, and imply that some currently claimed PCL\nproofs cannot be proven within the logic, or make use of unsound axioms. Where\npossible, we propose solutions for these problems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2007 13:32:06 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2007 12:14:41 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2007 21:51:00 GMT"}, {"version": "v4", "created": "Mon, 1 Oct 2007 13:28:21 GMT"}, {"version": "v5", "created": "Fri, 2 Nov 2007 11:10:19 GMT"}, {"version": "v6", "created": "Wed, 19 Dec 2007 15:58:11 GMT"}, {"version": "v7", "created": "Fri, 22 Feb 2008 16:12:11 GMT"}], "update_date": "2008-02-22", "authors_parsed": [["Cremers", "Cas", ""]]}, {"id": "0709.1201", "submitter": "Alessio Guglielmi", "authors": "Paola Bruscoli and Alessio Guglielmi", "title": "On the Proof Complexity of Deep Inference", "comments": "Minor improvements over the published version. Always updated version\n  at <http://cs.bath.ac.uk/ag/p/PrComplDI.pdf>", "journal-ref": "ACM Transactions on Computational Logic 10 (2:14) 2009, pp. 1-34", "doi": "10.1145/1462179.1462186", "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain two results about the proof complexity of deep inference: 1)\ndeep-inference proof systems are as powerful as Frege ones, even when both are\nextended with the Tseitin extension rule or with the substitution rule; 2)\nthere are analytic deep-inference proof systems that exhibit an exponential\nspeed-up over analytic Gentzen proof systems that they polynomially simulate.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2007 11:35:28 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2008 15:34:38 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2009 18:55:21 GMT"}], "update_date": "2009-04-19", "authors_parsed": [["Bruscoli", "Paola", ""], ["Guglielmi", "Alessio", ""]]}, {"id": "0709.1205", "submitter": "Alessio Guglielmi", "authors": "Alessio Guglielmi and Tom Gundersen", "title": "Normalisation Control in Deep Inference via Atomic Flows", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 1 (March 31,\n  2008) lmcs:1081", "doi": "10.2168/LMCS-4(1:9)2008", "report-no": null, "categories": "math.LO cs.LO", "license": null, "abstract": "  We introduce `atomic flows': they are graphs obtained from derivations by\ntracing atom occurrences and forgetting the logical structure. We study simple\nmanipulations of atomic flows that correspond to complex reductions on\nderivations. This allows us to prove, for propositional logic, a new and very\ngeneral normalisation theorem, which contains cut elimination as a special\ncase. We operate in deep inference, which is more general than other syntactic\nparadigms, and where normalisation is more difficult to control. We argue that\natomic flows are a significant technical advance for normalisation theory,\nbecause 1) the technique they support is largely independent of syntax; 2)\nindeed, it is largely independent of logical inference rules; 3) they\nconstitute a powerful geometric formalism, which is more intuitive than syntax.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2007 12:24:19 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2008 12:34:27 GMT"}, {"version": "v3", "created": "Mon, 31 Mar 2008 10:59:23 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Guglielmi", "Alessio", ""], ["Gundersen", "Tom", ""]]}, {"id": "0709.1308", "submitter": "Giorgi Japaridze", "authors": "Giorgi Japaridze", "title": "Cirquent calculus deepened", "comments": "Significant improvements over the previous versions", "journal-ref": "Journal of Logic and Computation 18 (2008), pp. 983-1028", "doi": "10.1093/logcom/exn019", "report-no": null, "categories": "cs.LO math.LO", "license": null, "abstract": "  Cirquent calculus is a new proof-theoretic and semantic framework, whose main\ndistinguishing feature is being based on circuits, as opposed to the more\ntraditional approaches that deal with tree-like objects such as formulas or\nsequents. Among its advantages are greater efficiency, flexibility and\nexpressiveness. This paper presents a detailed elaboration of a deep-inference\ncirquent logic, which is naturally and inherently resource conscious. It shows\nthat classical logic, both syntactically and semantically, is just a special,\nconservative fragment of this more general and, in a sense, more basic logic --\nthe logic of resources in the form of cirquent calculus. The reader will find\nvarious arguments in favor of switching to the new framework, such as arguments\nshowing the insufficiency of the expressive power of linear logic or other\nformula-based approaches to developing resource logics, exponential\nimprovements over the traditional approaches in both representational and proof\ncomplexities offered by cirquent calculus, and more. Among the main purposes of\nthis paper is to provide an introductory-style starting point for what, as the\nauthor wishes to hope, might have a chance to become a new line of research in\nproof theory -- a proof theory based on circuits instead of formulas.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2007 18:00:51 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2007 19:38:26 GMT"}, {"version": "v3", "created": "Tue, 1 Apr 2008 12:09:13 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Japaridze", "Giorgi", ""]]}, {"id": "0709.1401", "submitter": "Arnaud Spiwack", "authors": "Thierry Coquand and Arnaud Spiwack", "title": "A proof of strong normalisation using domain theory", "comments": "16 pages", "journal-ref": "Logical Methods in Computer Science, Volume 3, Issue 4 (December\n  4, 2007) lmcs:1099", "doi": "10.2168/LMCS-3(4:12)2007", "report-no": null, "categories": "cs.LO cs.PL", "license": null, "abstract": "  Ulrich Berger presented a powerful proof of strong normalisation using\ndomains, in particular it simplifies significantly Tait's proof of strong\nnormalisation of Spector's bar recursion. The main contribution of this paper\nis to show that, using ideas from intersection types and Martin-Lof's domain\ninterpretation of type theory one can in turn simplify further U. Berger's\nargument. We build a domain model for an untyped programming language where U.\nBerger has an interpretation only for typed terms or alternatively has an\ninterpretation for untyped terms but need an extra condition to deduce strong\nnormalisation. As a main application, we show that Martin-L\\\"{o}f dependent\ntype theory extended with a program for Spector double negation shift.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2007 14:08:26 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2007 14:43:36 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Coquand", "Thierry", ""], ["Spiwack", "Arnaud", ""]]}, {"id": "0709.1699", "submitter": "Paul Fodor", "authors": "Paul Fodor", "title": "Efficient Tabling Mechanisms for Transaction Logic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": null, "abstract": "  In this paper we present efficient evaluation algorithms for the Horn\nTransaction Logic (a generalization of the regular Horn logic programs with\nstate updates). We present two complementary methods for optimizing the\nimplementation of Transaction Logic. The first method is based on tabling and\nwe modified the proof theory to table calls and answers on states (practically,\nequivalent to dynamic programming). The call-answer table is indexed on the\ncall and a signature of the state in which the call was made. The answer\ncolumns contain the answer unification and a signature of the state after the\ncall was executed. The states are signed efficiently using a technique based on\ntries and counting. The second method is based on incremental evaluation and it\napplies when the data oracle contains derived relations. The deletions and\ninsertions (executed in the transaction oracle) change the state of the\ndatabase. Using the heuristic of inertia (only a part of the state changes in\nresponse to elementary updates), most of the time it is cheaper to compute only\nthe changes in the state than to recompute the entire state from scratch. The\ntwo methods are complementary by the fact that the first method optimizes the\nevaluation when a call is repeated in the same state, and the second method\noptimizes the evaluation of a new state when a call-state pair is not found by\nthe tabling mechanism (i.e. the first method). The proof theory of Transaction\nLogic with the application of tabling and incremental evaluation is sound and\ncomplete with respect to its model theory.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2007 19:00:02 GMT"}], "update_date": "2007-09-12", "authors_parsed": [["Fodor", "Paul", ""]]}, {"id": "0709.2961", "submitter": "Andreas Schutt", "authors": "Andreas Schutt and Peter J. Stuckey", "title": "Incremental Satisfiability and Implication for UTVPI Constraints", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LO", "license": null, "abstract": "  Unit two-variable-per-inequality (UTVPI) constraints form one of the largest\nclass of integer constraints which are polynomial time solvable (unless P=NP).\nThere is considerable interest in their use for constraint solving, abstract\ninterpretation, spatial databases, and theorem proving. In this paper we\ndevelop a new incremental algorithm for UTVPI constraint satisfaction and\nimplication checking that requires O(m + n log n + p) time and O(n+m+p) space\nto incrementally check satisfiability of m UTVPI constraints on n variables and\ncheck implication of p UTVPI constraints.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2007 06:58:05 GMT"}], "update_date": "2007-09-20", "authors_parsed": [["Schutt", "Andreas", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "0709.2962", "submitter": "Pascal Weil", "authors": "Zoltan Esik, Pascal Weil (LaBRI)", "title": "Algebraic characterization of logically defined tree languages", "comments": "46 pages. Version 3: various local improvements (more typos\n  corrected, and \"intuitive\" explanations added)", "journal-ref": "International Journal of Algebra and Computation 20 (2010) 195-239", "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algebraic characterization of the tree languages that are defined\nby logical formulas using certain Lindstr\\\"om quantifiers. An important\ninstance of our result concerns first-order definable tree languages. Our\ncharacterization relies on the usage of preclones, an algebraic structure\nintroduced by the authors in a previous paper, and of the block product\noperation on preclones. Our results generalize analogous results on finite word\nlanguages, but it must be noted that, as they stand, they do not yield an\nalgorithm to decide whether a given regular tree language is first-order\ndefinable.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2007 07:14:08 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2009 16:01:23 GMT"}, {"version": "v3", "created": "Thu, 22 Jan 2009 07:08:38 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Esik", "Zoltan", "", "LaBRI"], ["Weil", "Pascal", "", "LaBRI"]]}, {"id": "0709.3034", "submitter": "Anastasia Analyti", "authors": "Carlo Meghini, Yannis Tzitzikas, Anastasia Analyti", "title": "Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,\n  Complexity, and Optimizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.LO", "license": null, "abstract": "  In this study, we address the problem of answering queries over a\npeer-to-peer system of taxonomy-based sources. A taxonomy states subsumption\nrelationships between negation-free DNF formulas on terms and negation-free\nconjunctions of terms. To the end of laying the foundations of our study, we\nfirst consider the centralized case, deriving the complexity of the decision\nproblem and of query evaluation. We conclude by presenting an algorithm that is\nefficient in data complexity and is based on hypergraphs. More expressive forms\nof taxonomies are also investigated, which however lead to intractability. We\nthen move to the distributed case, and introduce a logical model of a network\nof taxonomy-based sources. On such network, a distributed version of the\ncentralized algorithm is then presented, based on a message passing paradigm,\nand its correctness is proved. We finally discuss optimization issues, and\nrelate our work to the literature.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2007 15:10:05 GMT"}], "update_date": "2007-09-20", "authors_parsed": [["Meghini", "Carlo", ""], ["Tzitzikas", "Yannis", ""], ["Analyti", "Anastasia", ""]]}, {"id": "0709.4118", "submitter": "Francesco Ranzato", "authors": "Francesco Ranzato and Francesco Tapparo", "title": "An efficient simulation algorithm based on abstract interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of algorithms for computing the simulation preorder are available.\nLet Sigma denote the state space, -> the transition relation and Psim the\npartition of Sigma induced by simulation equivalence. The algorithms by\nHenzinger, Henzinger, Kopke and by Bloom and Paige run in O(|Sigma||->|)-time\nand, as far as time-complexity is concerned, they are the best available\nalgorithms. However, these algorithms have the drawback of a space complexity\nthat is more than quadratic in the size of the state space. The algorithm by\nGentilini, Piazza, Policriti--subsequently corrected by van Glabbeek and\nPloeger--appears to provide the best compromise between time and space\ncomplexity. Gentilini et al.'s algorithm runs in O(|Psim|^2|->|)-time while the\nspace complexity is in O(|Psim|^2 + |Sigma|log|Psim|). We present here a new\nefficient simulation algorithm that is obtained as a modification of Henzinger\net al.'s algorithm and whose correctness is based on some techniques used in\napplications of abstract interpretation to model checking. Our algorithm runs\nin O(|Psim||->|)-time and O(|Psim||Sigma|log|Sigma|)-space. Thus, this\nalgorithm improves the best known time bound while retaining an acceptable\nspace complexity that is in general less than quadratic in the size of the\nstate space. An experimental evaluation showed good comparative results with\nrespect to Henzinger, Henzinger and Kopke's algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2007 09:54:31 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2008 19:57:53 GMT"}], "update_date": "2008-12-05", "authors_parsed": [["Ranzato", "Francesco", ""], ["Tapparo", "Francesco", ""]]}]