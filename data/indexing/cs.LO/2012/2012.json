[{"id": "2012.00118", "submitter": "G. Michele Pinna", "authors": "G. Michele Pinna", "title": "A new operational representation of dependencies in Event Structures", "comments": "30 pages, 8 figures, Coordination 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The execution of an event in a complex and distributed system where the\ndependencies vary during the evolution of the system can be represented in many\nways, and one of them is to use Context-Dependent Event structures. Event\nstructures are related to Petri nets. The aim of this paper is to propose what\ncan be the appropriate kind of Petri net corresponding to Context-Dependent\nEvent structures, giving an operational flavour to the dependencies represented\nin a Context/Dependent Event structure. Dependencies are often operationally\nrepresented, in Petri nets, by tokens produced by activities and consumed by\nothers. Here we shift the perspective using contextual arcs to characterize\nwhat has happened so far and in this way to describe the dependencies among the\nvarious activities.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:39:05 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 14:51:14 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Pinna", "G. Michele", ""]]}, {"id": "2012.00224", "submitter": "Brett McLean", "authors": "C\\'elia Borlido and Brett McLean", "title": "Difference-restriction algebras of partial functions with operators:\n  discrete duality and completion", "comments": "30 pages. Small improvements throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit an adjunction between a category of abstract algebras of partial\nfunctions and a category of set quotients. The algebras are those atomic\nalgebras representable as a collection of partial functions closed under\nrelative complement and domain restriction; the morphisms are the complete\nhomomorphisms. This generalises the discrete adjunction between the atomic\nBoolean algebras and the category of sets. We define the compatible completion\nof a representable algebra, and show that the monad induced by our adjunction\nyields the compatible completion of any atomic representable algebra. As a\ncorollary, the adjunction restricts to a duality on the compatibly complete\natomic representable algebras, generalising the discrete duality between\ncomplete atomic Boolean algebras and sets. We then extend these adjunction,\nduality, and completion results to representable algebras equipped with\narbitrary additional completely additive and compatibility preserving\noperators.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 02:42:30 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 14:14:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Borlido", "C\u00e9lia", ""], ["McLean", "Brett", ""]]}, {"id": "2012.00382", "submitter": "Valeria Vignudelli", "authors": "Matteo Mio, Ralph Sarkis, Valeria Vignudelli", "title": "Combining nondeterminism, probability, and termination: equational and\n  metric reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study monads resulting from the combination of nondeterministic and\nprobabilistic behaviour with the possibility of termination, which is essential\nin program semantics. Our main contributions are presentation results for the\nmonads, providing equational reasoning tools for establishing equivalences and\ndistances of programs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 10:21:58 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 11:18:36 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 15:25:43 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Mio", "Matteo", ""], ["Sarkis", "Ralph", ""], ["Vignudelli", "Valeria", ""]]}, {"id": "2012.00856", "submitter": "Matt Luckcuck", "authors": "Matt Luckcuck", "title": "Using Formal Methods for Autonomous Systems: Five Recipes for Formal\n  Verification", "comments": "Accepted at Journal of Risk and Reliability", "journal-ref": null, "doi": "10.1177/1748006X211034970", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Methods are mathematically-based techniques for software design and\nengineering, which enable the unambiguous description of and reasoning about a\nsystem's behaviour. Autonomous systems use software to make decisions without\nhuman control, are often embedded in a robotic system, are often\nsafety-critical, and are increasingly being introduced into everyday settings.\nAutonomous systems need robust development and verification methods, but formal\nmethods practitioners are often asked: Why use Formal Methods for Autonomous\nSystems? To answer this question, this position paper describes five recipes\nfor formally verifying aspects of an autonomous system, collected from the\nliterature. The recipes are examples of how Formal Methods can be an effective\ntool for the development and verification of autonomous systems. During design,\nthey enable unambiguous description of requirements; in development, formal\nspecifications can be verified against requirements; software components may be\nsynthesised from verified specifications; and behaviour can be monitored at\nruntime and compared to its original specification. Modern Formal Methods often\ninclude highly automated tool support, which enables exhaustive checking of a\nsystem's state space. This paper argues that Formal Methods are a powerful tool\nfor the repertoire of development techniques for safe autonomous systems,\nalongside other robust software engineering techniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:54:27 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 17:16:53 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 10:47:16 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Luckcuck", "Matt", ""]]}, {"id": "2012.01102", "submitter": "Ross Horne", "authors": "Matteo Acclavio, Ross Horne and Lutz Strassburger", "title": "An Analytic Propositional Proof System on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a proof system that operates on graphs instead of\nformulas. Starting from the well-known relationship between formulas and\ncographs, we drop the cograph-conditions and look at arbitrary undirected)\ngraphs. This means that we lose the tree structure of the formulas\ncorresponding to the cographs, and we can no longer use standard proof\ntheoretical methods that depend on that tree structure. In order to overcome\nthis difficulty, we use a modular decomposition of graphs and some techniques\nfrom deep inference where inference rules do not rely on the main connective of\na formula. For our proof system we show the admissibility of cut and a\ngeneralization of the splitting property. Finally, we show that our system is a\nconservative extension of multiplicative linear logic with mix, and we argue\nthat our graphs form a notion of generalized connective.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:39:57 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 09:33:31 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Acclavio", "Matteo", ""], ["Horne", "Ross", ""], ["Strassburger", "Lutz", ""]]}, {"id": "2012.01176", "submitter": "EPTCS", "authors": "Matt Luckcuck (University of Manchester, UK), Marie Farrell\n  (University of Manchester, UK)", "title": "Proceedings Second Workshop on Formal Methods for Autonomous Systems", "comments": null, "journal-ref": "EPTCS 329, 2020", "doi": "10.4204/EPTCS.329", "report-no": null, "categories": "cs.LO cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems are highly complex and present unique challenges for the\napplication of formal methods. Autonomous systems act without human\nintervention, and are often embedded in a robotic system, so that they can\ninteract with the real world. As such, they exhibit the properties of\nsafety-critical, cyber-physical, hybrid, and real-time systems.\n  The goal of FMAS is to bring together leading researchers who are tackling\nthe unique challenges of autonomous systems using formal methods, to present\nrecent and ongoing work. We are interested in the use of formal methods to\nspecify, model, or verify autonomous or robotic systems; in whole or in part.\nWe are also interested in successful industrial applications and potential\nfuture directions for this emerging application of formal methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:08:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Luckcuck", "Matt", "", "University of Manchester, UK"], ["Farrell", "Marie", "", "University of Manchester, UK"]]}, {"id": "2012.01247", "submitter": "D. Wesley Fussner", "authors": "Wesley Fussner", "title": "Poset products as relational models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a relational semantics based on poset products, and provide\nsufficient conditions guaranteeing its soundness and completeness for various\nsubstructural logics. We also demonstrate that our relational semantics unifies\nand generalizes two semantics already appearing in the literature: Aguzzoli,\nBianchi, and Marra's temporal flow semantics for H\\'ajek's basic logic, and\nLewis-Smith, Oliva, and Robinson's semantics for intuitionistic Lukasiewicz\nlogic. As a consequence of our general theory, we recover the soundness and\ncompleteness results of these prior studies in a uniform fashion, and extend\nthem to infinitely-many other substructural logics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:49:39 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Fussner", "Wesley", ""]]}, {"id": "2012.01309", "submitter": "Manfred Kufleitner", "authors": "Viktor Henriksson and Manfred Kufleitner", "title": "Nesting negations in FO2 over infinite words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider two-variable first-order logic FO2 over infinite words.\nRestricting the number of nested negations defines an infinite hierarchy; its\nlevels are often called the half-levels of the FO2 quantifier alternation\nhierarchy. For every level of this hierarchy, we give an effective\ncharacterization. For the lower levels, this characterization is a combination\nof an algebraic and a topological property. For the higher levels, algebraic\nproperties turn out to be sufficient. Within two-variable first-order logic,\neach algebraic property is a single ordered identity of omega-terms. The\ntopological properties are the same as for the lower half-levels of the\nquantifier alternation hierarchy without the two-variable restriction (i.e.,\nthe Cantor topology and the alphabetic topology).\n  Our result generalizes the corresponding result for finite words. The proof\nuses novel techniques and is based on a refinement of Mal'cev products for\nordered monoids.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:29:35 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Henriksson", "Viktor", ""], ["Kufleitner", "Manfred", ""]]}, {"id": "2012.01323", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte and Markus Hecher and Florim Hamiti", "title": "The Model Counting Competition 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computational problems in modern society account to probabilistic\nreasoning, statistics, and combinatorics. A variety of these real-world\nquestions can be solved by representing the question in (Boolean) formulas and\nassociating the number of models of the formula directly with the answer to the\nquestion. Since there has been an increasing interest in practical problem\nsolving for model counting over the last years, the Model Counting (MC)\nCompetition was conceived in fall 2019. The competition aims to foster\napplications, identify new challenging benchmarks, and to promote new solvers\nand improve established solvers for the model counting problem and versions\nthereof. We hope that the results can be a good indicator of the current\nfeasibility of model counting and spark many new applications. In this paper,\nwe report on details of the Model Counting Competition 2020, about carrying out\nthe competition, and the results. The competition encompassed three versions of\nthe model counting problem, which we evaluated in separate tracks. The first\ntrack featured the model counting problem (MC), which asks for the number of\nmodels of a given Boolean formula. On the second track, we challenged\ndevelopers to submit programs that solve the weighted model counting problem\n(WMC). The last track was dedicated to projected model counting (PMC). In\ntotal, we received a surprising number of 9 solvers in 34 versions from 8\ngroups.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:52:07 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Hamiti", "Florim", ""]]}, {"id": "2012.01466", "submitter": "Ziyuan Gao", "authors": "David Belanger, Ziyuan Gao, Sanjay Jain, Wei Li and Frank Stephan", "title": "Learnability and Positive Equivalence Relations", "comments": "31 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior work of Gavryushkin, Khoussainov, Jain and Stephan investigated what\nalgebraic structures can be realised in worlds given by a positive (=\nrecursively enumerable) equivalence relation which partitions the natural\nnumbers into infinitely many equivalence classes. The present work investigates\nthe infinite one-one numbered recursively enumerable (r.e.) families realised\nby such relations and asks how the choice of the equivalence relation impacts\nthe learnability properties of these classes when studying learnability in the\nlimit from positive examples, also known as learning from text. For all choices\nof such positive equivalence relations, for each of the following entries,\nthere are one-one numbered r.e. families which satisfy it: (a) they are\nbehaviourally correctly learnable but not vacillatorily learnable; (b) they are\nexplanatorily learnable but not confidently learnable; (c) they are not\nbehaviourally correctly learnable. Furthermore, there is a positive equivalence\nrelation which enforces that (d) every vacillatorily learnable one-one numbered\nfamily of languages closed under this equivalence relation is already\nexplanatorily learnable and cannot be confidently learnable.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 04:06:47 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 02:11:36 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Belanger", "David", ""], ["Gao", "Ziyuan", ""], ["Jain", "Sanjay", ""], ["Li", "Wei", ""], ["Stephan", "Frank", ""]]}, {"id": "2012.01648", "submitter": "EPTCS", "authors": "Rafael C. Cardoso (The University of Manchester), Louise A. Dennis\n  (The University of Manchester), Marie Farrell (The University of Manchester),\n  Michael Fisher (The University of Manchester), Matt Luckcuck (The University\n  of Manchester)", "title": "Towards Compositional Verification for Modular Robotic Systems", "comments": "In Proceedings FMAS 2020, arXiv:2012.01176", "journal-ref": "EPTCS 329, 2020, pp. 15-22", "doi": "10.4204/EPTCS.329.2", "report-no": null, "categories": "cs.SE cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software engineering of modular robotic systems is a challenging task,\nhowever, verifying that the developed components all behave as they should\nindividually and as a whole presents its own unique set of challenges. In\nparticular, distinct components in a modular robotic system often require\ndifferent verification techniques to ensure that they behave as expected.\nEnsuring whole system consistency when individual components are verified using\na variety of techniques and formalisms is difficult. This paper discusses how\nto use compositional verification to integrate the various verification\ntechniques that are applied to modular robotic software, using a First-Order\nLogic (FOL) contract that captures each component's assumptions and guarantees.\nThese contracts can then be used to guide the verification of the individual\ncomponents, be it by testing or the use of a formal method. We provide an\nillustrative example of an autonomous robot used in remote inspection. We also\ndiscuss a way of defining confidence for the verification associated with each\ncomponent.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:21:51 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Cardoso", "Rafael C.", "", "The University of Manchester"], ["Dennis", "Louise A.", "", "The University of Manchester"], ["Farrell", "Marie", "", "The University of Manchester"], ["Fisher", "Michael", "", "The University of Manchester"], ["Luckcuck", "Matt", "", "The University\n  of Manchester"]]}, {"id": "2012.01651", "submitter": "EPTCS", "authors": "Fatma Kachi (LIRE Laboratory, University of Constantine2-Abdelhamid\n  Mehri), Chafia Bouanaka (LIRE Laboratory, University of\n  Constantine2-Abdelhamid Mehri), Souheir Merkouche (University of\n  Constantine2-Abdelhamid Mehri)", "title": "A Formal Model for Quality-Driven Decision Making in Self-Adaptive\n  Systems", "comments": "In Proceedings FMAS 2020, arXiv:2012.01176", "journal-ref": "EPTCS 329, 2020, pp. 48-64", "doi": "10.4204/EPTCS.329.5", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining an acceptable level of quality of service in modern complex\nsystems is challenging, particularly in the presence of various forms of\nuncertainty caused by changing execution context, unpredicted events, etc.\nAlthough self-adaptability is a well-established approach for modelling such\nsystems, and thus enabling them to achieve functional and/or quality of service\nobjectives by autonomously modifying their behavior at runtime, guaranteeing a\ncontinuous satisfaction of quality objectives is still challenging and needs a\nrigorous definition and analysis of system behavioral properties. Formal\nmethods constitute a promising and effective solution in this direction in\norder to rigorously specify mathematical models of a software system and to\nanalyze its behavior. They are also largely adopted to analyze and provide\nguarantees on the required functional/non-functional properties of\nself-adaptive systems. Therefore, we introduce a formal model for\nquality-driven self-adaptive systems under uncertainty. We combine high-level\nPetri nets and plausible Petri nets in order to model complex data structures\nenabling system quality attributes quantification and to improve the\ndecision-making process through selecting the most plausible plans with regard\nto the system's actual context.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:22:38 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Kachi", "Fatma", "", "LIRE Laboratory, University of Constantine2-Abdelhamid\n  Mehri"], ["Bouanaka", "Chafia", "", "LIRE Laboratory, University of\n  Constantine2-Abdelhamid Mehri"], ["Merkouche", "Souheir", "", "University of\n  Constantine2-Abdelhamid Mehri"]]}, {"id": "2012.01657", "submitter": "EPTCS", "authors": "Okan \\\"Ozkan (Carl von Ossietzky University of Oldenburg, Germany)", "title": "Modeling Adverse Conditions in the Framework of Graph Transformation\n  Systems", "comments": "In Proceedings GCM 2020, arXiv:2012.01181", "journal-ref": "EPTCS 330, 2020, pp. 35-54", "doi": "10.4204/EPTCS.330.3", "report-no": null, "categories": "cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of adverse conditions addresses systems interacting with an\nadversary environment and finds use also in the development of new\ntechnologies. We present an approach for modeling adverse conditions by graph\ntransformation systems. In contrast to other approaches for\ngraph-transformational interacting systems, the presented main constructs are\ngraph transformation systems. We introduce joint graph transformation systems\nwhich involve a system, an interfering environment, and an automaton modeling\ntheir interaction. For joint graph transformation systems, we introduce notions\nof (partial) correctness under adverse conditions, which contain the\ncorrectness of the system and a recovery condition. As main result, we show\nthat two instances of correctness, namely k-step correctness (recovery in at\nmost k steps after an environment intervention) and last-minute correctness\n(recovery until next environment intervention) are expressible in LTL (linear\ntemporal logic), and that a weaker notion of k-step correctness is expressible\nin CTL (computation tree logic).\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:27:39 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["\u00d6zkan", "Okan", "", "Carl von Ossietzky University of Oldenburg, Germany"]]}, {"id": "2012.01661", "submitter": "EPTCS", "authors": "Russ Harmer (Univ Lyon, EnsL, UCBL, CNRS, LIP, France), Eugenia\n  Oshurko (Univ Lyon, EnsL, UCBL, CNRS, LIP, France)", "title": "Reversibility and Composition of Rewriting in Hierarchies", "comments": "In Proceedings GCM 2020, arXiv:2012.01181", "journal-ref": "EPTCS 330, 2020, pp. 145-162", "doi": "10.4204/EPTCS.330.9", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how graph transformations based on sesqui-pushout\nrewriting can be reversed and how the composition of rewrites can be\nconstructed. We illustrate how such reversibility and composition can be used\nto design an audit trail system for individual graphs and graph hierarchies.\nThis provides us with a compact way to maintain the history of updates of an\nobject, including its multiple versions. The main application of the designed\nframework is an audit trail of updates to knowledge represented by hierarchies\nof graphs. Therefore, we introduce the notion of rule hierarchy that represents\na transformation of the entire hierarchy, study how rule hierarchies can be\napplied to hierarchies and analyse the conditions under which this application\nis reversible. We then present a theory for constructing the composition of\nconsecutive hierarchy rewrites. The prototype audit trail system for\ntransformations in hierarchies of simple graphs with attributes is implemented\nas part of the ReGraph Python library.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:29:28 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Harmer", "Russ", "", "Univ Lyon, EnsL, UCBL, CNRS, LIP, France"], ["Oshurko", "Eugenia", "", "Univ Lyon, EnsL, UCBL, CNRS, LIP, France"]]}, {"id": "2012.01662", "submitter": "EPTCS", "authors": "Gia S. Wulandari (University of York, UK, Telkom University, Bandung,\n  Indonesia), Detlef Plump (University of York, UK)", "title": "Verifying Graph Programs with First-Order Logic", "comments": "In Proceedings GCM 2020, arXiv:2012.01181. arXiv admin note:\n  substantial text overlap with arXiv:2010.14549", "journal-ref": "EPTCS 330, 2020, pp. 181-200", "doi": "10.4204/EPTCS.330.11", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Hoare-style verification for the graph programming language GP 2.\nIn previous work, graph properties were specified by so-called E-conditions\nwhich extend nested graph conditions. However, this type of assertions is not\neasy to comprehend by programmers that are used to formal specifications in\nstandard first-order logic. In this paper, we present an approach to verify GP\n2 programs with a standard first-order logic. We show how to construct a\nstrongest liberal postcondition with respect to a rule schema and a\nprecondition. We then extend this construction to obtain strongest liberal\npostconditions for arbitrary loop-free programs. Compared with previous work,\nthis allows to reason about a vastly generalised class of graph programs. In\nparticular, many programs with nested loops can be verified with the new\ncalculus.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:30:12 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wulandari", "Gia S.", "", "University of York, UK, Telkom University, Bandung,\n  Indonesia"], ["Plump", "Detlef", "", "University of York, UK"]]}, {"id": "2012.01847", "submitter": "Fabio Zanasi", "authors": "Filippo Bonchi, Fabio Gadducci, Aleks Kissinger, Pawel Sobocinski, and\n  Fabio Zanasi", "title": "String Diagram Rewrite Theory I: Rewriting with Frobenius Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  String diagrams are a powerful and intuitive graphical syntax, originated in\nthe study of symmetric monoidal categories. In the last few years, they have\nfound application in the modelling of various computational structures, in\nfields as diverse as Computer Science, Physics, Control Theory, Linguistics,\nand Biology.\n  In many such proposals, the transformations of the described systems are\nmodelled as rewrite rules of diagrams. These developments demand a mathematical\nfoundation for string diagram rewriting: whereas rewrite theory for terms is\nwell-understood, the two-dimensional nature of string diagrams poses additional\nchallenges.\n  This work systematises and expands a series of recent conference papers\nlaying down such foundation. As first step, we focus on the case of rewrite\nsystems for string diagrammatic theories which feature a Frobenius algebra.\nThis situation ubiquitously appear in various approaches: for instance, in the\nalgebraic semantics of linear dynamical systems, Frobenius structures model the\nwiring of circuits; in categorical quantum mechanics, they model interacting\nquantum observables.\n  Our work introduces a combinatorial interpretation of string diagram\nrewriting modulo Frobenius structures, in terms of double-pushout hypergraph\nrewriting. Furthermore, we prove this interpretation to be sound and complete.\nIn the last part, we also see that the approach can be generalised to model\nrewriting modulo multiple Frobenius structures. As a proof of concept, we show\nhow to derive from these results a termination strategy for Interacting\nBialgebras, an important rewrite theory in the study of quantum circuits and\nsignal flow graphs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 11:46:06 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bonchi", "Filippo", ""], ["Gadducci", "Fabio", ""], ["Kissinger", "Aleks", ""], ["Sobocinski", "Pawel", ""], ["Zanasi", "Fabio", ""]]}, {"id": "2012.02143", "submitter": "Vasco Brattka", "authors": "Vasco Brattka", "title": "The Discontinuity Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matthias Schr\\\"oder has asked the question whether there is a weakest\ndiscontinuous problem in the continuous version of the Weihrauch lattice. Such\na problem can be considered as the weakest unsolvable problem. We introduce the\ndiscontinuity problem, and we show that it is reducible exactly to the\neffectively discontinuous problems, defined in a suitable way. However, in\nwhich sense this answers Schr\\\"oder's question sensitively depends on the\naxiomatic framework that is chosen, and it is a positive answer if we work in\nZermelo-Fraenkel set theory with dependent choice and the axiom of determinacy\nAD. On the other hand, using the full axiom of choice, one can construct\nproblems which are discontinuous, but not effectively so. Hence, the exact\nsituation at the bottom of the Weihrauch lattice sensitively depends on the\naxiomatic setting that we choose. We prove our result using a variant of Wadge\ngames for mathematical problems. While the existence of a winning strategy for\nplayer II characterizes continuity of the problem (as already shown by Nobrega\nand Pauly), the existence of a winning strategy for player I characterizes\neffective discontinuity of the problem. By Weihrauch determinacy we understand\nthe condition that every problem is either continuous or effectively\ndiscontinuous. This notion of determinacy is a fairly strong notion, as it is\nnot only implied by the axiom of determinacy AD, but it also implies Wadge\ndeterminacy. We close with a brief discussion of generalized notions of\nproductivity.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:22:53 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Brattka", "Vasco", ""]]}, {"id": "2012.02154", "submitter": "Kartik Singhal", "authors": "Kartik Singhal", "title": "Quantum Hoare Type Theory", "comments": "33 pages, 3 figures, 12 code listings. Draft master's paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.ET cs.LO quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As quantum computers become real, it is high time we come up with effective\ntechniques that help programmers write correct quantum programs. Inspired by\nHoare Type Theory in classical computing, we propose Quantum Hoare Type Theory\n(QHTT) in which precise specifications about the modification to the quantum\nstate can be provided within the type of a computation. These specifications\nwithin a Hoare type are given in the form of Hoare-logic style pre- and\npostconditions following the propositions-as-types principle. The type-checking\nprocess verifies that the implementation conforms to the provided\nspecification. QHTT has the potential to be a unified system for programming,\nspecifying, and reasoning about quantum programs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:41:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Singhal", "Kartik", ""]]}, {"id": "2012.02192", "submitter": "EPTCS", "authors": "Andrea Corradini (Dipartimento di Informatica, University of Pisa,\n  Italy), Maryam Ghaffari Saadat (Department of Informatics, University of\n  Leicester, UK), Reiko Heckel (Department of Informatics, University of\n  Leicester, UK)", "title": "Encoding Incremental NACs in Safe Graph Grammars using Complementation", "comments": "In Proceedings GCM 2020, arXiv:2012.01181", "journal-ref": "EPTCS 330, 2020, pp. 88-107", "doi": "10.4204/EPTCS.330.6", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modelling complex systems with graph grammars (GGs), it is convenient to\nrestrict the application of rules using attribute constraints and negative\napplication conditions (NACs). However, having both attributes and NACs in GGs\nrenders the behavioural analysis (e.g. unfolding) of such systems more\ncomplicated. We address this issue by an approach to encode NACs using a\ncomplementation technique. We consider the correctness of our encoding under\nthe assumption that the grammar is safe and NACs are incremental, and outline\nhow this result can be extended to unsafe, attributed grammars.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:28:29 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Corradini", "Andrea", "", "Dipartimento di Informatica, University of Pisa,\n  Italy"], ["Saadat", "Maryam Ghaffari", "", "Department of Informatics, University of\n  Leicester, UK"], ["Heckel", "Reiko", "", "Department of Informatics, University of\n  Leicester, UK"]]}, {"id": "2012.03745", "submitter": "EPTCS", "authors": "Aaron Dutle (NASA), C\\'esar Mu\\~noz (NASA), Esther Conrad (NASA),\n  Alwyn Goodloe (NASA), Laura Titolo (National Institute of Aerospace), Ivan\n  Perez (National Institute of Aerospace), Swee Balachandran (National\n  Institute of Aerospace), Dimitra Giannakopoulou (NASA), Anastasia Mavridou\n  (KBR Inc.), Thomas Pressburger (NASA)", "title": "From Requirements to Autonomous Flight: An Overview of the Monitoring\n  ICAROUS Project", "comments": "In Proceedings FMAS 2020, arXiv:2012.01176", "journal-ref": "EPTCS 329, 2020, pp. 23-30", "doi": "10.4204/EPTCS.329.3", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Independent Configurable Architecture for Reliable Operations of Unmanned\nSystems (ICAROUS) is a software architecture incorporating a set of algorithms\nto enable autonomous operations of unmanned aircraft applications. This paper\nprovides an overview of Monitoring ICAROUS, a project whose objective is to\nprovide a formal approach to generating runtime monitors for autonomous systems\nfrom requirements written in a structured natural language. This approach\nintegrates FRET, a formal requirement elicitation and authoring tool, and\nCopilot, a runtime verification framework. FRET is used to specify formal\nrequirements in structured natural language. These requirements are translated\ninto temporal logic formulae. Copilot is then used to generate executable\nruntime monitors from these temporal logic specifications. The generated\nmonitors are directly integrated into ICAROUS to perform runtime verification\nduring flight.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:22:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dutle", "Aaron", "", "NASA"], ["Mu\u00f1oz", "C\u00e9sar", "", "NASA"], ["Conrad", "Esther", "", "NASA"], ["Goodloe", "Alwyn", "", "NASA"], ["Titolo", "Laura", "", "National Institute of Aerospace"], ["Perez", "Ivan", "", "National Institute of Aerospace"], ["Balachandran", "Swee", "", "National\n  Institute of Aerospace"], ["Giannakopoulou", "Dimitra", "", "NASA"], ["Mavridou", "Anastasia", "", "KBR Inc."], ["Pressburger", "Thomas", "", "NASA"]]}, {"id": "2012.03910", "submitter": "Sebastian Biewer", "authors": "Sebastian Biewer, Rayna Dimitrova, Michael Fries, Maciej Gazda, Thomas\n  Heinze, Holger Hermanns and Mohammad Reza Mousavi", "title": "Conformance Relations and Hyperproperties for Doping Detection in Time\n  and Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and generalised notion of doping cleanness for\ncyber-physical systems that allows for perturbing the inputs and observing the\nperturbed outputs both in the time- and value-domains. We instantiate our\ndefinition using existing notions of conformance for cyber-physical systems. As\na formal basis for monitoring conformance-based cleanness, we develop the\ntemporal logic HyperSTL*, an extension of Signal Temporal Logics with trace\nquantifiers and a freeze operator. We show that our generalised definitions are\nessential in a data-driven method for doping detection and apply our\ndefinitions to a case study concerning diesel emission tests.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:41:17 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 15:40:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Biewer", "Sebastian", ""], ["Dimitrova", "Rayna", ""], ["Fries", "Michael", ""], ["Gazda", "Maciej", ""], ["Heinze", "Thomas", ""], ["Hermanns", "Holger", ""], ["Mousavi", "Mohammad Reza", ""]]}, {"id": "2012.03916", "submitter": "Fabio Zanasi", "authors": "Tao Gu, Fabio Zanasi", "title": "Coalgebraic Semantics for Probabilistic Logic Programming", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 2 (April 12,\n  2021) lmcs:7365", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic logic programming is increasingly important in artificial\nintelligence and related fields as a formalism to reason about uncertainty. It\ngeneralises logic programming with the possibility of annotating clauses with\nprobabilities. This paper proposes a coalgebraic semantics on probabilistic\nlogic programming. Programs are modelled as coalgebras for a certain functor F,\nand two semantics are given in terms of cofree coalgebras. First, the\nF-coalgebra yields a semantics in terms of derivation trees. Second, by\nembedding F into another type G, as cofree G-coalgebra we obtain a `possible\nworlds' interpretation of programs, from which one may recover the usual\ndistribution semantics of probabilistic logic programming. Furthermore, we show\nthat a similar approach can be used to provide a coalgebraic semantics to\nweighted logic programming.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:45:35 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 10:35:23 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gu", "Tao", ""], ["Zanasi", "Fabio", ""]]}, {"id": "2012.04025", "submitter": "Fatemeh Ghassemi", "authors": "Mahsa Zarneshan, Fatemeh Ghassemi, Ehsan Khamespanah, Marjan Sirjani,\n  John Hatcliff", "title": "Specification and Verification of Timing Properties in Interoperable\n  Medical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To support the dynamic composition of various devices/apps into a medical\nsystem at point-of-care, a set of communication patterns to describe the\ncommunication needs of devices has been proposed. To address timing\nrequirements, each pattern breaks common timing properties into finer ones that\ncan be enforced locally by the components. Common timing requirements for the\nunderlying communication substrate are derived from these local properties. The\nlocal properties of devices are assured by the vendors at the development time.\nAlthough organizations procure devices that are compatible in terms of their\nlocal properties and middleware, they may not operate as desired. The latency\nof the organization network interacts with the local properties of devices. To\nvalidate the interaction among the timing properties of components and the\nnetwork, we formally specify such systems in Timed Rebeca. We use model\nchecking to verify the derived timing requirements of the communication\nsubstrate in terms of the network and device models. We provide a set of\ntemplates as a guideline to specify medical systems in terms of the formal\nmodel of patterns. A composite medical system using several devices is subject\nto state-space explosion. We extend the reduction technique of Timed Rebeca\nbased on the static properties of patterns. We prove that our reduction is\nsound and show the applicability of our approach in reducing the state space by\nmodeling two clinical scenarios made of several instances of patterns.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 20:03:18 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:24:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zarneshan", "Mahsa", ""], ["Ghassemi", "Fatemeh", ""], ["Khamespanah", "Ehsan", ""], ["Sirjani", "Marjan", ""], ["Hatcliff", "John", ""]]}, {"id": "2012.04715", "submitter": "Curtis Bright", "authors": "Curtis Bright, Kevin K. H. Cheung, Brett Stevens, Ilias Kotsireas,\n  Vijay Ganesh", "title": "A SAT-based Resolution of Lam's Problem", "comments": "To appear at the Thirty-Fifth AAAI Conference on Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI cs.LO cs.SC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1989, computer searches by Lam, Thiel, and Swiercz experimentally resolved\nLam's problem from projective geometry$\\unicode{x2014}$the long-standing\nproblem of determining if a projective plane of order ten exists. Both the\noriginal search and an independent verification in 2011 discovered no such\nprojective plane. However, these searches were each performed using highly\nspecialized custom-written code and did not produce nonexistence certificates.\nIn this paper, we resolve Lam's problem by translating the problem into Boolean\nlogic and use satisfiability (SAT) solvers to produce nonexistence certificates\nthat can be verified by a third party. Our work uncovered consistency issues in\nboth previous searches$\\unicode{x2014}$highlighting the difficulty of relying\non special-purpose search code for nonexistence results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:06:25 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Bright", "Curtis", ""], ["Cheung", "Kevin K. H.", ""], ["Stevens", "Brett", ""], ["Kotsireas", "Ilias", ""], ["Ganesh", "Vijay", ""]]}, {"id": "2012.04739", "submitter": "Michal Knapik", "authors": "Laure Petrucci and Micha{\\l} Knapik", "title": "Modular Analysis of Tree-Topology Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate networks of automata that synchronise over common action\nlabels. A graph synchronisation topology between the automata is defined in\nsuch a way that two automata are connected iff they can synchronise over an\naction. We show a very effective reduction of networks of automata with\ntree-like synchronisation topologies. The reduction preserves a certain form of\nreachability, but not safety. The procedure is implemented in an open-source\ntool.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:03:04 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Petrucci", "Laure", ""], ["Knapik", "Micha\u0142", ""]]}, {"id": "2012.04752", "submitter": "Boris Eng", "authors": "Boris Eng (LIPN)", "title": "A gentle introduction to Girard's Transcendental Syntax for the linear\n  logician", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Girard's Transcendental Syntax suggests a new framework for proof theory\nwhere logic (proofs, formulas, truth, ...) is no more primitive but computation\nis. Logic is grounded on a model of computation I call \"stellar resolution\"\nwhich is basically logic-free Robinson's first-order clausal resolution with a\ndynamics related to tiling models. This model naturally encodes the\ncut-elimination for proof-structures. By using realisability techniques for\nlinear logic, it is possible to reconstruct formulas/types and logical\ncorrectness in order to obtain models of linear logic. Girard's philosophical\njustification of these works comes from Kantian inspirations: the\nTranscendental Syntax appears as a way to talk about the \"conditions of\npossibility of logic\", that is the conditions from which logical constructions\nemerge out of the meaningless (computation). We illustrate this foundational\nproject with a reconstruction of Intuitionistic MELL(+MIX) and describe few\nother novelties such as the treatment of second order and Girard's logical\nconstants \"fu\" and \"wo\".\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:09:28 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 09:29:47 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 13:25:10 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 08:02:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Eng", "Boris", "", "LIPN"]]}, {"id": "2012.05267", "submitter": "Tom Hanika", "authors": "Tom Hanika and Johannes Hirth", "title": "On the Lattice of Conceptual Measurements", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for data set scaling based on scale-measures from\nformal concept analysis, i.e., continuous maps between closure systems, and\nderive a canonical representation. Moreover, we prove said scale-measures are\nlattice ordered with respect to the closure systems. This enables exploring the\nset of scale-measures through by the use of meet and join operations.\nFurthermore we show that the lattice of scale-measures is isomorphic to the\nlattice of sub-closure systems that arises from the original data. Finally, we\nprovide another representation of scale-measures using propositional logic in\nterms of data set features. Our theoretical findings are discussed by means of\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:11:50 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hanika", "Tom", ""], ["Hirth", "Johannes", ""]]}, {"id": "2012.05604", "submitter": "Chun-Yu Lin", "authors": "Chun-Yu Lin and Churn-Jung Liau", "title": "A Note on the Completeness of Many-Valued Coalgebraic Modal Logic", "comments": "17 pages, preprint for journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we investigate the many-valued version of coalgebraic modal\nlogic through predicate lifting approach. Coalgebras, understood as generic\ntransition systems, can serve as semantic structures for various kinds of modal\nlogics. A well-known result in coalgebraic modal logic is that its completeness\ncan be determined at the one-step level. We generalize the result to the\nfinitely many-valued case by using the canonical model construction method. We\nprove the result for coalgebraic modal logics based on three different\nmany-valued algebraic structures, including the finitely-valued {\\L}ukasiewicz\nalgebra, the commutative integral Full-Lambek algebra (FL$_{ew}$-algebra)\nexpanded with canonical constants and Baaz Delta, and the FL$_{ew}$-algebra\nexpanded with valuation operations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:43:49 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 03:50:38 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Lin", "Chun-Yu", ""], ["Liau", "Churn-Jung", ""]]}, {"id": "2012.05682", "submitter": "Johannes Greiner", "authors": "Manuel Bodirsky, Johannes Greiner, Jakub Rydval", "title": "Tractable Combinations of Temporal CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) of a first-order theory $T$ is the\ncomputational problem of deciding whether a given conjunction of atomic\nformulas is satisfiable in some model of $T$. We study the computational\ncomplexity of $\\mathrm{CSP}(T_1 \\cup T_2)$ where $T_1$ and $T_2$ are theories\nwith disjoint finite relational signatures. We prove that if $T_1$ and $T_2$\nare the theories of temporal structures, i.e., structures where all relations\nhave a first-order definition in $(\\mathbb{Q};<)$, then $\\mathrm{CSP}(T_1 \\cup\nT_2)$ is in P or NP-complete. To this end we prove a purely algebraic statement\nabout the structure of the lattice of locally closed clones over the domain\n${\\mathbb Q}$ that contain $\\mathrm{Aut}(\\mathbb{Q};<)$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:08:34 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 09:11:43 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Greiner", "Johannes", ""], ["Rydval", "Jakub", ""]]}, {"id": "2012.05863", "submitter": "Aleksandar S. Dimovski", "authors": "Aleksandar S. Dimovski, Sven Apel, Axel Legay", "title": "A Decision Tree Lifted Domain for Analyzing Program Families with\n  Numerical Features (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lifted (family-based) static analysis by abstract interpretation is capable\nof analyzing all variants of a program family simultaneously, in a single run\nwithout generating any of the variants explicitly. The elements of the\nunderlying lifted analysis domain are tuples, which maintain one property per\nvariant. Still, explicit property enumeration in tuples, one by one for all\nvariants, immediately yields combinatorial explosion. This is particularly\napparent in the case of program families that, apart from Boolean features,\ncontain also numerical features with big domains, thus admitting astronomic\nconfiguration spaces.\n  The key for an efficient lifted analysis is proper handling of\nvariability-specific constructs of the language (e.g., feature-based runtime\ntests and #if directives). In this work, we introduce a new symbolic\nrepresentation of the lifted abstract domain that can efficiently analyze\nprogram families with numerical features. This makes sharing between property\nelements corresponding to different variants explicitly possible. The elements\nof the new lifted domain are constraint-based decision trees, where decision\nnodes are labeled with linear constraints defined over numerical features and\nthe leaf nodes belong to an existing single-program analysis domain. To\nillustrate the potential of this representation, we have implemented an\nexperimental lifted static analyzer, called SPLNUM^2Analyzer, for inferring\ninvariants of C programs. It uses existing numerical domains (e.g., intervals,\noctagons, polyhedra) from the APRON library as parameters. An empirical\nevaluation on benchmarks from SV-COMP and BusyBox yields promising preliminary\nresults indicating that our decision trees-based approach is effective and\noutperforms the tuple-based approach, which is used as a baseline lifted\nanalysis based on abstract interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:21:15 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Dimovski", "Aleksandar S.", ""], ["Apel", "Sven", ""], ["Legay", "Axel", ""]]}, {"id": "2012.05887", "submitter": "Alejandro D\\'iaz-Caro", "authors": "Alejandro D\\'iaz-Caro and Octavio Malherbe", "title": "Quantum Control in the Unitary Sphere: Lambda-S1 and its Categorical\n  Model", "comments": "23 pages plus appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a recent paper, a realizability technique has been used to give a\nsemantics of a quantum lambda calculus. Such a technique gives rise to an\ninfinite number of valid typing rules, without giving preference to any subset\nof those. In this paper, we introduce a valid subset of typing rules, defining\nan expressive enough quantum calculus. Then, we propose a categorical semantics\nfor it. Such a semantics consists of an adjunction between the category of\ndistributive-action spaces of value distributions (that is, linear combinations\nof values in the lambda calculus), and the category of sets of value\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:48:10 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 15:16:12 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["D\u00edaz-Caro", "Alejandro", ""], ["Malherbe", "Octavio", ""]]}, {"id": "2012.06370", "submitter": "Sarah Winkler", "authors": "Sarah Winkler and Georg Moser", "title": "Runtime Complexity Analysis of Logically Constrained Rewriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logically constrained rewrite systems (LCTRSs) are a versatile and efficient\nrewriting formalism that can be used to model programs from various programming\nparadigms, as well as simplification systems in compilers and SMT solvers. In\nthis paper, we investigate techniques to analyse the worst-case runtime\ncomplexity of LCTRSs. For that, we exploit synergies between previously\ndeveloped decomposition techniques for standard term rewriting by Avanzini et\nal. in conjunction with alternating time and size bound approximations for\ninteger programs by Brockschmidt et al. and adapt these techniques suitably to\nLCTRSs. Furthermore, we provide novel modularization techniques to exploit loop\nbounds from recurrence equations which yield sublinear bounds. We have\nimplemented the method in TCT to test the viability of our method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:19:07 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Winkler", "Sarah", ""], ["Moser", "Georg", ""]]}, {"id": "2012.06468", "submitter": "Maurice Laveaux", "authors": "Maurice Laveaux and Tim A.C. Willemse", "title": "Decompositional Minimisation of Monolithic Processes", "comments": "24 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional minimisation can be an effective technique to reduce the state\nspace explosion problem. This technique considers a parallel composition of\nseveral processes. In its simplest form, each sequential process is replaced by\nan abstraction, simpler than the corresponding process while still preserving\nthe property that is checked. However, this technique cannot be applied in a\nsetting where parallel composition is first translated to a non-deterministic\nsequential monolithic process. The advantage of this monolithic process is that\nit facilitates static analysis of global behaviour. Therefore, we present a\ntechnique that considers a monolithic process with data and decomposes it into\ntwo processes where each process defines behaviour for a subset of the\nparameters of the monolithic process. We prove that these processes preserve\nthe properties of the monolithic process under a suitable synchronisation\ncontext. Moreover, we prove that state invariants can be used to improve its\neffectiveness. Finally, we apply the decomposition technique to several\nspecifications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:39:25 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Laveaux", "Maurice", ""], ["Willemse", "Tim A. C.", ""]]}, {"id": "2012.06530", "submitter": "Tom Hirschowitz", "authors": "Andr\\'e Hirschowitz, Tom Hirschowitz and Ambroise Lafont", "title": "Modules over monads and operational semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is a contribution to the search for efficient and high-level\nmathematical tools to specify and reason about (abstract) programming languages\nor calculi. Generalising the reduction monads of Ahrens et al., we introduce\ntransition monads, thus covering new applications such as\nlambda-bar-mu-calculus, pi-calculus, Positive GSOS specifications, differential\nlambda-calculus, and the big-step, simply-typed, call-by-value lambda-calculus.\nMoreover, we design a suitable notion of signature for transition monads.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:51:50 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Hirschowitz", "Andr\u00e9", ""], ["Hirschowitz", "Tom", ""], ["Lafont", "Ambroise", ""]]}, {"id": "2012.06561", "submitter": "Pavel Naumov", "authors": "Pavel Naumov, Kevin Ros", "title": "Comprehension and Knowledge", "comments": "To appear in Proceedings 35th AAAI Conference on Artificial\n  Intelligence (AAAI 21), February 2-9, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of an agent to comprehend a sentence is tightly connected to the\nagent's prior experiences and background knowledge. The paper suggests to\ninterpret comprehension as a modality and proposes a complete bimodal logical\nsystem that describes an interplay between comprehension and knowledge\nmodalities.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:42:08 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 22:24:35 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Naumov", "Pavel", ""], ["Ros", "Kevin", ""]]}, {"id": "2012.06651", "submitter": "Pavel Naumov", "authors": "Sophia Epstein, Pavel Naumov", "title": "Epistemic Logic of Know-Who", "comments": "To appear in Proceedings of 35th AAAI Conference on Artificial\n  Intelligence (AAAI 21), February 2-9, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper suggests a definition of \"know who\" as a modality using\nGrove-Halpern semantics of names. It also introduces a logical system that\ndescribes the interplay between modalities \"knows who\", \"knows\", and \"for all\nagents\". The main technical result is a completeness theorem for the proposed\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 21:45:04 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 02:09:32 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Epstein", "Sophia", ""], ["Naumov", "Pavel", ""]]}, {"id": "2012.06779", "submitter": "Gaurav Sood", "authors": "Olaf Beyersdorff, Joshua Blinkhorn, Meena Mahajan, Tom\\'a\\v{s} Peitl,\n  Gaurav Sood", "title": "Hard QBFs for Merge Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the first proof size lower bounds for the proof system Merge\nResolution (MRes [Olaf Beyersdorff et al., 2020]), a refutational proof system\nfor prenex quantified Boolean formulas (QBF) with a CNF matrix. Unlike most QBF\nresolution systems in the literature, proofs in MRes consist of resolution\nsteps together with information on countermodels, which are syntactically\nstored in the proofs as merge maps. As demonstrated in [Olaf Beyersdorff et\nal., 2020], this makes MRes quite powerful: it has strategy extraction by\ndesign and allows short proofs for formulas which are hard for classical QBF\nresolution systems.\n  Here we show the first exponential lower bounds for MRes, thereby uncovering\nlimitations of MRes. Technically, the results are either transferred from\nbounds from circuit complexity (for restricted versions of MRes) or directly\nobtained by combinatorial arguments (for full MRes). Our results imply that the\nMRes approach is largely orthogonal to other QBF resolution models such as the\nQCDCL resolution systems QRes and QURes and the expansion systems\n$\\forall$Exp+Res and IR.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 10:39:14 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Beyersdorff", "Olaf", ""], ["Blinkhorn", "Joshua", ""], ["Mahajan", "Meena", ""], ["Peitl", "Tom\u00e1\u0161", ""], ["Sood", "Gaurav", ""]]}, {"id": "2012.07833", "submitter": "Edward Haeusler", "authors": "Edward Hermann Haeusler", "title": "Going from the huge to the small: Efficient succinct representation of\n  proofs in Minimal implicational logic", "comments": "Companion to arXiv:2009.09802v1. This versions explains better\n  Lemma14. This Lemma now is Lemma16. Two new lemmas were introduced in the new\n  version to help writing the better explanation on former Lemma14. Some reader\n  asked a better explanation. I am grateful to prof. Lew Gordeev to have raised\n  questions that help me to deliver this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A previous article shows that any linear height bounded normal proof of a\ntautology in the Natural Deduction for Minimal implicational logic\n$M_{\\supset}$ is as huge as it is redundant. More precisely, any proof in a\nfamily of super-polynomially sized and linearly height bounded proofs have a\nsub-derivation that occurs super-polynomially many times in it. In this\narticle, we show that by collapsing all the repeated sub-derivations we obtain\na smaller structure, a rooted Directed Acyclic Graph (r-DAG), that is\npolynomially upper-bounded on the size of $\\alpha$ and it is a certificate that\n$\\alpha$ is a tautology that can be verified in polynomial time. In other\nwords, for every huge proof of a tautology in $M_{\\supset}$, we obtain a\nsuccinct certificate for its validity. Moreover, we show an algorithm able to\ncheck this validity in polynomial time on the certificate's size. Comments on\nhow the results in this article are related to a proof of the conjecture\n$NP=CoNP$ appears in conclusion.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 21:13:58 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 15:19:51 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Haeusler", "Edward Hermann", ""]]}, {"id": "2012.07983", "submitter": "Zhiwei Zhang", "authors": "Anastasios Kyrillidis, Moshe Y. Vardi, Zhiwei Zhang", "title": "On Continuous Local BDD-Based Search for Hybrid SAT Solving", "comments": "AAAI 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG cs.LO math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the potential of continuous local search (CLS) in SAT solving by\nproposing a novel approach for finding a solution of a hybrid system of Boolean\nconstraints. The algorithm is based on CLS combined with belief propagation on\nbinary decision diagrams (BDDs). Our framework accepts all Boolean constraints\nthat admit compact BDDs, including symmetric Boolean constraints and\nsmall-coefficient pseudo-Boolean constraints as interesting families. We\npropose a novel algorithm for efficiently computing the gradient needed by CLS.\nWe study the capabilities and limitations of our versatile CLS solver, GradSAT,\nby applying it on many benchmark instances. The experimental results indicate\nthat GradSAT can be a useful addition to the portfolio of existing SAT and\nMaxSAT solvers for solving Boolean satisfiability and optimization problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:36:20 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 22:38:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Vardi", "Moshe Y.", ""], ["Zhang", "Zhiwei", ""]]}, {"id": "2012.08268", "submitter": "Sean Tull", "authors": "Sean Tull", "title": "Monoidal Categories for Formal Concept Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate monoidal categories of formal contexts, in which states\ncorrespond to formal concepts. In particular we examine the category of bonds\nor Chu correspondences between contexts, which is known to be equivalent to the\n*-autonomous category of complete sup-lattices. We show that a second monoidal\nstructure exists on both categories, corresponding to the direct product of\nformal contexts defined by Ganter and Wille, and discuss the use of these\ncategories as compositional models of meaning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:06:57 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Tull", "Sean", ""]]}, {"id": "2012.08370", "submitter": "Peter Dybjer", "authors": "Marc Bezem, Thierry Coquand, Peter Dybjer, Mart\\'in Escard\\'o", "title": "A Note on Generalized Algebraic Theories and Categories with Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new syntax independent definition of the notion of a generalized\nalgebraic theory as an initial object in a category of categories with families\n(cwfs) with extra structure. To this end we define inductively how to build a\nvalid signature $\\Sigma$ for a generalized algebraic theory and the associated\ncategory of cwfs with a $\\Sigma$-structure and cwf-morphisms that preserve this\nstructure on the nose. Our definition refers to uniform families of contexts,\ntypes, and terms, a purely semantic notion. Furthermore, we show how to\nsyntactically construct initial cwfs with $\\Sigma$-structures. This result can\nbe viewed as a generalization of Birkhoff's completeness theorem for equational\nlogic. It is obtained by extending Castellan, Clairambault, and Dybjer's\nconstruction of an initial cwf. We provide examples of generalized algebraic\ntheories for monoids, categories, categories with families, and categories with\nfamilies with extra structure for some type formers of dependent type theory.\nThe models of these are internal monoids, internal categories, and internal\ncategories with families (with extra structure) in a category with families.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 15:35:27 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:53:16 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Bezem", "Marc", ""], ["Coquand", "Thierry", ""], ["Dybjer", "Peter", ""], ["Escard\u00f3", "Mart\u00edn", ""]]}, {"id": "2012.08626", "submitter": "Roberto Casadei PhD", "authors": "Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Mirko Viroli", "title": "Computation Against a Neighbour", "comments": "50 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works in contexts like the Internet of Things (IoT) and large-scale\nCyber-Physical Systems (CPS) propose the idea of programming distributed\nsystems by focussing on their global behaviour across space and time. In this\nview, a potentially vast and heterogeneous set of devices is considered as an\n\"aggregate\" to be programmed as a whole, while abstracting away the details of\nindividual behaviour and exchange of messages, which are expressed\ndeclaratively. One such a paradigm, known as aggregate programming, builds on\ncomputational models inspired by field-based coordination. Existing models such\nas the field calculus capture interaction with neighbours by a so-called\n\"neighbouring field\" (a map from neighbours to values). This requires ad-hoc\nmechanisms to smoothly compose with standard values, thus complicating\nprogramming and introducing clutter in aggregate programs, libraries and\ndomain-specific languages (DSLs). To address this key issue we introduce the\nnovel notion of \"computation against a neighbour\", whereby the evaluation of\ncertain subexpressions of the aggregate program are affected by recent\ncorresponding evaluations in neighbours. We capture this notion in the\nneighbours calculus (NC), a new field calculus variant which is shown to\nsmoothly support declarative specification of interaction with neighbours, and\ncorrespondingly facilitate the embedding of field computations as internal DSLs\nin common general-purpose programming languages -- as exemplified by a Scala\nimplementation, called ScaFi. This paper formalises NC, thoroughly compares it\nwith respect to the classic field calculus, and shows its expressiveness by\nmeans of a case study in edge computing, developed in ScaFi.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 21:33:55 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Audrito", "Giorgio", ""], ["Casadei", "Roberto", ""], ["Damiani", "Ferruccio", ""], ["Viroli", "Mirko", ""]]}, {"id": "2012.08990", "submitter": "Jannis Limperg", "authors": "Jannis Limperg", "title": "A Novice-Friendly Induction Tactic for Lean", "comments": "13 pages. To be published in Proceedings of the 10th ACM SIGPLAN\n  International Conference on Certified Programs and Proofs (CPP '21).\n  Supplement available at https://doi.org/10.5281/zenodo.4327209", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theorem provers based on dependent type theory such as Coq and Lean,\ninduction is a fundamental proof method and induction tactics are omnipresent\nin proof scripts. Yet the ergonomics of existing induction tactics are not\nideal: they do not reliably support inductive predicates and relations; they\nsometimes generate overly specific or unnecessarily complex induction\nhypotheses; and they occasionally choose confusing names for the hypotheses\nthey introduce.\n  This paper describes a new induction tactic, implemented in Lean 3, which\naddresses these issues. The tactic is particularly suitable for educational\nuse, but experts should also find it more convenient than existing induction\ntactics. In addition, the tactic serves as a moderately complex case study for\nthe metaprogramming framework of Lean 3. The paper describes some difficulties\nencountered during the implementation and suggests improvements to the\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:36:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Limperg", "Jannis", ""]]}, {"id": "2012.08994", "submitter": "Alejandro D\\'iaz-Caro", "authors": "Alejandro D\\'iaz-Caro and Gilles Dowek", "title": "A New Connective in Natural Deduction, and its Application to Quantum\n  Computing", "comments": "Long version. 23 pages plus appendix. To appear at ICTAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an unsuspected connection between non-harmonious logical\nconnectives, such as Prior's tonk, and quantum computing. We argue that\nnon-harmonious connectives model the information erasure, the\nnon-reversibility, and the non-determinism that occur, among other places, in\nquantum measurement. We introduce a propositional logic with a non-harmonious\nconnective sup and show that its proof language forms the core of a quantum\nprogramming language.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:40:07 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:12:19 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 13:37:41 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["D\u00edaz-Caro", "Alejandro", ""], ["Dowek", "Gilles", ""]]}, {"id": "2012.09138", "submitter": "Danil Annenkov", "authors": "Danil Annenkov, Mikkel Milo, Jakob Botsch Nielsen, Bas Spitters", "title": "Extracting Smart Contracts Tested and Verified in Coq", "comments": "Coq implementation:\n  https://github.com/AU-COBRA/ConCert/tree/artifact-2020 Update: fixed the\n  \"author running\" list, fixed a mistake in an evaluation rule for lambda-box\n  (Section 3.1, item 2)", "journal-ref": "CPP'2021: Proceedings of the 10th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, January 18--19, 2021, Virtual,\n  Denmark", "doi": "10.1145/3437992.3439934", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement extraction of Coq programs to functional languages based on\nMetaCoq's certified erasure. As part of this, we implement an optimisation pass\nremoving unused arguments. We prove the pass correct wrt. a conventional\ncall-by-value operational semantics of functional languages. We apply this to\ntwo functional smart contract languages, Liquidity and Midlang, and to the\nfunctional language Elm. Our development is done in the context of the ConCert\nframework that enables smart contract verification. We contribute a verified\nboardroom voting smart contract featuring maximum voter privacy such that each\nvote is kept private except under collusion of all other parties. We also\nintegrate property-based testing into ConCert using QuickChick and our\ndevelopment is the first to support testing properties of interacting smart\ncontracts. We test several complex contracts such as a DAO-like contract, an\nescrow contract, an implementation of a Decentralized Finance (DeFi) contract\nwhich includes a custom token standard (Tezos FA2), and more. In total, this\ngives us a way to write dependent programs in Coq, test them\nsemi-automatically, verify, and then extract to functional smart contract\nlanguages, while retaining a small trusted computing base of only MetaCoq and\nthe pretty-printers into these languages.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:23:58 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 14:07:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Annenkov", "Danil", ""], ["Milo", "Mikkel", ""], ["Nielsen", "Jakob Botsch", ""], ["Spitters", "Bas", ""]]}, {"id": "2012.09274", "submitter": "Stylianos Loukas Vasileiou", "authors": "Stylianos Loukas Vasileiou, Alessandro Previti, William Yeoh", "title": "On Exploiting Hitting Sets for Model Reconciliation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human-aware planning, a planning agent may need to provide an explanation\nto a human user on why its plan is optimal. A popular approach to do this is\ncalled model reconciliation, where the agent tries to reconcile the differences\nin its model and the human's model such that the plan is also optimal in the\nhuman's model. In this paper, we present a logic-based framework for model\nreconciliation that extends beyond the realm of planning. More specifically,\ngiven a knowledge base $KB_1$ entailing a formula $\\varphi$ and a second\nknowledge base $KB_2$ not entailing it, model reconciliation seeks an\nexplanation, in the form of a cardinality-minimal subset of $KB_1$, whose\nintegration into $KB_2$ makes the entailment possible. Our approach, based on\nideas originating in the context of analysis of inconsistencies, exploits the\nexisting hitting set duality between minimal correction sets (MCSes) and\nminimal unsatisfiable sets (MUSes) in order to identify an appropriate\nexplanation. However, differently from those works targeting inconsistent\nformulas, which assume a single knowledge base, MCSes and MUSes are computed\nover two distinct knowledge bases. We conclude our paper with an empirical\nevaluation of the newly introduced approach on planning instances, where we\nshow how it outperforms an existing state-of-the-art solver, and generic\nnon-planning instances from recent SAT competitions, for which no other solver\nexists.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 21:25:53 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 23:19:36 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Vasileiou", "Stylianos Loukas", ""], ["Previti", "Alessandro", ""], ["Yeoh", "William", ""]]}, {"id": "2012.09313", "submitter": "Michael Warren", "authors": "Chris R. Serrano and Pape M. Sylla and Michael A. Warren", "title": "Generate and Verify: Semantically Meaningful Formal Analysis of Neural\n  Network Perception Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing remains the primary method to evaluate the accuracy of neural network\nperception systems. Prior work on the formal verification of neural network\nperception models has been limited to notions of local adversarial robustness\nfor classification with respect to individual image inputs. In this work, we\npropose a notion of global correctness for neural network perception models\nperforming regression with respect to a generative neural network with a\nsemantically meaningful latent space. That is, against an infinite set of\nimages produced by a generative model over an interval of its latent space, we\nemploy neural network verification to prove that the model will always produce\nestimates within some error bound of the ground truth. Where the perception\nmodel fails, we obtain semantically meaningful counter-examples which carry\ninformation on concrete states of the system of interest that can be used\nprogrammatically without human inspection of corresponding generated images.\nOur approach, Generate and Verify, provides a new technique to gather insight\ninto the failure cases of neural network perception systems and provide\nmeaningful guarantees of correct behavior in safety critical applications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 23:09:53 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Serrano", "Chris R.", ""], ["Sylla", "Pape M.", ""], ["Warren", "Michael A.", ""]]}, {"id": "2012.09388", "submitter": "Jiatu Li", "authors": "Jiatu Li", "title": "Formalization of PAL$\\cdot$S5 in Proof Assistant", "comments": "For proof codes, see https://github.com/ljt12138/Formalization-PAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As an experiment to the application of proof assistant for logic research, we\nformalize the model and proof system for multi-agent modal logic S5 with\nPAL-style dynamic modality in Lean theorem prover. We provide a formal proof\nfor the reduction axiom of public announcement, and the soundness and\ncompleteness of modal logic S5, which can be typechecked with Lean 3.19.0. The\ncomplete proof is now available at Github.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:19:06 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Li", "Jiatu", ""]]}, {"id": "2012.09539", "submitter": "Bettina K\\\"onighofer", "authors": "Bettina K\\\"onighofer, Julian Rudolf, Alexander Palmisano, Martin\n  Tappler, Roderick Bloem", "title": "Online Shielding for Stochastic Systems", "comments": "18 Pages, 6 Figures, under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to develop trustworthy reinforcement\nlearning systems. To ensure safety especially during exploration, we\nautomatically synthesize a correct-by-construction runtime enforcer, called a\nshield, that blocks all actions that are unsafe with respect to a temporal\nlogic specification from the agent. Our main contribution is a new synthesis\nalgorithm for computing the shield online. Existing offline shielding\napproaches compute exhaustively the safety of all states-action combinations\nahead-of-time, resulting in huge offline computation times, large memory\nconsumption, and significant delays at run-time due to the look-ups in a huge\ndatabase. The intuition behind online shielding is to compute during run-time\nthe set of all states that could be reached in the near future. For each of\nthese states, the safety of all available actions is analysed and used for\nshielding as soon as one of the considered states is reached. Our proposed\nmethod is general and can be applied to a wide range of planning problems with\nstochastic behavior. For our evaluation, we selected a 2-player version of the\nclassical computer game SNAKE. The game requires fast decisions and the\nmultiplayer setting induces a large state space, computationally expensive to\nanalyze exhaustively. The safety objective of collision avoidance is easily\ntransferable to a variety of planning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 12:25:48 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["K\u00f6nighofer", "Bettina", ""], ["Rudolf", "Julian", ""], ["Palmisano", "Alexander", ""], ["Tappler", "Martin", ""], ["Bloem", "Roderick", ""]]}, {"id": "2012.09919", "submitter": "Marc Schoolderman", "authors": "Marc Schoolderman, Jonathan Moerman, Sjaak Smetsers, Marko van Eekelen", "title": "Efficient Verification of Optimized Code: Correct High-speed X25519", "comments": "19 pages, 5 figures. accepted at NFM 2021 (without appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code that is highly optimized poses a problem for program-level verification:\nprogrammers can employ various clever tricks that are non-trivial to reason\nabout. For cryptography on low-power devices, it is nonetheless crucial that\nimplementations be functionally correct, secure, and efficient. These are\nusually crafted in hand-optimized machine code that eschew conventional control\nflow as much as possible.\n  We have formally verified such code: a library which implements elliptic\ncurve cryptography on 8-bit AVR microcontrollers. The chosen implementation is\nthe most efficient currently known for this microarchitecture. It consists of\nover 3000 lines of assembly instructions. Building on earlier work, we use the\nWhy3 platform to model the code and prove verification conditions, using\nautomated provers. We expect the approach to be re-usable and adaptable, and it\nallows for validation. Furthermore, an error in the original implementation was\nfound and corrected, at the same time reducing its memory footprint. This shows\nthat practical verification of cutting-edge code is not only possible, but can\nin fact add to its efficiency -- and is clearly necessary.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:25:58 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 11:07:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schoolderman", "Marc", ""], ["Moerman", "Jonathan", ""], ["Smetsers", "Sjaak", ""], ["van Eekelen", "Marko", ""]]}, {"id": "2012.09975", "submitter": "Luca Reggio", "authors": "Mai Gehrke, Tom\\'a\\v{s} Jakl, Luca Reggio", "title": "A duality theoretic view on limits of finite structures: Extended\n  version", "comments": "38 pages. Extended version of paper arXiv:1907.04036 which appeared\n  in FoSSaCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A systematic theory of structural limits for finite models has been developed\nby Nesetril and Ossona de Mendez. It is based on the insight that the\ncollection of finite structures can be embedded, via a map they call the Stone\npairing, in a space of measures, where the desired limits can be computed. We\nshow that a closely related but finer grained space of measures arises -- via\nStone-Priestley duality and the notion of types from model theory -- by\nenriching the expressive power of first-order logic with certain \"probabilistic\noperators\". We provide a sound and complete calculus for this extended logic\nand expose the functorial nature of this construction.\n  The consequences are two-fold. On the one hand, we identify the logical gist\nof the theory of structural limits. On the other hand, our construction shows\nthat the duality theoretic variant of the Stone pairing captures the adding of\na layer of quantifiers, thus making a strong link to recent work on semiring\nquantifiers in logic on words. In the process, we identify the model theoretic\nnotion of types as the unifying concept behind this link. These results\ncontribute to bridging the strands of logic in computer science which focus on\nsemantics and on more algorithmic and complexity related areas, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 23:26:54 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Gehrke", "Mai", ""], ["Jakl", "Tom\u00e1\u0161", ""], ["Reggio", "Luca", ""]]}, {"id": "2012.10191", "submitter": "Paolo Liberatore", "authors": "Paolo Liberatore", "title": "Reconstructing a single-head formula to facilitate logical forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logical forgetting may take exponential time in general, but it does not when\nits input is a single-head propositional definite Horn formula. Single-head\nmeans that no variable is the head of multiple clauses. An algorithm to make a\nformula single-head if possible is shown. It improves over a previous one by\nbeing complete: it always finds a single-head formula equivalent to the given\none if any.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:25:49 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Liberatore", "Paolo", ""]]}, {"id": "2012.10511", "submitter": "Adam Petz", "authors": "Adam Petz and Perry Alexander", "title": "An Infrastructure for Faithful Execution of Remote Attestation Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote attestation is an emerging technology for establishing trust in a\nremote computing system. Copland is a domain-specific language for specifying\nlayered attestation protocols, characterizing attestation-relevant system\nevents, and describing evidence bundling. In this work we formally define and\nverify a Copland Compiler and Copland Virtual Machine for executing Copland\nprotocols. The compiler translates Copland into instructions that are executed\non the virtual machine. The compiler and virtual machine are implemented as\nmonadic, functional programs in the Coq proof assistant and verified with\nrespect to the Copland event and evidence semantics. In addition we introduce\nthe Attestation Manager Monad as an environment for managing Copland term\nexecution providing support for managing nonces, binding results of Copland\nprotocols to variables, and appraising evidence results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 20:57:34 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Petz", "Adam", ""], ["Alexander", "Perry", ""]]}, {"id": "2012.10783", "submitter": "Daniel Gratzer", "authors": "Daniel Gratzer, Jonathan Sterling", "title": "Syntactic categories for dependent type theory: sketching and adequacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We argue that locally Cartesian closed categories form a suitable doctrine\nfor defining dependent type theories, including non-extensional ones. Using the\ntheory of sketches, one may define syntactic categories for type theories in a\nstyle that resembles the use of Martin-L\\\"of's Logical Framework, following the\n\"judgments as types\" principle. The concentration of type theories into their\nlocally Cartesian closed categories of judgments is particularly convenient for\nproving syntactic metatheorems by semantic means (canonicity, normalization,\netc.). Perhaps surprisingly, the notion of a context plays no role in the\ndefinitions of type theories in this sense, but the structure of a class of\ndisplay maps can be imposed on a theory post facto wherever needed, as\nadvocated by the Edinburgh school and realized by the %worlds declarations of\nthe Twelf proof assistant.\n  Uemura has proposed representable map categories together with a stratified\nlogical framework for similar purposes. The stratification in Uemura's\nframework restricts the use of dependent products to be strictly positive, in\ncontrast to the tradition of Martin-L\\\"of's logical framework and\nSchroeder-Heister's analysis of higher-level deductions. We prove a semantic\nadequacy result for locally Cartesian closed categories relative to Uemura's\nrepresentable map categories: if a theory is definable in the framework of\nUemura, the locally Cartesian closed category that it generates is a\nconservative (fully faithful) extension of its syntactic representable map\ncategory. On this basis, we argue for the use of locally Cartesian closed\ncategories as a simpler alternative to Uemura's representable map categories.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 21:23:29 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 21:20:52 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Gratzer", "Daniel", ""], ["Sterling", "Jonathan", ""]]}, {"id": "2012.10869", "submitter": "Clemens Grabmayer", "authors": "Clemens Grabmayer", "title": "Structure-Constrained Process Graphs for the Process Semantics of\n  Regular Expressions", "comments": "Extended-report version of article in EPTCS post-proceedings of the\n  workshop TERMGRAPH 2020. 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Milner (1984) introduced a process semantics for regular expressions as\nprocess graphs. Unlike for the language semantics, where every regular (that\nis, DFA-accepted) language is the interpretation of some regular expression,\nthere are finite process graphs that are not bisimilar to the process\ninterpretation of any regular expression. For reasoning about graphs that are\nexpressible by regular expressions modulo bisimilarity it is desirable to have\nstructural representations of process graphs in the image of the\ninterpretation.\n  For `1-free' regular expressions, their process interpretations satisfy the\nstructural property LEE (loop existence and elimination). But this is not in\ngeneral the case for all regular expressions, as we show by examples. Yet as a\nremedy, we describe the possibility to recover the property LEE for a close\nvariant of the process interpretation. For this purpose we refine the process\nsemantics of regular expressions to yield process graphs with 1-transitions,\nsimilar to silent moves for finite-state automata.\n  This report accompanies the paper with the same title in the post-proceedings\nof the workshop TERMGRAPH 2020. Here we give the proofs of not only one but of\nboth of the two central theorems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 08:50:27 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Grabmayer", "Clemens", ""]]}, {"id": "2012.10940", "submitter": "Angelos Charalambidis", "authors": "Angelos Charalambidis, Giorgos Papadimitriou, Panos Rondogiannis,\n  Antonis Troumpoukis", "title": "Lexicographic Logic: a Many-valued Logic for Preference Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical formalisms provide a natural and concise means for specifying and\nreasoning about preferences. In this paper, we propose lexicographic logic, an\nextension of classical propositional logic that can express a variety of\npreferences, most notably lexicographic ones. The proposed logic supports a\nsimple new connective whose semantics can be defined in terms of finite lists\nof truth values. We demonstrate that, despite the well-known theoretical\nlimitations that pose barriers to the quantitative representation of\nlexicographic preferences, there exists a subset of the rational numbers over\nwhich the proposed new connective can be naturally defined. Lexicographic logic\ncan be used to define in a simple way some well-known preferential operators,\nlike \"$A$ and if possible $B$\", and \"$A$ or failing that $B$\". Moreover, many\nother hierarchical preferential operators can be defined using a systematic\napproach. We argue that the new logic is an effective formalism for ranking\nquery results according to the satisfaction level of user preferences.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:42:04 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Charalambidis", "Angelos", ""], ["Papadimitriou", "Giorgos", ""], ["Rondogiannis", "Panos", ""], ["Troumpoukis", "Antonis", ""]]}, {"id": "2012.10987", "submitter": "Wayne Witzel", "authors": "Wayne M. Witzel, Warren D. Craft, Robert D. Carr and Joaqu\\'in E.\n  Madrid Larra\\~naga", "title": "Prove-It: A Proof Assistant for Organizing and Verifying General\n  Mathematical Knowledge", "comments": "Updated the links to our pyproveit.org website. 38 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Prove-It, a Python-based general-purpose interactive\ntheorem-proving assistant designed with the goal of making formal theorem\nproving as easy and natural as informal theorem proving (with moderate\ntraining). Prove-It uses a highly-flexible Jupyter notebook-based user\ninterface that documents interactions and proof steps using LaTeX. We review\nProve-It's highly expressive representation of expressions, judgments,\ntheorems, and proofs; demonstrate the system by constructing a traditional\nproof-by-contradiction that $\\sqrt{2}\\notin\\mathbb{Q}$; and discuss how the\nsystem avoids inconsistencies such as Russell's and Curry's paradoxes.\nExtensive documentation is provided in the appendices about core elements of\nthe system. Current development and future work includes promising applications\nto quantum circuit manipulation and quantum algorithm verification.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 18:15:12 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 16:38:05 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Witzel", "Wayne M.", ""], ["Craft", "Warren D.", ""], ["Carr", "Robert D.", ""], ["Larra\u00f1aga", "Joaqu\u00edn E. Madrid", ""]]}, {"id": "2012.11067", "submitter": "Alexey Ignatiev", "authors": "Alexey Ignatiev, Nina Narodytska, Nicholas Asher, Joao Marques-Silva", "title": "On Relating 'Why?' and 'Why Not?' Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations of Machine Learning (ML) models often address a 'Why?' question.\nSuch explanations can be related with selecting feature-value pairs which are\nsufficient for the prediction. Recent work has investigated explanations that\naddress a 'Why Not?' question, i.e. finding a change of feature values that\nguarantee a change of prediction. Given their goals, these two forms of\nexplaining predictions of ML models appear to be mostly unrelated. However,\nthis paper demonstrates otherwise, and establishes a rigorous formal\nrelationship between 'Why?' and 'Why Not?' explanations. Concretely, the paper\nproves that, for any given instance, 'Why?' explanations are minimal hitting\nsets of 'Why Not?' explanations and vice-versa. Furthermore, the paper devises\nnovel algorithms for extracting and enumerating both forms of explanations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 01:07:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ignatiev", "Alexey", ""], ["Narodytska", "Nina", ""], ["Asher", "Nicholas", ""], ["Marques-Silva", "Joao", ""]]}, {"id": "2012.11220", "submitter": "Lucas Carvalho Cordeiro", "authors": "Luiz Sena, Erickson Alves, Iury Bessa, Eddie Filho, and Lucas Cordeiro", "title": "Incremental Verification of Fixed-Point Implementations of Neural\n  Networks", "comments": "arXiv admin note: text overlap with arXiv:1907.12933", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementations of artificial neural networks (ANNs) might lead to failures,\nwhich are hardly predicted in the design phase since ANNs are highly parallel\nand their parameters are barely interpretable. Here, we develop and evaluate a\nnovel symbolic verification framework using incremental bounded model checking\n(BMC), satisfiability modulo theories (SMT), and invariant inference, to obtain\nadversarial cases and validate coverage methods in a multi-layer perceptron\n(MLP). We exploit incremental BMC based on interval analysis to compute\nboundaries from a neuron's input. Then, the latter are propagated to\neffectively find a neuron's output since it is the input of the next one. This\npaper describes the first bit-precise symbolic verification framework to reason\nover actual implementations of ANNs in CUDA, based on invariant inference,\ntherefore providing further guarantees about finite-precision arithmetic and\nits rounding errors, which are routinely ignored in the existing literature. We\nhave implemented the proposed approach on top of the efficient SMT-based\nbounded model checker (ESBMC), and its experimental results show that it can\nsuccessfully verify safety properties, in actual implementations of ANNs, and\ngenerate real adversarial cases in MLPs. Our approach was able to verify and\nproduce adversarial examples for 85.8% of 21 test cases considering different\ninput images, and 100% of the properties related to covering methods. Although\nour verification time is higher than existing approaches, our methodology can\nconsider fixed-point implementation aspects that are disregarded by the\nstate-of-the-art verification methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:03:44 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sena", "Luiz", ""], ["Alves", "Erickson", ""], ["Bessa", "Iury", ""], ["Filho", "Eddie", ""], ["Cordeiro", "Lucas", ""]]}, {"id": "2012.11223", "submitter": "Lucas Carvalho Cordeiro", "authors": "Kaled M. Alshmrany, Rafael S. Menezes, Mikhail R. Gadelha, and Lucas\n  C. Cordeiro", "title": "FuSeBMC: A White-Box Fuzzer for Finding Security Vulnerabilities in C\n  Programs", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and evaluate a novel white-box fuzzer for C programs named\nFuSeBMC, which combines fuzzing and symbolic execution, and applies Bounded\nModel Checking (BMC) to find security vulnerabilities in C programs. FuSeBMC\nexplores and analyzes C programs (1) to find execution paths that lead to\nproperty violations and (2) to incrementally inject labels to guide the fuzzer\nand the BMC engine to produce test-cases for code coverage. FuSeBMC\nsuccessfully participates in Test-Comp'21 and achieves first place in the\nCover-Error category and second place in the Overall category.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:07:20 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Alshmrany", "Kaled M.", ""], ["Menezes", "Rafael S.", ""], ["Gadelha", "Mikhail R.", ""], ["Cordeiro", "Lucas C.", ""]]}, {"id": "2012.11245", "submitter": "Mohannad Aldughaim", "authors": "Mohannad Aldughaim, Kaled Alshmrany, Mohamed Mustafa, Lucas Cordeiro\n  and Alexandru Stancu", "title": "Bounded Model Checking of Software Using Interval Methods via\n  Contractors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded model checking (BMC) is a vital technique to find property violations\nin programs. BMC can quickly find an execution path starting from an initial\nstate to the bad state that refutes a given property. However, BMC techniques\nstruggle to falsify programs that contain loops. BMC needs to unfold the\nprogram loops up to the bound k, which sometimes leads to a considerable\nstate-space to be explored. Here, we develop an innovative software\nverification approach that exploits interval methods via contractors to prune\nthe state-space exploration of programs that contain loops. In particular, this\nis the first work that exploits interval methods via contractors to analyze the\nloop variables search-space and identify where the property is guaranteed to\nhold and prune the domain where it holds. Experimental results show a\nperformance boost in terms of space and time as contractors removed 99% of the\nsearch-space in some examples and made them substantially faster to verify with\nBMC.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:49:34 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 11:20:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Aldughaim", "Mohannad", ""], ["Alshmrany", "Kaled", ""], ["Mustafa", "Mohamed", ""], ["Cordeiro", "Lucas", ""], ["Stancu", "Alexandru", ""]]}, {"id": "2012.12133", "submitter": "Igor Sedl\\'ar", "authors": "Igor Sedl\\'ar", "title": "Finitely-valued propositional dynamic logic", "comments": null, "journal-ref": "In: Nicola Olivetti, Rineke Verbrugge, Sara Negri, Gabriel Sandu\n  (Eds.), Advances in Modal Logic, Volume 13, pp. 561-579. College\n  Publications, 2020", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a many-valued generalization of Propositional Dynamic Logic where\nformulas in states and accessibility relations between states of a Kripke model\nare evaluated in a finite FL-algebra. One natural interpretation of this\nframework is related to reasoning about costs of performing structured actions.\nWe prove that PDL over any finite FL-algebra is decidable. We also establish a\ngeneral completeness result for a class of PDLs based on commutative integral\nFL-algebras with canonical constants.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:13:24 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Sedl\u00e1r", "Igor", ""]]}, {"id": "2012.12398", "submitter": "Valentin Goranko", "authors": "Valentin Goranko", "title": "Model checking and model synthesisfrom partial models: a logic-based\n  perspective", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  I consider the following generic scenario: an abstract model M of some 'real'\nsystem is only partially presented, or partially known to us, and we have to\nensure that the actual system satisfies a given specification, formalised in\nsome logical language. This scenario has at least two essentially different\ninterpretations, leading to two, essentially different, formal logical and\nalgorithmic problems: \"Model Synthesis from Partial Models\", where some\n'admissible' extension of M to a full model must satisfy the specification, and\n\"Model Checking of Partial Models\", where all 'admissible' extensions of M to a\nfull model must satisfy the specification. These problems naturally extend the\nclassical logical decision problems of Satisfiability, Validity, and Model\nChecking.\n  Here I briefly discuss both problems in the contexts of classical, modal and\ntemporal logics. I make some observations, state some open questions, and\noutline a general tableaux-style procedure that solves the problem of\nunconstrained model synthesis from finite partial models for several well-known\nmodal and temporal logics, incl. K, LTL, CTL, ATL.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:50:12 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Goranko", "Valentin", ""]]}, {"id": "2012.12716", "submitter": "F.J Wang", "authors": "Fujun Wang, Zining Cao, Lixing Tan, Zhen Li", "title": "Formal modeling and performance evaluation for hybrid systems:a\n  probabilistic hybrid process algebra-based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic behavior is omnipresent in computer controlled systems, in\nparticular, so-called safety-critical hybrid systems, because of various\nreasons, like uncertain environments, or fundamental properties of nature. In\nthis paper, we extend existing hybrid process algebra $ACP_{hs}^{srt}$ with\nprobability without replacing nondeterministic choice operator. In view of some\nshortcomings in existing approximate probabilistic bisimulation, we relax the\nconstrains and propose a novel approximate probabilistic bisimulation relation.\nAfter that, we present a performance evaluation language, CTRML, to reason over\nprobabilistic systems, which extend the results to real number. Along with the\nspecification language, we present a set of algorithms for the evaluation of\nthe language. Additionally, we transfer the hybrid process algebra to\nprobabilistic transition system and show experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:43:02 GMT"}], "update_date": "2021-01-04", "authors_parsed": [["Wang", "Fujun", ""], ["Cao", "Zining", ""], ["Tan", "Lixing", ""], ["Li", "Zhen", ""]]}, {"id": "2012.12830", "submitter": "Jonni Virtema", "authors": "Miika Hannula and Jonni Virtema", "title": "Tractability frontiers in probabilistic team semantics and existential\n  second-order logic over the reals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic team semantics is a framework for logical analysis of\nprobabilistic dependencies. Our focus is on the complexity and expressivity of\nprobabilistic inclusion logic and its extensions. We identify a natural\nfragment of existential second-order logic with additive real arithmetic that\ncaptures exactly the expressivity of probabilistic inclusion logic. We\nfurthermore relate these formalisms to linear programming, and doing so obtain\nPTIME data complexity for the logics. Moreover, on finite structures, we show\nthat the full existential second-order logic with additive real arithmetic can\nonly express NP properties.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:47:59 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 13:45:18 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Hannula", "Miika", ""], ["Virtema", "Jonni", ""]]}, {"id": "2012.12982", "submitter": "Gaia Belardinelli", "authors": "Gaia Belardinelli, Rasmus K. Rendsvig", "title": "Awareness Logic: A Kripke-based Rendition of the Heifetz-Meier-Schipper\n  Model", "comments": "18 pages, 2 figures, proceedings of DaLi conference 2020", "journal-ref": "Martins M.A., Sedl\\'ar I. (eds) Dynamic Logic. New Trends and\n  Applications. DaLi 2020. Lecture Notes in Computer Science, vol 12569, pp\n  33-50. Springer, Cham", "doi": "10.1007/978-3-030-65840-3_3", "report-no": null, "categories": "cs.AI cs.LO cs.MA econ.TH math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heifetz, Meier and Schipper (HMS) present a lattice model of awareness. The\nHMS model is syntax-free, which precludes the simple option to rely on formal\nlanguage to induce lattices, and represents uncertainty and unawareness with\none entangled construct, making it difficult to assess the properties of\neither. Here, we present a model based on a lattice of Kripke models, induced\nby atom subset inclusion, in which uncertainty and unawareness are separate. We\nshow the models to be equivalent by defining transformations between them which\npreserve formula satisfaction, and obtain completeness through our and HMS'\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 21:24:06 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Belardinelli", "Gaia", ""], ["Rendsvig", "Rasmus K.", ""]]}, {"id": "2012.13129", "submitter": "Ankush Das", "authors": "Ankush Das and Frank Pfenning", "title": "Rast: A Language for Resource-Aware Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional session types prescribe bidirectional communication protocols for\nconcurrent computations, where well-typed programs are guaranteed to adhere to\nthe protocols. However, simple session types cannot capture properties beyond\nthe basic type of the exchanged messages. In response, recent work has extended\nsession types with refinements from linear arithmetic, capturing intrinsic\nattributes of processes and data. These refinements then play a central role in\ndescribing sequential and parallel complexity bounds on session-typed programs.\nThe Rast language provides an open-source implementation of session-typed\nconcurrent programs extended with arithmetic refinements as well as ergometric\nand temporal types to capture work and span of program execution. To further\nsupport generic programming, Rast also enhances arithmetically refined session\ntypes with recently developed nested parametric polymorphism. Type checking\nrelies on Cooper's algorithm for quantifier elimination in Presburger\narithmetic with a few significant optimizations, and a heuristic extension to\nnonlinear constraints. Rast furthermore includes a reconstruction engine so\nthat most program constructs pertaining the layers of refinements and resources\nare inserted automatically. We provide a variety of examples to demonstrate the\nexpressivity of the language.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:30:00 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Das", "Ankush", ""], ["Pfenning", "Frank", ""]]}, {"id": "2012.13219", "submitter": "Mustafa Hashmi", "authors": "Ho-Pun Lam and Mustafa Hashmi and Akhil Kumar", "title": "Towards a Formal Framework for Partial Compliance of Business Processes", "comments": "15 page, 4 figures, 2 tables; Under consideration at AICOL 2020,\n  co-located with Jurix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary \"YES-NO\" notions of process compliance are not very helpful to\nmanagers for assessing the operational performance of their company because a\nlarge number of cases fall in the grey area of partial compliance. Hence, it is\nnecessary to have ways to quantify partial compliance in terms of metrics and\nbe able to classify actual cases by assigning a numeric value of compliance to\nthem. In this paper, we formulate an evaluation framework to quantify the level\nof compliance of business processes across different levels of abstraction\n(such as task,trace and process level) and across multiple dimensions of each\ntask (such as temporal, monetary, role-, data-, and quality-related) to provide\nmanagers more useful information about their operations and to help them\nimprove their decision making processes. Our approach can also add social value\nby making social services provided by local, state and federal governments more\nflexible and improving the lives of citizens.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 12:38:40 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lam", "Ho-Pun", ""], ["Hashmi", "Mustafa", ""], ["Kumar", "Akhil", ""]]}, {"id": "2012.13289", "submitter": "Mieke Massink", "authors": "Gina Belmonte and Giovanna Broccia and Vincenzo Ciancia and Diego\n  Latella and Mieke Massink", "title": "Using Spatial Logic and Model Checking for Nevus Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO eess.IV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatial and spatio-temporal model checking techniques have a wide range of\napplication domains, among which large scale distributed systems and signal and\nimage analysis. In the latter domain, automatic and semi-automatic contouring\nin Medical Imaging has shown to be a very promising and versatile application\nthat can greatly facilitate the work of professionals in this domain, while\nsupporting explainability, easy replicability and exchange of medical image\nanalysis methods. In recent work we have applied this model-checking technique\nto the (3D) contouring of tumours and related oedema in magnetic resonance\nimages of the brain. In the current work we address the contouring of (2D)\nimages of nevi. One of the challenges of treating nevi images is their\nconsiderable inhomogeneity in shape, colour, texture and size. To deal with\nthis challenge we use a texture similarity operator, in combination with\nspatial logic operators. We apply our technique on images of a large public\ndatabase and compare the results with associated ground truth segmentation\nprovided by domain experts.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 15:17:35 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Belmonte", "Gina", ""], ["Broccia", "Giovanna", ""], ["Ciancia", "Vincenzo", ""], ["Latella", "Diego", ""], ["Massink", "Mieke", ""]]}, {"id": "2012.13638", "submitter": "Marco Favorito", "authors": "Marco Favorito", "title": "A Standard Grammar for Temporal Logics on Finite Traces", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The heterogeneity of tools that support temporal logic formulae poses several\nchallenges in terms of interoperability. In particular, a standard syntax for\ntemporal logic on finite traces, despite similar to the one for infinite\ntraces, is currently missing. This document proposes a standard grammar for\nseveral temporal logic formalisms interpreted over finite traces, like Linear\nTemporal Logic (LTLf), Linear Dynamic Logic (LDLf), Pure-Past Linear Temporal\nLogic (PLTLf) and Pure-Past Linear Dynamic Logic (PLDLf).\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 22:36:43 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:07:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Favorito", "Marco", ""]]}, {"id": "2012.13806", "submitter": "Danilo Pianini", "authors": "Danilo Pianini, Roberto Casadei, Mirko Viroli, Stefano Mariani, Franco\n  Zambonelli", "title": "Time-Fluid Field-Based Coordination through Programmable Distributed\n  Schedulers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging application scenarios, such as cyber-physical systems (CPSs), the\nInternet of Things (IoT), and edge computing, call for coordination approaches\naddressing openness, self-adaptation, heterogeneity, and deployment\nagnosticism. Field-based coordination is one such approach, promoting the idea\nof programming system coordination declaratively from a global perspective, in\nterms of functional manipulation and evolution in \"space and time\" of\ndistributed data structures called fields. More specifically regarding time, in\nfield-based coordination (as in many other distributed approaches to\ncoordination) it is assumed that local activities in each device are regulated\nby a fair and unsynchronised fixed clock working at the platform level. In this\nwork, we challenge this assumption, and propose an alternative approach where\nscheduling is programmed in a natural way (along with usual field-based\ncoordination) in terms of causality fields, each enacting a programmable\ndistributed notion of a computation \"cause\" (why and when a field computation\nhas to be locally computed) and how it should change across time and space.\nStarting from low-level platform triggers, such causality fields can be\norganised into multiple layers, up to high-level, collectively-computed time\nabstractions, to be used at the application level. This reinterpretation of\ntime in terms of articulated causality relations allows us to express what we\ncall \"time-fluid\" coordination, where scheduling can be finely tuned so as to\nselect the triggers to react to, generally allowing to adaptively balance\nperformance (system reactivity) and cost (resource usage) of computations. We\nformalise the proposed scheduling framework for field-based coordination in the\ncontext of the field calculus, discuss an implementation in the aggregate\ncomputing framework, and finally evaluate the approach via simulation on\nseveral case studies.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 20:24:29 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 17:09:58 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 08:46:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Pianini", "Danilo", ""], ["Casadei", "Roberto", ""], ["Viroli", "Mirko", ""], ["Mariani", "Stefano", ""], ["Zambonelli", "Franco", ""]]}, {"id": "2012.13858", "submitter": "Jim de Groot", "authors": "Jim de Groot, Dirk Pattinson", "title": "Modal meet-implication logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We extend the meet-implication fragment of propositional intuitionistic logic\nwith a meet-preserving modality. We give semantics based on semilattices and a\nduality result with a suitable notion of descriptive frame. As a consequence we\nobtain completeness and identify a common (modal) fragment of a large class of\nmodal intuitionistic logics.\n  We recognise this logic as a dialgebraic logic, and as a consequence obtain\nexpressivity-somewhere-else. Within the dialgebraic framework, we then\ninvestigate the extension of the meet-implication fragment of propositional\nintuitionistic logic with a monotone modality and prove completeness and\nexpressivity-somewhere-else for it.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 03:18:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["de Groot", "Jim", ""], ["Pattinson", "Dirk", ""]]}, {"id": "2012.13925", "submitter": "Anthony Bordg", "authors": "Anthony Bordg, Hanna Lachnitt, Yijun He", "title": "Certified Quantum Computation in Isabelle/HOL", "comments": "J Autom Reasoning (2020)", "journal-ref": null, "doi": "10.1007/s10817-020-09584-7", "report-no": null, "categories": "cs.LO quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we present an ongoing effort to formalise quantum algorithms\nand results in quantum information theory using the proof assistant\nIsabelle/HOL. Formal methods being critical for the safety and security of\nalgorithms and protocols, we foresee their widespread use for quantum computing\nin the future. We have developed a large library for quantum computing in\nIsabelle based on a matrix representation for quantum circuits, successfully\nformalising the no-cloning theorem, quantum teleportation, Deutsch's algorithm,\nthe Deutsch-Jozsa algorithm and the quantum Prisoner's Dilemma. We discuss the\ndesign choices made and report on an outcome of our work in the field of\nquantum game theory.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:45:49 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bordg", "Anthony", ""], ["Lachnitt", "Hanna", ""], ["He", "Yijun", ""]]}, {"id": "2012.14095", "submitter": "J\\'an Pich", "authors": "J\\'an Pich", "title": "Learning algorithms from circuit lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit known constructions of efficient learning algorithms from various\nnotions of constructive circuit lower bounds such as distinguishers breaking\npseudorandom generators or efficient witnessing algorithms which find errors of\nsmall circuits attempting to compute hard functions. As our main result we\nprove that if it is possible to find efficiently, in a particular interactive\nway, errors of many p-size circuits attempting to solve hard problems, then\np-size circuits can be PAC learned over the uniform distribution with\nmembership queries by circuits of subexponential size. The opposite implication\nholds as well. This provides a new characterisation of learning algorithms and\nextends the natural proofs barrier of Razborov and Rudich. The proof is based\non a method of exploiting Nisan-Wigderson generators introduced by\nKraj\\'{i}\\v{c}ek (2010) and used to analyze complexity of circuit lower bounds\nin bounded arithmetic.\n  An interesting consequence of known constructions of learning algorithms from\ncircuit lower bounds is a learning speedup of Oliveira and Santhanam (2016). We\npresent an alternative proof of this phenomenon and discuss its potential to\nadvance the program of hardness magnification.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 04:47:36 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Pich", "J\u00e1n", ""]]}, {"id": "2012.14133", "submitter": "Brijesh Dongol", "authors": "Sadegh Dalvandi and Brijesh Dongol", "title": "Verifying C11-Style Weak Memory Libraries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deductive verification of concurrent programs under weak memory has thus far\nbeen limited to simple programs over a monolithic state space. For\nscalabiility, we also require modular techniques with verifiable library\nabstractions. This paper addresses this challenge in the context of RC11 RAR, a\nsubset of the C11 memory model that admits relaxed and release-acquire\naccesses, but disallows, so called, load-buffering cycles. We develop a simple\nframework for specifying abstract objects that precisely characterises the\nobservability guarantees of abstract method calls. We show how this framework\ncan be integrated with an operational semantics that enables verification of\nclient programs that execute abstract method calls from a library it uses.\nFinally, we show how implementations of such abstractions in RC11 RAR can be\nverified by developing a (contextual) refinement framework for abstract\nobjects. Our framework, including the operational semantics, verification\ntechnique for client-library programs, and simulation between abstract\nlibraries and their implementations, has been mechanised in Isabelle/HOL.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 08:02:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Dalvandi", "Sadegh", ""], ["Dongol", "Brijesh", ""]]}, {"id": "2012.14195", "submitter": "Sebastian Enqvist", "authors": "Sebastian Enqvist and Valentin Goranko", "title": "The temporal logic of coalitional goal assignments in concurrent\n  multi-player games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a natural extension of the Alternating time temporal\nlogic ATL, called Temporal Logic of Coalitional Goal Assignments (TLCGA). It\nfeatures just one, but quite expressive, coalitional strategic operator, viz.\nthe coalitional goal assignment operator, which is based on a mapping assigning\nto each set of players in the game its coalitional goal, formalised by a path\nformula of the language of TLCGA, i.e. a formula prefixed with a temporal\noperator X;U, or G, representing a temporalised objective for the respective\ncoalition, describing the property of the plays on which that objective is\nsatisfied. We establish fixpoint characterizations of the temporal goal\nassignments in a mu-calculus extension of TLCGA, discuss its expressiveness and\nillustrate it with some examples, prove bisimulation invariance and\nHennessy-Milner property for it with respect to a suitably defined notion of\nbisimulation, construct a sound and complete axiomatic system for TLCGA, and\nobtain its decidability via finite model property.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:20:20 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Enqvist", "Sebastian", ""], ["Goranko", "Valentin", ""]]}, {"id": "2012.14361", "submitter": "Radu Iosif", "authors": "Mnacho Echenim and Radu Iosif and Nicolas Peltier", "title": "Unifying Decidable Entailments in Separation Logic with Inductive\n  Definitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The entailment problem $\\varphi \\models \\psi$ in Separation Logic\n\\cite{IshtiaqOHearn01,Reynolds02}, between separated conjunctions of equational\n($x \\iseq y$ and $x \\not\\iseq y$), spatial ($x \\mapsto (y_1,\\ldots,y_\\rank)$)\nand predicate ($p(x_1,\\ldots,x_n)$) atoms, interpreted by a finite set of\ninductive rules, is undecidable in general. Certain restrictions on the set of\ninductive definitions lead to decidable classes of entailment problems.\nCurrently, there are two such decidable classes, based on two restrictions,\ncalled \\emph{establishment}\n\\cite{IosifRogalewiczSimacek13,KatelaanMathejaZuleger19,PZ20} and\n\\emph{restrictedness} \\cite{EIP21a}, respectively. Both classes are shown to be\nin \\twoexptime\\ by the independent proofs from \\cite{PZ20} and \\cite{EIP21a},\nrespectively, and a many-one reduction of established to restricted entailment\nproblems has been given \\cite{EIP21a}. In this paper, we strictly generalize\nthe restricted class, by distinguishing the conditions that apply only to the\nleft- ($\\varphi$) and the right- ($\\psi$) hand side of entailments,\nrespectively. We provide a many-one reduction of this generalized class, called\n\\emph{safe}, to the established class. Together with the reduction of\nestablished to restricted entailment problems, this new reduction closes the\nloop and shows that the three classes of entailment problems (respectively\nestablished, restricted and safe) form a single, unified, \\twoexptime-complete\nclass.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 17:00:11 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 09:39:14 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Echenim", "Mnacho", ""], ["Iosif", "Radu", ""], ["Peltier", "Nicolas", ""]]}, {"id": "2012.14421", "submitter": "Anupam Das", "authors": "Anupam Das", "title": "A circular version of G\\\"odel's T and its abstraction complexity", "comments": "74 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Circular and non-wellfounded proofs have become an increasingly popular tool\nfor metalogical treatments of systems with forms of induction and/or recursion.\nIn this work we investigate the expressivity of a variant CT of G\\\"odel's\nsystem T where programs are circularly typed, rather than including an explicit\nrecursion combinator. In particular, we examine the abstraction complexity\n(i.e. type level) of C, and show that the G\\\"odel primitive recursive\nfunctionals may be typed more succinctly with circular derivations, using types\nprecisely one level lower than in T. In fact we give a logical correspondence\nbetween the two settings, interpreting the quantifier-free type 1 theory of\nlevel n+1 T into that of level n C and vice-versa.\n  We also obtain some further results and perspectives on circular\n'derivations', namely strong normalisation and confluence, models based on\nhereditary computable functionals, continuity at type 2, and a translation to\nterms of $\\T$ computing the same functional, at all types.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:57:19 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 12:58:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Das", "Anupam", ""]]}, {"id": "2012.14582", "submitter": "Hazem Torfah", "authors": "Tom Baumeister, Bernd Finkbeiner, Hazem Torfah", "title": "Explainable Reactive Synthesis", "comments": "Published at ATVA 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59152-6_23", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive synthesis transforms a specification of a reactive system, given in\na temporal logic, into an implementation. The main advantage of synthesis is\nthat it is automatic. The main disadvantage is that the implementation is\nusually very difficult to understand. In this paper, we present a new synthesis\nprocess that explains the synthesized implementation to the user. The process\nstarts with a simple version of the specification and a corresponding simple\nimplementation. Then, desired properties are added one by one, and the\ncorresponding transformations, repairing the implementation, are explained in\nterms of counterexample traces. We present SAT-based algorithms for the\nsynthesis of repairs and explanations. The algorithms are evaluated on a range\nof examples including benchmarks taken from the SYNTCOMP competition.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:16:49 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Baumeister", "Tom", ""], ["Finkbeiner", "Bernd", ""], ["Torfah", "Hazem", ""]]}, {"id": "2012.14586", "submitter": "Hazem Torfah", "authors": "Bernd Finkbeiner, Lennart Haas, Hazem Torfah", "title": "Canonical Representations of k-Safety Hyperproperties", "comments": "Published in: 2019 IEEE 32nd Computer Security Foundations Symposium\n  (CSF)", "journal-ref": null, "doi": "10.1109/CSF.2019.00009", "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperproperties elevate the traditional view of trace properties form sets of\ntraces to sets of sets of traces and provide a formalism for expressing\ninformation-flow policies. For trace properties, algorithms for verification,\nmonitoring, and synthesis are typically based on a representation of the\nproperties as omega-automata. For hyperproperties, a similar, canonical\nautomata-theoretic representation is, so far, missing. This is a serious\nobstacle for the development of algorithms, because basic constructions, such\nas learning algorithms, cannot be applied.\n  In this paper, we present a canonical representation for the widely used\nclass of regular k-safety hyperproperties, which includes important polices\nsuch as noninterference. We show that a regular k-safety hyperproperty S can be\nrepresented by a finite automaton, where each word accepted by the automaton\nrepresents a violation of S. The representation provides an automata-theoretic\napproach to regular k-safety hyperproperties and allows us to compare regular\nk-safety hyperproperties, simplify them, and learn such hyperproperties. We\ninvestigate the problem of constructing automata for regular k-safety\nhyperproperties in general and from formulas in HyperLTL, and provide\ncomplexity bounds for the different translations. We also present a learning\nalgorithm for regular k-safety hyperproperties based on the L* learning\nalgorithm for deterministic finite automata.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:41:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Finkbeiner", "Bernd", ""], ["Haas", "Lennart", ""], ["Torfah", "Hazem", ""]]}, {"id": "2012.14590", "submitter": "Hazem Torfah", "authors": "Rayna Dimitrova, Bernd Finkbeiner, Hazem Torfah", "title": "Approximate Automata for Omega-Regular Languages", "comments": "Published at ATVA 2020", "journal-ref": null, "doi": "10.1007/978-3-030-31784-3_19", "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automata over infinite words, also known as omega-automata, play a key role\nin the verification and synthesis of reactive systems. The spectrum of\nomega-automata is defined by two characteristics: the acceptance condition\n(e.g. B\\\"uchi or parity) and the determinism (e.g., deterministic or\nnondeterministic) of an automaton. These characteristics play a crucial role in\napplications of automata theory. For example, certain acceptance conditions can\nbe handled more efficiently than others by dedicated tools and algorithms.\nFurthermore, some applications, such as synthesis and probabilistic model\nchecking, require that properties are represented as some type of deterministic\nomega-automata. However, properties cannot always be represented by automata\nwith the desired acceptance condition and determinism. In this paper we study\nthe problem of approximating linear-time properties by automata in a given\nclass. Our approximation is based on preserving the language up to a\nuser-defined precision given in terms of the size of the finite lasso\nrepresentation of infinite executions that are preserved. We study the state\ncomplexity of different types of approximating automata, and provide\nconstructions for the approximation within different automata classes, for\nexample, for approximating a given automaton by one with a simpler acceptance\ncondition.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:51:27 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dimitrova", "Rayna", ""], ["Finkbeiner", "Bernd", ""], ["Torfah", "Hazem", ""]]}, {"id": "2012.14592", "submitter": "Hazem Torfah", "authors": "Rayna Dimitrova, Bernd Finkbeiner, Hazem Torfah", "title": "Synthesizing Approximate Implementations for Unrealizable Specifications", "comments": "Published at CAV 2019", "journal-ref": null, "doi": "10.1007/978-3-030-25540-4_13", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unrealizability of a specification is often due to the assumption that\nthe behavior of the environment is unrestricted. In this paper, we present\nalgorithms for synthesis in bounded environments, where the environment can\nonly generate input sequences that are ultimately periodic words (lassos) with\nfinite representations of bounded size. We provide automata-theoretic and\nsymbolic approaches for solving this synthesis problem, and also study the\nsynthesis of approximative implementations from unrealizable specifications.\nSuch implementations may violate the specification in general, but are\nguaranteed to satisfy the specification on at least a specified portion of the\nbounded-size lassos. We evaluate the algorithms on different arbiter\nspecifications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:57:45 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dimitrova", "Rayna", ""], ["Finkbeiner", "Bernd", ""], ["Torfah", "Hazem", ""]]}, {"id": "2012.15080", "submitter": "Vivek Nigam", "authors": "Yuri Gil Dantas, Vivek Nigam, Harald Ruess", "title": "Security Engineering for ISO 21434", "comments": "This is a White Paper. This is a preliminary version. Its figures and\n  template are to be finalized by our marketing department. V3 corrects a\n  number of typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ISO 21434 is a new standard that has been proposed to address the future\nchallenges of automotive cybersecurity. This white paper takes a closer look at\nthe ISO 21434 helping engineers to understand the ISO 21434 parts, the key\nactivities to be carried out and the main artefacts that shall be produced. As\nany certification, obtaining the ISO 21434 certification can be daunting at\nfirst sight. Engineers have to deploy processes that include several security\nrisk assessment methods to produce security arguments and evidence supporting\nitem security claims. In this white paper, we propose a security engineering\napproach that can ease this process by relying on Rigorous Security Assessments\nand Incremental Assessment Maintenance methods supported by automation. We\ndemonstrate by example that the proposed approach can greatly increase the\nquality of the produced artefacts, the efficiency to produce them, as well as\nenable continuous security assessment. Finally, we point out some key research\ndirections that we are investigating to fully realize the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 08:36:45 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 05:56:05 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 08:51:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dantas", "Yuri Gil", ""], ["Nigam", "Vivek", ""], ["Ruess", "Harald", ""]]}, {"id": "2012.15090", "submitter": "Juerg Kohlas", "authors": "Juerg Kohlas and Juerg Schmid", "title": "Commutative Information Algebras: Representation and Duality Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LO math.IT math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information algebras arise from the idea that information comes in pieces\nwhich can be aggregated or combined into new pieces, that information refers to\nquestions and that from any piece of information, the part relevant to a given\nquestion can be extracted. This leads to a certain type of algebraic\nstructures, basically semilattices endowed with with additional unary\noperations. These operations essentially are (dual) existential quantifiers on\nthe underlying semilattice. The archetypical instances of such algebras are\nsemilattices of subsets of some universe, together with the saturation\noperators associated with a family of equivalence relations on this universe.\nSuch algebras will be called {\\em set algebras} in our context. Our first\nresult is a basic representation theorem: Every abstract information algebra is\nisomorphic to a set algebra. When it comes to combine pieces of information,\nthe idea to model the logical connectives {\\em and}, {\\em or} or {\\em not} is\nquite natural. Accordingly, we are especially interested in information\nalgebras where the underlying semilattice is a lattice, typically distributive\nor even Boolean. A major part of this paper is therefore devoted to developing\nexplicitly a full-fledged natural duality theory extending Stone resp.\nPriestley duality in a suitable way in order to take into account the\nadditional operations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:23:58 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kohlas", "Juerg", ""], ["Schmid", "Juerg", ""]]}, {"id": "2012.15291", "submitter": "Lorenzo Clemente", "authors": "Lorenzo Clemente and S{\\l}awomir Lasota", "title": "Reachability relations of timed pushdown automata", "comments": "Author's version of JCSS article", "journal-ref": "Journal of Computer and System Sciences, Volume 117, May 2021,\n  Pages 202-241", "doi": "10.1016/j.jcss.2020.11.003", "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Timed pushdown automata (TPDA) are an expressive formalism combining\nrecursion with a rich logic of timing constraints. We prove that reachability\nrelations of TPDA are expressible in linear arithmetic, a rich logic\ngeneralising Presburger arithmetic and rational arithmetic. The main technical\ningredients are a novel quantifier elimination result for clock constraints\n(used to simplify the syntax of TPDA transitions), the use of clock difference\nrelations to express reachability relations of the fractional clock values, and\nan application of Parikh's theorem to reconstruct the integral clock values.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:09:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Clemente", "Lorenzo", ""], ["Lasota", "S\u0142awomir", ""]]}, {"id": "2012.15704", "submitter": "Thomas Neele", "authors": "Thomas Neele, Antti Valmari, Tim A. C. Willemse", "title": "A Detailed Account of The Inconsistent Labelling Problem of\n  Stutter-Preserving Partial-Order Reduction", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.09829", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most popular state-space reduction techniques for model checking\nis partial-order reduction (POR). Of the many different POR implementations,\nstubborn sets are a very versatile variant and have thus seen many different\napplications over the past 32 years. One of the early stubborn sets works shows\nhow the basic conditions for reduction can be augmented to preserve\nstutter-trace equivalence, making stubborn sets suitable for model checking of\nlinear-time properties. In this paper, we identify a flaw in the reasoning and\nshow with a counter-example that stutter-trace equivalence is not necessarily\npreserved. We propose a stronger reduction condition and provide extensive new\ncorrectness proofs to ensure the issue is resolved. Furthermore, we analyse in\nwhich formalisms the problem may occur. The impact on practical implementations\nis limited, since they all compute a correct approximation of the theory.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 16:43:11 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:04:34 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 16:51:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Neele", "Thomas", ""], ["Valmari", "Antti", ""], ["Willemse", "Tim A. C.", ""]]}]