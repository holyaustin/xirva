[{"id": "1701.00140", "submitter": "Neil J. Ross", "authors": "Matthew Amy (Institute for Quantum Computing and David R. Cheriton\n  School of Computer Science, University of Waterloo, Waterloo, Canada),\n  Jianxin Chen (Institute for Advanced Computer Studies and Joint Center for\n  Quantum Information and Computer Science, University of Maryland, College\n  Park, USA), Neil J. Ross (Institute for Advanced Computer Studies and Joint\n  Center for Quantum Information and Computer Science, University of Maryland,\n  College Park, USA)", "title": "A Finite Presentation of CNOT-Dihedral Operators", "comments": "In Proceedings QPL 2017, arXiv:1802.09737", "journal-ref": "EPTCS 266, 2018, pp. 84-97", "doi": "10.4204/EPTCS.266.5", "report-no": null, "categories": "quant-ph cs.ET cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a finite presentation by generators and relations of the unitary\noperators expressible over the {CNOT, T, X} gate set, also known as\nCNOT-dihedral operators. To this end, we introduce a notion of normal form for\nCNOT-dihedral circuits and prove that every CNOT-dihedral operator admits a\nunique normal form. Moreover, we show that in the presence of certain\nstructural rules only finitely many circuit identities are required to reduce\nan arbitrary CNOT-dihedral circuit to its normal form.\n  By appropriately restricting our relations, we obtain a finite presentation\nof unitary operators expressible over the {CNOT, T} gate set as a corollary.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 16:43:54 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 14:44:04 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 17:21:13 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 03:45:48 GMT"}, {"version": "v5", "created": "Sun, 28 Apr 2019 15:43:52 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Amy", "Matthew", "", "Institute for Quantum Computing and David R. Cheriton\n  School of Computer Science, University of Waterloo, Waterloo, Canada"], ["Chen", "Jianxin", "", "Institute for Advanced Computer Studies and Joint Center for\n  Quantum Information and Computer Science, University of Maryland, College\n  Park, USA"], ["Ross", "Neil J.", "", "Institute for Advanced Computer Studies and Joint\n  Center for Quantum Information and Computer Science, University of Maryland,\n  College Park, USA"]]}, {"id": "1701.00148", "submitter": "EPTCS", "authors": "Sibylle Schwarz, Janis Voigtl\\\"ander", "title": "Proceedings 29th and 30th Workshops on (Constraint) Logic Programming\n  and 24th International Workshop on Functional and (Constraint) Logic\n  Programming", "comments": null, "journal-ref": "EPTCS 234, 2017", "doi": "10.4204/EPTCS.234", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Workshops on (Constraint) Logic Programming (WLP) are the annual meeting\nof the German Society of Logic Programming (Gesellschaft f\\\"ur Logische\nProgrammierung e.V., GLP) and bring together researchers interested in logic\nprogramming, constraint programming, answer set programming, and related areas\nlike databases and artificial intelligence (not only from Germany).\n  The International Workshops on Functional and (Constraint) Logic Programming\n(WFLP) aim at bringing together researchers, students, and practitioners\ninterested in functional programming, logic programming, and their integration.\n  The workshops have a tradition of co-location to promote the\ncross-fertilizing exchange of ideas and experiences among and between the\ncommunities interested in the foundations, applications, and combinations of\nhigh-level, declarative programming languages and related areas.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 17:36:07 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Schwarz", "Sibylle", ""], ["Voigtl\u00e4nder", "Janis", ""]]}, {"id": "1701.00227", "submitter": "Ale\\v{s} Bizjak", "authors": "Carsten R\\\"osnick-Neugebauer", "title": "Closed Sets and Operators thereon: Representations, Computability and\n  Complexity", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 2 (April 10,\n  2018) lmcs:4432", "doi": "10.23638/LMCS-14(2:1)2018", "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The TTE approach to Computable Analysis is the study of so-called\nrepresentations (encodings for continuous objects such as reals, functions, and\nsets) with respect to the notions of computability they induce. A rich variety\nof such representations had been devised over the past decades, particularly\nregarding closed subsets of Euclidean space plus subclasses thereof (like\ncompact subsets). In addition, they had been compared and classified with\nrespect to both non-uniform computability of single sets and uniform\ncomputability of operators on sets. In this paper we refine these\ninvestigations from the point of view of computational complexity. Benefiting\nfrom the concept of second-order representations and complexity recently\ndevised by Kawamura & Cook (2012), we determine parameterized complexity bounds\nfor operators such as union, intersection, projection, and more generally\nfunction image and inversion. By indicating natural parameters in addition to\nthe output precision, we get a uniform view on results by Ko (1991-2013),\nBraverman (2004/05) and Zhao & M\\\"uller (2008), relating these problems to the\nP/UP/NP question in discrete complexity theory.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 11:25:42 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 15:04:02 GMT"}, {"version": "v3", "created": "Sat, 10 Mar 2018 12:21:51 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 07:07:33 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["R\u00f6snick-Neugebauer", "Carsten", ""]]}, {"id": "1701.00233", "submitter": "EPTCS", "authors": "Horatiu Cirstea (LORIA, Universit\\'e de Lorraine, France), Santiago\n  Escobar (Universitat Polit\\`ecnica de Val\\`encia, Spain)", "title": "Proceedings Third International Workshop on Rewriting Techniques for\n  Program Transformations and Evaluation", "comments": "Dedicated to the memory of Kristoffer H. Rose", "journal-ref": "EPTCS 235, 2017", "doi": "10.4204/EPTCS.235", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the formal proceedings of the Third International\nWorkshop on Rewriting Techniques for Program Transformations and Evaluation\n(WPTE 2016), held on 23rd June 2016 in Porto, Portugal, as a satellite event of\nthe First International Conference on Formal Structures for Computation and\nDeduction (FSCD 2016). The workshop brought together researchers working on\nprogram transformations, evaluation, and operationally based programming\nlanguage semantics, using rewriting methods, in order to share the techniques\nand recent developments and to exchange ideas to encourage further activation\nof research in this area.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 12:26:14 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Cirstea", "Horatiu", "", "LORIA, Universit\u00e9 de Lorraine, France"], ["Escobar", "Santiago", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia, Spain"]]}, {"id": "1701.00242", "submitter": "EPTCS", "authors": "Ross Duncan (University of Strathclyde), Chris Heunen (University of\n  Edinburgh)", "title": "Proceedings 13th International Conference on Quantum Physics and Logic", "comments": null, "journal-ref": "EPTCS 236, 2017", "doi": "10.4204/EPTCS.236", "report-no": null, "categories": "quant-ph cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the 13th International Conference on\nQuantum Physics and Logic (QPL 2016), which was held June 6-10, 2016 at the\nUniversity of Strathclyde. QPL is a conference that brings together researchers\nworking on mathematical foundations of quantum physics, quantum computing, and\nrelated areas, with a focus on structural perspectives and the use of logical\ntools, ordered algebraic and category-theoretic structures, formal languages,\nsemantical methods, and other computer science techniques applied to the study\nof physical behaviour in general.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 13:31:42 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Duncan", "Ross", "", "University of Strathclyde"], ["Heunen", "Chris", "", "University of\n  Edinburgh"]]}, {"id": "1701.00280", "submitter": "Ernst-Erich Doberkat", "authors": "Ernst-Erich Doberkat", "title": "Using Coalgebras and the Giry Monad for Interpreting Game Logics --- A\n  Tutorial", "comments": null, "journal-ref": null, "doi": "10.1007/s11704-016-6155-5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic interpretation of Parikh's game logic should not follow the\nusual pattern of Kripke models, which in turn are based on the Kleisli\nmorphisms for the Giry monad, rather, a specific and more general approach to\nprobabilistic nondeterminism is required. We outline this approach together\nwith its probabilistic and measure theoretic basis, introducing in a leisurely\npace the Giry monad and their Kleisli morphisms together with important\ntechniques for manipulating them. Proof establishing specific techniques are\ngiven, and pointers to the extant literature are provided. After working\nthrough this tutorial, the reader should find it easier to follow the original\nliterature in this and related areas, and it should be possible for her or him\nto appreciate measure theoretic arguments for original work in the areas of\nMarkov transition systems, and stochastic effectivity functions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 19:36:58 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Doberkat", "Ernst-Erich", ""]]}, {"id": "1701.00527", "submitter": "Giuseppe Vitiello", "authors": "Gianfranco Basti, Antonio Capolupo, Giuseppe Vitiello", "title": "Quantum Field Theory and Coalgebraic Logic in Theoretical Computer\n  Science", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LO math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we suggest that in the framework of the Category Theory it is\npossible to demonstrate the mathematical and logical \\textit{dual equivalence}\nbetween the category of the $q$-deformed Hopf Coalgebras and the category of\nthe $q$-deformed Hopf Algebras in QFT, interpreted as a thermal field theory.\nEach pair algebra-coalgebra characterizes, indeed, a QFT system and its\nmirroring thermal bath, respectively, so to model dissipative quantum systems\npersistently in far-from-equilibrium conditions, with an evident significance\nalso for biological sciences. The $q$-deformed Hopf Coalgebras and the\n$q$-deformed Hopf Algebras constitute two dual categories because characterized\nby the same functor $T$, related with the Bogoliubov transform, and by its\ncontravariant application $T^{op}$, respectively. The \\textit{q}-deformation\nparameter, indeed, is related to the Bogoliubov angle, and it is effectively a\nthermal parameter. Therefore, the different values of $q$ identify univocally,\nand then label, the vacua appearing in the foliation process of the quantum\nvacuum. This means that, in the framework of Universal Coalgebra, as general\ntheory of dynamic and computing systems (\"labelled state-transition systems\"),\nthe so labelled infinitely many quantum vacua can be interpreted as the Final\nCoalgebra of an \"Infinite State Black-Box Machine\". All this opens the way to\nthe possibility of designing a new class of universal quantum computing\narchitectures based on this coalgebraic formulation of QFT, as its ability of\nnaturally generating a Fibonacci progression demonstrates.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 22:11:35 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Basti", "Gianfranco", ""], ["Capolupo", "Antonio", ""], ["Vitiello", "Giuseppe", ""]]}, {"id": "1701.00623", "submitter": "EPTCS", "authors": "Stefan Brass (University of Halle), Heike Stephan (University of\n  Halle)", "title": "Bottom-Up Evaluation of Datalog: Preliminary Report", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 13-26", "doi": "10.4204/EPTCS.234.2", "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-up evaluation of Datalog has been studied for a long time, and is\nstandard material in textbooks. However, if one actually wants to develop a\ndeductive database system, it turns out that there are many implementation\noptions. For instance, the sequence in which rule instances are applied is not\ngiven. In this paper, we study a method that immediately uses a derived tuple\nto derive more tuples (called the Push method). In this way, storage space for\nintermediate results can be reduced. The main contribution of our method is the\nway in which we minimize the copying of values at runtime, and do much work\nalready at compile-time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:30:51 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Brass", "Stefan", "", "University of Halle"], ["Stephan", "Heike", "", "University of\n  Halle"]]}, {"id": "1701.00624", "submitter": "EPTCS", "authors": "Marija Kula\\v{s}", "title": "A Practical View on Renaming", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 27-41", "doi": "10.4204/EPTCS.234.3", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit variable renaming from a practitioner's point of view, presenting\nconcepts we found useful in dealing with operational semantics of pure Prolog.\nA concept of relaxed core representation is introduced, upon which a concept of\nprenaming is built. Prenaming formalizes the intuitive practice of renaming\nterms by just considering the necessary bindings, where now some passive\n\"bindings\" x/x may be necessary as well. As an application, a constructive\nversion of variant lemma for implemented Horn clause logic has been obtained.\nThere, prenamings made it possible to incrementally handle new (local)\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:31:07 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Kula\u0161", "Marija", ""]]}, {"id": "1701.00627", "submitter": "EPTCS", "authors": "Stefan Brass (University of Halle), Heike Stephan (University of\n  Halle)", "title": "Experiences with Some Benchmarks for Deductive Databases and\n  Implementations of Bottom-Up Evaluation", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 57-72", "doi": "10.4204/EPTCS.234.5", "report-no": null, "categories": "cs.DB cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenRuleBench is a large benchmark suite for rule engines, which includes\ndeductive databases. We previously proposed a translation of Datalog to C++\nbased on a method that \"pushes\" derived tuples immediately to places where they\nare used. In this paper, we report performance results of various\nimplementation variants of this method compared to XSB, YAP and DLV. We study\nonly a fraction of the OpenRuleBench problems, but we give a quite detailed\nanalysis of each such task and the factors which influence performance. The\nresults not only show the potential of our method and implementation approach,\nbut could be valuable for anybody implementing systems which should be able to\nexecute tasks of the discussed types.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:31:41 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Brass", "Stefan", "", "University of Halle"], ["Stephan", "Heike", "", "University of\n  Halle"]]}, {"id": "1701.00629", "submitter": "EPTCS", "authors": "Sebastian Krings (Heinrich-Heine Universit\\\"at D\\\"usseldorf), Michael\n  Leuschel (Heinrich-Heine Universit\\\"at D\\\"usseldorf)", "title": "Constraint Logic Programming over Infinite Domains with an Application\n  to Proof", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 73-87", "doi": "10.4204/EPTCS.234.6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a CLP(FD)-based constraint solver able to deal with unbounded\ndomains. It is based on constraint propagation, resorting to enumeration if all\nother methods fail. An important aspect is detecting when enumeration was\ncomplete and if this has an impact on the soundness of the result. We present a\ntechnique which guarantees soundness in the following way: if the constraint\nsolver finds a solution it is guaranteed to be correct; if the constraint\nsolver fails to find a solution it can either return the result \"definitely\nfalse\" in case it knows enumeration was exhaustive, or \"unknown\" in case it was\naborted. The technique can deal with nested universal and existential\nquantifiers. It can easily be extended to set comprehensions and other\noperators introducing new quantified variables. We show applications in data\nvalidation and proof.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:31:58 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Krings", "Sebastian", "", "Heinrich-Heine Universit\u00e4t D\u00fcsseldorf"], ["Leuschel", "Michael", "", "Heinrich-Heine Universit\u00e4t D\u00fcsseldorf"]]}, {"id": "1701.00630", "submitter": "EPTCS", "authors": "Frank Flederer (University of Wuerzburg), Ludwig Ostermayer\n  (University of Wuerzburg), Dietmar Seipel (University of Wuerzburg), Sergio\n  Montenegro (University of Wuerzburg)", "title": "Source Code Verification for Embedded Systems using Prolog", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 88-103", "doi": "10.4204/EPTCS.234.7", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System relevant embedded software needs to be reliable and, therefore, well\ntested, especially for aerospace systems. A common technique to verify programs\nis the analysis of their abstract syntax tree (AST). Tree structures can be\nelegantly analyzed with the logic programming language Prolog. Moreover, Prolog\noffers further advantages for a thorough analysis: On the one hand, it natively\nprovides versatile options to efficiently process tree or graph data\nstructures. On the other hand, Prolog's non-determinism and backtracking eases\ntests of different variations of the program flow without big effort. A\nrule-based approach with Prolog allows to characterize the verification goals\nin a concise and declarative way.\n  In this paper, we describe our approach to verify the source code of a flash\nfile system with the help of Prolog. The flash file system is written in C++\nand has been developed particularly for the use in satellites. We transform a\ngiven abstract syntax tree of C++ source code into Prolog facts and derive the\ncall graph and the execution sequence (tree), which then are further tested\nagainst verification goals. The different program flow branching due to control\nstructures is derived by backtracking as subtrees of the full execution\nsequence. Finally, these subtrees are verified in Prolog.\n  We illustrate our approach with a case study, where we search for incorrect\napplications of semaphores in embedded software using the real-time operating\nsystem RODOS. We rely on computation tree logic (CTL) and have designed an\nembedded domain specific language (DSL) in Prolog to express the verification\ngoals.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:32:17 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Flederer", "Frank", "", "University of Wuerzburg"], ["Ostermayer", "Ludwig", "", "University of Wuerzburg"], ["Seipel", "Dietmar", "", "University of Wuerzburg"], ["Montenegro", "Sergio", "", "University of Wuerzburg"]]}, {"id": "1701.00637", "submitter": "EPTCS", "authors": "Ken-etsu Fujita (Gunma University)", "title": "On Upper Bounds on the Church-Rosser Theorem", "comments": "In Proceedings WPTE 2016, arXiv:1701.00233", "journal-ref": "EPTCS 235, 2017, pp. 16-31", "doi": "10.4204/EPTCS.235.2", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Church-Rosser theorem in the type-free lambda-calculus is well\ninvestigated both for beta-equality and beta-reduction. We provide a new proof\nof the theorem for beta-equality with no use of parallel reductions, but simply\nwith Takahashi's translation (Gross-Knuth strategy). Based on this, upper\nbounds for reduction sequences on the theorem are obtained as the fourth level\nof the Grzegorczyk hierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:38:06 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Fujita", "Ken-etsu", "", "Gunma University"]]}, {"id": "1701.00638", "submitter": "EPTCS", "authors": "Karl Gmeiner (UAS Technikum Wien)", "title": "Confluence of Conditional Term Rewrite Systems via Transformations", "comments": "In Proceedings WPTE 2016, arXiv:1701.00233", "journal-ref": "EPTCS 235, 2017, pp. 32-45", "doi": "10.4204/EPTCS.235.3", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional term rewriting is an intuitive yet complex extension of term\nrewriting. In order to benefit from the simpler framework of unconditional\nrewriting, transformations have been defined to eliminate the conditions of\nconditional term rewrite systems.\n  Recent results provide confluence criteria for conditional term rewrite\nsystems via transformations, yet they are restricted to CTRSs with certain\nsyntactic properties like weak left-linearity. These syntactic properties imply\nthat the transformations are sound for the given CTRS.\n  This paper shows how to use transformations to prove confluence of\noperationally terminating, right-stable deterministic conditional term rewrite\nsystems without the necessity of soundness restrictions. For this purpose, it\nis shown that certain rewrite strategies, in particular almost U-eagerness and\ninnermost rewriting, always imply soundness.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:38:23 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Gmeiner", "Karl", "", "UAS Technikum Wien"]]}, {"id": "1701.00639", "submitter": "EPTCS", "authors": "Yutaro Nagae (Nagoya University), Masahiko Sakai (Nagoya University),\n  Hiroyuki Seki (Nagoya University)", "title": "An Extension of Proof Graphs for Disjunctive Parameterised Boolean\n  Equation Systems", "comments": "In Proceedings WPTE 2016, arXiv:1701.00233", "journal-ref": "EPTCS 235, 2017, pp. 46-61", "doi": "10.4204/EPTCS.235.4", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parameterised Boolean equation system (PBES) is a set of equations that\ndefines sets as the least and/or greatest fixed-points that satisfy the\nequations. This system is regarded as a declarative program defining functions\nthat take a datum and returns a Boolean value. The membership problem of PBESs\nis a problem to decide whether a given element is in the defined set or not,\nwhich corresponds to an execution of the program. This paper introduces reduced\nproof graphs, and studies a technique to solve the membership problem of PBESs,\nwhich is undecidable in general, by transforming it into a reduced proof graph.\n  A vertex X(v) in a proof graph represents that the data v is in the set X, if\nthe graph satisfies conditions induced from a given PBES. Proof graphs are,\nhowever, infinite in general. Thus we introduce vertices each of which stands\nfor a set of vertices of the original ones, which possibly results in a finite\ngraph. For a subclass of disjunctive PBESs, we clarify some conditions which\nreduced proof graphs should satisfy. We also show some examples having no\nfinite proof graph except for reduced one. We further propose a reduced\ndependency space, which contains reduced proof graphs as sub-graphs if a proof\ngraph exists. We provide a procedure to construct finite reduced dependency\nspaces, and show the soundness and completeness of the procedure.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:38:38 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Nagae", "Yutaro", "", "Nagoya University"], ["Sakai", "Masahiko", "", "Nagoya University"], ["Seki", "Hiroyuki", "", "Nagoya University"]]}, {"id": "1701.00650", "submitter": "EPTCS", "authors": "Ryota Nakayama (Nagoya University), Naoki Nishida (Nagoya University),\n  Masahiko Sakai (Nagoya University)", "title": "Sound Structure-Preserving Transformation for Weakly-Left-Linear\n  Deterministic Conditional Term Rewriting Systems", "comments": "In Proceedings WPTE 2016, arXiv:1701.00233", "journal-ref": "EPTCS 235, 2017, pp. 62-77", "doi": "10.4204/EPTCS.235.5", "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the SR transformation, a computationally\nequivalent transformation proposed by Serbanuta and Rosu, is a sound\nstructure-preserving transformation for weakly-left-linear deterministic\nconditional term rewriting systems. More precisely, we show that every\nweakly-left-linear deterministic conditional term rewriting system can be\nconverted to an equivalent weakly-left-linear and ultra-weakly-left-linear\ndeterministic conditional term rewriting system and prove that the SR\ntransformation is sound for weakly-left-linear and ultra-weakly-left-linear\ndeterministic conditional term rewriting systems. Here, soundness for a\nconditional term rewriting system means that reduction of the transformed\nunconditional term rewriting system creates no undesired reduction sequence for\nthe conditional system.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:10:52 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Nakayama", "Ryota", "", "Nagoya University"], ["Nishida", "Naoki", "", "Nagoya University"], ["Sakai", "Masahiko", "", "Nagoya University"]]}, {"id": "1701.00656", "submitter": "EPTCS", "authors": "Giovanni Car\\`u (University of Oxford)", "title": "On the Cohomology of Contextuality", "comments": "In Proceedings QPL 2016, arXiv:1701.00242", "journal-ref": "EPTCS 236, 2017, pp. 21-39", "doi": "10.4204/EPTCS.236.2", "report-no": null, "categories": "quant-ph cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Abramsky and Brandenburger used sheaf theory to give a\nmathematical formulation of non-locality and contextuality. By adopting this\nviewpoint, it has been possible to define cohomological obstructions to the\nexistence of global sections. In the present work, we illustrate new insights\ninto different aspects of this theory. We shed light on the power of detection\nof the cohomological obstruction by showing that it is not a complete invariant\nfor strong contextuality even under symmetry and connectedness restrictions on\nthe measurement cover, disproving a previous conjecture. We generalise\nobstructions to higher cohomology groups and show that they give rise to a\nrefinement of the notion of cohomological contextuality: different \"levels\" of\ncontextuality are organised in a hierarchy of logical implications. Finally, we\npresent an alternative description of the first cohomology group in terms of\ntorsors, resulting in a new interpretation of the cohomological obstructions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:14:35 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Car\u00f9", "Giovanni", "", "University of Oxford"]]}, {"id": "1701.00658", "submitter": "EPTCS", "authors": "Amar Hadzihasanovic", "title": "A Topological Perspective on Interacting Algebraic Theories", "comments": "In Proceedings QPL 2016, arXiv:1701.00242", "journal-ref": "EPTCS 236, 2017, pp. 70-86", "doi": "10.4204/EPTCS.236.5", "report-no": null, "categories": "math.CT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques from higher categories and higher-dimensional rewriting are\nbecoming increasingly important for understanding the finer, computational\nproperties of higher algebraic theories that arise, among other fields, in\nquantum computation. These theories have often the property of containing\nsimpler sub-theories, whose interaction is regulated in a limited number of\nways, which reveals a topological substrate when pictured by string diagrams.\nBy exploring the double nature of computads as presentations of higher\nalgebraic theories, and combinatorial descriptions of \"directed spaces\", we\ndevelop a basic language of directed topology for the compositional study of\nalgebraic theories. We present constructions of computads, all with clear\nanalogues in standard topology, that capture in great generality such notions\nas homomorphisms and actions, and the interactions of monoids and comonoids\nthat lead to the theory of Frobenius algebras and of bialgebras. After a number\nof examples, we describe how a fragment of the ZX calculus can be reconstructed\nin this framework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:15:16 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Hadzihasanovic", "Amar", ""]]}, {"id": "1701.00660", "submitter": "EPTCS", "authors": "Dan Marsden (University of Oxford)", "title": "Ambiguity and Incomplete Information in Categorical Models of Language", "comments": "In Proceedings QPL 2016, arXiv:1701.00242", "journal-ref": "EPTCS 236, 2017, pp. 95-107", "doi": "10.4204/EPTCS.236.7", "report-no": null, "categories": "cs.LO cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate notions of ambiguity and partial information in categorical\ndistributional models of natural language. Probabilistic ambiguity has\npreviously been studied using Selinger's CPM construction. This construction\nworks well for models built upon vector spaces, as has been shown in quantum\ncomputational applications. Unfortunately, it doesn't seem to provide a\nsatisfactory method for introducing mixing in other compact closed categories\nsuch as the category of sets and binary relations. We therefore lack a uniform\nstrategy for extending a category to model imprecise linguistic information.\n  In this work we adopt a different approach. We analyze different forms of\nambiguous and incomplete information, both with and without quantitative\nprobabilistic data. Each scheme then corresponds to a suitable enrichment of\nthe category in which we model language. We view different monads as\nencapsulating the informational behaviour of interest, by analogy with their\nuse in modelling side effects in computation. Previous results of Jacobs then\nallow us to systematically construct suitable bases for enrichment.\n  We show that we can freely enrich arbitrary dagger compact closed categories\nin order to capture all the phenomena of interest, whilst retaining the\nimportant dagger compact closed structure. This allows us to construct a model\nwith real convex combination of binary relations that makes non-trivial use of\nthe scalars. Finally we relate our various different enrichments, showing that\nfinite subconvex algebra enrichment covers all the effects under consideration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:16:07 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Marsden", "Dan", "", "University of Oxford"]]}, {"id": "1701.00662", "submitter": "EPTCS", "authors": "Mathys Rennela (Radboud University), Sam Staton (Oxford University),\n  Robert Furber (Aalborg University)", "title": "Infinite-Dimensionality in Quantum Foundations: W*-algebras as\n  Presheaves over Matrix Algebras", "comments": "In Proceedings QPL 2016, arXiv:1701.00242", "journal-ref": "EPTCS 236, 2017, pp. 161-173", "doi": "10.4204/EPTCS.236.11", "report-no": null, "categories": "math.OA cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, W*-algebras are presented as canonical colimits of diagrams of\nmatrix algebras and completely positive maps. In other words, matrix algebras\nare dense in W*-algebras.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:17:34 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Rennela", "Mathys", "", "Radboud University"], ["Staton", "Sam", "", "Oxford University"], ["Furber", "Robert", "", "Aalborg University"]]}, {"id": "1701.00877", "submitter": "Tom Hanika", "authors": "Daniel Borchmann, Tom Hanika, Sergei Obiedkov", "title": "On the Usability of Probably Approximately Correct Implication Bases", "comments": "17 pages, 8 figures; typos added, corrected x-label on graphs", "journal-ref": null, "doi": "10.1007/978-3-319-59271-8_5", "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the notion of probably approximately correct implication bases\nfrom the literature and present a first formulation in the language of formal\nconcept analysis, with the goal to investigate whether such bases represent a\nsuitable substitute for exact implication bases in practical use-cases. To this\nend, we quantitatively examine the behavior of probably approximately correct\nimplication bases on artificial and real-world data sets and compare their\nprecision and recall with respect to their corresponding exact implication\nbases. Using a small example, we also provide qualitative insight that\nimplications from probably approximately correct bases can still represent\nmeaningful knowledge from a given data set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 00:45:37 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 23:39:05 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Borchmann", "Daniel", ""], ["Hanika", "Tom", ""], ["Obiedkov", "Sergei", ""]]}, {"id": "1701.00976", "submitter": "Elem Guzel Kalayci", "authors": "Diego Calvanese and Elem G\\\"uzel Kalayc{\\i} and Vladislav Ryzhikov and\n  Guohui Xiao and Michael Zakharyaschev", "title": "Metric Temporal Logic for Ontology-Based Data Access over Log Data", "comments": null, "journal-ref": "In Proceedings of the 2nd International Workshop on Ontologies and\n  Logic Programming for Query Answering (ONTOLP-16), 2016", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new metric temporal logic HornMTL over dense time and its\ndatalog extension datalogMTL. The use of datalogMTL is demonstrated in the\ncontext of ontology-based data access over meteorological data. We show\ndecidability of answering ontology-mediated queries for a practically relevant\nnon-recursive fragment of datalogMTL. Finally, we discuss directions of the\nfuture work, including the potential use-cases in analyzing log data of engines\nand devices.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 11:59:30 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Calvanese", "Diego", ""], ["Kalayc\u0131", "Elem G\u00fczel", ""], ["Ryzhikov", "Vladislav", ""], ["Xiao", "Guohui", ""], ["Zakharyaschev", "Michael", ""]]}, {"id": "1701.01285", "submitter": "Daniel Murfet", "authors": "James Clift, Daniel Murfet", "title": "Cofree coalgebras and differential linear logic", "comments": "New introduction to the cofree coalgebra and relevant geometric\n  intuition, and a discussion of the differential lambda calculus", "journal-ref": "Math. Struct. Comp. Sci. 30 (2020) 416-457", "doi": "10.1017/S0960129520000134", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the semantics of intuitionistic linear logic in vector spaces\nwhich uses cofree coalgebras is also a model of differential linear logic, and\nthat the Cartesian closed category of cofree coalgebras is a model of the\nsimply-typed differential lambda calculus.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 11:55:58 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 21:18:07 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Clift", "James", ""], ["Murfet", "Daniel", ""]]}, {"id": "1701.01413", "submitter": "Matthieu Perrinel M.", "authors": "Matthieu Perrinel", "title": "Paths-based criteria and application to linear logic subsystems\n  characterizing polynomial time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several variants of linear logic have been proposed to characterize\ncomplexity classes in the proofs-as-programs correspondence. Light linear logic\n(LLL) ensures a polynomial bound on reduction time, and characterizes in this\nway polynomial time (Ptime). In this paper we study the complexity of linear\nlogic proof-nets and propose three semantic criteria based on context\nsemantics: stratification, dependence control and nesting. Stratification alone\nentails an elementary time bound, the three criteria entail together a\npolynomial time bound.\n  These criteria can be used to prove the complexity soundness of several\nexisting variants of linear logic. We define a decidable syntactic subsystem of\nlinear logic: SDNLL. We prove that the proof-nets of SDNLL satisfy the three\ncriteria, which implies that SDNLL is sound for Ptime. Several previous\nsubsystems of linear logic characterizing polynomial time (LLL, mL^4, maximal\nsystem of MS) are embedded in SDNLL, proving its Ptime completeness.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 18:25:37 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Perrinel", "Matthieu", ""]]}, {"id": "1701.02118", "submitter": "William Blum", "authors": "William Blum", "title": "Type homogeneity is not a restriction for safe recursion schemes", "comments": "The result presented in this paper was privately circulated for the\n  first time in 2009 and shared on my personal website but was never published\n  in a journal or conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knapik et al. introduced the safety restriction which constrains both the\ntypes and syntax of the production rules defining a higher-order recursion\nscheme. This restriction gives rise to an equi-expressivity result between\norder-n pushdown automata and order-n safe recursion schemes, when such devices\nare used as tree generators. We show that the typing constraint of safety,\ncalled homogeneity, is unnecessary in the sense that imposing the syntactic\nrestriction alone is sufficient to prove the equi-expressivity result for\ntrees.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 10:07:50 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Blum", "William", ""]]}, {"id": "1701.02162", "submitter": "Amaury Pouly", "authors": "Nathana\\\"el Fijalkow, Pierre Ohlmann, Jo\\\"el Ouaknine, Amaury Pouly,\n  James Worrell", "title": "Semialgebraic Invariant Synthesis for the Kannan-Lipton Orbit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO cs.SC math.AG math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{Orbit Problem} consists of determining, given a linear\ntransformation $A$ on $\\mathbb{Q}^d$, together with vectors $x$ and $y$,\nwhether the orbit of $x$ under repeated applications of $A$ can ever reach $y$.\nThis problem was famously shown to be decidable by Kannan and Lipton in the\n1980s.\n  In this paper, we are concerned with the problem of synthesising suitable\n\\emph{invariants} $\\mathcal{P} \\subseteq \\mathbb{R}^d$, \\emph{i.e.}, sets that\nare stable under $A$ and contain $x$ and not $y$, thereby providing compact and\nversatile certificates of non-reachability. We show that whether a given\ninstance of the Orbit Problem admits a semialgebraic invariant is decidable,\nand moreover in positive instances we provide an algorithm to synthesise\nsuitable invariants of polynomial size.\n  It is worth noting that the existence of \\emph{semilinear} invariants, on the\nother hand, is (to the best of our knowledge) not known to be decidable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 13:00:53 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Fijalkow", "Nathana\u00ebl", ""], ["Ohlmann", "Pierre", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Pouly", "Amaury", ""], ["Worrell", "James", ""]]}, {"id": "1701.02199", "submitter": "Jan Hidders", "authors": "Jacek Sroka and Jan Hidders", "title": "Finding AND-OR Hierarchies in Workflow Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the notion of AND-OR reduction, which reduces a WF net to\na smaller net by iteratively contracting certain well-formed subnets into\nsingle nodes until no more such contractions are possible. This reduction can\nreveal the hierarchical structure of a WF net, and since it preserves certain\nsemantical properties such as soundness, it can help with analysing and\nunderstanding why a WF net is sound or not. The reduction can also be used to\nverify if a WF net is an AND-OR net. This class of WF nets was introduced in\nearlier work, and arguably describes nets that follow good hierarchical design\nprinciples. It is shown that the AND-OR reduction is confluent up to\nisomorphism, which means that despite the inherent non-determinism that comes\nfrom the choice of subnets that are contracted, the final result of the\nreduction is always the same up to the choice of the identity of the nodes.\nBased on this result, a polynomial-time algorithm is presented that computes\nthis unique result of the AND-OR reduction. Finally, it is shown how this\nalgorithm can be used to verify if a WF net is an AND-OR net.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 16:10:48 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Sroka", "Jacek", ""], ["Hidders", "Jan", ""]]}, {"id": "1701.02227", "submitter": "Thorsten Wissmann", "authors": "Matthew Frank", "title": "Interpolating Between Choices for the Approximate Intermediate Value\n  Theorem", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 3 (July 14,\n  2020) lmcs:6636", "doi": "10.23638/LMCS-16(3:5)2020", "report-no": null, "categories": "math.LO cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proves the approximate intermediate value theorem, constructively\nand from notably weak hypotheses: from pointwise rather than uniform\ncontinuity, without assuming that reals are presented with rational\napproximants, and without using countable choice. The theorem is that if a\npointwise continuous function has both a negative and a positive value, then it\nhas values arbitrarily close to 0. The proof builds on the usual classical\nproof by bisection, which repeatedly selects the left or right half of an\ninterval; the algorithm here selects an interval of half the size in a\ncontinuous way, interpolating between those two possibilities.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 13:56:32 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 15:10:36 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 08:16:39 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Frank", "Matthew", ""]]}, {"id": "1701.02231", "submitter": "Thorsten Wissmann", "authors": "Cristina Feier and Antti Kuusisto and Carsten Lutz", "title": "Rewritability in Monadic Disjunctive Datalog, MMSNP, and Expressive\n  Description Logics", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (May 23,\n  2019) lmcs:5502", "doi": "10.23638/LMCS-15(2:15)2019", "report-no": null, "categories": "cs.LO cs.CC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study rewritability of monadic disjunctive Datalog programs, (the\ncomplements of) MMSNP sentences, and ontology-mediated queries (OMQs) based on\nexpressive description logics of the ALC family and on conjunctive queries. We\nshow that rewritability into FO and into monadic Datalog (MDLog) are decidable,\nand that rewritability into Datalog is decidable when the original query\nsatisfies a certain condition related to equality. We establish\n2NExpTime-completeness for all studied problems except rewritability into MDLog\nfor which there remains a gap between 2NExpTime and 3ExpTime. We also analyze\nthe shape of rewritings, which in the MMSNP case correspond to obstructions,\nand give a new construction of canonical Datalog programs that is more\nelementary than existing ones and also applies to formulas with free variables.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 18:17:15 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 13:13:21 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 17:23:18 GMT"}, {"version": "v4", "created": "Wed, 22 May 2019 06:03:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Feier", "Cristina", ""], ["Kuusisto", "Antti", ""], ["Lutz", "Carsten", ""]]}, {"id": "1701.02274", "submitter": "Florian Steinberg", "authors": "Matthias Schr\\\"oder and Florian Steinberg", "title": "Bounded time computation on metric spaces and Banach spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the framework by Kawamura and Cook for investigating computational\ncomplexity for operators occurring in analysis. This model is based on\nsecond-order complexity theory for functions on the Baire space, which is\nlifted to metric spaces by means of representations. Time is measured in terms\nof the length of the input encodings and the required output precision. We\npropose the notions of a complete representation and of a regular\nrepresentation. We show that complete representations ensure that any\ncomputable function has a time bound. Regular representations generalize\nKawamura and Cook's more restrictive notion of a second-order representation,\nwhile still guaranteeing fast computability of the length of the encodings.\nApplying these notions, we investigate the relationship between purely metric\nproperties of a metric space and the existence of a representation such that\nthe metric is computable within bounded time. We show that a bound on the\nrunning time of the metric can be straightforwardly translated into size bounds\nof compact subsets of the metric space. Conversely, for compact spaces and for\nBanach spaces we construct a family of admissible, complete, regular\nrepresentations that allow for fast computation of the metric and provide short\nencodings. Here it is necessary to trade the time bound off against the length\nof encodings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 17:42:12 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 16:18:40 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Schr\u00f6der", "Matthias", ""], ["Steinberg", "Florian", ""]]}, {"id": "1701.02275", "submitter": "Olga Tveretina", "authors": "Olga Tveretina", "title": "Resolution Simulates Ordered Binary Decision Diagrams for Formulas in\n  Conjunctive Normal Form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical question of propositional logic is one of the shortest proof of a\ntautology. A related fundamental problem is to determine the relative\nefficiency of standard proof systems, where the relative complexity is measured\nusing the notion of polynomial simulation.\n  Presently, the state-of-the-art satisfiability algorithms are based on\nresolution in combination with search. An Ordered Binary Decision Diagram\n(OBDD) is a data structure that is used to represent Boolean functions.\n  Groote and Zantema have proved that there is exponential separation between\nresolution and a proof system based on limited OBDD derivations. However,\nformal comparison of these methods is not straightforward because OBDDs work on\narbitrary formulas, whereas resolution can only be applied to formulas in\nConjunctive Normal Form (CNFs).\n  Contrary to popular belief, we argue that resolution simulates OBDDs\npolynomially if we limit both to CNFs and thus answer negatively the open\nquestion of Groote and Zantema whether there exist unsatisfiable CNFs having\npolynomial OBDD refutations and requiring exponentially long resolution\nrefutations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 17:42:33 GMT"}, {"version": "v2", "created": "Sun, 12 Feb 2017 19:41:16 GMT"}, {"version": "v3", "created": "Sat, 18 Mar 2017 16:41:05 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Tveretina", "Olga", ""]]}, {"id": "1701.02394", "submitter": "Paolo Baldan", "authors": "Paolo Baldan, Andrea Corradini, Fabio Gadducci", "title": "Domains and Event Structures for Fusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable event structures, and their duality with prime algebraic domains\narising as partial orders of configurations, are a landmark of concurrency\ntheory, providing a clear characterisation of causality in computations. They\nhave been used for defining a concurrent semantics of several formalisms, from\nPetri nets to (linear) graph rewriting systems, which in turn lay at the basis\nof many visual modelling frameworks. Stability however is restrictive when\ndealing with formalisms with \"fusion\", i.e., where a computational step can not\nonly consume and produce but also merge parts of the state. This happens, e.g.,\nfor graph rewriting systems with non-linear rules, which are needed to cover\nsome relevant applications (such as the graphical encoding of calculi with name\npassing). Guided by the need of capturing the semantics of formalisms with\nfusion we leave aside stability and we characterise, as a natural\ngeneralisation of prime algebraic domains, a class of domains, referred to as\nweak prime domains. We then identify a corresponding class of event structures,\nthat we call connected event structures, via a duality result formalised as an\nequivalence of categories. We show that connected event structures are exactly\nthe class of event structures that arise as the semantics of non-linear graph\nrewriting systems. Interestingly, the category of general unstable event\nstructures coreflects into our category of weak prime domains, so that our\nresult provides a characterisation of the partial orders of configurations of\nsuch event structures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 23:51:57 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 22:22:06 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 15:19:28 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Baldan", "Paolo", ""], ["Corradini", "Andrea", ""], ["Gadducci", "Fabio", ""]]}, {"id": "1701.02445", "submitter": "Vilem Vychodil", "authors": "Vilem Vychodil", "title": "Closure structures parameterized by systems of isotone Galois\n  connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study properties of classes of closure operators and closure systems\nparameterized by systems of isotone Galois connections. The parameterizations\nexpress stronger requirements on idempotency and monotony conditions of closure\noperators. The present approach extends previous approaches to fuzzy closure\noperators which appeared in analysis of object-attribute data with graded\nattributes and reasoning with if-then rules in graded setting and is also\nrelated to analogous results developed in linear temporal logic. In the paper,\nwe present foundations of the operators and include examples of general\nproblems in data analysis where such operators appear.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 06:26:21 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Vychodil", "Vilem", ""]]}, {"id": "1701.02494", "submitter": "Nils Vortmeier", "authors": "Thomas Schwentick, Nils Vortmeier, Thomas Zeume", "title": "Dynamic Complexity under Definable Changes", "comments": "Full version of an article to be published in ICDT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies dynamic complexity under definable change operations in\nthe DynFO framework by Patnaik and Immerman. It is shown that for changes\ndefinable by parameter-free first-order formulas, all (uniform) $AC^1$ queries\ncan be maintained by first-order dynamic programs. Furthermore, many\nmaintenance results for single-tuple changes are extended to more powerful\nchange operations: (1) The reachability query for undirected graphs is\nfirst-order maintainable under single tuple changes and first-order defined\ninsertions, likewise the reachability query for directed acyclic graphs under\nquantifier-free insertions. (2) Context-free languages are first-order\nmaintainable under $\\Sigma_1$-defined changes. These results are complemented\nby several inexpressibility results, for example, that the reachability query\ncannot be maintained by quantifier-free programs under definable,\nquantifier-free deletions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 09:43:10 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Schwentick", "Thomas", ""], ["Vortmeier", "Nils", ""], ["Zeume", "Thomas", ""]]}, {"id": "1701.02526", "submitter": "Shichao Liu", "authors": "Shichao Liu and Ying Jiang", "title": "Modeling and Reasoning About Wireless Networks: A Graph-based Calculus\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a graph-based process calculus for modeling and reasoning about\nwireless networks with local broadcasts. Graphs are used at syntactical level\nto describe the topological structures of networks. This calculus is equipped\nwith a reduction semantics and a labelled transition semantics. The former is\nused to define weak barbed congruence. The latter is used to define a\nparameterized weak bisimulation emphasizing locations and local broadcasts. We\nprove that weak bisimilarity implies weak barbed congruence. The potential\napplications are illustrated by some examples and two case studies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 11:45:34 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 08:04:33 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Liu", "Shichao", ""], ["Jiang", "Ying", ""]]}, {"id": "1701.02546", "submitter": "Patrick Totzke", "authors": "Richard Mayr, Sven Schewe, Patrick Totzke, Dominik Wojtczak", "title": "MDPs with Energy-Parity Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-parity objectives combine $\\omega$-regular with quantitative\nobjectives of reward MDPs. The controller needs to avoid to run out of energy\nwhile satisfying a parity objective.\n  We refute the common belief that, if an energy-parity objective holds\nalmost-surely, then this can be realised by some finite memory strategy. We\nprovide a surprisingly simple counterexample that only uses coB\\\"uchi\nconditions.\n  We introduce the new class of bounded (energy) storage objectives that, when\ncombined with parity objectives, preserve the finite memory property. Based on\nthese, we show that almost-sure and limit-sure energy-parity objectives, as\nwell as almost-sure and limit-sure storage parity objectives, are in\n$\\mathit{NP}\\cap \\mathit{coNP}$ and can be solved in pseudo-polynomial time for\nenergy-parity MDPs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 12:18:23 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 16:09:31 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 16:46:07 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Mayr", "Richard", ""], ["Schewe", "Sven", ""], ["Totzke", "Patrick", ""], ["Wojtczak", "Dominik", ""]]}, {"id": "1701.02547", "submitter": "Hongseok Yang", "authors": "Chris Heunen and Ohad Kammar and Sam Staton and Hongseok Yang", "title": "A Convenient Category for Higher-Order Probability Theory", "comments": null, "journal-ref": "Logic in Computer Science 2017", "doi": "10.1109/LICS.2017.8005137", "report-no": null, "categories": "cs.PL cs.AI cs.LO math.CT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order probabilistic programming languages allow programmers to write\nsophisticated models in machine learning and statistics in a succinct and\nstructured way, but step outside the standard measure-theoretic formalization\nof probability theory. Programs may use both higher-order functions and\ncontinuous distributions, or even define a probability distribution on\nfunctions. But standard probability theory does not handle higher-order\nfunctions well: the category of measurable spaces is not cartesian closed.\n  Here we introduce quasi-Borel spaces. We show that these spaces: form a new\nformalization of probability theory replacing measurable spaces; form a\ncartesian closed category and so support higher-order functions; form a\nwell-pointed category and so support good proof principles for equational\nreasoning; and support continuous probability distributions. We demonstrate the\nuse of quasi-Borel spaces for higher-order functions and probability by:\nshowing that a well-known construction of probability theory involving random\nfunctions gains a cleaner expression; and generalizing de Finetti's theorem,\nthat is a crucial theorem in probability theory, to quasi-Borel spaces.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 12:19:05 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 11:02:46 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 20:02:24 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2020 08:56:11 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Heunen", "Chris", ""], ["Kammar", "Ohad", ""], ["Staton", "Sam", ""], ["Yang", "Hongseok", ""]]}, {"id": "1701.02571", "submitter": "Bassel Mannaa", "authors": "Thierry Coquand and Bassel Mannaa and Fabian Ruch", "title": "Stack Semantics of Type Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a model of dependent type theory with one univalent universe and\npropositional truncation interpreting a type as a stack, generalising the\ngroupoid model of type theory. As an application, we show that countable choice\ncannot be proved in dependent type theory with one univalent universe and\npropositional truncation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 13:09:52 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 19:39:28 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Coquand", "Thierry", ""], ["Mannaa", "Bassel", ""], ["Ruch", "Fabian", ""]]}, {"id": "1701.02623", "submitter": "Andrei Bulatov", "authors": "Andrei A. Bulatov", "title": "Constraint Satisfaction Problems over semilattice block Mal'tsev\n  algebras", "comments": "This version features a different proof of the main result, which\n  uses an approach closer to that in [Andrei A. Bulatov: A dichotomy theorem\n  for nonuniform CSPs. CoRR abs/1703.03021 (2017)], and is much simplified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two well known types of algorithms for solving CSPs: local\npropagation and generating a basis of the solution space. For several years the\nfocus of the CSP research has been on `hybrid' algorithms that somehow combine\nthe two approaches. In this paper we present a new method of such hybridization\nthat allows us to solve certain CSPs that has been out of reach for a quite a\nwhile. We consider these method on a fairly restricted class of CSPs given by\nalgebras we will call semilattice block Mal'tsev. An algebra A is called\nsemilattice block Mal'tsev if it has a binary operation f, a ternary operation\nm, and a congruence s such that the quotient A/s with operation $f$ is a\nsemilattice, $f$ is a projection on every block of s, and every block of s is a\nMal'tsev algebra with Mal'tsev operation m. We show that the constraint\nsatisfaction problem over a semilattice block Mal'tsev algebra is solvable in\npolynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 14:54:48 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 03:36:56 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Bulatov", "Andrei A.", ""]]}, {"id": "1701.02648", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Why Can't You Behave? Non-termination Analysis of Direct Recursive Rules\n  with Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with rule-based programs that go wrong. The unwanted\nbehavior of rule applications is non-termination or failure of a computation.\nWe propose a static program analysis of the non-termination problem for\nrecursion in the Constraint Handling Rules (CHR) language.\n  CHR is an advanced concurrent declarative language involving constraint\nreasoning. It has been closely related to many other rule-based approaches, so\nthe results are of a more general interest. In such languages, non-termination\nis due to infinite applications of recursive rules. Failure is due to\naccumulation of contradicting constraints during the computation.\n  We give theorems with so-called misbehavior conditions for potential\nnon-termination and failure (as well as definite termination) of linear direct\nrecursive simplification rules. Logical relationships between the constraints\nin a recursive rule play a crucial role in this kind of program analysis. We\nthink that our approach can be extended to other types of recursion and to a\nmore general class of rules. Therefore this paper can serve as a basic\nreference and a starting point for further research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:50:12 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1701.02668", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Constraint Handling Rules - What Else?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Handling Rules (CHR) is both an effective concurrent declarative\nconstraint-based programming language and a versatile computational formalism.\nWhile conceptually simple, CHR is distinguished by a remarkable combination of\ndesirable features: - semantic foundation in classical and linear logic, -\neffective and efficient sequential and parallel execution model - guaranteed\nproperties like the anytime online algorithm properties - powerful analysis\nmethods for deciding essential program properties. This overview of CHR\nresearch and applications is by no complete. It concentrates on the years since\n2000. Up-to-date information on CHR can be found at the CHR web-site\nwww.constraint-handling-rules.org, including the slides of the keynote talk\nassociated with this article, an extensive bibliography, online demo versions\nand free downloads of the language.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:29:06 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1701.02673", "submitter": "Micha\\\"el Cadilhac", "authors": "Micha\\\"el Cadilhac, Charles Paperman", "title": "A Crevice on the Crane Beach: Finite-Degree Predicates", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order logic (FO) over words is shown to be equiexpressive with FO\nequipped with a restricted set of numerical predicates, namely the order, a\nbinary predicate MSB$_0$, and the finite-degree predicates: FO[Arb] = FO[<,\nMSB$_0$, Fin].\n  The Crane Beach Property (CBP), introduced more than a decade ago, is true of\na logic if all the expressible languages admitting a neutral letter are\nregular.\n  Although it is known that FO[Arb] does not have the CBP, it is shown here\nthat the (strong form of the) CBP holds for both FO[<, Fin] and FO[<, MSB$_0$].\nThus FO[<, Fin] exhibits a form of locality and the CBP, and can still express\na wide variety of languages, while being one simple predicate away from the\nexpressive power of FO[Arb]. The counting ability of FO[<, Fin] is studied as\nan application.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:40:19 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 15:42:44 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 10:29:16 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Cadilhac", "Micha\u00ebl", ""], ["Paperman", "Charles", ""]]}, {"id": "1701.02682", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "A Devil's Advocate against Termination of Direct Recursion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A devil's advocate is one who argues against a claim, not as a committed\nopponent but in order to determine the validity of the claim. We are interested\nin a devil's advocate that argues against termination of a program. He does so\nby producing a maleficent program that can cause the non-termination of the\noriginal program. By inspecting and running the malicious program, one may gain\ninsight into the potential reasons for non-termination and produce\ncounterexamples for termination.\n  We introduce our method using the concurrent programming language Constraint\nHandling Rules (CHR). Like in other declarative languages, non-termination\noccurs through unbounded recursion. Given a self-recursive rule, we\nautomatically generate one or more devil's rules from it. The construction of\nthe devil's rules is straightforward and involves no guessing. The devil's\nrules can be simple. For example, they are non-recursive for rules with single\nrecursion.\n  We show that the devil's rules are maximally vicious in the following sense:\nFor any program that contains the self-recursive rule and for any infinite\ncomputation through that rule in that program, there is a corresponding\ninfinite computation with the recursive rule and the devil's rules alone. In\nthat case, the malicious rules serve as a finite witness for non-termination.\nOn the other hand, if the devil's rules do not exhibit an infinite computation,\nthe recursive rule is unconditionally terminating. We also identify cases where\nthe static analysis of the devil's rule decides termination or non-termination\nof the recursive rule.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:51:30 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1701.02903", "submitter": "Guillermo P\\'erez", "authors": "Emmanuel Filiot, Isma\\\"el Jecker, Nathan Lhote, Guillermo A. P\\'erez,\n  and Jean-Fran\\c{c}ois Raskin", "title": "On Delay and Regret Determinization of Max-Plus Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decidability of the determinization problem for weighted automata over the\nsemiring $(\\mathbb{Z} \\cup {-\\infty}, \\max, +)$, WA for short, is a\nlong-standing open question. We propose two ways of approaching it by\nconstraining the search space of deterministic WA: k-delay and r-regret. A WA N\nis k-delay determinizable if there exists a deterministic automaton D that\ndefines the same function as N and for all words {\\alpha} in the language of N,\nthe accepting run of D on {\\alpha} is always at most k-away from a maximal\naccepting run of N on {\\alpha}. That is, along all prefixes of the same length,\nthe absolute difference between the running sums of weights of the two runs is\nat most k. A WA N is r-regret determinizable if for all words {\\alpha} in its\nlanguage, its non-determinism can be resolved on the fly to construct a run of\nN such that the absolute difference between its value and the value assigned to\n{\\alpha} by N is at most r.\n  We show that a WA is determinizable if and only if it is k-delay\ndeterminizable for some k. Hence deciding the existence of some k is as\ndifficult as the general determinization problem. When k and r are given as\ninput, the k-delay and r-regret determinization problems are shown to be\nEXPtime-complete. We also show that determining whether a WA is r-regret\ndeterminizable for some r is in EXPtime.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 09:48:53 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 18:52:37 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Filiot", "Emmanuel", ""], ["Jecker", "Isma\u00ebl", ""], ["Lhote", "Nathan", ""], ["P\u00e9rez", "Guillermo A.", ""], ["Raskin", "Jean-Fran\u00e7ois", ""]]}, {"id": "1701.02917", "submitter": "Noam Zeilberger", "authors": "Noam Zeilberger", "title": "A sequent calculus for the Tamari order", "comments": "12 pages + two page appendix with figures, submitted to LICS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sequent calculus with a simple restriction of Lambek's product\nrules that precisely captures the classical Tamari order, i.e., the partial\norder on fully-bracketed words (equivalently, binary trees) induced by a\nsemi-associative law (equivalently, tree rotation). We establish a focusing\nproperty for this sequent calculus (a strengthening of cut-elimination), which\nyields the following coherence theorem: every valid entailment in the Tamari\norder has exactly one focused derivation. One combinatorial application of this\ncoherence theorem is a new proof of the Tutte-Chapoton formula for the number\nof intervals in the Tamari lattice $Y_n$. We also apply the sequent calculus\nand the coherence theorem to build a surprising bijection between intervals of\nthe Tamari order and a certain fragment of lambda calculus, consisting of the\n$\\beta$-normal planar lambda terms with no closed proper subterms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 10:31:04 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Zeilberger", "Noam", ""]]}, {"id": "1701.02927", "submitter": "Sebastian Muskalla", "authors": "Mohamed Faouzi Atig, Roland Meyer, Sebastian Muskalla, Prakash\n  Saivasan", "title": "On the Upward/Downward Closures of Petri Nets", "comments": "The conference version of this paper has been published in the\n  proceedings of the 42nd International Symposium on Mathematical Foundations\n  of Computer Science, MFCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the size and the complexity of computing finite state automata (FSA)\nrepresenting and approximating the downward and the upward closure of Petri net\nlanguages with coverability as the acceptance condition. We show how to\nconstruct an FSA recognizing the upward closure of a Petri net language in\ndoubly-exponential time, and therefore the size is at most doubly exponential.\nFor downward closures, we prove that the size of the minimal automata can be\nnon-primitive recursive. In the case of BPP nets, a well-known subclass of\nPetri nets, we show that an FSA accepting the downward/upward closure can be\nconstructed in exponential time. Furthermore, we consider the problem of\nchecking whether a simple regular language is included in the downward/upward\nclosure of a Petri net/BPP net language. We show that this problem is\nEXPSPACE-complete (resp. NP-complete) in the case of Petri nets (resp. BPP\nnets). Finally, we show that it is decidable whether a Petri net language is\nupward/downward closed. To this end, we prove that one can decide whether a\ngiven regular language is a subset of a Petri net coverability language.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 11:20:31 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 17:05:17 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Atig", "Mohamed Faouzi", ""], ["Meyer", "Roland", ""], ["Muskalla", "Sebastian", ""], ["Saivasan", "Prakash", ""]]}, {"id": "1701.02947", "submitter": "Sebastian Muskalla", "authors": "Roland Meyer, Sebastian Muskalla, Elisabeth Neumann", "title": "Liveness Verification and Synthesis: New Algorithms for Recursive\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of liveness verification and liveness synthesis for\nrecursive programs. The liveness verification problem (LVP) is to decide\nwhether a given omega-context-free language is contained in a given\nomega-regular language. The liveness synthesis problem (LSP) is to compute a\nstrategy so that a given omega-context-free game, when played along the\nstrategy, is guaranteed to derive a word in a given omega-regular language. The\nproblems are known to be EXPTIME-complete and EXPTIME-complete, respectively.\nOur contributions are new algorithms with optimal time complexity. For LVP, we\ngeneralize recent lasso-finding algorithms (also known as Ramsey-based\nalgorithms) from finite to recursive programs. For LSP, we generalize a recent\nsummary-based algorithm from finite to infinite words. Lasso finding and\nsummaries have proven to be efficient in a number of implementations for the\nfinite state and finite word setting.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 12:21:51 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Meyer", "Roland", ""], ["Muskalla", "Sebastian", ""], ["Neumann", "Elisabeth", ""]]}, {"id": "1701.02996", "submitter": "Ventsislav Chonev", "authors": "Ventsislav Chonev", "title": "Reachability in Augmented Interval Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose augmented interval Markov chains (AIMCs): a\ngeneralisation of the familiar interval Markov chains (IMCs) where uncertain\ntransition probabilities are in addition allowed to depend on one another. This\nnew model preserves the flexibility afforded by IMCs for describing stochastic\nsystems where the parameters are unclear, for example due to measurement error,\nbut also allows us to specify transitions with probabilities known to be\nidentical, thereby lending further expressivity.\n  The focus of this paper is reachability in AIMCs. We study the qualitative,\nexact quantitative and approximate reachability problem, as well as natural\nsubproblems thereof, and establish several upper and lower bounds for their\ncomplexity. We prove the exact reachability problem is at least as hard as the\nfamous square-root sum problem, but, encouragingly, the approximate version\nlies in $\\mathbf{NP}$ if the underlying graph is known, whilst the restriction\nof the exact problem to a constant number of uncertain edges is in\n$\\mathbf{P}$. Finally, we show that uncertainty in the graph structure affects\ncomplexity by proving $\\mathbf{NP}$-completeness for the qualitative\nsubproblem, in contrast with an easily-obtained upper bound of $\\mathbf{P}$ for\nthe same subproblem with known graph structure.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 14:57:30 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Chonev", "Ventsislav", ""]]}, {"id": "1701.03297", "submitter": "Murray Elder", "authors": "Volker Diekert and Murray Elder", "title": "Solutions to twisted word equations and equations in virtually free\n  groups", "comments": "70 pages, 13 figures. An extended abstract of a preliminary version\n  of this paper was presented at ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.CC cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the problem solving equations in virtually free groups\ncan be reduced to the problem of solving twisted word equations with regular\nconstraints over free monoids with involution. In this paper we prove that the\nset of all solutions of a twisted word equation is an EDT0L language whose\nspecification can be computed in $\\mathsf{PSPACE}$. Within the same complexity\nbound we can decide whether the solution set is empty, finite, or infinite.\n  In the second part of the paper we apply the results for twisted equations to\nobtain in $\\mathsf{PSPACE}$ an EDT0L description of the solution set of\nequations with rational constraints for finitely generated virtually free\ngroups in standard normal forms with respect to a natural set of generators. If\nthe rational constraints are given by a homomorphism into a fixed (or \"small\nenough\") finite monoid, then our algorithms can be implemented in\n$\\mathsf{NSPACE}(n^2\\log n)$, that is, in quasi-quadratic nondeterministic\nspace.\n  Our results generalize the work by Lohrey and S\\'enizergues (ICALP 2006) and\nDahmani and Guirardel (J. of Topology 2010) with respect to both complexity and\nexpressive power. Neither paper gave any concrete complexity bound and the\nresults in these papers are stated for subsets of solutions only, whereas our\nresults concern all solutions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 10:51:33 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 07:32:48 GMT"}, {"version": "v3", "created": "Sun, 18 Aug 2019 01:31:16 GMT"}, {"version": "v4", "created": "Tue, 20 Aug 2019 08:22:35 GMT"}, {"version": "v5", "created": "Sun, 1 Dec 2019 22:03:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Diekert", "Volker", ""], ["Elder", "Murray", ""]]}, {"id": "1701.03320", "submitter": "EPTCS", "authors": "Ricardo Pe\\~na (Universidad Complutense de Madrid)", "title": "An Introduction to Liquid Haskell", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069", "journal-ref": "EPTCS 237, 2017, pp. 68-80", "doi": "10.4204/EPTCS.237.5", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a tutorial introducing the underlying technology and the use of\nthe tool Liquid Haskell, a type-checker for the functional language Haskell\nthat can help programmers to verify non-trivial properties of their programs\nwith a low effort.\n  The first sections introduce the technology of Liquid Types by explaining its\nprinciples and summarizing how its type inference algorithm manages to prove\nproperties. The remaining sections present a selection of Haskell examples and\nshow the kind of properties that can be proved with the system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 12:05:03 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Pe\u00f1a", "Ricardo", "", "Universidad Complutense de Madrid"]]}, {"id": "1701.03602", "submitter": "Andrea Asperti", "authors": "Andrea Asperti", "title": "Automatic verification and interactive theorem proving", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic verification deals with the validation by means of computers of\ncorrectness certificates. The related tools, usually called proof assistants or\ninteractive provers, provide an interactive environment for the creation of\nformal certificates whose correctness can be assessed in a purely automatic\nway. Such systems have applications both in mathematics, where certificates are\nproofs of theorems, and in computer science, where certificates testify the\ncorrectness of a given software with respect to its specification.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 09:28:04 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Asperti", "Andrea", ""]]}, {"id": "1701.03615", "submitter": "Keehang Kwon", "authors": "Keehang Kwon and Daeseong Kang", "title": "Towards a Decidable LogicWeb via Length-Bounded Derivations", "comments": "3 pages. A novel module language for logic programming is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LogicWeb has traditionally lacked devices for dealing with intractable\nqueries.\n  We address this limitation by adopting length-bounded inference, a form of\napproximate reasoning. A length-bounded inference is of the form $prov(P,G,n)$\nwhich is a success if a query $G$ can be proved from the web page $P$ within\n$n$ proof steps. It thus makes LogicWeb decidable and more tractable. During\nthe process, we propose a novel module language for logic programming as a\ndevice for structuring programs and queries.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 10:27:46 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 13:44:11 GMT"}, {"version": "v3", "created": "Wed, 18 Oct 2017 04:21:26 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Kwon", "Keehang", ""], ["Kang", "Daeseong", ""]]}, {"id": "1701.03670", "submitter": "Luc Dartois", "authors": "Luc Dartois and Emmanuel Filiot and Nathan Lhote", "title": "Logics for Word Transductions with Synthesis", "comments": "10 pages + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a logic, called LT, to express properties of transductions, i.e.\nbinary relations from input to output (finite) words. In LT, the input/output\ndependencies are modelled via an origin function which associates to any\nposition of the output word, the input position from which it originates. LT is\nwell-suited to express relations (which are not necessarily functional), and\ncan express all regular functional transductions, i.e. transductions definable\nfor instance by deterministic two-way transducers. Despite its high expressive\npower, LT has decidable satisfiability and equivalence problems, with tight\nnon-elementary and elementary complexities, depending on specific\nrepresentation of LT-formulas. Our main contribution is a synthesis result:\nfrom any transduction R defined in LT , it is possible to synthesise a regular\nfunctional transduction f such that for all input words u in the domain of R, f\nis defined and (u,f(u)) belongs to R. As a consequence, we obtain that any\nfunctional transduction is regular iff it is LT-definable. We also investigate\nthe algorithmic and expressiveness properties of several extensions of LT, and\nexplicit a correspondence between transductions and data words. As a\nside-result, we obtain a new decidable logic for data words.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 13:57:07 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 12:32:45 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 11:43:43 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 11:35:47 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Dartois", "Luc", ""], ["Filiot", "Emmanuel", ""], ["Lhote", "Nathan", ""]]}, {"id": "1701.03773", "submitter": "Ale\\v{s} Bizjak", "authors": "Tadeusz Litak, Dirk Pattinson, Katsuhiko Sano, Lutz Schr\\\"oder", "title": "Model Theory and Proof Theory of Coalgebraic Predicate Logic", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 1 (March 20,\n  2018) lmcs:4390", "doi": "10.23638/LMCS-14(1:22)2018", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a generalization of first-order logic originating in a neglected\nwork by C.C. Chang: a natural and generic correspondence language for any types\nof structures which can be recast as Set-coalgebras. We discuss axiomatization\nand completeness results for several natural classes of such logics. Moreover,\nwe show that an entirely general completeness result is not possible. We study\nthe expressive power of our language, both in comparison with coalgebraic\nhybrid logics and with existing first-order proposals for special classes of\nSet-coalgebras (apart from relational structures, also neighbourhood frames and\ntopological spaces). Basic model-theoretic constructions and results, in\nparticular ultraproducts, obtain for the two classes that allow\ncompleteness---and in some cases beyond that. Finally, we discuss a basic\nsequent system, for which we establish a syntactic cut-elimination result.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 18:43:56 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 01:48:23 GMT"}, {"version": "v3", "created": "Sat, 3 Mar 2018 08:12:06 GMT"}, {"version": "v4", "created": "Mon, 19 Mar 2018 10:14:38 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Litak", "Tadeusz", ""], ["Pattinson", "Dirk", ""], ["Sano", "Katsuhiko", ""], ["Schr\u00f6der", "Lutz", ""]]}, {"id": "1701.03836", "submitter": "Khaza Anuarul Hoque", "authors": "Khaza Anuarul Hoque, Otmane Ait Mohamed, Yvon Savaria", "title": "Formal Analysis of SEU Mitigation for Early Dependability and\n  Performability Analysis of FPGA-based Space Applications", "comments": "Accepted version for publication in the Journal of Applied Science,\n  Elsevier", "journal-ref": null, "doi": "10.1016/j.jal.2017.03.001", "report-no": null, "categories": "cs.PF cs.AR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SRAM-based FPGAs are increasingly popular in the aerospace industry due to\ntheir field programmability and low cost. However, they suffer from cosmic\nradiation induced Single Event Upsets (SEUs). In safety-critical applications,\nthe dependability of the design is a prime concern since failures may have\ncatastrophic consequences. An early analysis of the relationship between\ndependability metrics, performability-area trade-off, and different mitigation\ntechniques for such applications can reduce the design effort while increasing\nthe design confidence. This paper introduces a novel methodology based on\nprobabilistic model checking, for the analysis of the reliability,\navailability, safety and performance-area tradeoffs of safety-critical systems\nfor early design decisions. Starting from the high-level description of a\nsystem, a Markov reward model is constructed from the Control Data Flow Graph\n(CDFG) and a component characterization library targeting FPGAs. The proposed\nmodel and exhaustive analysis capture all the failure states (based on the\nfault detection coverage) and repairs possible in the system. We present\nquantitative results based on an FIR filter circuit to illustrate the\napplicability of the proposed approach and to demonstrate that a wide range of\nuseful dependability and performability properties can be analyzed using the\nproposed methodology. The modeling results show the relationship between\ndifferent mitigation techniques and fault detection coverage, exposing their\ndirect impact on the design for early decisions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 17:07:36 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 17:21:29 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Hoque", "Khaza Anuarul", ""], ["Mohamed", "Otmane Ait", ""], ["Savaria", "Yvon", ""]]}, {"id": "1701.04086", "submitter": "Barnaby Martin", "authors": "Catarina Carvalho, Barnaby Martin and Dmitriy Zhuk", "title": "The complexity of quantified constraints using the algebraic formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let A be an idempotent algebra on a finite domain. We combine results of\nChen, Zhuk and Carvalho et al. to argue that if A satisfies the polynomially\ngenerated powers property (PGP), then QCSP(Inv(A)) is in NP. We then use the\nresult of Zhuk to prove a converse, that if QCSP(Inv(A)) satisfies the\nexponentially generated powers property (EGP), then QCSP(Inv(A)) is co-NP-hard.\nSince Zhuk proved that only PGP and EGP are possible, we derive a full\ndichotomy for the QCSP, justifying the moral correctness of what we term the\nChen Conjecture.\n  We examine in closer detail the situation for domains of size three. Over any\nfinite domain, the only type of PGP that can occur is switchability.\nSwitchability was introduced by Chen as a generalisation of the already-known\nCollapsibility. For three-element domain algebras A that are Switchable, we\nprove that for every finite subset Delta of Inv(A), Pol(Delta) is Collapsible.\nThe significance of this is that, for QCSP on finite structures (over\nthree-element domain), all QCSP tractability explained by Switchability is\nalready explained by Collapsibility.\n  Finally, we present a three-element domain complexity classification\nvignette, using known as well as derived results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 17:54:19 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 13:05:46 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Carvalho", "Catarina", ""], ["Martin", "Barnaby", ""], ["Zhuk", "Dmitriy", ""]]}, {"id": "1701.04089", "submitter": "Charles Grellois", "authors": "Ugo Dal Lago and Charles Grellois", "title": "Probabilistic Termination by Monadic Affine Sized Typing (Long Version)", "comments": "63 pages. To appear in ESOP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a system of monadic affine sized types, which substantially\ngeneralise usual sized types, and allows this way to capture probabilistic\nhigher-order programs which terminate almost surely. Going beyond plain, strong\nnormalisation without losing soundness turns out to be a hard task, which\ncannot be accomplished without a richer, quantitative notion of types, but also\nwithout imposing some affinity constraints. The proposed type system is\npowerful enough to type classic examples of probabilistically terminating\nprograms such as random walks. The way typable programs are proved to be almost\nsurely terminating is based on reducibility, but requires a substantial\nadaptation of the technique.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 18:04:44 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Grellois", "Charles", ""]]}, {"id": "1701.04240", "submitter": "Andrea Asperti", "authors": "Andrea Asperti", "title": "About the efficient reduction of lambda terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is still a lot of confusion about \"optimal\" sharing in the lambda\ncalculus, and its actual efficiency. In this article, we shall try to clarify\nsome of these issues.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 10:57:03 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Asperti", "Andrea", ""]]}, {"id": "1701.04391", "submitter": "Daniel Selsam", "authors": "Daniel Selsam, Leonardo de Moura", "title": "Congruence Closure in Intensional Type Theory", "comments": "Appeared at International Joint Conference on Automated Reasoning\n  (IJCAR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congruence closure procedures are used extensively in automated reasoning and\nare a core component of most satisfiability modulo theories solvers. However,\nno known congruence closure algorithms can support any of the expressive logics\nbased on intensional type theory (ITT), which form the basis of many\ninteractive theorem provers. The main source of expressiveness in these logics\nis dependent types, and yet existing congruence closure procedures found in\ninteractive theorem provers based on ITT do not handle dependent types at all\nand only work on the simply-typed subsets of the logics. Here we present an\nefficient and proof-producing congruence closure procedure that applies to\nevery function in ITT no matter how many dependencies exist among its\narguments, and that only relies on the commonly assumed uniqueness of identity\nproofs axiom. We demonstrate its usefulness by solving interesting verification\nproblems involving functions with dependent types.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:40:09 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 17:34:51 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Selsam", "Daniel", ""], ["de Moura", "Leonardo", ""]]}, {"id": "1701.04481", "submitter": "EPTCS", "authors": "Paqui Lucio", "title": "A Tutorial on Using Dafny to Construct Verified Software", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069", "journal-ref": "EPTCS 237, 2017, pp. 1-19", "doi": "10.4204/EPTCS.237.1", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a tutorial for newcomers to the field of automated verification\ntools, though we assume the reader to be relatively familiar with Hoare-style\nverification. In this paper, besides introducing the most basic features of the\nlanguage and verifier Dafny, we place special emphasis on how to use Dafny as\nan assistant in the development of verified programs. Our main aim is to\nencourage the software engineering community to make the move towards using\nformal verification tools.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 22:44:47 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Lucio", "Paqui", ""]]}, {"id": "1701.04522", "submitter": "EPTCS", "authors": "Iliano Cervesato, Maribel Fern\\'andez", "title": "Proceedings Fourth International Workshop on Linearity", "comments": null, "journal-ref": "EPTCS 238, 2017", "doi": "10.4204/EPTCS.238", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at LINEARITY 2016, the Fourth\nInternational Workshop on Linearity, held on June 26, 2016 in Porto, Portugal.\nThe workshop was a one-day satellite event of FSCD 2016, the first\nInternational Conference on Formal Structures for Computation and Deduction.\n  The aim of this workshop was to bring together researchers who are developing\ntheory and applications of linear calculi, to foster their interaction and\nprovide a forum for presenting new ideas and work in progress, and enable\nnewcomers to learn about current activities in this area. Of interest were new\nresults that made a central use of linearity, ranging from foundational work to\napplications in any field. This included: sub-linear logics, linear term\ncalculi, linear type systems, linear proof-theory, linear programming\nlanguages, applications to concurrency, interaction-based systems, verification\nof linear systems, and biological and chemical models of computation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 03:45:13 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Cervesato", "Iliano", ""], ["Fern\u00e1ndez", "Maribel", ""]]}, {"id": "1701.04547", "submitter": "Gaoang Bian", "authors": "Gaoang Bian, Alessandro Abate", "title": "On the Relationship between Bisimulation and Trace Equivalence in an\n  Approximate Probabilistic Context (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a notion of approximate probabilistic trace equivalence\nfor labelled Markov chains, and relates this new concept to the known notion of\napproximate probabilistic bisimulation. In particular this work shows that the\nlatter notion induces a tight upper bound on the approximation between\nfinite-horizon traces, as expressed by a total variation distance. As such,\nthis work extends corresponding results for exact notions and analogous results\nfor non-probabilistic models. This bound can be employed to relate the\ncloseness in satisfaction probabilities over bounded linear-time properties,\nand allows for probabilistic model checking of concrete models via\nabstractions. The contribution focuses on both finite-state and\nuncountable-state labelled Markov chains, and claims two main applications:\nfirstly, it allows an upper bound on the trace distance to be decided for\nfinite state systems; secondly, it can be used to synthesise discrete\napproximations to continuous-state models with arbitrary precision.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 06:55:57 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 08:25:54 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 16:56:52 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Bian", "Gaoang", ""], ["Abate", "Alessandro", ""]]}, {"id": "1701.04626", "submitter": "Simone Bova", "authors": "Simone Bova and Stefan Szeider", "title": "Circuit Treewidth, Sentential Decision, and Query Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of a query over a probabilistic database boils down to\ncomputing the probability of a suitable Boolean function, the lineage of the\nquery over the database. The method of query compilation approaches the task in\ntwo stages: first, the query lineage is implemented (compiled) in a circuit\nform where probability computation is tractable; and second, the desired\nprobability is computed over the compiled circuit. A basic theoretical quest in\nquery compilation is that of identifying pertinent classes of queries whose\nlineages admit compact representations over increasingly succinct, tractable\ncircuit classes. Fostering previous work by Jha and Suciu (2012) and Petke and\nRazgon (2013), we focus on queries whose lineages admit circuit implementations\nwith small treewidth, and investigate their compilability within tame classes\nof decision diagrams. In perfect analogy with the characterization of bounded\ncircuit pathwidth by bounded OBDD width, we show that a class of Boolean\nfunctions has bounded circuit treewidth if and only if it has bounded SDD\nwidth. Sentential decision diagrams (SDDs) are central in knowledge\ncompilation, being essentially as tractable as OBDDs but exponentially more\nsuccinct. By incorporating constant width SDDs and polynomial size SDDs, we\nrefine the panorama of query compilation for unions of conjunctive queries with\nand without inequalities.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 11:34:07 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Bova", "Simone", ""], ["Szeider", "Stefan", ""]]}, {"id": "1701.04642", "submitter": "Zvonko Iljazovi\\'c", "authors": "Zvonko Iljazovi\\'c and Igor Su\\v{s}i\\'c", "title": "Computability of semicomputable manifolds in computable topological\n  spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study computable topological spaces and semicomputable and computable sets\nin these spaces. In particular, we investigate conditions under which\nsemicomputable sets are computable. We prove that a semicomputable compact\nmanifold $M$ is computable if its boundary $\\partial M$ is computable. We also\nshow how this result combined with certain construction which compactifies a\nsemicomputable set leads to the conclusion that some noncompact semicomputable\nmanifolds in computable metric spaces are computable.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 12:20:52 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Iljazovi\u0107", "Zvonko", ""], ["Su\u0161i\u0107", "Igor", ""]]}, {"id": "1701.04691", "submitter": "Anton Salikhmetov", "authors": "Anton Salikhmetov", "title": "Optimal Reduction without Oracle?", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest an approach to solve the problem of matching fans in interaction\nnet implementations of optimal reduction for the pure untyped lambda calculus\nwithout use of any additional agent types. Our implementation supports a wider\nclass of lambda terms than the abstract version of Lamping's algorithm and\nbeats the interaction net implementation of closed reduction by the total\nnumber of interactions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 14:27:12 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 15:30:18 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Salikhmetov", "Anton", ""]]}, {"id": "1701.04732", "submitter": "Thorsten Wissmann", "authors": "Aleks Kissinger, Sander Uijlen", "title": "A categorical semantics for causal structure", "comments": "Extended version of a LICS 2017 paper with the same title", "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 3 (August 9,\n  2019) lmcs:5681", "doi": "10.23638/LMCS-15(3:15)2019", "report-no": null, "categories": "quant-ph cs.LO math-ph math.CT math.MP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a categorical construction for modelling causal structures within\na general class of process theories that include the theory of classical\nprobabilistic processes as well as quantum theory. Unlike prior constructions\nwithin categorical quantum mechanics, the objects of this theory encode\nfine-grained causal relationships between subsystems and give a new method for\nexpressing and deriving consequences for a broad class of causal structures. We\nshow that this framework enables one to define families of processes which are\nconsistent with arbitrary acyclic causal orderings. In particular, one can\ndefine one-way signalling (a.k.a. semi-causal) processes, non-signalling\nprocesses, and quantum $n$-combs. Furthermore, our framework is general enough\nto accommodate recently-proposed generalisations of classical and quantum\ntheory where processes only need to have a fixed causal ordering locally, but\nglobally allow indefinite causal ordering.\n  To illustrate this point, we show that certain processes of this kind, such\nas the quantum switch, the process matrices of Oreshkov, Costa, and Brukner,\nand a classical three-party example due to Baumeler, Feix, and Wolf are all\ninstances of a certain family of processes we refer to as $\\textrm{SOC}_n$ in\nthe appropriate category of higher-order causal processes. After defining these\nfamilies of causal structures within our framework, we give derivations of\ntheir operational behaviour using simple, diagrammatic axioms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 15:42:22 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 16:55:29 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 15:10:30 GMT"}, {"version": "v4", "created": "Wed, 4 Apr 2018 22:10:16 GMT"}, {"version": "v5", "created": "Wed, 23 Jan 2019 13:39:35 GMT"}, {"version": "v6", "created": "Thu, 8 Aug 2019 14:52:58 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kissinger", "Aleks", ""], ["Uijlen", "Sander", ""]]}, {"id": "1701.04786", "submitter": "Ugo Dal Lago", "authors": "Flavien Breuvart, Ugo Dal Lago, Agathe Herrou", "title": "On Higher-Order Probabilistic Subrecursion (Long Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the expressive power of subrecursive probabilistic higher-order\ncalculi. More specifically, we show that endowing a very expressive\ndeterministic calculus like G\\\"odel's $\\mathbb{T}$ with various forms of\nprobabilistic choice operators may result in calculi which are not equivalent\nas for the class of distributions they give rise to, although they all\nguarantee almost-sure termination. Along the way, we introduce a probabilistic\nvariation of the classic reducibility technique, and we prove that the simplest\nform of probabilistic choice leaves the expressive power of $\\mathbb{T}$\nessentially unaltered. The paper ends with some observations about functional\nexpressivity: expectedly, all the considered calculi capture precisely the\nfunctions which $\\mathbb{T}$ itself represents, at least when standard notions\nof observations are considered.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 17:55:36 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 22:04:37 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 14:42:58 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 12:18:49 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Breuvart", "Flavien", ""], ["Lago", "Ugo Dal", ""], ["Herrou", "Agathe", ""]]}, {"id": "1701.04915", "submitter": "EPTCS", "authors": "Quentin Heath (LIX, Ecole Polytechnique), Dale Miller (Inria Saclay\n  and LIX, Ecole Polytechnique)", "title": "A Proof Theory for Model Checking: An Extended Abstract", "comments": "In Proceedings LINEARITY 2016, arXiv:1701.04522", "journal-ref": "EPTCS 238, 2017, pp. 1-10", "doi": "10.4204/EPTCS.238.1", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While model checking has often been considered as a practical alternative to\nbuilding formal proofs, we argue here that the theory of sequent calculus\nproofs can be used to provide an appealing foundation for model checking. Since\nthe emphasis of model checking is on establishing the truth of a property in a\nmodel, we rely on the proof theoretic notion of additive inference rules, since\nsuch rules allow provability to directly describe truth conditions.\nUnfortunately, the additive treatment of quantifiers requires inference rules\nto have infinite sets of premises and the additive treatment of model\ndescriptions provides no natural notion of state exploration. By employing a\nfocused proof system, it is possible to construct large scale, synthetic rules\nthat also qualify as additive but contain elements of multiplicative inference.\nThese additive synthetic rules -- essentially rules built from the description\nof a model -- allow a direct treatment of state exploration. This proof\ntheoretic framework provides a natural treatment of reachability and\nnon-reachability problems, as well as tabled deduction, bisimulation, and\nwinning strategies.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:29:28 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Heath", "Quentin", "", "LIX, Ecole Polytechnique"], ["Miller", "Dale", "", "Inria Saclay\n  and LIX, Ecole Polytechnique"]]}, {"id": "1701.04916", "submitter": "EPTCS", "authors": "Antoine Allioux (Institut de Recherche en Informatique Fondamentale,\n  Paris, France)", "title": "Krivine Machine and Taylor Expansion in a Non-uniform Setting", "comments": "In Proceedings LINEARITY 2016, arXiv:1701.04522", "journal-ref": "EPTCS 238, 2017, pp. 24-32", "doi": "10.4204/EPTCS.238.3", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Krivine machine is an abstract machine implementing the linear head\nreduction of lambda-calculus. Ehrhard and Regnier gave a resource sensitive\nversion returning the annotated form of a lambda-term accounting for the\nresources used by the linear head reduction. These annotations take the form of\nterms in the resource lambda-calculus.\n  We generalize this resource-driven Krivine machine to the case of the\nalgebraic lambda-calculus. The latter is an extension of the pure\nlambda-calculus allowing for the linear combination of lambda-terms with\ncoefficients taken from a semiring. Our machine associates a lambda-term M and\na resource annotation t with a scalar k in the semiring describing some\nquantitative properties of the linear head reduction of M.\n  In the particular case of non-negative real numbers and of algebraic terms M\nrepresenting probability distributions, the coefficient k gives the probability\nthat the linear head reduction actually uses exactly the resources annotated by\nt. In the general case, we prove that the coefficient k can be recovered from\nthe coefficient of t in the Taylor expansion of M and from the normal form of\nt.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:30:26 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Allioux", "Antoine", "", "Institut de Recherche en Informatique Fondamentale,\n  Paris, France"]]}, {"id": "1701.04917", "submitter": "EPTCS", "authors": "Lawrence Dunn (North Florida Community College), Jamie Vicary\n  (University of Oxford)", "title": "Surface Proofs for Nonsymmetric Linear Logic (Extended Abstract)", "comments": "In Proceedings LINEARITY 2016, arXiv:1701.04522", "journal-ref": "EPTCS 238, 2017, pp. 33-43", "doi": "10.4204/EPTCS.238.4", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a proof in multiplicative linear logic can be represented as a\ndecorated surface, such that two proofs are logically equivalent just when\ntheir surfaces are geometrically equivalent. This is an extended abstract for\narXiv:1601.05372.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:30:43 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Dunn", "Lawrence", "", "North Florida Community College"], ["Vicary", "Jamie", "", "University of Oxford"]]}, {"id": "1701.04918", "submitter": "EPTCS", "authors": "Stefano Guerrini (LIPN, Institut Galil\\'ee, Universit\\'e Paris Nord\n  13, Sorbonne Paris Cit\\'e)", "title": "Linear $\\beta$-reduction", "comments": "In Proceedings LINEARITY 2016, arXiv:1701.04522", "journal-ref": "EPTCS 238, 2017, pp. 44-53", "doi": "10.4204/EPTCS.238.5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear head reduction is a key tool for the analysis of reduction machines\nfor lambda-calculus and for game semantics. Its definition requires a notion of\nredex at a distance named primary redex in the literature. Nevertheless, a\nclear and complete syntactic analysis of this rule is missing. We present here\na general notion of beta-reduction at a distance and of linear reduction (i.e.,\nnot restricted to the head variable), and we analyse their relations and\nproperties. This analysis rests on a variant of the so-called sigma-equivalence\nthat is more suitable for the analysis of reduction machines, since the\nposition along the spine of primary redexes is not permuted. We finally show\nthat, in the simply typed case, the proof of strong normalisation of linear\nreduction can be obtained by a trivial tuning of Gandy's proof for strong\nnormalisation of beta-reduction.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:31:21 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Guerrini", "Stefano", "", "LIPN, Institut Galil\u00e9e, Universit\u00e9 Paris Nord\n  13, Sorbonne Paris Cit\u00e9"]]}, {"id": "1701.04919", "submitter": "EPTCS", "authors": "Masahito Hasegawa (RIMS, Kyoto University)", "title": "Linear Exponential Comonads without Symmetry", "comments": "In Proceedings LINEARITY 2016, arXiv:1701.04522", "journal-ref": "EPTCS 238, 2017, pp. 54-63", "doi": "10.4204/EPTCS.238.6", "report-no": null, "categories": "cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of linear exponential comonads on symmetric monoidal categories\nhas been used for modelling the exponential modality of linear logic. In this\npaper we introduce linear exponential comonads on general (possibly\nnon-symmetric) monoidal categories, and show some basic results on them.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:31:41 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Hasegawa", "Masahito", "", "RIMS, Kyoto University"]]}, {"id": "1701.04920", "submitter": "EPTCS", "authors": "Miguel Silva, M\\'ario Florido, Frank Pfenning", "title": "Non-Blocking Concurrent Imperative Programming with Session Types", "comments": "In Proceedings LINEARITY 2016, arXiv:1701.04522", "journal-ref": "EPTCS 238, 2017, pp. 64-72", "doi": "10.4204/EPTCS.238.7", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent C0 is an imperative programming language in the C family with\nsession-typed message-passing concurrency. The previously proposed semantics\nimplements asynchronous (non-blocking) output; we extend it here with\nnon-blocking input. A key idea is to postpone message reception as much as\npossible by interpreting receive commands as a request for a message. We\nimplemented our ideas as a translation from a blocking intermediate language to\na non-blocking language. Finally, we evaluated our techniques with several\nbenchmark programs and show the results obtained. While the abstract measure of\nspan always decreases (or remains unchanged), only a few of the examples reap a\npractical benefit.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:31:58 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Silva", "Miguel", ""], ["Florido", "M\u00e1rio", ""], ["Pfenning", "Frank", ""]]}, {"id": "1701.05063", "submitter": "Joelle Despeyroux", "authors": "Jo\\\"elle Despeyroux (CRISAM)", "title": "(Mathematical) Logic for Systems Biology (Invited Paper)", "comments": null, "journal-ref": "Computational Methods in Systems Biology, Sep 2016, Cambridge,\n  United Kingdom. pp.3 - 12, 2016", "doi": "10.1007/978-3-319-45177-0_1", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocates here the use of (mathematical) logic for systems biology, as a\nunified framework well suited for both modeling the dynamic behaviour of\nbiological systems, expressing properties of them, and verifying these\nproperties. The potential candidate logics should have a traditional proof\ntheoretic pedigree (including a sequent calculus presentation enjoying\ncut-elimination and focusing), and should come with (certified) proof tools.\nBeyond providing a reliable framework, this allows the adequate encodings of\nour biological systems. We present two candidate logics (two modal extensions\nof linear logic, called HyLL and SELL), along with biological examples. The\nexamples we have considered so far are very simple ones-coming with completely\nformal (interactive) proofs in Coq. Future works includes using automatic\nprovers, which would extend existing automatic provers for linear logic. This\nshould enable us to specify and study more realistic examples in systems\nbiology, biomedicine (diagnosis and prognosis), and eventually neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 13:59:49 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Despeyroux", "Jo\u00eblle", "", "CRISAM"]]}, {"id": "1701.05138", "submitter": "Mojtaba Aghaei", "authors": "Mojtaba Aghaei, Maryam Rostami Giv", "title": "Rejecting inadmissible rules in reduced normal forms in S4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods for checking admissibility of rules in the modal logic $S4$\nare presented in [1], [15]. These methods determine admissibility of rules in\n$S4$, but they don't determine or give substitutions rejecting inadmissible\nrules. In this paper, we investigate some relations between one of the above\nmethods, based on the reduced normal form rules, and sets of substitutions\nwhich reject them. We also generalize the method in [1], [15] for one rule to\nadmissibility of a set of rules.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:37:06 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Aghaei", "Mojtaba", ""], ["Giv", "Maryam Rostami", ""]]}, {"id": "1701.05251", "submitter": "Andrea Condoluci", "authors": "Andrea Condoluci", "title": "CERES in Propositional Proof Schemata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cut-elimination is one of the most famous problems in proof theory, and it\nwas defined and solved for first-order sequent calculus by Gentzen in his\ncelebrated Hauptsatz. Ceres is a different cut-elimination algorithm for first-\nand higher-order classical logic. Ceres was extended to proof schemata, which\nare templates for usual first-order proofs, with parameters for natural\nnumbers. However, while Ceres is known to be a complete cut-elimination\nalgorithm for first-order logic, it is not clear whether this holds for\nfirst-order schemata too: given in input a proof schema with cuts, does Ceres\nalways produce a schema for a cut-free proof? The difficult step is finding and\nrepresenting an appropriate refutation schema for the characteristic term\nschema of a proof schema. In this thesis, we progress in solving this problem\nby restricting Ceres to propositional schemata, which are templates for\npropositional proofs. By limiting adequately the expressivity of propositional\nschemata and proof schemata, we aim at providing a version of schematic Ceres\nwhich is a complete cut-elimination algorithm for propositional schemata. We\nfocus on one particular step of Ceres: resolution refutation schemata. First,\nwe prove that by naively adapting Ceres for first-order schemata to our case,\nwe end up with an incomplete algorithm. Then, we modify slightly the concept of\nresolution refutation schema: to refute a clause set, first we bring it to a\ngeneric form, and then we use a fixed refutation of that generic clause set.\nOur variation of schematic Ceres is the first step towards completeness with\nrespect to propositional schemata.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 22:35:12 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Condoluci", "Andrea", ""]]}, {"id": "1701.05303", "submitter": "Pawe{\\l} Parys", "authors": "Pawe{\\l} Parys", "title": "Intersection Types and Counting", "comments": "Full version (with appendix) of a conference paper from Eighth\n  Workshop on Intersection Types and Related Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the following meta-problem: given a quantitative\nproperty of trees, design a type system such that the desired property for the\ntree generated by an infinitary ground $\\lambda$-term corresponds to some\nproperty of a derivation of a type for this $\\lambda$-term, in this type\nsystem.\n  Our approach is presented in the particular case of the language finiteness\nproblem for nondeterministic higher-order recursion schemes (HORSes): given a\nnondeterministic HORS, decide whether the set of all finite trees generated by\nthis HORS is finite. We give a type system such that the HORS can generate a\ntree of an arbitrarily large finite size if and only if in the type system we\ncan obtain derivations that are arbitrarily large, in an appropriate sense; the\nlatter condition can be easily decided.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 05:33:29 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Parys", "Pawe\u0142", ""]]}, {"id": "1701.05324", "submitter": "Ross Horne", "authors": "Ki Yung Ahn, Ross Horne, Alwen Tiu", "title": "A Characterisation of Open Bisimilarity using an Intuitionistic Modal\n  Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open bisimilarity is defined for open process terms in which free variables\nmay appear. The insight is, in order to characterise open bisimilarity, we move\nto the setting of intuitionistic modal logics. The intuitionistic modal logic\nintroduced, called $\\mathcal{OM}$, is such that modalities are closed under\nsubstitutions, which induces a property known as intuitionistic hereditary.\nIntuitionistic hereditary reflects in logic the lazy instantiation of free\nvariables performed when checking open bisimilarity. The soundness proof for\nopen bisimilarity with respect to our intuitionistic modal logic is mechanised\nin Abella. The constructive content of the completeness proof provides an\nalgorithm for generating distinguishing formulae, which we have implemented. We\ndraw attention to the fact that there is a spectrum of bisimilarity congruences\nthat can be characterised by intuitionistic modal logics.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 08:11:56 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 18:35:07 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 12:05:34 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 07:08:02 GMT"}, {"version": "v5", "created": "Thu, 6 May 2021 10:16:47 GMT"}, {"version": "v6", "created": "Mon, 19 Jul 2021 09:49:05 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ahn", "Ki Yung", ""], ["Horne", "Ross", ""], ["Tiu", "Alwen", ""]]}, {"id": "1701.05382", "submitter": "Cynthia Kop", "authors": "Cynthia Kop and Jakob Grue Simonsen", "title": "The Power of Non-Determinism in Higher-Order Implicit Complexity", "comments": "pre-edition version of a paper accepted for publication at ESOP'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the power of non-determinism in purely functional programming\nlanguages with higher-order types. Specifically, we consider cons-free programs\nof varying data orders, equipped with explicit non-deterministic choice.\nCons-freeness roughly means that data constructors cannot occur in function\nbodies and all manipulation of storage space thus has to happen indirectly\nusing the call stack.\n  While cons-free programs have previously been used by several authors to\ncharacterise complexity classes, the work on non-deterministic programs has\nalmost exclusively considered programs of data order 0. Previous work has shown\nthat adding explicit non-determinism to cons-free programs taking data of order\n0 does not increase expressivity; we prove that this - dramatically - is not\nthe case for higher data orders: adding non-determinism to programs with data\norder at least 1 allows for a characterisation of the entire class of\nelementary-time decidable sets.\n  Finally we show how, even with non-deterministic choice, the original\nhierarchy of characterisations is restored by imposing different restrictions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 12:01:51 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 15:37:00 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Kop", "Cynthia", ""], ["Simonsen", "Jakob Grue", ""]]}, {"id": "1701.05389", "submitter": "Joachim Klein", "authors": "Christel Baier and Joachim Klein and Sascha Kl\\\"uppelholz and Sascha\n  Wunderlich", "title": "Maximizing the Conditional Expected Reward for Reaching the Goal", "comments": "103 pages, extended version with appendices of a paper accepted at\n  TACAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of computing maximal conditional expected\naccumulated rewards until reaching a target state (briefly called maximal\nconditional expectations) in finite-state Markov decision processes where the\ncondition is given as a reachability constraint. Conditional expectations of\nthis type can, e.g., stand for the maximal expected termination time of\nprobabilistic programs with non-determinism, under the condition that the\nprogram eventually terminates, or for the worst-case expected penalty to be\npaid, assuming that at least three deadlines are missed. The main results of\nthe paper are (i) a polynomial-time algorithm to check the finiteness of\nmaximal conditional expectations, (ii) PSPACE-completeness for the threshold\nproblem in acyclic Markov decision processes where the task is to check whether\nthe maximal conditional expectation exceeds a given threshold, (iii) a\npseudo-polynomial-time algorithm for the threshold problem in the general\n(cyclic) case, and (iv) an exponential-time algorithm for computing the maximal\nconditional expectation and an optimal scheduler.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 12:25:40 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Baier", "Christel", ""], ["Klein", "Joachim", ""], ["Kl\u00fcppelholz", "Sascha", ""], ["Wunderlich", "Sascha", ""]]}, {"id": "1701.05408", "submitter": "Thorsten Wissmann", "authors": "Ruggero Pagnan", "title": "Ologisms", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 3 (August\n  31, 2018) lmcs:4793", "doi": "10.23638/LMCS-14(3:12)2018", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce ologisms. They generate from ologs by extending their logical\nexpressivity, from the possibility of considering constraints of equational\nnature only to the possibility of considering constraints of syllogistic\nnature, in addition. This is obtained by taking advantage of the peculiar\nfeatures of an original diagrammatic logical calculus for the syllogistic, that\nmake it well-behaved with respect to the design of ologs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 13:30:10 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 08:24:31 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 10:03:40 GMT"}, {"version": "v4", "created": "Wed, 1 Aug 2018 11:27:06 GMT"}, {"version": "v5", "created": "Thu, 30 Aug 2018 09:53:40 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Pagnan", "Ruggero", ""]]}, {"id": "1701.05463", "submitter": "Artem Khyzha", "authors": "Artem Khyzha, Mike Dodds, Alexey Gotsman, Matthew Parkinson", "title": "Proving Linearizability Using Partial Orders (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability is the commonly accepted notion of correctness for concurrent\ndata structures. It requires that any execution of the data structure is\njustified by a linearization --- a linear order on operations satisfying the\ndata structure's sequential specification. Proving linearizability is often\nchallenging because an operation's position in the linearization order may\ndepend on future operations. This makes it very difficult to incrementally\nconstruct the linearization in a proof.\n  We propose a new proof method that can handle data structures with such\nfuture-dependent linearizations. Our key idea is to incrementally construct not\na single linear order of operations, but a partial order that describes\nmultiple linearizations satisfying the sequential specification. This allows\ndecisions about the ordering of operations to be delayed, mirroring the\nbehaviour of data structure implementations. We formalise our method as a\nprogram logic based on rely-guarantee reasoning, and demonstrate its\neffectiveness by verifying several challenging data structures: the\nHerlihy-Wing queue, the TS queue and the Optimistic set.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:13:14 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 15:58:01 GMT"}, {"version": "v3", "created": "Wed, 28 Jun 2017 23:04:36 GMT"}, {"version": "v4", "created": "Thu, 6 Jul 2017 13:35:08 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Khyzha", "Artem", ""], ["Dodds", "Mike", ""], ["Gotsman", "Alexey", ""], ["Parkinson", "Matthew", ""]]}, {"id": "1701.05487", "submitter": "Martin Grohe", "authors": "Martin Grohe and Martin Ritzert", "title": "Learning first-order definable concepts over structures of small degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a declarative framework for machine learning where concepts and\nhypotheses are defined by formulas of a logic over some background structure.\nWe show that within this framework, concepts defined by first-order formulas\nover a background structure of at most polylogarithmic degree can be learned in\npolylogarithmic time in the \"probably approximately correct\" learning sense.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:48:11 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Grohe", "Martin", ""], ["Ritzert", "Martin", ""]]}, {"id": "1701.05521", "submitter": "Ugo Dal Lago", "authors": "Rapha\\\"elle Crubill\\'e, Ugo Dal Lago", "title": "Metric Reasoning About $\\lambda$-Terms: The General Case (Long Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any setting in which observable properties have a quantitative flavour, it\nis natural to compare computational objects by way of \\emph{metrics} rather\nthan equivalences or partial orders. This holds, in particular, for\nprobabilistic higher-order programs. A natural notion of comparison, then,\nbecomes context distance, the metric analogue of Morris' context equivalence.\nIn this paper, we analyze the main properties of the context distance in\nfully-fledged probabilistic $\\lambda$-calculi, this way going beyond the state\nof the art, in which only affine calculi were considered. We first of all study\nto which extent the context distance trivializes, giving a sufficient condition\nfor trivialization. We then characterize context distance by way of a\ncoinductively defined, tuple-based notion of distance in one of those calculi,\ncalled $\\Lambda^\\oplus_!$. We finally derive pseudometrics for call-by-name and\ncall-by-value probabilistic $\\lambda$-calculi, and prove them fully-abstract.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:38:51 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Crubill\u00e9", "Rapha\u00eblle", ""], ["Lago", "Ugo Dal", ""]]}, {"id": "1701.05617", "submitter": "Auke Booij", "authors": "Auke Bart Booij, Mart\\'in H\\\"otzel Escard\\'o, Peter LeFanu Lumsdaine,\n  Michael Shulman", "title": "Parametricity, automorphisms of the universe, and excluded middle", "comments": "15 pages, to appear in post-proceedings of TYPES 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that one can construct non-parametric functions by assuming\nclassical axioms. Our work is a converse to that: we prove classical axioms in\ndependent type theory assuming specific instances of non-parametricity. We also\naddress the interaction between classical axioms and the existence of\nautomorphisms of a type universe. We work over intensional Martin-L\\\"of\ndependent type theory, and in some results assume further principles including\nfunction extensionality, propositional extensionality, propositional\ntruncation, and the univalence axiom.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 21:57:36 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 12:14:15 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Booij", "Auke Bart", ""], ["Escard\u00f3", "Mart\u00edn H\u00f6tzel", ""], ["Lumsdaine", "Peter LeFanu", ""], ["Shulman", "Michael", ""]]}, {"id": "1701.05738", "submitter": "Jan K\\v{r}et\\'insk\\'y", "authors": "Jan K\\v{r}et\\'insk\\'y and Tobias Meggendorfer and Clara Waldmann and\n  Maximilian Weininger", "title": "Index appearance record for transforming Rabin automata into parity\n  automata", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-54577-5_26", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transforming deterministic $\\omega$-automata into deterministic parity\nautomata is traditionally done using variants of appearance records. We present\na more efficient variant of this approach, tailored to Rabin automata, and\nseveral optimizations applicable to all appearance records. We compare the\nmethods experimentally and find out that our method produces smaller automata\nthan previous approaches. Moreover, the experiments demonstrate the potential\nof our method for LTL synthesis, using LTL-to-Rabin translators. It leads to\nsignificantly smaller parity automata when compared to state-of-the-art\napproaches on complex formulae.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 09:52:01 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["K\u0159et\u00ednsk\u00fd", "Jan", ""], ["Meggendorfer", "Tobias", ""], ["Waldmann", "Clara", ""], ["Weininger", "Maximilian", ""]]}, {"id": "1701.05888", "submitter": "Joseph Tassarotti", "authors": "Joseph Tassarotti, Ralf Jung, Robert Harper", "title": "A Higher-Order Logic for Concurrent Termination-Preserving Refinement", "comments": "78 pages, extended version of a conference paper for ESOP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compiler correctness proofs for higher-order concurrent languages are\ndifficult: they involve establishing a termination-preserving refinement\nbetween a concurrent high-level source language and an implementation that uses\nlow-level shared memory primitives. However, existing logics for proving\nconcurrent refinement either neglect properties such as termination, or only\nhandle first-order state. In this paper, we address these limitations by\nextending Iris, a recent higher-order concurrent separation logic, with support\nfor reasoning about termination-preserving refinements. To demonstrate the\npower of these extensions, we prove the correctness of an efficient\nimplementation of a higher-order, session-typed language. To our knowledge,\nthis is the first program logic capable of giving a compiler correctness proof\nfor such a language. The soundness of our extensions and our compiler\ncorrectness proof have been mechanized in Coq.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 18:42:43 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Tassarotti", "Joseph", ""], ["Jung", "Ralf", ""], ["Harper", "Robert", ""]]}, {"id": "1701.06103", "submitter": "Jan K\\v{r}et\\'insk\\'y", "authors": "Javier Esparza and Jan K\\v{r}et\\'insk\\'y and Jean-Fran\\c{c}ois Raskin\n  and Salomon Sickert", "title": "From LTL and Limit-Deterministic B\\\"uchi Automata to Deterministic\n  Parity Automata", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-54577-5_25", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controller synthesis for general linear temporal logic (LTL) objectives is a\nchallenging task. The standard approach involves translating the LTL objective\ninto a deterministic parity automaton (DPA) by means of the Safra-Piterman\nconstruction. One of the challenges is the size of the DPA, which often grows\nvery fast in practice, and can reach double exponential size in the length of\nthe LTL formula. In this paper we describe a single exponential translation\nfrom limit-deterministic B\\\"uchi automata (LDBA) to DPA, and show that it can\nbe concatenated with a recent efficient translation from LTL to LDBA to yield a\ndouble exponential, \\enquote{Safraless} LTL-to-DPA construction. We also report\non an implementation, a comparison with the SPOT library, and performance on\nseveral sets of formulas, including instances from the 2016 SyntComp\ncompetition.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 23:40:49 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Esparza", "Javier", ""], ["K\u0159et\u00ednsk\u00fd", "Jan", ""], ["Raskin", "Jean-Fran\u00e7ois", ""], ["Sickert", "Salomon", ""]]}, {"id": "1701.06142", "submitter": "Thorsten Wi{\\ss}mann", "authors": "Franco Barbanera and Ugo de'Liguoro", "title": "Retractability, games and orchestrators for session contracts", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 3 (August\n  25, 2017) lmcs:3880", "doi": "10.23638/LMCS-13(3:15)2017", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session contracts is a formalism enabling to investigate client/server\ninteraction protocols and to interpret session types. We extend session\ncontracts in order to represent outputs whose actual sending in an interaction\ndepends on a third party or on a mutual agreement between the partners. Such\ncontracts are hence adaptable, or as we say \"affectible\". In client/server\nsystems, in general, compliance stands for the satisfaction of all client's\nrequests by the server. We define an abstract notion of \"affectible compliance\"\nand show it to have a precise three-party game-theoretic interpretation. This\nin turn is shown to be equivalent to a compliance based on interactions that\ncan undergo a sequence of failures and rollbacks, as well as to a compliance\nbased on interactions which can be mediated by an orchestrator. Besides, there\nis a one-to-one effective correspondence between winning strategies and\norchestrators. The relation of subcontract for affectible contracts is also\ninvestigated.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 09:41:38 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 19:21:47 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 12:06:53 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Barbanera", "Franco", ""], ["de'Liguoro", "Ugo", ""]]}, {"id": "1701.06209", "submitter": "Xiaobin Zhang", "authors": "Xiaobin Zhang, Bo Wu, and Hai Lin", "title": "Counterexample-guided Abstraction Refinement for POMDPs", "comments": "corrected typos; updated refinement algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially Observable Markov Decision Process (POMDP) is widely used to model\nprobabilistic behavior for complex systems. Compared with MDPs, POMDP models a\nsystem more accurate but solving a POMDP generally takes exponential time in\nthe size of its state space. This makes the formal verification and synthesis\nproblems much more challenging for POMDPs, especially when multiple system\ncomponents are involved. As a promising technique to reduce the verification\ncomplexity, the abstraction method tries to find an abstract system with a\nsmaller state space but preserves enough properties for the verification\npurpose. While abstraction based verification has been explored extensively for\nMDPs, in this paper, we present the first result of POMDP abstraction and its\nrefinement techniques. The main idea follows the counterexample-guided\nabstraction refinement (CEGAR) framework. Starting with a coarse guess for the\nPOMDP abstraction, we iteratively use counterexamples from formal verification\nto refine the abstraction until the abstract system can be used to infer the\nverification result for the original POMDP. Our main contributions have two\nfolds: 1) we propose a novel abstract system model for POMDP and a new\nsimulation relation to capture the partial observability then prove the\npreservation on a fragment of Probabilistic Computation Tree Logic (PCTL); 2)\nto find a proper abstract system that can prove or disprove the satisfaction\nrelation on the concrete POMDP, we develop a novel refinement algorithm. Our\nwork leads to a sound and complete CEGAR framework for POMDP.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 19:20:00 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 05:34:26 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 22:07:15 GMT"}, {"version": "v4", "created": "Fri, 10 Mar 2017 03:07:19 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Zhang", "Xiaobin", ""], ["Wu", "Bo", ""], ["Lin", "Hai", ""]]}, {"id": "1701.06244", "submitter": "Kristina Sojakova", "authors": "Patricia Johann and Kristina Sojakova", "title": "Cubical Categories for Higher-Dimensional Parametricity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reynolds' theory of relational parametricity formalizes parametric\npolymorphism for System F, thus capturing the idea that polymorphically typed\nSystem F programs always map related inputs to related results. This paper\nshows that Reynolds' theory can be seen as the instantiation at dimension 1 of\na theory of relational parametricity for System F that holds at all higher\ndimensions, including infinite dimension. This theory is formulated in terms of\nthe new notion of a p-dimensional cubical category, which we use to define a\np-dimensional parametric model of System F for any p, where p is a natural\nnumber or infinity. We show that every p-dimensional parametric model of System\nF yields a split $\\lambda$ 2-fibration in which types are interpreted as face\nmap- and degeneracy-preserving cubical functors and terms are interpreted as\nface map- and degeneracy-preserving cubical natural transformations. We\ndemonstrate that our theory is \"good\" by showing that the PER model of\nBainbridge et al. is derivable as another 1-dimensional instance, and that all\ninstances at all dimensions derive higher-dimensional analogues of expected\nresults for parametric models, such as a Graph Lemma and the existence of\ninitial algebras and final coalgebras. Finally, our technical development\nresolves a number of significant technical issues arising in Ghani et al.'s\nrecent bifibrational treatment of relational parametricity, which allows us to\nclarify their approach and strengthen their main result. Once clarified, their\nbifibrational framework, too, can be seen as a 1-dimensional instance of our\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 01:16:43 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Johann", "Patricia", ""], ["Sojakova", "Kristina", ""]]}, {"id": "1701.06282", "submitter": "Tomas Fiedor", "authors": "Tom\\'a\\v{s} Fiedor, Luk\\'a\\v{s} Hol\\'ik, Petr Jank\\r{u}, Ond\\v{r}ej\n  Leng\\'al and Tom\\'a\\v{s} Vojnar", "title": "Lazy Automata Techniques for WS1S", "comments": "Technical Report for a paper to be published in TACAS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new decision procedure for the logic WS1S. It originates from\nthe classical approach, which first builds an automaton accepting all models of\na formula and then tests whether its language is empty. The main novelty is to\ntest the emptiness on the fly, while constructing a symbolic, term-based\nrepresentation of the automaton, and prune the constructed state space from\nparts irrelevant to the test. The pruning is done by a generalization of two\ntechniques used in antichain-based language inclusion and universality checking\nof finite automata: subsumption and early termination. The richer structure of\nthe WS1S decision problem allows us, however, to elaborate on these techniques\nin novel ways. Our experiments show that the proposed approach can in many\ncases significantly outperform the classical decision procedure (implemented in\nthe MONA tool) as well as recently proposed alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 07:30:12 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 08:49:11 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Fiedor", "Tom\u00e1\u0161", ""], ["Hol\u00edk", "Luk\u00e1\u0161", ""], ["Jank\u016f", "Petr", ""], ["Leng\u00e1l", "Ond\u0159ej", ""], ["Vojnar", "Tom\u00e1\u0161", ""]]}, {"id": "1701.06477", "submitter": "Justin Hsu", "authors": "Gilles Barthe and Thomas Espitau and Benjamin Gr\\'egoire and Justin\n  Hsu and Pierre-Yves Strub", "title": "Proving uniformity and independence by self-composition and coupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof by coupling is a classical proof technique for establishing\nprobabilistic properties of two probabilistic processes, like stochastic\ndominance and rapid mixing of Markov chains. More recently, couplings have been\ninvestigated as a useful abstraction for formal reasoning about relational\nproperties of probabilistic programs, in particular for modeling\nreduction-based cryptographic proofs and for verifying differential privacy. In\nthis paper, we demonstrate that probabilistic couplings can be used for\nverifying non-relational probabilistic properties. Specifically, we show that\nthe program logic pRHL---whose proofs are formal versions of proofs by\ncoupling---can be used for formalizing uniformity and probabilistic\nindependence. We formally verify our main examples using the EasyCrypt proof\nassistant.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 16:23:29 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 20:51:43 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Barthe", "Gilles", ""], ["Espitau", "Thomas", ""], ["Gr\u00e9goire", "Benjamin", ""], ["Hsu", "Justin", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1701.06532", "submitter": "Jan Jakubuv", "authors": "Jan Jakub\\r{u}v, Josef Urban", "title": "ENIGMA: Efficient Learning-based Inference Guiding Machine", "comments": "Submitted to LPAR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ENIGMA is a learning-based method for guiding given clause selection in\nsaturation-based theorem provers. Clauses from many proof searches are\nclassified as positive and negative based on their participation in the proofs.\nAn efficient classification model is trained on this data, using fast\nfeature-based characterization of the clauses . The learned model is then\ntightly linked with the core prover and used as a basis of a new parameterized\nevaluation heuristic that provides fast ranking of all generated clauses. The\napproach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing\na large increase of E's performance.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:03:52 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Jakub\u016fv", "Jan", ""], ["Urban", "Josef", ""]]}, {"id": "1701.06612", "submitter": "Florian Lonsing", "authors": "Florian Lonsing and Uwe Egly", "title": "Evaluating QBF Solvers: Quantifier Alternations Matter", "comments": "preprint of a paper to be published at CP 2018, LNCS, Springer,\n  including appendix", "journal-ref": null, "doi": "10.1007/978-3-319-98334-9_19", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an experimental study of the effects of quantifier alternations on\nthe evaluation of quantified Boolean formula (QBF) solvers. The number of\nquantifier alternations in a QBF in prenex conjunctive normal form (PCNF) is\ndirectly related to the theoretical hardness of the respective QBF\nsatisfiability problem in the polynomial hierarchy. We show empirically that\nthe performance of solvers based on different solving paradigms substantially\nvaries depending on the numbers of alternations in PCNFs. In related\ntheoretical work, quantifier alternations have become the focus of\nunderstanding the strengths and weaknesses of various QBF proof systems\nimplemented in solvers. Our results motivate the development of methods to\nevaluate orthogonal solving paradigms by taking quantifier alternations into\naccount. This is necessary to showcase the broad range of existing QBF solving\nparadigms for practical QBF applications. Moreover, we highlight the potential\nof combining different approaches and QBF proof systems in solvers.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 20:10:06 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 13:59:21 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 12:13:00 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lonsing", "Florian", ""], ["Egly", "Uwe", ""]]}, {"id": "1701.06644", "submitter": "Tingting Han", "authors": "Taolue Chen, Tingting Han, Yongzhi Cao", "title": "Polynomial-time Algorithms for Computing Distances of Fuzzy Transition\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behaviour distances to measure the resemblance of two states in a\n(nondeterministic) fuzzy transition system have been proposed recently in the\nliterature. Such a distance, defined as a pseudo-ultrametric over the state\nspace of the model, provides a quantitative analogue of bisimilarity. In this\npaper, we focus on the problem of computing these distances. We first extend\nthe definition of the pseudo-ultrametric by introducing discount such that the\ndiscounting factor being equal to 1 captures the original definition. We then\nprovide polynomial-time algorithms to calculate the behavioural distances, in\nboth the non-discounted and the discounted setting. The algorithm is strongly\npolynomial in the former case. Furthermore, we give a polynomial-time algorithm\nto compute bisimulation over fuzzy transition systems which captures the\ndistance being equal to 0.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 21:55:56 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Chen", "Taolue", ""], ["Han", "Tingting", ""], ["Cao", "Yongzhi", ""]]}, {"id": "1701.06745", "submitter": "EPTCS", "authors": "Serge Autexier, Pedro Quaresma", "title": "Proceedings of the 12th Workshop on User Interfaces for Theorem Provers", "comments": null, "journal-ref": "EPTCS 239, 2017", "doi": "10.4204/EPTCS.239", "report-no": null, "categories": "cs.HC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UITP workshop series brings together researchers interested in designing,\ndeveloping and evaluating user interfaces for automated reasoning tools, such\nas interactive proof assistants, automated theorem provers, model finders,\ntools for formal methods, and tools for visualising and manipulating logical\nformulas and proofs. The twelth edition of UITP took place in Coimbra,\nPortugal, and was part of the International Joint Conference on Automated\nReasoning (IJCAR'16). The workshop consisted of an invited talk, six\npresentations of submitted papers and lively hands-on session for reasoning\ntools and their user-interface. These post-proceedings contain four contributed\npapers accepted for publication after a second round of reviewing after the\nworkshop as well as the invited paper.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 06:26:01 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Autexier", "Serge", ""], ["Quaresma", "Pedro", ""]]}, {"id": "1701.06790", "submitter": "Rachid Echahed", "authors": "Rachid Echahed and Aude Maignan", "title": "Parallel Graph Rewriting with Overlapping Rules", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of simultaneous transformations of networks represented\nas graphs. Roughly speaking, one may distinguish two kinds of simultaneous or\nparallel rewrite relations over complex structures such as graphs: (i) those\nwhich transform disjoint subgraphs in parallel and hence can be simulated by\nsuccessive mere sequential and local transformations and (ii) those which\ntransform overlapping subgraphs simultaneously. In the latter situations,\nparallel transformations cannot be simulated in general by means of successive\nlocal rewrite steps. We investigate this last problem in the framework of\noverlapping graph transformation systems. As parallel transformation of a graph\ndoes not produce a graph in general, we propose first some sufficient\nconditions that ensure the closure of graphs by parallel rewrite relations.\nThen we mainly introduce and discuss two parallel rewrite relations over\ngraphs. One relation is functional and thus deterministic, the other one is not\nfunctional for which we propose sufficient conditions which ensure its\nconfluence.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 10:02:55 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Echahed", "Rachid", ""], ["Maignan", "Aude", ""]]}, {"id": "1701.06924", "submitter": "John Van De Wetering", "authors": "John van de Wetering", "title": "Ordering information on distributions", "comments": "Master Thesis. 78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis details a class of partial orders on the space of probability\ndistributions and the space of density operators which capture the idea of\ninformation content. Some links to domain theory and computational linguistics\nare also discussed. Chapter 1 details some useful theorems from order theory.\nIn Chapter 2 we define a notion of an information ordering on the space of\nprobability distributions and see that this gives rise to a large class of\norderings. In Chapter 3 we extend the idea of an information ordering to the\nspace of density operators and in particular look at the maximum eigenvalue\norder. We will discuss whether this order might be unique given certain\nrestrictions. In Chapter 4 we discuss a possible application in distributional\nlanguage models, namely in the study of entailment and disambiguation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 12:21:26 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["van de Wetering", "John", ""]]}, {"id": "1701.06937", "submitter": "Micha{\\l} Pilipczuk", "authors": "Miko{\\l}aj Boja\\'nczyk and Micha{\\l} Pilipczuk", "title": "Optimizing tree decompositions in MSO", "comments": "Version 1: Extended abstract appeared in the proceedings of STACS\n  2017. The version from STACS 2017 did not include Sections 6 and 8\n  (implementation of MSO transductions). Version 2: Fixed an issue in Section\n  7, as a result the bounds in the Dealternation Lemma needed to be increased\n  from $O(k^2)$ and $O(k^3)$ to $O(k^3)$ and $O(k^4)$, respectively. Also,\n  updated the introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic algorithm of Bodlaender and Kloks [J. Algorithms, 1996] solves\nthe following problem in linear fixed-parameter time: given a tree\ndecomposition of a graph of (possibly suboptimal) width $k$, compute an\noptimum-width tree decomposition of the graph. In this work, we prove that this\nproblem can also be solved in MSO in the following sense: for every positive\ninteger $k$, there is an MSO transduction from tree decompositions of width $k$\nto tree decompositions of optimum width. Together with our recent results [LICS\n2016], this implies that for every $k$ there exists an MSO transduction which\ninputs a graph of treewidth $k$, and nondeterministically outputs its tree\ndecomposition of optimum width. We also show that MSO transductions can be\nimplemented in linear fixed-parameter time, which enables us to derive the\nalgorithmic result of Bodlaender and Kloks as a corollary of our main result.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 15:39:49 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 19:45:51 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Boja\u0144czyk", "Miko\u0142aj", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1701.06972", "submitter": "Sarah Loos", "authors": "Sarah Loos, Geoffrey Irving, Christian Szegedy, Cezary Kaliszyk", "title": "Deep Network Guided Proof Search", "comments": null, "journal-ref": "In Thomas Eiter and David Sands, editors, 21st International\n  Conference on Logic for Programming, Artificial Intelligence and Reasoning\n  (LPAR-21). EPiC Series in Computing, vol. 46, pages 85-105, EasyChair, 2017.\n  ISSN 2398-7340", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques lie at the heart of several significant AI advances\nin recent years including object recognition and detection, image captioning,\nmachine translation, speech recognition and synthesis, and playing the game of\nGo. Automated first-order theorem provers can aid in the formalization and\nverification of mathematical theorems and play a crucial role in program\nanalysis, theory reasoning, security, interpolation, and system verification.\nHere we suggest deep learning based guidance in the proof search of the theorem\nprover E. We train and compare several deep neural network models on the traces\nof existing ATP proofs of Mizar statements and use them to select processed\nclauses during proof search. We give experimental evidence that with a hybrid,\ntwo-phase approach, deep learning based guidance can significantly reduce the\naverage number of proof search steps while increasing the number of theorems\nproved. Using a few proof guidance strategies that leverage deep neural\nnetworks, we have found first-order proofs of 7.36% of the first-order logic\ntranslations of the Mizar Mathematical Library theorems that did not previously\nhave ATP generated proofs. This increases the ratio of statements in the corpus\nwith ATP generated proofs from 56% to 59%.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 16:39:05 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Loos", "Sarah", ""], ["Irving", "Geoffrey", ""], ["Szegedy", "Christian", ""], ["Kaliszyk", "Cezary", ""]]}, {"id": "1701.07124", "submitter": "EPTCS", "authors": "Sylvain Conchon (LRI, Universit\\'e Paris-Sud), Mohamed Iguernlala\n  (OCamlPro SAS), Alain Mebsout (The University of Iowa)", "title": "AltGr-Ergo, a Graphical User Interface for the SMT Solver Alt-Ergo", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 1-13", "doi": "10.4204/EPTCS.239.1", "report-no": null, "categories": "cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to undecidability and complexity of first-order logic, SMT solvers may\nnot terminate on some problems or require a very long time. When this happens,\none would like to find the reasons why the solver fails. To this end, we have\ndesigned AltGr-Ergo, an interactive graphical interface for the SMT solver\nAlt-Ergo which allows users and tool developers to help the solver finish some\nproofs. AltGr-Ergo gives real time feedback in order to evaluate and quantify\nprogress made by the solver, and also offers various syntactic manipulation\noptions to allow a finer grained interaction with Alt-Ergo. This paper\ndescribes these features and their implementation, and gives usage scenarios\nfor most of them.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:20:59 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Conchon", "Sylvain", "", "LRI, Universit\u00e9 Paris-Sud"], ["Iguernlala", "Mohamed", "", "OCamlPro SAS"], ["Mebsout", "Alain", "", "The University of Iowa"]]}, {"id": "1701.07125", "submitter": "EPTCS", "authors": "Emilio Jes\\'us Gallego Arias (MINES ParisTech, PSL Research\n  University, France), Beno\\^it Pin (MINES ParisTech, PSL Research University,\n  France), Pierre Jouvelot (MINES ParisTech, PSL Research University, France)", "title": "jsCoq: Towards Hybrid Theorem Proving Interfaces", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 15-27", "doi": "10.4204/EPTCS.239.2", "report-no": null, "categories": "cs.PL cs.HC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe jsCcoq, a new platform and user environment for the Coq\ninteractive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015\nspecification, and it is typically run inside a standards-compliant browser,\nwithout the need of external servers or services. Targeting educational use,\njsCoq allows the user to start interaction with proof scripts right away,\nthanks to its self-contained nature. Indeed, a full Coq environment is packed\nalong the proof scripts, easing distribution and installation. Starting to use\njsCoq is as easy as clicking on a link. The current release ships more than 10\npopular Coq libraries, and supports popular books such as Software Foundations\nor Certified Programming with Dependent Types. The new target platform has\nopened up new interaction and display possibilities. It has also fostered the\ndevelopment of some new Coq-related technology. In particular, we have\nimplemented a new serialization-based protocol for interaction with the proof\nassistant, as well as a new package format for library distribution.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:21:14 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Arias", "Emilio Jes\u00fas Gallego", "", "MINES ParisTech, PSL Research\n  University, France"], ["Pin", "Beno\u00eet", "", "MINES ParisTech, PSL Research University,\n  France"], ["Jouvelot", "Pierre", "", "MINES ParisTech, PSL Research University, France"]]}, {"id": "1701.07126", "submitter": "EPTCS", "authors": "Sven Linker (University of Liverpool, UK), Jim Burton (University of\n  Brighton, UK), Mateja Jamnik (University of Cambridge, UK)", "title": "Tactical Diagrammatic Reasoning", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 29-42", "doi": "10.4204/EPTCS.239.3", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although automated reasoning with diagrams has been possible for some years,\ntools for diagrammatic reasoning are generally much less sophisticated than\ntheir sentential cousins. The tasks of exploring levels of automation and\nabstraction in the construction of proofs and of providing explanations of\nsolutions expressed in the proofs remain to be addressed. In this paper we take\nan interactive proof assistant for Euler diagrams, Speedith, and add tactics to\nits reasoning engine, providing a level of automation in the construction of\nproofs. By adding tactics to Speedith's repertoire of inferences, we ease the\ninteraction between the user and the system and capture a higher level\nexplanation of the essence of the proof. We analysed the design options for\ntactics by using metrics which relate to human readability, such as the number\nof inferences and the amount of clutter present in diagrams. Thus, in contrast\nto the normal case with sentential tactics, our tactics are designed to not\nonly prove the theorem, but also to support explanation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:21:27 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Linker", "Sven", "", "University of Liverpool, UK"], ["Burton", "Jim", "", "University of\n  Brighton, UK"], ["Jamnik", "Mateja", "", "University of Cambridge, UK"]]}, {"id": "1701.07127", "submitter": "EPTCS", "authors": "Martin Ring (DFKI), Christoph L\\\"uth (DFKI and Universit\\\"at Bremen)", "title": "Interactive Proof Presentations with Cobra", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 43-52", "doi": "10.4204/EPTCS.239.4", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Cobra, a modern proof presentation framework, leveraging\ncutting-edge presentation technology together with a state of the art\ninteractive theorem prover to present formalized mathematics as active\ndocuments. Cobra provides both an easy way to present proofs and a novel\napproach to auditorium interaction. The presentation is checked live by the\ntheorem prover, and moreover allows for live changes both by the presenter and\nthe audience.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:21:43 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Ring", "Martin", "", "DFKI"], ["L\u00fcth", "Christoph", "", "DFKI and Universit\u00e4t Bremen"]]}, {"id": "1701.07470", "submitter": "Georg Zetzsche", "authors": "Simon Halfon, Philippe Schnoebelen, Georg Zetzsche", "title": "Decidability, Complexity, and Expressiveness of First-Order Logic Over\n  the Subword Ordering", "comments": "26 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider first-order logic over the subword ordering on finite words,\nwhere each word is available as a constant. Our first result is that the\n$\\Sigma_1$ theory is undecidable (already over two letters).\n  We investigate the decidability border by considering fragments where all but\na certain number of variables are alternation bounded, meaning that the\nvariable must always be quantified over languages with a bounded number of\nletter alternations. We prove that when at most two variables are not\nalternation bounded, the $\\Sigma_1$ fragment is decidable, and that it becomes\nundecidable when three variables are not alternation bounded. Regarding higher\nquantifier alternation depths, we prove that the $\\Sigma_2$ fragment is\nundecidable already for one variable without alternation bound and that when\nall variables are alternation bounded, the entire first-order theory is\ndecidable.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:12:02 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Halfon", "Simon", ""], ["Schnoebelen", "Philippe", ""], ["Zetzsche", "Georg", ""]]}, {"id": "1701.07601", "submitter": "Dusko Pavlovic", "authors": "Dusko Pavlovic and Peter-Michael Seidel", "title": "Quotients in monadic programming: Projective algebras are equivalent to\n  coalgebras", "comments": "31 pages, 21 diagram; in this version: fixed typos; short version in\n  LICS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In monadic programming, datatypes are presented as free algebras, generated\nby data values, and by the algebraic operations and equations capturing some\ncomputational effects. These algebras are free in the sense that they satisfy\njust the equations imposed by their algebraic theory, and remain free of any\nadditional equations. The consequence is that they do not admit quotient types.\nThis is, of course, often inconvenient. Whenever a computation involves data\nwith multiple representatives, and they need to be identified according to some\nequations that are not satisfied by all data, the monadic programmer has to\nleave the universe of free algebras, and resort to explicit destructors. We\ncharacterize the situation when these destructors are preserved under all\noperations, and the resulting quotients of free algebras are also their\nsubalgebras. Such quotients are called *projective*. Although popular in\nuniversal algebra, projective algebras did not attract much attention in the\nmonadic setting, where they turn out to have a surprising avatar: for any given\nmonad, a suitable category of projective algebras is equivalent with the\ncategory of coalgebras for the comonad induced by any monad resolution. For a\nmonadic programmer, this equivalence provides a convenient way to implement\npolymorphic quotients as coalgebras. The dual correspondence of injective\ncoalgebras and all algebras leads to a different family of quotient types,\nwhich seems to have a different family of applications. Both equivalences also\nentail several general corollaries concerning monadicity and comonadicity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 07:38:53 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 20:17:08 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 18:26:09 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Pavlovic", "Dusko", ""], ["Seidel", "Peter-Michael", ""]]}, {"id": "1701.07842", "submitter": "Nicholas V. Lewchenko", "authors": "Arjun Radhakrishna, Nicholas V. Lewchenko, Shawn Meier, Sergio Mover,\n  Krishna Chaitanya Sripada, Damien Zufferey, Bor-Yuh Evan Chang, and Pavol\n  \\v{C}ern\\'y", "title": "DroidStar: Callback Typestates for Android Classes", "comments": "Appearing at ICSE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-driven programming frameworks, such as Android, are based on components\nwith asynchronous interfaces. The protocols for interacting with these\ncomponents can often be described by finite-state machines we dub *callback\ntypestates*. Callback typestates are akin to classical typestates, with the\ndifference that their outputs (callbacks) are produced asynchronously. While\nuseful, these specifications are not commonly available, because writing them\nis difficult and error-prone.\n  Our goal is to make the task of producing callback typestates significantly\neasier. We present a callback typestate assistant tool, DroidStar, that\nrequires only limited user interaction to produce a callback typestate. Our\napproach is based on an active learning algorithm, L*. We improved the\nscalability of equivalence queries (a key component of L*), thus making active\nlearning tractable on the Android system.\n  We use DroidStar to learn callback typestates for Android classes both for\ncases where one is already provided by the documentation, and for cases where\nthe documentation is unclear. The results show that DroidStar learns callback\ntypestates accurately and efficiently. Moreover, in several cases, the\nsynthesized callback typestates uncovered surprising and undocumented\nbehaviors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 19:06:45 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 23:43:09 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 18:45:04 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Radhakrishna", "Arjun", ""], ["Lewchenko", "Nicholas V.", ""], ["Meier", "Shawn", ""], ["Mover", "Sergio", ""], ["Sripada", "Krishna Chaitanya", ""], ["Zufferey", "Damien", ""], ["Chang", "Bor-Yuh Evan", ""], ["\u010cern\u00fd", "Pavol", ""]]}, {"id": "1701.07925", "submitter": "EPTCS", "authors": "Catherine Dubois (ENSIIE), Paolo Masci (HASLab, INESC TEC), Dominique\n  M\\'ery (Universit\\'e de Lorraine)", "title": "Proceedings of the Third Workshop on Formal Integrated Development\n  Environment", "comments": null, "journal-ref": "EPTCS 240, 2017", "doi": "10.4204/EPTCS.240", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of F-IDE 2016, the third international\nworkshop on Formal Integrated Development Environment, which was held as an FM\n2016 satellite event, on November 8, 2016, in Limassol (Cyprus). High levels of\nsafety, security and also privacy standards require the use of formal methods\nto specify and develop compliant software (sub)systems. Any standard comes with\nan assessment process, which requires a complete documentation of the\napplication in order to ease the justification of design choices and the review\nof code and proofs. Thus tools are needed for handling specifications, program\nconstructs and verification artifacts. The aim of the F-IDE workshop is to\nprovide a forum for presenting and discussing research efforts as well as\nexperience returns on design, development and usage of formal IDE aiming at\nmaking formal methods \"easier\" for both specialists and non-specialists.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 02:23:26 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Dubois", "Catherine", "", "ENSIIE"], ["Masci", "Paolo", "", "HASLab, INESC TEC"], ["M\u00e9ry", "Dominique", "", "Universit\u00e9 de Lorraine"]]}, {"id": "1701.07937", "submitter": "Taichi Uemura", "authors": "Taichi Uemura", "title": "Homotopies for Free!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show \"free theorems\" in the style of Wadler for polymorphic functions in\nhomotopy type theory as consequences of the abstraction theorem. As an\napplication, it follows that every space defined as a higher inductive type has\nthe same homotopy groups as some type of polymorphic functions defined without\nunivalence or higher inductive types.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 04:06:49 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 09:29:34 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Uemura", "Taichi", ""]]}, {"id": "1701.08030", "submitter": "Valentin Touzeau", "authors": "Valentin Touzeau (VERIMAG - IMAG), Claire Ma\\\"iza (VERIMAG - IMAG),\n  David Monniaux (VERIMAG - IMAG)", "title": "Model Checking of Cache for WCET Analysis Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On real-time systems running under timing constraints, scheduling can be\nperformed when one is aware of the worst case execution time (WCET) of tasks.\nUsually, the WCET of a task is unknown and schedulers make use of safe\nover-approximations given by static WCET analysis. To reduce the\nover-approximation, WCET analysis has to gain information about the underlying\nhardware behavior, such as pipelines and caches. In this paper, we focus on the\ncache analysis, which classifies memory accesses as hits/misses according to\nthe set of possible cache states. We propose to refine the results of classical\ncache analysis using a model checker, introducing a new cache model for the\nleast recently used (LRU) policy.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:33:58 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 09:04:04 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Touzeau", "Valentin", "", "VERIMAG - IMAG"], ["Ma\u00efza", "Claire", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"]]}, {"id": "1701.08186", "submitter": "Giulio Guerrieri", "authors": "Beniamino Accattoli, Giulio Guerrieri", "title": "Implementing Open Call-by-Value (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of the call-by-value lambda-calculus relies on weak evaluation and\nclosed terms, that are natural hypotheses in the study of programming\nlanguages. To model proof assistants, however, strong evaluation and open terms\nare required. Open call-by-value is the intermediate setting of weak evaluation\nwith open terms, on top of which Gr\\'egoire and Leroy designed the abstract\nmachine of Coq. This paper provides a theory of abstract machines for open\ncall-by-value. The literature contains machines that are either simple but\ninefficient, as they have an exponential overhead, or efficient but heavy, as\nthey rely on a labelling of environments and a technical optimization. We\nintroduce a machine that is simple and efficient: it does not use labels and it\nimplements open call-by-value within a bilinear overhead. Moreover, we provide\na new fine understanding of how different optimizations impact on the\ncomplexity of the overhead.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 20:41:19 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 16:41:47 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Guerrieri", "Giulio", ""]]}, {"id": "1701.08189", "submitter": "Ulrik Buchholtz", "authors": "Ulrik Buchholtz and Edward Morehouse", "title": "Varieties of Cubical Sets", "comments": "16 pages; to appear in proceedings of RAMiCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.LO math.AT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a variety of notions of cubical sets, based on sites organized\nusing substructural algebraic theories presenting PRO(P)s or Lawvere theories.\nWe prove that all our sites are test categories in the sense of Grothendieck,\nmeaning that the corresponding presheaf categories of cubical sets model\nclassical homotopy theory. We delineate exactly which ones are even strict test\ncategories, meaning that products of cubical sets correspond to products of\nhomotopy types.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 20:50:56 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 14:20:37 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Buchholtz", "Ulrik", ""], ["Morehouse", "Edward", ""]]}, {"id": "1701.08301", "submitter": "A Mani", "authors": "A. Mani", "title": "Pure Rough Mereology and Counting", "comments": "IEEE Women in Engineering Conference, WIECON-ECE'2017 (Accepted for\n  IEEEXplore)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LO math.IT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of mereology (parts and wholes) in the context of formal approaches\nto vagueness can be approached in a number of ways. In the context of rough\nsets, mereological concepts with a set-theoretic or valuation based ontology\nacquire complex and diverse behavior. In this research a general rough set\nframework called granular operator spaces is extended and the nature of\nparthood in it is explored from a minimally intrusive point of view. This is\nused to develop counting strategies that help in classifying the framework. The\ndeveloped methodologies would be useful for drawing involved conclusions about\nthe nature of data (and validity of assumptions about it) from antichains\nderived from context. The problem addressed is also about whether counting\nprocedures help in confirming that the approximations involved in formation of\ndata are indeed rough approximations?\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 16:38:39 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mani", "A.", ""]]}, {"id": "1701.08330", "submitter": "Ale\\v{s} Bizjak", "authors": "Valentina Castiglioni, Daniel Gebler and Simone Tini", "title": "SOS-based Modal Decomposition on Nondeterministic Probabilistic\n  Processes", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 2 (June 25,\n  2018) lmcs:4641", "doi": "10.23638/LMCS-14(2:18)2018", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method for the decomposition of modal formulae on processes with\nnondeterminism and probability with respect to Structural Operational\nSemantics. The purpose is to reduce the satisfaction problem of a formula for a\nprocess to verifying whether its subprocesses satisfy certain formulae obtained\nfrom the decomposition. To deal with the probabilistic behavior of processes,\nand thus with the decomposition of formulae characterizing it, we introduce a\nSOS-like machinery allowing for the specification of the behavior of open\ndistribution terms. By our decomposition, we obtain (pre)congruence formats for\nprobabilistic bisimilarity, ready similarity and similarity.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 23:17:10 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 08:59:44 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 09:36:56 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Castiglioni", "Valentina", ""], ["Gebler", "Daniel", ""], ["Tini", "Simone", ""]]}, {"id": "1701.08345", "submitter": "Azadeh Farzan", "authors": "Azadeh Farzan and Victor Nicolet", "title": "Automated Synthesis of Divide and Conquer Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper focuses on automated synthesis of divide-and-conquer parallelism,\nwhich is a common parallel programming skeleton supported by many\ncross-platform multithreaded libraries. The challenges of producing (manually\nor automatically) a correct divide-and-conquer parallel program from a given\nsequential code are two-fold: (1) assuming that individual worker threads\nexecute a code identical to the sequential code, the programmer has to provide\nthe extra code for dividing the tasks and combining the computation results,\nand (2) sometimes, the sequential code may not be usable as is, and may need to\nbe modified by the programmer. We address both challenges in this paper. We\npresent an automated synthesis technique for the case where no modifications to\nthe sequential code are required, and we propose an algorithm for modifying the\nsequential code to make it suitable for parallelization when some modification\nis necessary. The paper presents theoretical results for when this {\\em\nmodification} is efficiently possible, and experimental evaluation of the\ntechnique and the quality of the produced parallel programs.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 02:05:03 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Farzan", "Azadeh", ""], ["Nicolet", "Victor", ""]]}, {"id": "1701.08369", "submitter": "Dan Frumin", "authors": "Daniil Frumin and Benno van den Berg", "title": "A homotopy-theoretic model of function extensionality in the effective\n  topos", "comments": "v2: The section \"A non-contractible uniform object.\" was removed due\n  to an error in Lemma 6.5, Proposition 7.3 was changed to account for the fact\n  that only the \"only if\" direction holds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a way of constructing a Quillen model structure on a full\nsubcategory of an elementary topos, starting with an interval object with\nconnections and a certain dominance. The advantage of this method is that it\ndoes not require the underlying topos to be cocomplete. The resulting model\ncategory structure gives rise to a model of homotopy type theory with identity\ntypes, $\\Sigma$- and $\\Pi$-types, and functional extensionality.\n  We apply the method to the effective topos with the interval object $\\nabla\n2$. In the resulting model structure we identify uniform inhabited objects as\ncontractible objects, and show that discrete objects are fibrant. Moreover, we\nshow that the unit of the discrete reflection is a homotopy equivalence and the\nhomotopy category of fibrant assemblies is equivalent to the category of modest\nsets. We compare our work with the path object category construction on the\neffective topos by Jaap van Oosten.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 12:35:57 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 14:05:34 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Frumin", "Daniil", ""], ["Berg", "Benno van den", ""]]}, {"id": "1701.08402", "submitter": "Martin Ziegler", "authors": "Chansu Park and Ji-Won Park and Sewon Park and Dongseong Seon and\n  Martin Ziegler", "title": "Computable Operations on Compact Subsets of Metric Spaces with\n  Applications to Fr\\'echet Distance and Shape Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Theory of Computation on real numbers, continuous real\nfunctions, and bounded closed Euclidean subsets, to compact metric spaces\n$(X,d)$: thereby generically including computational and optimization problems\nover higher types, such as the compact 'hyper' spaces of (i) nonempty closed\nsubsets of $X$ w.r.t. Hausdorff metric, and of (ii) equicontinuous functions on\n$X$. The thus obtained Cartesian closure is shown to exhibit the same\nstructural properties as in the Euclidean case, particularly regarding function\npre/image. This allows us to assert the computability of (iii) Fr\\'echet\nDistances between curves and between loops, as well as of (iv)\nconstrained/Shape Optimization.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 17:13:28 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 20:16:02 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Park", "Chansu", ""], ["Park", "Ji-Won", ""], ["Park", "Sewon", ""], ["Seon", "Dongseong", ""], ["Ziegler", "Martin", ""]]}, {"id": "1701.08466", "submitter": "EPTCS", "authors": "Andrew Healy (Maynooth University), Rosemary Monahan (Maynooth\n  University), James F. Power (Maynooth University)", "title": "Predicting SMT Solver Performance for Software Verification", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 20-37", "doi": "10.4204/EPTCS.240.2", "report-no": null, "categories": "cs.SE cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Why3 IDE and verification system facilitates the use of a wide range of\nSatisfiability Modulo Theories (SMT) solvers through a driver-based\narchitecture. We present Where4: a portfolio-based approach to discharge Why3\nproof obligations. We use data analysis and machine learning techniques on\nstatic metrics derived from program source code. Our approach benefits software\nengineers by providing a single utility to delegate proof obligations to the\nsolvers most likely to return a useful result. It does this in a time-efficient\nway using existing Why3 and solver installations - without requiring low-level\nknowledge about SMT solver operation from the user.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:32:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Healy", "Andrew", "", "Maynooth University"], ["Monahan", "Rosemary", "", "Maynooth\n  University"], ["Power", "James F.", "", "Maynooth University"]]}, {"id": "1701.08468", "submitter": "EPTCS", "authors": "Gioacchino Mauro, Harold Thimbleby, Andrea Domenici, Cinzia\n  Bernardeschi", "title": "Extending a User Interface Prototyping Tool with Automatic MISRA C Code\n  Generation", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 53-66", "doi": "10.4204/EPTCS.240.4", "report-no": null, "categories": "cs.SE cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with systems, particularly safety-critical systems, that\ninvolve interaction between users and devices, such as the user interface of\nmedical devices. We therefore developed a MISRA C code generator for formal\nmodels expressed in the PVSio-web prototyping toolkit. PVSio-web allows\ndevelopers to rapidly generate realistic interactive prototypes for verifying\nusability and safety requirements in human-machine interfaces. The visual\nappearance of the prototypes is based on a picture of a physical device, and\nthe behaviour of the prototype is defined by an executable formal model. Our\napproach transforms the PVSio-web prototyping tool into a model-based\nengineering toolkit that, starting from a formally verified user interface\ndesign model, will produce MISRA C code that can be compiled and linked into a\nfinal product. An initial validation of our tool is presented for the data\nentry system of an actual medical device.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:33:06 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mauro", "Gioacchino", ""], ["Thimbleby", "Harold", ""], ["Domenici", "Andrea", ""], ["Bernardeschi", "Cinzia", ""]]}, {"id": "1701.08469", "submitter": "EPTCS", "authors": "Stefan Mitsch (Computer Science Department, Carnegie Mellon\n  University), Andr\\'e Platzer (Computer Science Department, Carnegie Mellon\n  University)", "title": "The KeYmaera X Proof IDE - Concepts on Usability in Hybrid Systems\n  Theorem Proving", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 67-81", "doi": "10.4204/EPTCS.240.5", "report-no": null, "categories": "cs.LO cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid systems verification is quite important for developing correct\ncontrollers for physical systems, but is also challenging. Verification\nengineers, thus, need to be empowered with ways of guiding hybrid systems\nverification while receiving as much help from automation as possible. Due to\nundecidability, verification tools need sufficient means for intervening during\nthe verification and need to allow verification engineers to provide system\ndesign insights.\n  This paper presents the design ideas behind the user interface for the hybrid\nsystems theorem prover KeYmaera X. We discuss how they make it easier to prove\nhybrid systems as well as help learn how to conduct proofs in the first place.\nUnsurprisingly, the most difficult user interface challenges come from the\ndesire to integrate automation and human guidance. We also share thoughts how\nthe success of such a user interface design could be evaluated and anecdotal\nobservations about it.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:33:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mitsch", "Stefan", "", "Computer Science Department, Carnegie Mellon\n  University"], ["Platzer", "Andr\u00e9", "", "Computer Science Department, Carnegie Mellon\n  University"]]}, {"id": "1701.08470", "submitter": "EPTCS", "authors": "Lilian Burdy (Clearsy System Engineering), David D\\'eharbe (Clearsy\n  System Engineering), \\'Etienne Prun (Clearsy System Engineering)", "title": "Interfacing Automatic Proof Agents in Atelier B: Introducing \"iapa\"", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 82-90", "doi": "10.4204/EPTCS.240.6", "report-no": null, "categories": "cs.SE cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of automatic theorem provers to discharge proof obligations\nis necessary to apply formal methods in an efficient manner. Tools supporting\nformal methods, such as Atelier~B, generate proof obligations fully\nautomatically. Consequently, such proof obligations are often cluttered with\ninformation that is irrelevant to establish their validity.\n  We present iapa, an \"Interface to Automatic Proof Agents\", a new tool that is\nbeing integrated to Atelier~B, through which the user will access proof\nobligations, apply operations to simplify these proof obligations, and then\ndispatch the resulting, simplified, proof obligations to a portfolio of\nautomatic theorem provers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:33:45 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Burdy", "Lilian", "", "Clearsy System Engineering"], ["D\u00e9harbe", "David", "", "Clearsy\n  System Engineering"], ["Prun", "\u00c9tienne", "", "Clearsy System Engineering"]]}, {"id": "1701.08516", "submitter": "Jan van den Heuvel", "authors": "Jan van den Heuvel and Stephan Kreutzer and Micha{\\l} Pilipczuk and\n  Daniel A. Quiroz and Roman Rabinovich and Sebastian Siebertz", "title": "Model-Checking for Successor-Invariant First-Order Formulas on Graph\n  Classes of Bounded Expansion", "comments": "20 pages, submitted to LICS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successor-invariant first-order formula is a formula that has access to an\nauxiliary successor relation on a structure's universe, but the model relation\nis independent of the particular interpretation of this relation. It is well\nknown that successor-invariant formulas are more expressive on finite\nstructures than plain first-order formulas without a successor relation. This\nnaturally raises the question whether this increase in expressive power comes\nat an extra cost to solve the model-checking problem, that is, the problem to\ndecide whether a given structure together with some (and hence every) successor\nrelation is a model of a given formula. It was shown earlier that adding\nsuccessor-invariance to first-order logic essentially comes at no extra cost\nfor the model-checking problem on classes of finite structures whose underlying\nGaifman graph is planar [Engelmann et al., 2012], excludes a fixed minor\n[Eickmeyer et al., 2013] or a fixed topological minor [Eickmeyer and\nKawarabayashi, 2016; Kreutzer et al., 2016]. In this work we show that the\nmodel-checking problem for successor-invariant formulas is fixed-parameter\ntractable on any class of finite structures whose underlying Gaifman graphs\nform a class of bounded expansion. Our result generalises all earlier results\nand comes close to the best tractability results on nowhere dense classes of\ngraphs currently known for plain first-order logic.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 09:20:02 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 10:35:23 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Heuvel", "Jan van den", ""], ["Kreutzer", "Stephan", ""], ["Pilipczuk", "Micha\u0142", ""], ["Quiroz", "Daniel A.", ""], ["Rabinovich", "Roman", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1701.08524", "submitter": "Uli Fahrenberg", "authors": "David Cachera, Uli Fahrenberg, Axel Legay", "title": "An $\\omega$-Algebra for Real-Time Energy Problems", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (May 24,\n  2019) lmcs:5507", "doi": "10.23638/LMCS-15(2:17)2019", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a $^*$-continuous Kleene $\\omega$-algebra of real-time energy\nfunctions. Together with corresponding automata, these can be used to model\nsystems which can consume and regain energy (or other types of resources)\ndepending on available time. Using recent results on $^*$-continuous Kleene\n$\\omega$-algebras and computability of certain manipulations on real-time\nenergy functions, it follows that reachability and B\\\"uchi acceptance in\nreal-time energy automata can be decided in a static way which only involves\nmanipulations of real-time energy functions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 09:40:46 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 23:16:28 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 08:54:53 GMT"}, {"version": "v4", "created": "Thu, 23 May 2019 08:27:00 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Cachera", "David", ""], ["Fahrenberg", "Uli", ""], ["Legay", "Axel", ""]]}, {"id": "1701.08622", "submitter": "Christoph Rauch", "authors": "Panos Rondogiannis, Ioanna Symeonidou", "title": "Extensional Semantics for Higher-Order Logic Programs with Negation", "comments": "24 pages. Submitted to Logical Methods in Computer Science", "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 2 (June 29,\n  2018) lmcs:4067", "doi": "10.23638/LMCS-14(2:19)2018", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop an extensional semantics for higher-order logic programs with\nnegation, generalizing the technique that was introduced in [Bezem99,Bezem01]\nfor positive higher-order programs. In this way we provide an alternative\nextensional semantics for higher-order logic programs with negation to the one\nproposed in [CharalambidisER14]. As an immediate useful consequence of our\ndevelopments, we define for the language we consider the notions of\nstratification and local stratification, which generalize the familiar such\nnotions from classical logic programming. We demonstrate that for stratified\nand locally stratified higher-order logic programs, the proposed semantics\nnever assigns the unknown truth value. We conclude the paper by providing a\nnegative result: we demonstrate that the well-known stable model semantics of\nclassical logic programming, if extended according to the technique of\n[Bezem99,Bezem01] to higher-order logic programs, does not in general lead to\nextensional stable models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 14:53:20 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 16:50:55 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 13:44:01 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Rondogiannis", "Panos", ""], ["Symeonidou", "Ioanna", ""]]}, {"id": "1701.08682", "submitter": "Ale\\v{s} Bizjak", "authors": "Parosh Aziz Abdulla, Mohamed Faouzi Atig, Ahmed Bouajjani, Tuan Phong\n  Ngo", "title": "A Load-Buffer Semantics for Total Store Ordering", "comments": "Logic in computer science", "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 1 (January\n  23, 2018) lmcs:4228", "doi": "10.23638/LMCS-14(1:9)2018", "report-no": null, "categories": "cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of verifying safety properties of concurrent programs\nrunning over the Total Store Order (TSO) memory model. Known decision\nprocedures for this model are based on complex encodings of store buffers as\nlossy channels. These procedures assume that the number of processes is fixed.\nHowever, it is important in general to prove the correctness of a\nsystem/algorithm in a parametric way with an arbitrarily large number of\nprocesses.\n  In this paper, we introduce an alternative (yet equivalent) semantics to the\nclassical one for the TSO semantics that is more amenable to efficient\nalgorithmic verification and for the extension to parametric verification. For\nthat, we adopt a dual view where load buffers are used instead of store\nbuffers. The flow of information is now from the memory to load buffers. We\nshow that this new semantics allows (1) to simplify drastically the safety\nanalysis under TSO, (2) to obtain a spectacular gain in efficiency and\nscalability compared to existing procedures, and (3) to extend easily the\ndecision procedure to the parametric case, which allows obtaining a new\ndecidability result, and more importantly, a verification algorithm that is\nmore general and more efficient in practice than the one for bounded instances.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:26:43 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 16:08:39 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 12:57:27 GMT"}, {"version": "v4", "created": "Mon, 22 Jan 2018 08:39:46 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Abdulla", "Parosh Aziz", ""], ["Atig", "Mohamed Faouzi", ""], ["Bouajjani", "Ahmed", ""], ["Ngo", "Tuan Phong", ""]]}]