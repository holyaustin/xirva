[{"id": "1801.00285", "submitter": "Thorsten Wissmann", "authors": "Paula Severi", "title": "A Light Modality for Recursion", "comments": "32 pages 1 figure in pdf format", "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (February\n  5, 2019) lmcs:5166", "doi": "10.23638/LMCS-15(1:8)2019", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the interplay between a modality for controlling the behaviour\nof recursive functional programs on infinite structures which are completely\nsilent in the syntax. The latter means that programs do not contain \"marks\"\nshowing the application of the introduction and elimination rules for the\nmodality. This shifts the burden of controlling recursion from the programmer\nto the compiler. To do this, we introduce a typed lambda calculus a la Curry\nwith a silent modality and guarded recursive types. The typing discipline\nguarantees normalisation and can be transformed into an algorithm which infers\nthe type of a program.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 14:02:33 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 16:16:44 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 10:41:17 GMT"}, {"version": "v4", "created": "Mon, 23 Jul 2018 19:54:35 GMT"}, {"version": "v5", "created": "Sat, 2 Feb 2019 13:57:14 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Severi", "Paula", ""]]}, {"id": "1801.00471", "submitter": "Brandon Bohrer", "authors": "Brandon Bohrer and Karl Crary", "title": "TWAM: A Certifying Abstract Machine for Logic Programs", "comments": "41 pages, under submission to ACM Transactions on Computational Logic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type-preserving (or typed) compilation uses typing derivations to certify\ncorrectness properties of compilation. We have designed and implemented a\ntype-preserving compiler for a simply-typed dialect of Prolog we call T-Prolog.\nThe crux of our approach is a new certifying abstract machine which we call the\nTyped Warren Abstract Machine (TWAM). The TWAM has a dependent type system\nstrong enough to specify the semantics of a logic program in the logical\nframework LF. We present a soundness metatheorem which constitutes a partial\ncorrectness guarantee: well-typed programs implement the logic program\nspecified by their type. This metatheorem justifies our design and\nimplementation of a certifying compiler from T-Prolog to TWAM.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 16:46:28 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Bohrer", "Brandon", ""], ["Crary", "Karl", ""]]}, {"id": "1801.00969", "submitter": "Nikolay Shilov", "authors": "Nikolay V. Shilov (1), Igor S. Anureev (2), Mikhail Berdyshev (1),\n  Dmitry Kondratev (2), Aleksey V. Promsky (2) ((1) Innopolis University, (2)\n  A.P. Ershov Institute of Informatics Systems)", "title": "Towards platform-independent verification of the standard mathematical\n  functions: the square root function", "comments": "25 pages, 7 figures, full version with complete proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents (human-oriented) specification and (pen-and-paper)\nverification of the square root function. The function implements Newton method\nand uses a look-up table for initial approximations. Specification is done in\nterms of total correctness assertions with use of precise arithmetic and the\nmathematical square root $\\sqrt{\\dots}$, algorithms are presented in\npseudo-code with explicit distinction between precise and machine arithmetic,\nverification is done in Floyd-Hoare style and adjustment (matching) of runs of\nalgorithms with precise arithmetic and with machine arithmetic. The primary\npurpose of the paper is to make explicit properties of the machine arithmetic\nthat are sufficient to make verification presented in the paper. Computer-aided\nimplementation and validation of the proofs (using some proof-assistant) is the\ntopic for further studies.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 11:53:44 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 06:53:09 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Shilov", "Nikolay V.", ""], ["Anureev", "Igor S.", ""], ["Berdyshev", "Mikhail", ""], ["Kondratev", "Dmitry", ""], ["Promsky", "Aleksey V.", ""]]}, {"id": "1801.01073", "submitter": "Petr Jancar", "authors": "Petr Jancar, Petr Osicka, Zdenek Sawa", "title": "EXPSPACE-hardness of behavioural equivalences of succinct one-counter\n  nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We note that the remarkable EXPSPACE-hardness result in [G\\\"oller, Haase,\nOuaknine, Worrell, ICALP 2010] ([GHOW10] for short) allows us to answer an open\ncomplexity question for simulation preorder of succinct one counter nets (i.e.,\none counter automata with no zero tests where counter increments and decrements\nare integers written in binary). This problem, as well as bisimulation\nequivalence, turn out to be EXPSPACE-complete. The technique of [GHOW10] was\nreferred to by Hunter [RP 2015] for deriving EXPSPACE-hardness of reachability\ngames on succinct one-counter nets. We first give a direct self-contained\nEXPSPACE-hardness proof for such reachability games (by adjusting a known\nPSPACE-hardness proof for emptiness of alternating finite automata with\none-letter alphabet); then we reduce reachability games to (bi)simulation games\nby using a standard \"defender-choice\" technique.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 16:44:58 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Jancar", "Petr", ""], ["Osicka", "Petr", ""], ["Sawa", "Zdenek", ""]]}, {"id": "1801.01180", "submitter": "Thorsten Wissmann", "authors": "Rob van Glabbeek, Bas Luttik, Linda Spaninks", "title": "Rooted Divergence-Preserving Branching Bisimilarity is a Congruence", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 3 (August\n  28, 2020) lmcs:6741", "doi": "10.23638/LMCS-16(3:14)2020", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove that rooted divergence-preserving branching bisimilarity is a\ncongruence for the process specification language consisting of nil, action\nprefix, choice, and the recursion construct.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 21:21:05 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:37:38 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 07:14:48 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 14:05:17 GMT"}, {"version": "v5", "created": "Wed, 26 Aug 2020 17:35:06 GMT"}, {"version": "v6", "created": "Thu, 27 Aug 2020 15:39:25 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["van Glabbeek", "Rob", ""], ["Luttik", "Bas", ""], ["Spaninks", "Linda", ""]]}, {"id": "1801.01231", "submitter": "Amar Hadzihasanovic", "authors": "Giovanni de Felice, Amar Hadzihasanovic, Kang Feng Ng", "title": "A diagrammatic calculus of fermionic quantum circuits", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 3 (September\n  2, 2019) lmcs:5736", "doi": "10.23638/LMCS-15(3:26)2019", "report-no": null, "categories": "cs.LO quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the fermionic ZW calculus, a string-diagrammatic language for\nfermionic quantum computing (FQC). After defining a fermionic circuit model, we\npresent the basic components of the calculus, together with their\ninterpretation, and show how the main physical gates of interest in FQC can be\nrepresented in our language. We then list our axioms, and derive some\nadditional equations. We prove that the axioms provide a complete equational\naxiomatisation of the monoidal category whose objects are systems of finitely\nmany local fermionic modes (LFMs), with maps that preserve or reverse the\nparity of states, and the tensor product as monoidal product. We achieve this\nthrough a procedure that rewrites any diagram in a normal form. As an example,\nwe show how the statistics of a fermionic Mach-Zehnder interferometer can be\ncalculated in the diagrammatic language. We conclude by giving a diagrammatic\ntreatment of the dual-rail encoding, a standard method in optical quantum\ncomputing used to perform universal quantum computation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 02:43:52 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 04:48:02 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 02:10:21 GMT"}, {"version": "v4", "created": "Thu, 4 Jul 2019 23:55:05 GMT"}, {"version": "v5", "created": "Fri, 30 Aug 2019 13:44:44 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["de Felice", "Giovanni", ""], ["Hadzihasanovic", "Amar", ""], ["Ng", "Kang Feng", ""]]}, {"id": "1801.01568", "submitter": "Evan Cavallo", "authors": "Evan Cavallo and Robert Harper", "title": "Computational Higher Type Theory IV: Inductive Types", "comments": "Major revision to include indexed cubical inductive types. 48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the fourth in a series of papers extending Martin-L\\\"of's meaning\nexplanation of dependent type theory to higher-dimensional types. In this\ninstallment, we show how to define cubical type systems supporting a general\nschema of indexed cubical inductive types whose constructors may take dimension\nparameters and have a specified boundary. Using this schema, we are able to\nspecify and implement many of the higher inductive types which have been\npostulated in homotopy type theory, including homotopy pushouts, the torus,\n$W$-quotients, truncations, arbitrary localizations. By including indexed\ninductive types, we enable the definition of identity types.\n  The addition of higher inductive types makes computational higher type theory\na model of homotopy type theory, capable of interpreting almost all of the\nconstructions in the HoTT Book (with the exception of inductive-inductive\ntypes). This is the first such model with an explicit canonicity theorem, which\nspecifies the canonical values of higher inductive types and confirms that\nevery term in an inductive type evaluates to such a value.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 22:45:13 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 20:38:38 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 18:01:48 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Cavallo", "Evan", ""], ["Harper", "Robert", ""]]}, {"id": "1801.02068", "submitter": "Mircea Andrecut Dr", "authors": "M. Andrecut", "title": "On the inherent competition between valid and spurious inductive\n  inferences in Boolean data", "comments": "12 pages, 2 figures, Int. J. Mod. Phys. C, 2017", "journal-ref": null, "doi": "10.1142/S0129183117501467", "report-no": null, "categories": "physics.data-an cs.AI cs.LO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive inference is the process of extracting general rules from specific\nobservations. This problem also arises in the analysis of biological networks,\nsuch as genetic regulatory networks, where the interactions are complex and the\nobservations are incomplete. A typical task in these problems is to extract\ngeneral interaction rules as combinations of Boolean covariates, that explain a\nmeasured response variable. The inductive inference process can be considered\nas an incompletely specified Boolean function synthesis problem. This\nincompleteness of the problem will also generate spurious inferences, which are\na serious threat to valid inductive inference rules. Using random Boolean data\nas a null model, here we attempt to measure the competition between valid and\nspurious inductive inference rules from a given data set. We formulate two\ngreedy search algorithms, which synthesize a given Boolean response variable in\na sparse disjunct normal form, and respectively a sparse generalized algebraic\nnormal form of the variables from the observation data, and we evaluate\nnumerically their performance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 19:06:15 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Andrecut", "M.", ""]]}, {"id": "1801.02075", "submitter": "Thomas Preu{\\ss}er", "authors": "Thomas B. Preu{\\ss}er", "title": "QBM - Mapping User-Specified Functions to Programmable Logic through a\n  QBF Satisfiability Problem", "comments": "Instance in Prenex CNF Track of QBFEVAL'17 competition:\n  http://www.qbflib.org/family_detail.php?idFamily=775", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a brief overview on the background behind the test set formulas\ngenerated by the QBM tool. After establishing its application context, its\nformal approach to the generation of QBF formulas and the concrete test set\nformulas are described. Finally, some related work will be credited and the\nsource to obtain the open-source tool will be identified.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 19:31:27 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Preu\u00dfer", "Thomas B.", ""]]}, {"id": "1801.02367", "submitter": "Philipp Ruemmer", "authors": "Hossein Hojjat (Rochester Institute of Technology) and Philipp\n  R\\\"ummer (Uppsala University)", "title": "Deciding and Interpolating Algebraic Data Types by Reduction (Technical\n  Report)", "comments": "Extended version of a paper presented at SYNASC 2017, Timisoara,\n  Romania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive algebraic data types (term algebras, ADTs) are one of the most\nwell-studied theories in logic, and find application in contexts including\nfunctional programming, modelling languages, proof assistants, and\nverification. At this point, several state-of-the-art theorem provers and SMT\nsolvers include tailor-made decision procedures for ADTs, and version 2.6 of\nthe SMT-LIB standard includes support for ADTs. We study an extremely simple\napproach to decide satisfiability of ADT constraints, the reduction of ADT\nconstraints to equisatisfiable constraints over uninterpreted functions (EUF)\nand linear integer arithmetic (LIA). We show that the reduction approach gives\nrise to both decision and Craig interpolation procedures in (extensions of)\nADTs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 10:16:18 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hojjat", "Hossein", "", "Rochester Institute of Technology"], ["R\u00fcmmer", "Philipp", "", "Uppsala University"]]}, {"id": "1801.02387", "submitter": "Ale\\v{s} Bizjak", "authors": "Arnon Avron and Liron Cohen", "title": "Applicable Mathematics in a Minimal Computational Theory of Sets", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 4 (October\n  16, 2018) lmcs:4891", "doi": "10.23638/LMCS-14(4:1)2018", "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In previous papers on this project a general static logical framework for\nformalizing and mechanizing set theories of different strength was suggested,\nand the power of some predicatively acceptable theories in that framework was\nexplored. In this work we first improve that framework by enriching it with\nmeans for coherently extending by definitions its theories, without destroying\nits static nature or violating any of the principles on which it is based. Then\nwe turn to investigate within the enriched framework the power of the minimal\n(predicatively acceptable) theory in it that proves the existence of infinite\nsets. We show that that theory is a computational theory, in the sense that\nevery element of its minimal transitive model is denoted by some of its closed\nterms. (That model happens to be the second universe in Jensen's hierarchy.)\nThen we show that already this minimal theory suffices for developing very\nlarge portions (if not all) of scientifically applicable mathematics. This\nrequires treating the collection of real numbers as a proper class, that is: a\nunary predicate which can be introduced in the theory by the static extension\nmethod described in the first part of the paper.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 11:30:03 GMT"}, {"version": "v2", "created": "Sun, 27 May 2018 10:08:20 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 07:42:34 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Avron", "Arnon", ""], ["Cohen", "Liron", ""]]}, {"id": "1801.02457", "submitter": "Tuba Yavuz", "authors": "Tuba Yavuz and Chelsea Metcalf", "title": "Heuristics for Selecting Predicates for Partial Predicate Abstraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of configuring partial predicate\nabstraction that combines two techniques that have been effective in analyzing\ninfinite-state systems: predicate abstraction and fixpoint approximations. A\nfundamental problem in partial predicate abstraction is deciding the variables\nto be abstracted and the predicates to be used. In this paper, we consider\nsystems modeled using linear integer arithmetic and investigate an alternative\napproach to counter-example guided abstraction refinement. We devise two\nheuristics that search for predicates that are likely to be precise. The first\nheuristic performs the search on the problem instance to be verified. The other\nheuristic leverages verification results on the smaller instances of the\nproblem. We report experimental results for CTL model checking and discuss\nadvantages and disadvantages of each approach.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 17:49:37 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Yavuz", "Tuba", ""], ["Metcalf", "Chelsea", ""]]}, {"id": "1801.02484", "submitter": "Srinivas Pinisetty", "authors": "Srinivas Pinisetty, Thibaud Antignac, David Sands, Gerardo Schneider", "title": "Monitoring Data Minimisation", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CR cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data minimisation is a privacy enhancing principle, stating that personal\ndata collected should be no more than necessary for the specific purpose\nconsented by the user. Checking that a program satisfies the data minimisation\nprinciple is not easy, even for the simple case when considering deterministic\nprograms-as-functions. In this paper we prove (im)possibility results\nconcerning runtime monitoring of (non-)minimality for deterministic programs\nboth when the program has one input source (monolithic) and for the more\ngeneral case when inputs come from independent sources (distributed case). We\npropose monitoring mechanisms where a monitor observes the inputs and the\noutputs of a program, to detect violation of data minimisation policies. We\nshow that monitorability of (non) minimality is decidable only for specific\ncases, and detection of satisfaction of different notions of minimality in\nundecidable in general. That said, we show that under certain conditions\nmonitorability is decidable and we provide an algorithm and a bound to check\nsuch properties in a pre-deployment controlled environment, also being able to\ncompute a minimiser for the given program. Finally, we provide a\nproof-of-concept implementation for both offline and online monitoring and\napply that to some case studies.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:04:15 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Pinisetty", "Srinivas", ""], ["Antignac", "Thibaud", ""], ["Sands", "David", ""], ["Schneider", "Gerardo", ""]]}, {"id": "1801.02857", "submitter": "Ale\\v{s} Bizjak", "authors": "Youssef Arbach, David S. Karcher, Kirstin Peters, Uwe Nestmann", "title": "Dynamic Causality in Event Structures", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 1 (February\n  27, 2018) lmcs:4317", "doi": "10.23638/LMCS-14(1:17)2018", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event Structures (ESs) address the representation of direct relationships\nbetween individual events, usually capturing the notions of causality and\nconflict. Up to now, such relationships have been static, i.e., they cannot\nchange during a system run. Thus, the common ESs only model a static view on\nsystems. We make causality dynamic by allowing causal dependencies between some\nevents to be changed by occurrences of other events. We first model and study\nthe case in which events may entail the removal of causal dependencies, then we\nconsider the addition of causal dependencies, and finally we combine both\napproaches in the so-called Dynamic Causality ESs. For all three newly defined\ntypes of ESs, we study their expressive power in comparison to the well-known\nPrime ESs, Dual ESs, Extended Bundle ESs, and ESs for Resolvable Conflicts.\nInterestingly, Dynamic Causality ESs subsume Extended Bundle ESs and Dual ESs\nbut are incomparable with ESs for Resolvable Conflicts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 10:00:00 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 16:27:39 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Arbach", "Youssef", ""], ["Karcher", "David S.", ""], ["Peters", "Kirstin", ""], ["Nestmann", "Uwe", ""]]}, {"id": "1801.03833", "submitter": "Pierre-Loic Garoche", "authors": "Guillaume Davy (Toulouse), Eric F\\'eron (GATECH), Pierre-Lo\\\"ic\n  Garoche (Toulouse), Didier Henrion (LAAS-MAC)", "title": "Experiments in Verification of Linear Model Predictive Control:\n  Automatic Generation and Formal Verification of an Interior Point Method\n  Algorithm", "comments": null, "journal-ref": "22nd International Conference on Logic for Programming Artificial\n  Intelligence and Reasoning (LPAR-22), Nov 2018, Awassa, Ethiopia.\n  https://easychair.org/smart-program/LPAR-22/", "doi": null, "report-no": "Rapport LAAS n{\\textdegree} 18009", "categories": "cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical control of cyber-physical systems used to rely on basic linear\ncontrollers. These controllers provided a safe and robust behavior but lack the\nability to perform more complex controls such as aggressive maneuvering or\nperforming fuel-efficient controls. Another approach called optimal control is\ncapable of computing such difficult trajectories but lacks the ability to adapt\nto dynamic changes in the environment. In both cases, the control was designed\noffline, relying on more or less complex algorithms to find the appropriate\nparameters. More recent kinds of approaches such as Linear Model-Predictive\nControl (MPC) rely on the online use of convex optimization to compute the best\ncontrol at each sample time. In these settings, optimization algorithms are\nspecialized for the specific control problem and embed on the device. This\npaper proposes to revisit the code generation of an interior point method\n(IPM)algorithm, an efficient family of convex optimization, focusing on the\nproof of its implementation at code level. Our approach relies on the code\nspecialization phase to produce additional annotations formalizing the intented\nspecification of the algorithm. Deductive methods are then used to prove\nautomatically the validity of these assertions. Since the algorithm is complex,\nadditional lemmas are also produced, allowing the complete proofto be checked\nby SMT solvers only. This work is the first to address the effective formal\nproof of an IPM algorithm. Theapproach could also be generalized more\nsystematically to code generation frameworks, producing proof certificate along\nthe code, for numerical intensive software.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 15:53:47 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 08:08:16 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Davy", "Guillaume", "", "Toulouse"], ["F\u00e9ron", "Eric", "", "GATECH"], ["Garoche", "Pierre-Lo\u00efc", "", "Toulouse"], ["Henrion", "Didier", "", "LAAS-MAC"]]}, {"id": "1801.03859", "submitter": "Tom van Dijk", "authors": "Tom van Dijk", "title": "Oink: an Implementation and Evaluation of Modern Parity Game Solvers", "comments": "Accepted at TACAS 2018", "journal-ref": null, "doi": "10.1007/978-3-319-89960-2_16", "report-no": null, "categories": "cs.LO cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parity games have important practical applications in formal verification and\nsynthesis, especially to solve the model-checking problem of the modal\nmu-calculus. They are also interesting from the theory perspective, as they are\nwidely believed to admit a polynomial solution, but so far no such algorithm is\nknown. In recent years, a number of new algorithms and improvements to existing\nalgorithms have been proposed. We implement a new and easy to extend tool Oink,\nwhich is a high-performance implementation of modern parity game algorithms. We\nfurther present a comprehensive empirical evaluation of modern parity game\nalgorithms and solvers, both on real world benchmarks and randomly generated\ngames. Our experiments show that our new tool Oink outperforms the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 16:40:28 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 17:46:44 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 11:36:47 GMT"}, {"version": "v4", "created": "Mon, 5 Mar 2018 17:13:51 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["van Dijk", "Tom", ""]]}, {"id": "1801.03886", "submitter": "Thorsten Wissmann", "authors": "Kazuyuki Asada, Naoki Kobayashi, Ryoma Sin'ya, Takeshi Tsukada", "title": "Almost Every Simply Typed Lambda-Term Has a Long Beta-Reduction Sequence", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (February\n  22, 2019) lmcs:5203", "doi": "10.23638/LMCS-15(1:16)2019", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that the length of a beta-reduction sequence of a simply\ntyped lambda-term of order k can be huge; it is as large as k-fold exponential\nin the size of the lambda-term in the worst case. We consider the following\nrelevant question about quantitative properties, instead of the worst case: how\nmany simply typed lambda-terms have very long reduction sequences? We provide a\npartial answer to this question, by showing that asymptotically almost every\nsimply typed lambda-term of order k has a reduction sequence as long as\n(k-1)-fold exponential in the term size, under the assumption that the arity of\nfunctions and the number of variables that may occur in every subterm are\nbounded above by a constant. To prove it, we have extended the infinite monkey\ntheorem for strings to a parametrized one for regular tree languages, which may\nbe of independent interest. The work has been motivated by quantitative\nanalysis of the complexity of higher-order model checking.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 17:23:57 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 15:03:34 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 09:42:31 GMT"}, {"version": "v4", "created": "Mon, 21 Jan 2019 09:02:09 GMT"}, {"version": "v5", "created": "Thu, 21 Feb 2019 14:11:20 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Asada", "Kazuyuki", ""], ["Kobayashi", "Naoki", ""], ["Sin'ya", "Ryoma", ""], ["Tsukada", "Takeshi", ""]]}, {"id": "1801.04026", "submitter": "Peter H\\\"ofner", "authors": "Rudolf Berghammer, Hitoshi Furusawa, Walter Guttmann, Peter H\\\"ofner", "title": "Relational Characterisations of Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary relations are one of the standard ways to encode, characterise and\nreason about graphs. Relation algebras provide equational axioms for a large\nfragment of the calculus of binary relations. Although relations are standard\ntools in many areas of mathematics and computing, researchers usually fall back\nto point-wise reasoning when it comes to arguments about paths in a graph. We\npresent a purely algebraic way to specify different kinds of paths in relation\nalgebras. We study the relationship between paths with a designated root vertex\nand paths without such a vertex. Since we stay in first-order logic this\ndevelopment helps with mechanising proofs.To demonstrate the applicability of\nthe algebraic framework we verify the correctness of three basic graph\nalgorithms. All results of this paper are formally verified using the\ninteractive proof assistant Isabelle/HOL.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 00:59:57 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 00:28:37 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Berghammer", "Rudolf", ""], ["Furusawa", "Hitoshi", ""], ["Guttmann", "Walter", ""], ["H\u00f6fner", "Peter", ""]]}, {"id": "1801.04066", "submitter": "Vivek Nigam", "authors": "Vivek Nigam, Carolyn Talcott, Abr\\~aao Aires Urquiza", "title": "Symbolic Timed Observational Equivalence", "comments": "New version with corrected Typos, improved motivation, explanation,\n  better notation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intruders can infer properties of a system by measuring the time it takes for\nthe system to respond to some request of a given protocol, that is, by\nexploiting time side channels. These properties may help intruders distinguish\nwhether a system is a honeypot or concrete system helping him avoid defense\nmechanisms, or track a user among others violating his privacy. Observational\nequivalence is the technical machinery used for verifying whether two systems\nare distinguishable. Moreover, efficient symbolic methods have been developed\nfor automating the check of observational equivalence of systems. This paper\nintroduces a novel definition of timed observational equivalence which also\ndistinguishes systems according to their time side channels. Moreover, as our\ndefinition uses symbolic time constraints, it can be automated by using\nSMT-solvers.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 06:11:57 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 08:43:40 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Nigam", "Vivek", ""], ["Talcott", "Carolyn", ""], ["Urquiza", "Abr\u00e3ao Aires", ""]]}, {"id": "1801.04163", "submitter": "Michael Lettmann", "authors": "Michael Peter Lettmann and Nicolas Peltier", "title": "A Tableaux Calculus for Reducing Proof Size", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tableau calculus is proposed, based on a compressed representation of\nclauses, where literals sharing a similar shape may be merged. The inferences\napplied on these literals are fused when possible, which reduces the size of\nthe proof. It is shown that the obtained proof procedure is sound,\nrefutationally complete and allows to reduce the size of the tableau by an\nexponential factor. The approach is compatible with all usual refinements of\ntableaux.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 13:39:46 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Lettmann", "Michael Peter", ""], ["Peltier", "Nicolas", ""]]}, {"id": "1801.04185", "submitter": "Johannes Reich", "authors": "Johannes Reich and Tizian Schr\\\"oder", "title": "A reference model for interaction semantics", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a reference model for interaction semantics\namong communicating discrete systems to guide the discourse on\ninteroperability.\n  The necessary set of unifying concepts is small and comprises essentially the\nnotion of discrete systems interacting by exchanging information. It is based\non a simple, but nevertheless complete classification of system interactions\nwith respect to information transport and processing. Information transport can\nonly be uni- or bidirectional and information processing is subclassified along\nthe binary dimensions of state, determinism and synchronicity.\n  For interactions with bidirectional information flow we are able to define a\ncriterion for a layered structure of systems: we name a bidirectional\ninteraction \"horizontal\" if all interacting systems behave the same with\nrespect to state, determinism and synchronicity and we name it \"vertical\" ---\nproviding a semantic direction --- if there is a behavioral asymmetry between\nthe interacting systems with respect to these properties.\n  It is shown that horizontal interactions are essentially stateful,\nasynchronous and nondeterministic and are described by protocols. Vertical\ninteractions are essentially top-down-usage, described by object models or\noperations, and bottom-up-observation, described by anonymous events.\n  The reference model thereby helps us to understand the significant\nrelationships that are created between interacting discrete systems by their\ninteractions and guides us on how to talk about discrete system\ninteroperability.\n  To show its conceptual power, we apply the reference model to assess several\nother architectural models, communication technologies and so called software\ndesign or architectural styles like SOA and REST.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 12:49:52 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 17:39:47 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Reich", "Johannes", ""], ["Schr\u00f6der", "Tizian", ""]]}, {"id": "1801.04263", "submitter": "Nathalie Cauchi", "authors": "Nathalie Cauchi, Khaza Anuarul Hoque, Alessandro Abate, Marielle\n  Stoelinga", "title": "Efficient Probabilistic Model Checking of Smart Building Maintenance\n  using Fault Maintenance Trees", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems, like Smart Buildings and power plants, have to meet\nhigh standards, both in terms of reliability and availability. Such metrics are\ntypically evaluated using Fault trees (FTs) and do not consider maintenance\nstrategies which can significantly improve lifespan and reliability. Fault\nMaintenance trees (FMTs) -- an extension of FTs that also incorporate\nmaintenance and degradation models, are a novel technique that serve as a good\nplanning platform for balancing total costs and dependability of a system. In\nthis work, we apply the FMT formalism to a Smart Building application. We\npropose a framework for modelling FMTs using probabilistic model checking and\npresent an algorithm for performing abstraction of the FMT in order to reduce\nthe size of its equivalent Continuous Time Markov Chain. This allows us to\napply the probabilistic model checking more efficiently. We demonstrate the\napplicability of our proposed approach by evaluating various dependability\nmetrics and maintenance strategies of a Heating, Ventilation and\nAir-Conditioning system's FMT.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 18:43:32 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Cauchi", "Nathalie", ""], ["Hoque", "Khaza Anuarul", ""], ["Abate", "Alessandro", ""], ["Stoelinga", "Marielle", ""]]}, {"id": "1801.04315", "submitter": "Wil Van Der Aalst", "authors": "Wil M.P. van der Aalst", "title": "Markings in Perpetual Free-Choice Nets Are Fully Characterized by Their\n  Enabled Transitions", "comments": "The proof of Theorem 3 has been changed. The original proof was\n  incomplete. The original proof could be completed, but this complicates\n  things and turns out to be rather indirect. Therefore, the new proof uses a\n  more direct and self-contained approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A marked Petri net is lucent if there are no two different reachable markings\nenabling the same set of transitions, i.e., states are fully characterized by\nthe transitions they enable. This paper explores the class of marked Petri nets\nthat are lucent and proves that perpetual marked free-choice nets are lucent.\nPerpetual free-choice nets are free-choice Petri nets that are live and bounded\nand have a home cluster, i.e., there is a cluster such that from any reachable\nstate there is a reachable state marking the places of this cluster. A home\ncluster in a perpetual net serves as a \"regeneration point\" of the process,\ne.g., to start a new process instance (case, job, cycle, etc.). Many\n\"well-behaved\" process models fall into this class. For example, the class of\nshort-circuited sound workflow nets is perpetual. Also, the class of processes\nsatisfying the conditions of the $\\alpha$ algorithm for process discovery falls\ninto this category. This paper shows that the states in a perpetual marked\nfree-choice net are fully characterized by the transitions they enable, i.e.,\nthese process models are lucent. Having a one-to-one correspondence between the\nactions that can happen and the state of the process, is valuable in a variety\nof application domains. The full characterization of markings in terms of\nenabled transitions makes perpetual free-choice nets interesting for workflow\nanalysis and process mining. In fact, we anticipate new verification, process\ndiscovery, and conformance checking techniques for the subclasses identified.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 21:02:56 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 21:36:57 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 15:19:31 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["van der Aalst", "Wil M. P.", ""]]}, {"id": "1801.04337", "submitter": "Howard Straubing", "authors": "Howard Straubing", "title": "Forest Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Tilson's theory of the algebra of finite categories, in particular,\nthe Derived Category Theorem, to the setting of forest algebras. As an\nillustration of the usefulness of this method, we provide a new proof of a\nresult of Place and Segoufin characterizing locally testable tree languages.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 22:10:30 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Straubing", "Howard", ""]]}, {"id": "1801.04387", "submitter": "Daniel Hern\\'andez", "authors": "Daniel Hern\\'andez and Claudio Gutierrez and Renzo Angles", "title": "The Problem of Correlation and Substitution in SPARQL -- Extended\n  Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Implementations of a standard language are expected to give same outputs to\nidentical queries. In this paper we study why different implementations of\nSPARQL (Fuseki, Virtuoso, Blazegraph and rdf4j) behave differently when\nevaluating queries with correlated variables. We show that at the core of this\nproblem lies the historically troubling notion of logical substitution. We\npresent a formal framework to study this issue based on Datalog that besides\nclarifying the problem, gives a solid base to define and implement nesting.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 06:30:59 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 21:30:17 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Hern\u00e1ndez", "Daniel", ""], ["Gutierrez", "Claudio", ""], ["Angles", "Renzo", ""]]}, {"id": "1801.04886", "submitter": "Khaza Anuarul Hoque", "authors": "Khaza Anuarul Hoque, Otmane Ait Mohamed, Yvon Savaria", "title": "Dependability modeling and optimization of triple modular redundancy\n  partitioning for SRAM-based FPGAs", "comments": "Published in Reliability Engineering & System Safety Volume 182,\n  February 2019, Pages 107-119", "journal-ref": "Reliability Engineering & System Safety Volume 182, February 2019,\n  Pages 107-119", "doi": "10.1016/j.ress.2018.10.011", "report-no": null, "categories": "cs.DC cs.AR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SRAM-based FPGAs are popular in the aerospace industry for their field\nprogrammability and low cost. However, they suffer from cosmic\nradiation-induced Single Event Upsets (SEUs). Triple Modular Redundancy (TMR)\nis a well-known technique to mitigate SEUs in FPGAs that is often used with\nanother SEU mitigation technique known as configuration scrubbing. Traditional\nTMR provides protection against a single fault at a time, while partitioned TMR\nprovides improved reliability and availability. In this paper, we present a\nmethodology to analyze TMR partitioning at early design stage using\nprobabilistic model checking. The proposed formal model can capture both single\nand multiple-cell upset scenarios, regardless of any assumption of equal\npartition sizes. Starting with a high-level description of a design, a Markov\nmodel is constructed from the Data Flow Graph (DFG) using a specified number of\npartitions, a component characterization library and a user defined scrub rate.\nSuch a model and exhaustive analysis captures all the considered failures and\nrepairs possible in the system within the radiation environment. Various\nreliability and availability properties are then verified automatically using\nthe PRISM model checker exploring the relationship between the scrub frequency\nand the number of TMR partitions required to meet the design requirements.\nAlso, the reported results show that based on a known voter failure rate, it is\npossible to find an optimal number of partitions at early design stages using\nour proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 19:00:22 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 20:38:27 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 17:11:31 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Hoque", "Khaza Anuarul", ""], ["Mohamed", "Otmane Ait", ""], ["Savaria", "Yvon", ""]]}, {"id": "1801.04979", "submitter": "Maria Spichkova", "authors": "Maria Spichkova", "title": "Formal specification of the FlexRay protocol using FocusST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FlexRay is a communication protocol developed by the FlexRay Consortium. The\ncore members of the Consortium are Freescale Semiconductor, Robert Bosch GmbH,\nNXP Semiconductors, BMW, Volkswagen, Daimler, and General Motors, and the\nprotocol was respectively oriented towards embedded systems in the automotive\ndomain. This paper presents a formal specification of the FlexRay protocol\nusing the FocusST framework. This work extends our previous research of formal\nspecifications of this protocol using Focus formal language.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 09:30:50 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Spichkova", "Maria", ""]]}, {"id": "1801.05052", "submitter": "Ale\\v{s} Bizjak", "authors": "Lorenzo Bettini, Viviana Bono, Mariangiola Dezani-Ciancaglini, Paola\n  Giannini, Betti Venneri", "title": "Java & Lambda: a Featherweight Story", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 3 (September\n  5, 2018) lmcs:4803", "doi": "10.23638/LMCS-14(3:17)2018", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present FJ&$\\lambda$, a new core calculus that extends Featherweight Java\n(FJ) with interfaces, supporting multiple inheritance in a restricted form,\n$\\lambda$-expressions, and intersection types. Our main goal is to formalise\nhow lambdas and intersection types are grafted on Java 8, by studying their\nproperties in a formal setting. We show how intersection types play a\nsignificant role in several cases, in particular in the typecast of a\n$\\lambda$-expression and in the typing of conditional expressions. We also\nembody interface \\emph{default methods} in FJ&$\\lambda$, since they increase\nthe dynamism of $\\lambda$-expressions, by allowing these methods to be called\non $\\lambda$-expressions.\n  The crucial point in Java 8 and in our calculus is that $\\lambda$-expressions\ncan have various types according to the context requirements (target types):\nindeed, Java code does not compile when $\\lambda$-expressions come without\ntarget types. In particular, in the operational semantics we must record target\ntypes by decorating $\\lambda$-expressions, otherwise they would be lost in the\nruntime expressions.\n  We prove the subject reduction property and progress for the resulting\ncalculus, and we give a type inference algorithm that returns the type of a\ngiven program if it is well typed. The design of FJ&$\\lambda$ has been driven\nby the aim of making it a subset of Java 8, while preserving the elegance and\ncompactness of FJ. Indeed, FJ&$\\lambda$ programs are typed and behave the same\nas Java programs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 22:31:54 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 11:23:05 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 20:12:25 GMT"}, {"version": "v4", "created": "Mon, 3 Sep 2018 07:17:23 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Bettini", "Lorenzo", ""], ["Bono", "Viviana", ""], ["Dezani-Ciancaglini", "Mariangiola", ""], ["Giannini", "Paola", ""], ["Venneri", "Betti", ""]]}, {"id": "1801.05150", "submitter": "Flavien Breuvart", "authors": "Flavien Breuvart", "title": "On the characterization of models of H* : The operational aspect", "comments": "arXiv admin note: text overlap with arXiv:1603.07259", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We give a characterization, with respect to a large class of models of\nuntyped $\\lambda$-calculus, of those models that are fully abstract for\nhead-normalization, i.e., whose equational theory is $\\mathcal{H}^*$. An\nextensional K-model $D$ is fully abstract if and only if it is hyperimmune,\ni.e., non-well founded chains of elements of $D$ cannot be captured by any\nrecursive function.\n  This article share its first title with its companion paper and a short\nversion. It is a standalone paper that present a purely syntactical proof of\nthe result as opposed to its companion paper that present an independent and\npurely semantical proof of the exact same result.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:33:08 GMT"}], "update_date": "2018-01-20", "authors_parsed": [["Breuvart", "Flavien", ""]]}, {"id": "1801.05153", "submitter": "Flavien Breuvart", "authors": "Flavien Breuvart", "title": "Refining Properties of Filter Models: Sensibility, Approximability and\n  Reducibility", "comments": "long version, draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the tedious link between the properties of\nsensibility and approximability of models of untyped {\\lambda}-calculus.\nApproximability is known to be a slightly, but strictly stronger property that\nsensibility. However, we will see that so far, each and every (filter) model\nthat have been proven sensible are in fact approximable. We explain this result\nas a weakness of the sole known approach of sensibility: the Tait reducibility\ncandidates and its realizability variants. In fact, we will reduce the\napproximability of a filter model D for the {\\lambda}-calculus to the\nsensibility of D but for an extension of the {\\lambda}-calculus that we call\n{\\lambda}-calculus with D-tests. Then we show that traditional proofs of\nsensibility of D for the {\\lambda}-calculus are smoothly extendable for this\n{\\lambda}-calculus with D-tests.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:38:45 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 11:30:53 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Breuvart", "Flavien", ""]]}, {"id": "1801.05950", "submitter": "Lindsey Kuper", "authors": "Lindsey Kuper, Guy Katz, Justin Gottschlich, Kyle Julian, Clark\n  Barrett, Mykel Kochenderfer", "title": "Toward Scalable Verification for Safety-Critical Deep Networks", "comments": "Accepted for presentation at SysML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of deep neural networks for safety-critical applications,\nsuch as autonomous driving and flight control, raises concerns about their\nsafety and reliability. Formal verification can address these concerns by\nguaranteeing that a deep learning system operates as intended, but the state of\nthe art is limited to small systems. In this work-in-progress report we give an\noverview of our work on mitigating this difficulty, by pursuing two\ncomplementary directions: devising scalable verification techniques, and\nidentifying design choices that result in deep learning systems that are more\namenable to verification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 06:27:57 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 21:25:11 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Kuper", "Lindsey", ""], ["Katz", "Guy", ""], ["Gottschlich", "Justin", ""], ["Julian", "Kyle", ""], ["Barrett", "Clark", ""], ["Kochenderfer", "Mykel", ""]]}, {"id": "1801.05965", "submitter": "Thorsten Wissmann", "authors": "Manuel Bodirsky, Johannes Greiner", "title": "The Complexity of Combinations of Qualitative Constraint Satisfaction\n  Problems", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 1 (February\n  20, 2020) lmcs:6129", "doi": "10.23638/LMCS-16(1:21)2020", "report-no": null, "categories": "math.LO cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The CSP of a first-order theory $T$ is the problem of deciding for a given\nfinite set $S$ of atomic formulas whether $T \\cup S$ is satisfiable. Let $T_1$\nand $T_2$ be two theories with countably infinite models and disjoint\nsignatures. Nelson and Oppen presented conditions that imply decidability (or\npolynomial-time decidability) of $\\mathrm{CSP}(T_1 \\cup T_2)$ under the\nassumption that $\\mathrm{CSP}(T_1)$ and $\\mathrm{CSP}(T_2)$ are decidable (or\npolynomial-time decidable). We show that for a large class of\n$\\omega$-categorical theories $T_1, T_2$ the Nelson-Oppen conditions are not\nonly sufficient, but also necessary for polynomial-time tractability of\n$\\mathrm{CSP}(T_1 \\cup T_2)$ (unless P=NP).\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 08:13:30 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 07:45:03 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 09:29:28 GMT"}, {"version": "v4", "created": "Tue, 11 Feb 2020 12:00:46 GMT"}, {"version": "v5", "created": "Wed, 19 Feb 2020 14:42:28 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Greiner", "Johannes", ""]]}, {"id": "1801.05994", "submitter": "Ale\\v{s} Bizjak", "authors": "Ga\\\"elle Fontaine and Yde Venema", "title": "Some model theory for the modal $\\mu$-calculus: syntactic\n  characterisations of semantic properties", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 1 (February\n  6, 2018) lmcs:4261", "doi": "10.23638/LMCS-14(1:14)2018", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the theory of the modal $\\mu$-calculus by proving\nsome model-theoretic results. More in particular, we discuss a number of\nsemantic properties pertaining to formulas of the modal $\\mu$-calculus. For\neach of these properties we provide a corresponding syntactic fragment, in the\nsense that a $\\mu$-formula $\\xi$ has the given property iff it is equivalent to\na formula $\\xi'$ in the corresponding fragment. Since this formula $\\xi'$ will\nalways be effectively obtainable from $\\xi$, as a corollary, for each of the\nproperties under discussion, we prove that it is decidable in elementary time\nwhether a given $\\mu$-calculus formula has the property or not.\n  The properties that we study all concern the way in which the meaning of a\nformula $\\xi$ in a model depends on the meaning of a single, fixed proposition\nletter $p$. For example, consider a formula $\\xi$ which is monotone in $p$;\nsuch a formula a formula $\\xi$ is called continuous (respectively, fully\nadditive), if in addition it satisfies the property that, if $\\xi$ is true at a\nstate $s$ then there is a finite set (respectively, a singleton set) $U$ such\nthat $\\xi$ remains true at $s$ if we restrict the interpretation of $p$ to the\nset $U$. Each of the properties that we consider is, in a similar way,\nassociated with one of the following special kinds of subset of a tree model:\nsingletons, finite sets, finitely branching subtrees, noetherian subtrees\n(i.e., without infinite paths), and branches.\n  Our proofs for these characterization results will be automata-theoretic in\nnature; we will see that the effectively defined maps on formulas are in fact\ninduced by rather simple transformations on modal automata. Thus our results\ncan also be seen as a contribution to the model theory of modal automata.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 12:56:51 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 10:39:17 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Fontaine", "Ga\u00eblle", ""], ["Venema", "Yde", ""]]}, {"id": "1801.06793", "submitter": "Moez AbdelGawad", "authors": "Moez AbdelGawad and Robert Cartwright", "title": "NOOP: A Domain-Theoretic Model of Nominally-Typed OOP", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of industrial-strength object-oriented (OO) software is written\nusing nominally-typed OO programming languages. Extant domain-theoretic models\nof OOP developed to analyze OO type systems miss, however, a crucial feature of\nthese mainstream OO languages: nominality. This paper presents the construction\nof NOOP as the first domain-theoretic model of OOP that includes full\nclass/type names information found in nominally-typed OOP. Inclusion of nominal\ninformation in objects of NOOP and asserting that type inheritance in\nstatically-typed OO programming languages is an inherently nominal notion allow\nreadily proving that type inheritance and subtyping are completely identified\nin these languages. This conclusion is in full agreement with intuitions of\ndevelopers and language designers of these OO languages, and contrary to the\nbelief that \"inheritance is not subtyping,\" which came from assuming\nnon-nominal (a.k.a., structural) models of OOP.\n  To motivate the construction of NOOP, this paper briefly presents the\nbenefits of nominal-typing to mainstream OO developers and OO language\ndesigners, as compared to structural-typing. After presenting NOOP, the paper\nfurther briefly compares NOOP to the most widely known domain-theoretic models\nof OOP. Leveraging the development of NOOP, the comparisons presented in this\npaper provide clear, brief and precise technical and mathematical accounts for\nthe relation between nominal and structural OO type systems. NOOP, thus,\nprovides a firmer semantic foundation for analyzing and progressing\nnominally-typed OO programming languages.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 09:17:31 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["AbdelGawad", "Moez", ""], ["Cartwright", "Robert", ""]]}, {"id": "1801.06883", "submitter": "Harley Eades PhD", "authors": "Valeria de Paiva and Harley Eades III", "title": "Dialectica Categories for the Lambek Calculus", "comments": null, "journal-ref": "In: Artemov S., Nerode A. (eds) Logical Foundations of Computer\n  Science. LFCS 2018. Lecture Notes in Computer Science, vol 10703, . Springer,\n  Cham", "doi": "10.1007/978-3-319-72056-2_16", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the old work of de Paiva on the models of the Lambek Calculus in\ndialectica models making sure that the syntactic details that were sketchy on\nthe first version got completed and verified. We extend the Lambek Calculus\nwith a \\kappa modality, inspired by Yetter's work, which makes the calculus\ncommutative. Then we add the of-course modality !, as Girard did, to\nre-introduce weakening and contraction for all formulas and get back the full\npower of intuitionistic and classical logic. We also present the categorical\nsemantics, proved sound and complete. Finally we show the traditional\nproperties of type systems, like subject reduction, the Church-Rosser theorem\nand normalization for the calculi of extended modalities, which we did not have\nbefore.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 19:32:37 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["de Paiva", "Valeria", ""], ["Eades", "Harley", "III"]]}, {"id": "1801.06886", "submitter": "Harley Eades PhD", "authors": "Harley Eades III", "title": "An Intuitionistic Linear Logical Semantics of SAND Attack Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new logical foundation of SAND attack trees in\nintuitionistic linear logic. This new foundation is based on a new logic called\nthe Attack Tree Linear Logic (ATLL). Before introducing ATLL we given several\nnew logical models of attack trees, the first, is a very basic model based in\ntruth tables. Then we lift this semantics into a semantics of attack trees\nbased on lineales which introduces implication, but this can be further lifted\ninto a dialectica model which ATLL is based. One important feature of ATLL is\nthat it supports full distributivity of sequential conjunction over choice.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 19:40:42 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Eades", "Harley", "III"]]}, {"id": "1801.07438", "submitter": "David Cerna", "authors": "David M. Cerna and Temur Kutsia", "title": "Higher-Order Equational Pattern Anti-Unification [Preprint]", "comments": "Submitted to FSCD 2018", "journal-ref": null, "doi": "10.4230/LIPIcs.FSCD.2018.12", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider anti-unification for simply typed lambda terms in associative,\ncommutative, and associative-commutative theories and develop a sound and\ncomplete algorithm which takes two lambda terms and computes their\ngeneralizations in the form of higher-order patterns. The problem is finitary:\nthe minimal complete set of generalizations contains finitely many elements. We\ndefine the notion of optimal solution and investigate special fragments of the\nproblem for which the optimal solution can be computed in linear or polynomial\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 08:44:54 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Cerna", "David M.", ""], ["Kutsia", "Temur", ""]]}, {"id": "1801.07485", "submitter": "Florian Steinberg", "authors": "Bruce M. Kapron and Florian Steinberg", "title": "Type-two polynomial-time and restricted lookahead", "comments": null, "journal-ref": "Theoretical Computer Science, Volume 813, 2020, Pages 1-19", "doi": "10.1016/j.tcs.2019.07.003", "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an alternate characterization of type-two polynomial-time\ncomputability, with the goal of making second-order complexity theory more\napproachable. We rely on the usual oracle machines to model programs with\nsubroutine calls. In contrast to previous results, the use of higher-order\nobjects as running times is avoided, either explicitly or implicitly. Instead,\nregular polynomials are used. This is achieved by refining the notion of\noracle-polynomial-time introduced by Cook. We impose a further restriction on\nthe oracle interactions to force feasibility. Both the restriction as well as\nits purpose are very simple: it is well-known that Cook's model allows\npolynomial depth iteration of functional inputs with no restrictions on size,\nand thus does not guarantee that polynomial-time computability is preserved. To\nmend this we restrict the number of lookahead revisions, that is the number of\ntimes a query can be asked that is bigger than any of the previous queries. We\nprove that this leads to a class of feasible functionals and that all feasible\nproblems can be solved within this class if one is allowed to separate a task\ninto efficiently solvable subtasks. Formally put: the closure of our class\nunder lambda-abstraction and application includes all feasible operations. We\nalso revisit the very similar class of strongly polynomial-time computable\noperators previously introduced by Kawamura and Steinberg. We prove it to be\nstrictly included in our class and, somewhat surprisingly, to have the same\nclosure property. This can be attributed to properties of the limited recursion\noperator: It is not strongly polynomial-time computable but decomposes into two\nsuch operations and lies in our class.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 11:24:49 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 19:57:51 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kapron", "Bruce M.", ""], ["Steinberg", "Florian", ""]]}, {"id": "1801.07528", "submitter": "Thorsten Wissmann", "authors": "Predrag Jani\\v{c}i\\'c, Filip Mari\\'c, Marko Malikovi\\'c", "title": "Computer-Assisted Proving of Combinatorial Conjectures Over Finite\n  Domains: A Case Study of a Chess Conjecture", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (March 29,\n  2019) lmcs:5328", "doi": "10.23638/LMCS-15(1:34)2019", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are several approaches for using computers in deriving mathematical\nproofs. For their illustration, we provide an in-depth study of using computer\nsupport for proving one complex combinatorial conjecture -- correctness of a\nstrategy for the chess KRK endgame. The final, machine verifiable, result\npresented in this paper is that there is a winning strategy for white in the\nKRK endgame generalized to $n \\times n$ board (for natural $n$ greater than\n$3$). We demonstrate that different approaches for computer-based theorem\nproving work best together and in synergy and that the technology currently\navailable is powerful enough for providing significant help to humans deriving\ncomplex proofs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 13:26:35 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 15:36:45 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 15:23:37 GMT"}, {"version": "v4", "created": "Wed, 27 Mar 2019 18:34:10 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Jani\u010di\u0107", "Predrag", ""], ["Mari\u0107", "Filip", ""], ["Malikovi\u0107", "Marko", ""]]}, {"id": "1801.07647", "submitter": "Eugen Z\\u{a}linescu", "authors": "Serdar Erbatur and Martin Hofmann and Eugen Zalinescu", "title": "Enforcing Programming Guidelines with Region Types and Effects", "comments": "long version of APLAS'17 paper", "journal-ref": null, "doi": "10.1007/978-3-319-71237-6_5", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a new type and effect system for Java which can be\nused to ensure adherence to guidelines for secure web programming. The system\nis based on the region and effect system by Beringer, Grabowski, and Hofmann.\nIt improves upon it by being parametrized over an arbitrary guideline supplied\nin the form of a finite monoid or automaton and a type annotation or mockup\ncode for external methods. Furthermore, we add a powerful type inference based\non precise interprocedural analysis and provide an implementation in the Soot\nframework which has been tested on a number of benchmarks including large parts\nof the Stanford SecuriBench.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:38:18 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Erbatur", "Serdar", ""], ["Hofmann", "Martin", ""], ["Zalinescu", "Eugen", ""]]}, {"id": "1801.07664", "submitter": "Andrew Pitts", "authors": "Daniel R. Licata, Ian Orton, Andrew M. Pitts, Bas Spitters", "title": "Internal Universes in Models of Homotopy Type Theory", "comments": "In H. Kirchner (ed), Proceedings of the 3rd International Conference\n  on Formal Structures for Computation and Deduction (FSCD 2018), Leibniz\n  International Proceedings in Informatics (LIPIcs), Vol. 108, pp. 22:1-22:17,\n  2018", "journal-ref": "Leibniz International Proceedings in Informatics (LIPIcs), Vol.\n  108, pp. 22:1-22:17, 2018", "doi": "10.4230/LIPIcs.FSCD.2018.22", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We begin by recalling the essentially global character of universes in\nvarious models of homotopy type theory, which prevents a straightforward\naxiomatization of their properties using the internal language of the presheaf\ntoposes from which these model are constructed. We get around this problem by\nextending the internal language with a modal operator for expressing properties\nof global elements. In this setting we show how to construct a universe that\nclassifies the Cohen-Coquand-Huber-M\\\"ortberg (CCHM) notion of fibration from\ntheir cubical sets model, starting from the assumption that the interval is\ntiny - a property that the interval in cubical sets does indeed have. This\nleads to an elementary axiomatization of that and related models of homotopy\ntype theory within what we call crisp type theory.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 17:21:55 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 11:50:58 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 09:42:09 GMT"}, {"version": "v4", "created": "Thu, 5 Jul 2018 08:54:10 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Licata", "Daniel R.", ""], ["Orton", "Ian", ""], ["Pitts", "Andrew M.", ""], ["Spitters", "Bas", ""]]}, {"id": "1801.08099", "submitter": "Mohammadhosein Hasanbeig", "authors": "Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening", "title": "Logically-Constrained Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first model-free Reinforcement Learning (RL) algorithm to\nsynthesise policies for an unknown Markov Decision Process (MDP), such that a\nlinear time property is satisfied. The given temporal property is converted\ninto a Limit Deterministic Buchi Automaton (LDBA) and a robust reward function\nis defined over the state-action pairs of the MDP according to the resulting\nLDBA. With this reward function, the policy synthesis procedure is\n\"constrained\" by the given specification. These constraints guide the MDP\nexploration so as to minimize the solution time by only considering the portion\nof the MDP that is relevant to satisfaction of the LTL property. This improves\nperformance and scalability of the proposed method by avoiding an exhaustive\nupdate over the whole state space while the efficiency of standard methods such\nas dynamic programming is hindered by excessive memory requirements, caused by\nthe need to store a full-model in memory. Additionally, we show that the RL\nprocedure sets up a local value iteration method to efficiently calculate the\nmaximum probability of satisfying the given property, at any given state of the\nMDP. We prove that our algorithm is guaranteed to find a policy whose traces\nprobabilistically satisfy the LTL property if such a policy exists, and\nadditionally we show that our method produces reasonable control policies even\nwhen the LTL property cannot be satisfied. The performance of the algorithm is\nevaluated via a set of numerical examples. We observe an improvement of one\norder of magnitude in the number of iterations required for the synthesis\ncompared to existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:50:30 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 16:45:23 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2018 16:50:43 GMT"}, {"version": "v4", "created": "Thu, 17 May 2018 14:06:28 GMT"}, {"version": "v5", "created": "Wed, 4 Jul 2018 15:33:43 GMT"}, {"version": "v6", "created": "Mon, 22 Oct 2018 10:35:40 GMT"}, {"version": "v7", "created": "Thu, 6 Dec 2018 11:38:58 GMT"}, {"version": "v8", "created": "Sat, 16 Feb 2019 18:11:58 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Hasanbeig", "Mohammadhosein", ""], ["Abate", "Alessandro", ""], ["Kroening", "Daniel", ""]]}, {"id": "1801.08212", "submitter": "Sergio Miguel Tome", "authors": "Sergio Miguel Tom\\'e", "title": "Multi-optional Many-sorted Past Present Future structures and its\n  description", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cognitive theory of true conditions (CTTC) is a proposal to describe the\nmodel-theoretic semantics of symbolic cognitive architectures and design the\nimplementation of cognitive abilities. The CTTC is formulated mathematically\nusing the multi-optional many-sorted past present future(MMPPF) structures.\nThis article defines mathematically the MMPPF structures and the formal\nlanguages proposed to describe them by the CTTC.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 21:59:15 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Tom\u00e9", "Sergio Miguel", ""]]}, {"id": "1801.08350", "submitter": "Thorsten Wissmann", "authors": "Emmanuel Hainry and Romain P\\'echoux", "title": "Theory of higher order interpretations and application to Basic Feasible\n  Functions", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (December\n  14, 2020) lmcs:6973", "doi": "10.23638/LMCS-16(4:14)2020", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Interpretation methods and their restrictions to polynomials have been deeply\nused to control the termination and complexity of first-order term rewrite\nsystems. This paper extends interpretation methods to a pure higher order\nfunctional language. We develop a theory of higher order functions that is\nwell-suited for the complexity analysis of this programming language. The\ninterpretation domain is a complete lattice and, consequently, we express\nprogram interpretation in terms of a least fixpoint. As an application, by\nbounding interpretations by higher order polynomials, we characterize Basic\nFeasible Functions at any order.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 11:11:10 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 10:08:28 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 14:07:28 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 16:09:10 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hainry", "Emmanuel", ""], ["P\u00e9choux", "Romain", ""]]}, {"id": "1801.08441", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Finitary-based Domain Theory in Coq: An Early Report", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domain theory every finite computable object can be represented by a\nsingle mathematical object instead of a set of objects, using the notion of\nfinitary-basis. In this article we report on our effort to formalize domain\ntheory in Coq in terms of finitary-basis.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 11:13:59 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1801.08446", "submitter": "Tomas Grimm", "authors": "Tomas Grimm, Djones Lettnin, Michael H\\\"ubner", "title": "A Scalable Approach for Hardware Semiformal Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current verification flow of complex systems uses different engines\nsynergistically: virtual prototyping, formal verification, simulation,\nemulation and FPGA prototyping. However, none is able to verify a complete\narchitecture. Furthermore, hybrid approaches aiming at complete verification\nuse techniques that lower the overall complexity by increasing the abstraction\nlevel. This work focuses on the verification of complex systems at the RT level\nto handle the hardware peculiarities. Our results show an improvement of 100\\%\ncompared to the commercial tool's results for the prototype we used to validate\nour approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 11:03:43 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Grimm", "Tomas", ""], ["Lettnin", "Djones", ""], ["H\u00fcbner", "Michael", ""]]}, {"id": "1801.08450", "submitter": "Thorsten Wissmann", "authors": "Ian A. Mason and Carolyn L. Talcott", "title": "Reasoning about effects: from lists to cyber-physical agents", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (April 30,\n  2019) lmcs:5411", "doi": "10.23638/LMCS-15(2:8)2019", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Theories for reasoning about programs with effects initially focused on basic\nmanipulation of lists and other mutable data. The next challenge was to\nconsider higher-order programming, adding functions as first class objects to\nmutable data. Reasoning about actors added the challenge of dealing with\ndistributed open systems of entities interacting asynchronously. The advent of\ncyber-physical agents introduces the need to consider uncertainty, faults,\nphysical as well as logical effects. In addition cyber-physical agents have\nsensors and actuators giving rise to a much richer class of effects with\nbroader scope: think of self-driving cars, autonomous drones, or smart medical\ndevices.\n  This paper gives a retrospective on reasoning about effects highlighting key\nprinciples and techniques and closing with challenges for future work.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:35:32 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 11:16:34 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 18:22:20 GMT"}, {"version": "v4", "created": "Mon, 29 Apr 2019 16:52:44 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mason", "Ian A.", ""], ["Talcott", "Carolyn L.", ""]]}, {"id": "1801.08451", "submitter": "Christoph Rauch", "authors": "Thomas Kahl", "title": "Higher-dimensional automata modeling shared-variable systems", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 3 (September\n  6, 2019) lmcs:5749", "doi": "10.23638/LMCS-15(3:28)2019", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of this paper is to provide a construction to model\nshared-variable systems using higher-dimensional automata which is\ncompositional in the sense that the parallel composition of completely\nindependent systems is modeled by the standard tensor product of HDAs and\nnondeterministic choice is represented by the coproduct.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 15:18:36 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 19:02:35 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 10:34:18 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kahl", "Thomas", ""]]}, {"id": "1801.08707", "submitter": "Victor Marsault", "authors": "Victor Marsault", "title": "On p/q-recognisable sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let p/q be a rational number. Numeration in base p/q is defined by a function\nthat evaluates each finite word over A_p={0,1,...,p-1} to some rational number.\nWe let N_p/q denote the image of this evaluation function. In particular, N_p/q\ncontains all nonnegative integers and the literature on base p/q usually\nfocuses on the set of words that are evaluated to nonnegative integers; it is a\nrather chaotic language which is not context-free. On the contrary, we study\nhere the subsets of (N_p/q)^d that are p/q-recognisable, i.e. realised by\nfinite automata over (A_p)^d. First, we give a characterisation of these sets\nas those definable in a first-order logic, similar to the one given by the\nB\\\"uchi-Bruy\\`ere Theorem for integer bases numeration systems. Second, we show\nthat the natural order relation and the modulo-q operator are not\np/q-recognisable.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 08:21:30 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 08:33:36 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 14:20:21 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 14:18:45 GMT"}, {"version": "v5", "created": "Tue, 27 Jul 2021 13:49:07 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Marsault", "Victor", ""]]}, {"id": "1801.08718", "submitter": "Alberto Griggio", "authors": "Alessandro Cimatti and Alberto Griggio and Ahmed Irfan and Marco\n  Roveri and Roberto Sebastiani", "title": "Invariant Checking of NRA Transition Systems via Incremental Reduction\n  to LRA with EUF", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-54577-5_4", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model checking invariant properties of designs, represented as transition\nsystems, with non-linear real arithmetic (NRA), is an important though very\nhard problem. On the one hand NRA is a hard-to-solve theory; on the other hand\nmost of the powerful model checking techniques lack support for NRA. In this\npaper, we present a counterexample-guided abstraction refinement (CEGAR)\napproach that leverages linearization techniques from differential calculus to\nenable the use of mature and efficient model checking algorithms for transition\nsystems on linear real arithmetic (LRA) with uninterpreted functions (EUF). The\nresults of an empirical evaluation confirm the validity and potential of this\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 08:55:28 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Cimatti", "Alessandro", ""], ["Griggio", "Alberto", ""], ["Irfan", "Ahmed", ""], ["Roveri", "Marco", ""], ["Sebastiani", "Roberto", ""]]}, {"id": "1801.08723", "submitter": "Alberto Griggio", "authors": "Alessandro Cimatti and Alberto Griggio and Ahmed Irfan and Marco\n  Roveri and Roberto Sebastiani", "title": "Satisfiability Modulo Transcendental Functions via Incremental\n  Linearization", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-63046-5_7", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an abstraction-refinement approach to Satisfiability\nModulo the theory of transcendental functions, such as exponentiation and\ntrigonometric functions. The transcendental functions are represented as\nuninterpreted in the abstract space, which is described in terms of the\ncombined theory of linear arithmetic on the rationals with uninterpreted\nfunctions, and are incrementally axiomatized by means of upper- and\nlower-bounding piecewise-linear functions. Suitable numerical techniques are\nused to ensure that the abstractions of the transcendental functions are sound\neven in presence of irrationals. Our experimental evaluation on benchmarks from\nverification and mathematics demonstrates the potential of our approach,\nshowing that it compares favorably with delta-satisfiability /interval\npropagation and methods based on theorem proving.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 09:23:23 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Cimatti", "Alessandro", ""], ["Griggio", "Alberto", ""], ["Irfan", "Ahmed", ""], ["Roveri", "Marco", ""], ["Sebastiani", "Roberto", ""]]}, {"id": "1801.08766", "submitter": "Alexander Weigl", "authors": "Bernhard Beckert and Timo Bingmann and Moritz Kiefer and Peter Sanders\n  and Mattias Ulbrich and Alexander Weigl", "title": "Relational Equivalence Proofs Between Imperative and MapReduce\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce frameworks are widely used for the implementation of distributed\nalgorithms. However, translating imperative algorithms into these frameworks\nrequires significant structural changes to the algorithm. As the costs of\nrunning faulty algorithms at scale can be severe, it is highly desirable to\nverify the correctness of the translation, i.e., to prove that the MapReduce\nversion is equivalent to the imperative original. We present a novel approach\nfor proving equivalence between imperative and MapReduce algorithms based on\npartitioning the equivalence proof into a sequence of equivalence proofs\nbetween intermediate programs with smaller differences. Our approach is based\non the insight that two kinds of sub-proofs are required: (1) uniform\ntransformations changing the controlflow structure that are mostly independent\nof the particular context in which they are applied; and (2) context-dependent\ntransformations that are not uniform but that preserve the overall structure\nand can be proved correct using coupling invariants. We demonstrate the\nfeasibility of our approach by evaluating it on two prototypical algorithms\ncommonly used as examples in MapReduce frameworks: k-means and PageRank. To\ncarry out the proofs, we use the interactive theorem prover Coq with partial\nproof automation. The results show that our approach and its prototypical\nimplementation based on Coq enables equivalence proofs of non-trivial\nalgorithms and could be automated to a large degree.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 11:44:15 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Beckert", "Bernhard", ""], ["Bingmann", "Timo", ""], ["Kiefer", "Moritz", ""], ["Sanders", "Peter", ""], ["Ulbrich", "Mattias", ""], ["Weigl", "Alexander", ""]]}, {"id": "1801.09072", "submitter": "Francesco Gavazzo", "authors": "Francesco Gavazzo", "title": "Quantitative Behavioural Reasoning for Higher-order Effectful Programs:\n  Applicative Distances (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the quantitative refinements of Abramsky's applicative\nsimilarity and bisimilarity in the context of a generalisation of Fuzz, a\ncall-by-value $\\lambda$-calculus with a linear type system that can express\nprograms sensitivity, enriched with algebraic operations \\emph{\\`a la} Plotkin\nand Power. To do so a general, abstract framework for studying behavioural\nrelations taking values over quantales is defined according to Lawvere's\nanalysis of generalised metric spaces. Barr's notion of relator (or lax\nextension) is then extended to quantale-valued relations adapting and extending\nresults from the field of monoidal topology. Abstract notions of\nquantale-valued effectful applicative similarity and bisimilarity are then\ndefined and proved to be a compatible generalised metric (in the sense of\nLawvere) and pseudometric, respectively, under mild conditions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 11:36:11 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 10:07:00 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 10:06:46 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Gavazzo", "Francesco", ""]]}, {"id": "1801.09225", "submitter": "Yuito Murase", "authors": "Yuito Murase and Yuichi Nishiwaki", "title": "Polymorphic Context for Contextual Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the Curry-Howard isomorphism between logics and calculi, necessity\nmodality in logic is interpreted as types representing program code.\nParticularly, \\lamcirc, which was proposed in influential work by Davies, and\nits successors have been widely used as a logical foundation for syntactic\nmeta-programming. However, it is less known how to extend calculi based on\nmodal type theory to handle more practical operations including manipulation of\nvariable binding structures.\n  This paper constructs such a modal type theory in two steps. First, we\nreconstruct contextual modal type theory by Nanevski, et al.\\ as a Fitch-style\nsystem, which introduces hypothetical judgment with hierarchical context. The\nresulting type theory, \\multilayer contextual modal type theory \\fcmtt, is\ngeneralized to accommodate not only S4 but also K, T, and K4 modalities, and\nproven to enjoy many desired properties. Second, we extend \\fcmtt with\npolymorphic context, which is an internalization of contextual weakening, to\nobtain a novel modal type theory \\envpoly. Despite the fact that it came from\nobservation in logic, polymorphic context allows both binding manipulation and\nhygienic code generation. We claim this by showing a sound translation from\n\\lamcirc to \\envpoly.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 13:26:44 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Murase", "Yuito", ""], ["Nishiwaki", "Yuichi", ""]]}, {"id": "1801.09443", "submitter": "Murdoch Gabbay", "authors": "Murdoch J. Gabbay", "title": "Equivariant ZFA and the foundations of nominal techniques", "comments": null, "journal-ref": null, "doi": "10.1093/logcom/exz015", "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an accessible presentation to the foundations of nominal techniques,\nlying between Zermelo-Fraenkel set theory and Fraenkel-Mostowski set theory,\nand which has several nice properties including being consistent with the Axiom\nof Choice. We give two presentations of equivariance, accompanied by detailed\nyet user-friendly discussions of its theoretical significance and practical\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 10:56:56 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 16:40:58 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 06:13:13 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Gabbay", "Murdoch J.", ""]]}, {"id": "1801.09618", "submitter": "Nathana\\\"el Fijalkow", "authors": "Nathana\\\"el Fijalkow", "title": "An Optimal Value Iteration Algorithm for Parity Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for a polynomial time algorithm for solving parity games gained\nmomentum in 2017 when two different quasipolynomial time algorithms were\nconstructed. In this paper, we further analyse the second algorithm due to\nJurdzi\\'nski and Lazi\\'c and called the succinct progress measure algorithm. It\nwas presented as an improvement over a previous algorithm called the small\nprogress measure algorithm, using a better data structure.\n  The starting point of this paper is the observation that the underlying data\nstructure for both progress measure algorithms are (subgraph-)universal trees.\nWe show that in fact any universal tree gives rise to a value iteration\nalgorithm \\`a la succinct progress measure, and the complexity of the algorithm\nis proportional to the size of the chosen universal tree. We then show that\nboth algorithms are instances of this generic algorithm for two constructions\nof universal trees, the first of exponential size (for small progress measure)\nand the second of quasipolynomial size (for succinct progress measure).\n  The technical result of this paper is to show that the latter construction is\nasymptotically tight: universal trees have at least quasipolynomial size. This\nsuggests that the succinct progress measure algorithm of Jurdzi\\'nski and\nLazi\\'c is in this framework optimal, and that the polynomial time algorithm\nfor parity games is hiding someplace else.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 16:47:33 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Fijalkow", "Nathana\u00ebl", ""]]}, {"id": "1801.09644", "submitter": "Francesco Ciraulo", "authors": "Francesco Ciraulo", "title": "$\\sigma$-locales in Formal Topology", "comments": "Paper presented at the conference Continuity, Computability,\n  Constructivity - From Logic to Algorithms (CCC 2017), Nancy, France, June\n  26-30 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $\\sigma$-frame is a poset with countable joins and finite meets in which\nbinary meets distribute over countable joins. The aim of this paper is to show\nthat $\\sigma$-frames, actually $\\sigma$-locales, can be seen as a branch of\nFormal Topology, that is, intuitionistic and predicative point-free topology.\nEvery $\\sigma$-frame $L$ is the lattice of Lindel\\\"of elements (those for which\neach of their covers admits a countable subcover) of a formal topology of a\nspecific kind which, in its turn, is a presentation of the free frame over $L$.\nWe then give a constructive characterization of the smallest (strongly) dense\n$\\sigma$-sublocale of a given $\\sigma$-locale, thus providing a\n``$\\sigma$-version'' of a Boolean locale. Our development depends on the axiom\nof countable choice.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:39:16 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 23:20:21 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 03:19:23 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 12:56:06 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Ciraulo", "Francesco", ""]]}, {"id": "1801.10063", "submitter": "Stefan Gerdjikov", "authors": "Stefan Gerdjikov", "title": "Characterisation of (Sub)sequential Rational Functions over a General\n  Class Monoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we describe a general class of monoids for which\n(sub)sequential rational can be characterised in terms of a congruence relation\nin the flavour of Myhill-Nerode relation. The class of monoids that we consider\ncan be described in terms of natural algebraic axioms, contains the free\nmonoids, groups, the tropical monoid, and is closed under Cartesian.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 09:46:22 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Gerdjikov", "Stefan", ""]]}, {"id": "1801.10140", "submitter": "Cristian Mattarei", "authors": "Cristian Mattarei, Clark Barrett, Shu-yu Guo, Bradley Nelson, Ben\n  Smith, JF Bastien", "title": "EMME: a formal tool for ECMAScript Memory Model Evaluation", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all web-based interfaces are written in JavaScript. Given its\nprevalence, the support for high performance JavaScript code is crucial. The\nECMA Technical Committee 39 (TC39) has recently extended the ECMAScript\nlanguage (i.e., JavaScript) to support shared memory accesses between different\nthreads. The extension is given in terms of a natural language memory model\nspecification. In this paper we describe a formal approach for validating both\nthe memory model and its implementations in various JavaScript engines. We\nfirst introduce a formal version of the memory model and report results on\nchecking the model for consistency and other properties. We then introduce our\ntool, EMME, built on top of the Alloy analyzer, which leverages the model to\ngenerate all possible valid executions of a given JavaScript program. Finally,\nwe report results using EMME together with small test programs to analyze\nindustrial JavaScript engines. We show that EMME can find bugs as well as\nmissed opportunities for optimization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 18:46:58 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 06:22:11 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Mattarei", "Cristian", ""], ["Barrett", "Clark", ""], ["Guo", "Shu-yu", ""], ["Nelson", "Bradley", ""], ["Smith", "Ben", ""], ["Bastien", "JF", ""]]}, {"id": "1801.10280", "submitter": "Robert Kenny", "authors": "Robert Kenny", "title": "Dugundji systems and a retract characterization of effective\n  zero-dimensionality", "comments": "33 pages, major revised version, intended for postproceedings of CCC\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper (as in [Ken15]), we consider an effective version of the\ncharacterization of separable metric spaces as zero-dimensional iff every\nnonempty closed subset is a retract of the space (actually, it is a relative\nresult for closed zero-dimensional subspaces of a fixed space that we have\nproved). This uses (in the converse direction) local compactness & bilocated\nsets as in [Ken15], but in the forward direction the newer version has a\nsimpler proof and no compactness assumption. Furthermore, the proof of the\nforward implication relates to so-called Dugundji systems: we elaborate both a\ngeneral construction of such systems for a proper nonempty closed subspace\n(using a computable form of countable paracompactness), and modifications -- to\nmake the sets pairwise disjoint if the subspace is zero-dimensional, or to\navoid the restriction to proper subspaces. In a different direction, a second\ntheorem applies in $p$-adic analysis the ideas of the first theorem to compute\na more general form of retraction, given a Dugundji system (possibly without\ndisjointness).\n  Finally, we complement the effective retract characterization of\nzero-dimensional subspaces mentioned above by improving to equivalence the\nimplications (or Weihrauch reductions in some cases), for closed\nat-most-zero-dimensional subsets with some negative information, among separate\nconditions of computability of operations $N,M,B,S$ introduced in [Ken15,\\S 4]\nand corresponding to vanishing large inductive dimension, vanishing small\ninductive dimension, existence of a countable basis of relatively clopen sets,\nand the reduction principle for sequences of open sets. Thus, similarly to the\nrobust notion of effective zero-dimensionality of computable metric spaces in\n[Ken15], there is a robust notion of `uniform effective zero-dimensionality'\nfor a represented pointclass consisting of at-most-zero-dimensional closed\nsubsets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 02:32:51 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 14:39:58 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 04:55:02 GMT"}, {"version": "v4", "created": "Mon, 24 May 2021 08:15:24 GMT"}, {"version": "v5", "created": "Tue, 25 May 2021 03:51:51 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Kenny", "Robert", ""]]}, {"id": "1801.10387", "submitter": "Cameron Freer", "authors": "Nathanael L. Ackerman, Jeremy Avigad, Cameron E. Freer, Daniel M. Roy,\n  Jason M. Rute", "title": "On the computability of graphons", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relative computability of exchangeable binary relational\ndata when presented in terms of the distribution of an invariant measure on\ngraphs, or as a graphon in either $L^1$ or the cut distance. We establish basic\ncomputable equivalences, and show that $L^1$ representations contain\nfundamentally more computable information than the other representations, but\nthat $0'$ suffices to move between computable such representations. We show\nthat $0'$ is necessary in general, but that in the case of random-free\ngraphons, no oracle is necessary. We also provide an example of an\n$L^1$-computable random-free graphon that is not weakly isomorphic to any\ngraphon with an a.e. continuous version.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 10:24:18 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Ackerman", "Nathanael L.", ""], ["Avigad", "Jeremy", ""], ["Freer", "Cameron E.", ""], ["Roy", "Daniel M.", ""], ["Rute", "Jason M.", ""]]}, {"id": "1801.10513", "submitter": "Maximilian Dor\\'e", "authors": "Maximilian Dor\\'e and Krysia Broda", "title": "The Elfe System - Verifying mathematical proofs of undergraduate\n  students", "comments": null, "journal-ref": "10th International Conference on Computer Supported Education\n  (CSEDU 2018), Mar 2018, Funchal, Portugal", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elfe is an interactive system for teaching basic proof methods in discrete\nmathematics. The user inputs a mathematical text written in fair English which\nis converted to a special data-structure of first-order formulas. Certain proof\nobligations implied by this intermediate representation are checked by\nautomated theorem provers which try to either prove the obligations or find\ncountermodels if an obligation is wrong. The result of the verification process\nis then returned to the user. Elfe is implemented in Haskell and can be\naccessed via a reactive web interface or from the command line. Background\nlibraries for sets, relations and functions have been developed. It has been\ntested by students in the beginning of their mathematical studies.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 15:58:37 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Dor\u00e9", "Maximilian", ""], ["Broda", "Krysia", ""]]}, {"id": "1801.10519", "submitter": "Andr\\'es Ezequiel Viso", "authors": "Delia Kesner and Alejandro R\\'ios and Andr\\'es Viso", "title": "Call-by-Need, Neededness and All That", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-89366-2_13", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that call-by-need is observationally equivalent to weak-head needed\nreduction. The proof of this result uses a semantical argument based on a\n(non-idempotent) intersection type system called $\\mathcal{V}$. Interestingly,\nsystem $\\mathcal{V}$ also allows to syntactically identify all the weak-head\nneeded redexes of a term.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 16:03:32 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 13:09:59 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Kesner", "Delia", ""], ["R\u00edos", "Alejandro", ""], ["Viso", "Andr\u00e9s", ""]]}]