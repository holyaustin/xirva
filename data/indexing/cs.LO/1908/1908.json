[{"id": "1908.00158", "submitter": "Nengkun Yu", "authors": "Nengkun Yu", "title": "Quantum Temporal Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a model of quantum concurrent program, which can\nbe used to model the behaviour of reactive quantum systems and to design\nquantum compilers. We investigate quantum temporal logic, QTL, for the\nspecification of quantum concurrent systems by suggesting the time-dependence\nof events. QTL employs the projections on subspaces as atomic propositions,\nwhich was established in the Birkhoff and von Neumann's classic treatise on\nquantum logic. For deterministic functional quantum program, We prove a quantum\nB\\\"{o}hm-Jacopini theorem which states that any such program is equivalent to a\nQ-While program. The decidability of basic QTL formulae for general quantum\nconcurrent program is studied.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 00:47:19 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 07:28:52 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yu", "Nengkun", ""]]}, {"id": "1908.00183", "submitter": "EPTCS", "authors": "Carmen Leticia Garc\\'ia-Mata (Tecnol\\'ogico Nacional de M\\'exico -\n  Tecnol\\'ogico de Chihuahua), Pedro Rafael M\\'arquez-Guti\\'errez\n  (Tecnol\\'ogico Nacional de M\\'exico - Tecnol\\'ogico de Chihuahua)", "title": "Solving a Flowshop Scheduling Problem with Answer Set Programming:\n  Exploiting the Problem to Reduce the Number of Combinations", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 347-353", "doi": "10.4204/EPTCS.306.41", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning and scheduling have been a central theme of research in computer\nscience. In particular, the simplicity of the theoretical approach of a no-wait\nflowshop scheduling problem does not allow to perceive the problem complexity\nat first sight. In this paper the applicability of the Answer Set Programming\nlanguage is explored for the solution of the Automated Wet-etching scheduling\nproblem in Semiconductor Manufacturing Systems. A method based in ranges is\nproposed in order to reduce the huge number of combinations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 02:25:34 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 08:25:46 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Garc\u00eda-Mata", "Carmen Leticia", "", "Tecnol\u00f3gico Nacional de M\u00e9xico -\n  Tecnol\u00f3gico de Chihuahua"], ["M\u00e1rquez-Guti\u00e9rrez", "Pedro Rafael", "", "Tecnol\u00f3gico Nacional de M\u00e9xico - Tecnol\u00f3gico de Chihuahua"]]}, {"id": "1908.00860", "submitter": "Saket Dingliwal", "authors": "Saket Dingliwal, Ronak Agarwal, Happy Mittal, Parag Singla", "title": "Advances in Symmetry Breaking for SAT Modulo Theories", "comments": "SMT 2019, SMT, CVC4, Symmetry-breaking, starAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry breaking is a popular technique to reduce the search space for SAT\nsolving by exploiting the underlying symmetry over variables and clauses in a\nformula. The key idea is to first identify sets of assignments which fall in\nthe same symmetry class, and then impose ordering constraints, called Symmetry\nBreaking Predicates (SBPs), such that only one (or a small subset) of these\nassignments is allowed to be a solution of the original SAT formula. While this\ntechnique has been exploited extensively in the SAT literature, there is little\nwork on using symmetry breaking for SAT Modulo Theories (SMT). In SMT, logical\nconstraints in SAT theories are combined with another set of theory operations\ndefined over non-Boolean variables such as integers, reals, etc. SMT solvers\ntypically use a combination of SAT solving techniques augmented with calls to\nthe theory solver. In this work, we take up the advances in SAT symmetry\nbreaking and apply them to the domain of SMT. Our key technical contribution is\nthe formulation of symmetry breaking over the Boolean skeleton variables, which\nare placeholders for actual theory operations in SMT solving. These SBPs are\nthen applied over the SAT solving part of the SMT solver. We implement our SBP\nideas on top of CVC4, which is a state-of-the-art SMT solver. Our approach can\nresult in significantly faster solutions on several benchmark problems compared\nto the state-of-the-art. Our final solver is a hybrid of the original CVC4\nsolver, and an SBP based solver, and can solve up to 3.8% and 3.1% more\nproblems in the QF_NIA category of 2018 and 2019 SMT benchmarks, respectively,\ncompared to CVC4, the top performer in this category.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:53:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 06:01:31 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Dingliwal", "Saket", ""], ["Agarwal", "Ronak", ""], ["Mittal", "Happy", ""], ["Singla", "Parag", ""]]}, {"id": "1908.01055", "submitter": "Daniel Rogozin", "authors": "Daniel Rogozin", "title": "Quantale semantics of Lambek calculus with subexponential modalities", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CT math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the polymodal version of Lambek calculus with\nsubexponential modalities initially introduced by Kanovich, Kuznetsov, Nigam,\nand Scedrov and its quantale semantics. In our approach, subexponential\nmodalities have an interpretation in terms of quantic conuclei. We show that\nthis extension of Lambek calculus is complete w.r.t quantales with quantic\nconuclei. Also, we prove a representation theorem for quantales with quantic\nconuclei and show that Lambek calculus with subexponentials is relationally\ncomplete. Finally, we extend this representation theorem to the category of\nquantales with quantic conuclei.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 01:35:58 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 18:04:33 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Rogozin", "Daniel", ""]]}, {"id": "1908.01056", "submitter": "Tsong Ming Liaw", "authors": "Tsong-Ming Liaw, Simon C. Lin", "title": "A General Theory of Concept Lattice (I): Emergence of General Concept\n  Lattice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first part of the treatise on A General Theory of Concept Lattice\n(I-V), this work develops the general concept lattice for the problem\nconcerning categorization of objects according to their properties. Unlike the\nconventional approaches, such as the formal concept lattice and the rough set\nlattice, the general concept lattice is designed to adhere to the general\nprinciple that the information content should be invariant regardless how the\nvariables/parameters are presented. Here, one will explicitly demonstrate the\nexistence of such a construction by a sequence of fulfillment compatible with\nthe conventional lattice structure. The general concept lattice promises to be\na comprehensive categorization for all the distinctive object classes according\nto whatever properties they are equipped with. It will be shown that one can\nalways regain the formal concept lattice and rough set lattice from the general\nconcept lattice.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 05:38:33 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liaw", "Tsong-Ming", ""], ["Lin", "Simon C.", ""]]}, {"id": "1908.01624", "submitter": "Marc Hartung", "authors": "Marc Hartung and Florian Schintke", "title": "Learned Clause Minimization in Parallel SAT Solvers", "comments": "accepted at Pragmatics of SAT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned clauses minimization (LCM) let to performance improvements of modern\nSAT solvers especially in solving hard SAT instances. Despite the success of\nLCM approaches in sequential solvers, they are not widely incorporated in\nparallel SAT solvers. In this paper we explore the potential of LCM for\nparallel SAT solvers by defining multiple LCM approaches based on clause\nvivification, comparing their runtime in different SAT solvers and discussing\nreasons for performance gains and losses. Results show that LCM only boosts\nperformance of parallel SAT solvers on a fraction of SAT instances. More\ncommonly applying LCM decreases performance. Only certain LCM approaches are\nable to improve the overall performance of parallel SAT solvers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 13:53:24 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Hartung", "Marc", ""], ["Schintke", "Florian", ""]]}, {"id": "1908.01909", "submitter": "Farzaneh Derakhshan", "authors": "Farzaneh Derakhshan, Frank Pfenning", "title": "Circular Proofs as Session-Typed Processes: A Local Validity Condition", "comments": "The revised version, 48 pages, submitted to Logical Methods in\n  Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof theory provides a foundation for studying and reasoning about\nprogramming languages, most directly based on the well-known Curry-Howard\nisomorphism between intuitionistic logic and the typed lambda-calculus. More\nrecently, a correspondence between intuitionistic linear logic and the\nsession-typed pi-calculus has been discovered. In this paper, we establish an\nextension of the latter correspondence for a fragment of substructural logic\nwith least and greatest fixed points. We describe the computational\ninterpretation of the resulting infinitary proof system as session-typed\nprocesses, and provide an effectively decidable local criterion to recognize\nmutually recursive processes corresponding to valid circular proofs as\nintroduced by Fortier and Santocanale. We show that our algorithm imposes a\nstricter requirement than Fortier and Santocanale's guard condition, but is\nlocal and compositional and therefore more suitable as the basis for a\nprogramming language.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 00:07:04 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 02:32:01 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Derakhshan", "Farzaneh", ""], ["Pfenning", "Frank", ""]]}, {"id": "1908.01930", "submitter": "Yassmeen Elderhalli", "authors": "Yassmeen Elderhalli, Osman Hasan and Sofiene Tahar", "title": "A Formally Verified HOL Algebra for Dynamic Reliability Block Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic reliability block diagrams (DRBDs) are introduced to overcome the\nmodeling limitations of traditional reliability block diagrams, such as the\ninability to capture redundant components. However, so far there is no\nalgebraic framework that allows conducting the analysis of a given DRBD based\non its structure function and enables verifying its soundness using\nhigher-order logic (HOL) theorem proving. In this work, we propose a new\nalgebra to formally express the structure function and the reliability of a\nDRBD with spare constructs based on basic system blocks and newly introduced\nDRBD operators. We present several simplification properties that allow\nreducing the structure of a given DRBD. We provide the HOL formalization of the\nproposed algebra, and formally verify its corresponding properties using the\nHOL4 theorem prover. This includes formally verifying generic reliability\nexpressions of the spare construct, series, parallel and deeper structures in\nan extensible manner that allows verifying the reliability of complex systems.\nFinally, we demonstrate the applicability of this algebra by formally analyzing\nthe terminal reliability analysis of a shuffle-exchange network in HOL4.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 02:13:34 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Elderhalli", "Yassmeen", ""], ["Hasan", "Osman", ""], ["Tahar", "Sofiene", ""]]}, {"id": "1908.02078", "submitter": "Enrique Martin-Martin", "authors": "Elvira Albert, Samir Genaim, Ra\\'ul Guti\\'errez, Enrique Martin-Martin", "title": "A Transformational Approach to Resource Analysis with Typed-norms\n  Inference", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to automatically infer the resource consumption of programs,\nanalyzers track how data sizes change along program's execution. Typically,\nanalyzers measure the sizes of data by applying norms which are mappings from\ndata to natural numbers that represent the sizes of the corresponding data.\nWhen norms are defined by taking type information into account, they are named\ntyped-norms. This article presents a transformational approach to resource\nanalysis with typed-norms that are inferred by a data-flow analysis. The\nanalysis is based on a transformation of the program into an intermediate\nabstract program in which each variable is abstracted with respect to all\nconsidered norms which are valid for its type. We also present the data-flow\nanalysis to automatically infer the required, useful, typed-norms from\nprograms. Our analysis is formalized on a simple rule-based representation to\nwhich programs written in different programming paradigms (e.g., functional,\nlogic, imperative) can be automatically translated. Experimental results on\nstandard benchmarks used by other type-based analyzers show that our approach\nis both efficient and accurate in practice.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:13:12 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Albert", "Elvira", ""], ["Genaim", "Samir", ""], ["Guti\u00e9rrez", "Ra\u00fal", ""], ["Martin-Martin", "Enrique", ""]]}, {"id": "1908.02087", "submitter": "Silvano Dal Zilio", "authors": "Yannick Pencol\\'e (LAAS-DISCO), \\'Eric Lubat (LAAS-VERTICS), Silvano\n  Dal Zilio (LAAS-VERTICS), Didier Le Botlan (LAAS-VERTICS), Audine Subias\n  (LAAS-DISCO)", "title": "A State Class Construction for Computing the Intersection of Time Petri\n  Nets Languages", "comments": "FORMATS, Aug 2019, Amsterdam, Netherlands", "journal-ref": null, "doi": "10.1007/978-3-030-29662-9_5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for computing the language intersection of two Time\nPetri nets (TPN); that is the sequence of labels in timed traces common to the\nexecution of two TPN. Our approach is based on a new product construction\nbetween nets and relies on the State Class construction, a widely used method\nfor checking the behaviour of TPN. We prove that this new construct does not\nadd additional expressive power, and yet that it can leads to very concise\nrepresentation of the result. We have implemented our approach in a new tool,\ncalled Twina. We report on some experimental results obtained with this tool\nand show how to apply our approach on two interesting problems: rst, to dene an\nequivalent of the twin-plant diagnosability methods for TPN; then as a way to\ncheck timed properties without interfering with a system.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:45:28 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Pencol\u00e9", "Yannick", "", "LAAS-DISCO"], ["Lubat", "\u00c9ric", "", "LAAS-VERTICS"], ["Zilio", "Silvano Dal", "", "LAAS-VERTICS"], ["Botlan", "Didier Le", "", "LAAS-VERTICS"], ["Subias", "Audine", "", "LAAS-DISCO"]]}, {"id": "1908.02202", "submitter": "David Spivak", "authors": "David I. Spivak", "title": "Generalized Lens Categories via functors $\\mathcal{C}^{\\rm\n  op}\\to\\mathsf{Cat}$", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lenses have a rich history and have recently received a great deal of\nattention from applied category theorists. We generalize the notion of lens by\ndefining a category $\\mathsf{Lens}_F$ for any category $\\mathcal{C}$ and\nfunctor $F\\colon \\mathcal{C}^{\\rm op}\\to\\mathsf{Cat}$, using a variant of the\nGrothendieck construction. All of the mathematics in this note is\nstraightforward; the purpose is simply to see lenses in a broader context where\nsome closely-related examples, such as ringed spaces and open continuous\ndynamical systems, can be included.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 15:10:42 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 01:08:51 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 18:27:06 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Spivak", "David I.", ""]]}, {"id": "1908.02488", "submitter": "Bastien Maubert", "authors": "Sophia Knight and Bastien Maubert", "title": "Dealing with imperfect information in Strategy Logic", "comments": "Workshop Strategic Reasoning 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of Strategy Logic (SL), in which one can both reason\nabout strategizing under imperfect information and about players' knowledge.\nOne original aspect of our approach is that we do not force strategies to be\nuniform, i.e. consistent with the players' information, at the semantic level;\ninstead, one can express in the logic itself that a strategy should be uniform.\nTo do so, we first develop a \"branching-time\" version of SL with perfect\ninformation, that we call BSL, in which one can quantify over the different\noutcomes defined by a partial assignment of strategies to the players; this\ncontrasts with SL, where temporal operators are allowed only when all\nstrategies are fixed, leaving only one possible play. Next, we further extend\nBSL by adding distributed knowledge operators, the semantics of which rely on\nequivalence relations on partial plays. The logic we obtain subsumes most\nstrategic logics with imperfect information, epistemic or not.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 08:40:01 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Knight", "Sophia", ""], ["Maubert", "Bastien", ""]]}, {"id": "1908.03171", "submitter": "Patrick Lambrix", "authors": "Patrick Lambrix", "title": "Completing and Debugging Ontologies: state of the art and challenges", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As semantically-enabled applications require high-quality ontologies,\ndeveloping and maintaining ontologies that are as correct and complete as\npossible is an important although difficult task in ontology engineering. A key\nstep is ontology debugging and completion. In general, there are two steps:\ndetecting defects and repairing defects. In this paper we discuss the state of\nthe art regarding the repairing step. We do this by formalizing the repairing\nstep as an abduction problem and situating the state of the art with respect to\nthis framework. We show that there are still many open research problems and\nshow opportunities for further work and advancing the field.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 17:02:37 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:36:24 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Lambrix", "Patrick", ""]]}, {"id": "1908.03489", "submitter": "Matteo Rucco", "authors": "Matteo Rucco, Luca Tesei, and Emanuela Merelli", "title": "Topological Run-time Monitoring for Complex Systems", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new data-driven run-time monitoring system for\nanalysing the behaviour of time evolving complex systems. The monitor controls\nthe evolution of the whole system but it is mined from the data produced by its\nsingle interacting components. Relevant behavioural changes happening at the\ncomponent level and that are responsible for global system evolution are\ncaptured by the monitor. Topological Data Analysis is used for shaping and\nanalysing the data for mining an automaton mimicking the global system\ndynamics, the so-called Persistent Entropy Automaton (PEA). A slight augmented\nPEA, the monitor, can be used to run current or past executions of the system\nto mine temporal invariants, for instance through statistical reasoning. Such\ninvariants can be formulated as properties of a temporal logic, e.g. bounded\nLTL, that can be run-time model-checked. We have performed a feasibility\nassessment of the PEA and the associated monitoring system by analysing a\nsimulated biological complex system, namely the human immune system. The\napplication of the monitor to simulated traces reveals temporal properties that\nshould be satisfied in order to reach immunization memory.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 09:42:19 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Rucco", "Matteo", ""], ["Tesei", "Luca", ""], ["Merelli", "Emanuela", ""]]}, {"id": "1908.03501", "submitter": "Peter Hertling", "authors": "Peter Hertling and Gisela Krommes", "title": "EXPSPACE-Completeness of the Logics K4xS5 and S4xS5 and the Logic of\n  Subset Spaces, Part 1: ESPACE-Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the satisfiability problems of the product logics K4xS5 and\nS4xS5 and of the logic SSL of subset spaces are in N2EXPTIME. We improve this\nupper bound for the complexity of these problems by presenting\nESPACE-algorithms for these problems. In another paper we show that these\nproblems are EXPSPACE-hard. This shows that all three problems are\nEXPSPACE-complete.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:43:50 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Hertling", "Peter", ""], ["Krommes", "Gisela", ""]]}, {"id": "1908.03509", "submitter": "Peter Hertling", "authors": "Peter Hertling and Gisela Krommes", "title": "EXPSPACE-Completeness of the Logics K4xS5 and S4xS5 and the Logic of\n  Subset Spaces, Part 2: EXPSPACE-Hardness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the satisfiability problems of the product logics K4xS5 and\nS4xS5 are NEXPTIME-hard and that the satisfiability problem of the logic SSL of\nsubset spaces is PSPACE-hard. We improve these lower bounds for the complexity\nof these problems by showing that all three problems are EXPSPACE-hard under\nlogspace reduction. In another paper we show that these problems are in ESPACE.\nThis shows that all three problems are EXPSPACE-complete.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:57:36 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Hertling", "Peter", ""], ["Krommes", "Gisela", ""]]}, {"id": "1908.03719", "submitter": "Esra Erdem", "authors": "Esra Erdem, Andrea Formisano, German Vidal, and Fangkai Yang", "title": "Introduction to the 35th International Conference on Logic Programming\n  Special Issue", "comments": "The 35th International Conference on Logic Programming (ICLP 2019),\n  Las Cruces, New Mexico, USA, September 20--25, 2019. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proud to introduce this special issue of Theory and Practice of Logic\nProgramming (TPLP), dedicated to the regular papers accepted for the 35th\nInternational Conference on Logic Programming (ICLP). The ICLP meetings started\nin Marseille in 1982 and since then constitute the main venue for presenting\nand discussing work in the area of logic programming. Under consideration for\nacceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:25:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Erdem", "Esra", ""], ["Formisano", "Andrea", ""], ["Vidal", "German", ""], ["Yang", "Fangkai", ""]]}, {"id": "1908.03800", "submitter": "Toru Takisaka", "authors": "Bakhadyr Khoussainov, Toru Takisaka", "title": "Large Scale Geometries of Infinite Strings", "comments": "12 pages", "journal-ref": "32nd Annual ACM/IEEE Symposium on Logic in Computer Science, LICS\n  2017, Reykjavik, Iceland, June 20-23, 2017", "doi": "10.1109/LICS.2017.8005078", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce geometric consideration into the theory of formal languages. We\naim to shed light on our understanding of global patterns that occur on\ninfinite strings. We utilise methods of geometric group theory. Our emphasis is\non large scale geometries. Two infinite strings have the same large scale\ngeometry if there are colour preserving bi-Lipschitz maps with distortions\nbetween the strings. Call these maps quasi-isometries. Introduction of large\nscale geometries poses several questions. The first question asks to study the\npartial order induced by quasi-isometries. This partial order compares large\nscale geometries; as such it presents an algebraic tool for classification of\nglobal patterns. We prove there is a greatest large scale geometry and\ninfinitely many minimal large scale geometries. The second question is related\nto understanding the quasi-isometric maps on various classes of strings. The\nthird question investigates the sets of large scale geometries of strings\naccepted by computational models, e.g. B\\\"uchi automata. We provide an\nalgorithm that describes large scale geometries of strings accepted by B\\\"uchi\nautomata. This links large scale geometries with automata theory. The fourth\nquestion studies the complexity of the quasi-isometry problem. We show the\nproblem is $\\Sigma_3^0$-complete thus providing a bridge with computability\ntheory. Finally, the fifth question asks to build algebraic structures that are\ninvariants of large scale geometries. We invoke asymptotic cones, a key concept\nin geometric group theory, defined via model-theoretic notion of ultra-product.\nPartly, we study asymptotic cones of algorithmically random strings thus\nconnecting the topic with algorithmic randomness.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 19:31:10 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Khoussainov", "Bakhadyr", ""], ["Takisaka", "Toru", ""]]}, {"id": "1908.03999", "submitter": "Jason Teutsch", "authors": "Jason Teutsch, Michael Straka, Dan Boneh", "title": "Retrofitting a two-way peg between blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In December 2015, a bounty emerged to establish both reliable communication\nand secure transfer of value between the Dogecoin and Ethereum blockchains.\nThis prized \"Dogethereum bridge\" would allow parties to \"lock\" a DOGE coin on\nDogecoin and in exchange receive a newly minted WOW token in Ethereum. Any\nsubsequent owner of the WOW token could burn it and, in exchange, earn the\nright to \"unlock\" a DOGE on Dogecoin.\n  We describe an efficient, trustless, and retrofitting Dogethereum\nconstruction which requires no fork but rather employs economic collateral to\nachieve a \"lock\" operation in Dogecoin. The protocol relies on bulletproofs,\nTruebit, and parametrized tokens to efficiently and trustlessly relay events\nfrom the \"true\" Dogecoin blockchain into Ethereum. The present construction not\nonly enables cross-platform exchange but also allows Ethereum smart contracts\nto trustlessly access Dogecoin. A similar technique adds Ethereum-based smart\ncontracts to Bitcoin and Bitcoin data to Ethereum smart contracts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 04:41:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Teutsch", "Jason", ""], ["Straka", "Michael", ""], ["Boneh", "Dan", ""]]}, {"id": "1908.04038", "submitter": "Ramanathan Thinniyam", "authors": "Ramanathan S. Thinniyam, Georg Zetzsche", "title": "Regular Separability and Intersection Emptiness are Independent Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of \\emph{regular separability} asks, given two languages $K$ and\n$L$, whether there exists a regular language $S$ with $K\\subseteq S$ and $S\\cap\nL=\\emptyset$. This problem has recently been studied for various classes of\nlanguages. All the results on regular separability obtained so far exhibited a\nnoteworthy correspondence with the intersection emptiness problem: In eachcase,\nregular separability is decidable if and only if intersection emptiness is\ndecidable. This raises the question whether under mild assumptions, regular\nseparability can be reduced to intersection emptiness and vice-versa.\n  We present counterexamples showing that none of the two problems can be\nreduced to the other. More specifically, we describe language classes\n$\\mathcal{C_1}$, $\\mathcal{D_1}$, $\\mathcal{C_2}$, $\\mathcal{D_2}$ such that\n(i)~intersection emptiness is decidable for $\\mathcal{C_1}$ and\n$\\mathcal{D_1}$, but regular separability is undecidable for $\\mathcal{C_1}$\nand $\\mathcal{D_1}$ and (ii)~regular separability is decidable for\n$\\mathcal{C_2}$ and $\\mathcal{D_2}$, but intersection emptiness is undecidable\nfor $\\mathcal{C_2}$ and $\\mathcal{D_2}$.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 07:42:29 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Thinniyam", "Ramanathan S.", ""], ["Zetzsche", "Georg", ""]]}, {"id": "1908.04260", "submitter": "Tsong Ming Liaw", "authors": "Tsong-Ming Liaw, Simon C. Lin", "title": "A General Theory of Concept Lattice (II): Tractable Lattice Construction\n  and Implication Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the second part of the treatise 'A General Theory of Concept Lattice',\nthis paper speaks of the tractability of the general concept lattice for both\nits lattice structure and logic content. The general concept lattice permits a\nfeasible construction that can be completed in a single scan of the formal\ncontext, though the conventional formal-concept lattice and rough-set lattice\ncan be regained from the general concept lattice. The logic implication\ndeducible from the general concept lattice takes the form of {\\mu}_1\n{\\rightarrow} {\\mu}_2 where {\\mu}_1,{\\mu}_2 {\\in} M^{\\ast} are composite\nattributes out of the concerned formal attributes M. Remarkable is that with a\nsingle formula based on the contextual truth 1_{\\eta} one can deduce all the\nimplication relations extractable from the formal context. For concreteness, it\ncan be shown that any implication A {\\rightarrow} B (A, B being subsets of the\nformal attributes M) discussed in the formal-concept lattice corresponds to a\nspecial case of {\\mu}_1 {\\rightarrow} {\\mu}_2 by means of {\\mu}_1 = {\\prod} A\nand {\\mu}_2 = {\\prod} B. Thus, one may elude the intractability due to\nsearching for the Guigues-Duquenne basis appropriate for the implication\nrelations deducible from the formal-concept lattice. Likewise, one may identify\nthose {\\mu}_1 {\\rightarrow} {\\mu}_2 where {\\mu}_1 = {\\sum} A and {\\mu}_2 =\n{\\sum} B with the implications that can be acquired from the rough-set lattice.\n(Here, the product {\\prod} stands for the conjunction and the summation {\\sum}\nthe disjunction.)}\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 00:36:12 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Liaw", "Tsong-Ming", ""], ["Lin", "Simon C.", ""]]}, {"id": "1908.04264", "submitter": "Emanuela Merelli", "authors": "Emanuela Merelli, Anita Wasilewska", "title": "Topological Interpretation of Interactive Computation", "comments": "18 figures, 19 pages. Scott Smolka Festschrift", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is a great pleasure to write this tribute in honor of Scott A. Smolka on\nhis 65th birthday. We revisit Goldin, Smolka hypothesis that persistent Turing\nmachine (PTM) can capture the intuitive notion of sequential interaction\ncomputation. We propose a topological setting to model the abstract concept of\nenvironment. We use it to define a notion of a topological Turing machine (TTM)\nas a universal model for interactive computation and possible model for\nconcurrent computation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 19:29:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Merelli", "Emanuela", ""], ["Wasilewska", "Anita", ""]]}, {"id": "1908.04291", "submitter": "Dan Ghica", "authors": "Dan R. Ghica", "title": "The far side of the cube", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game-semantic models usually start from the core model of the prototypical\nlanguage PCF, which is characterised by a range of combinatorial constraints on\nthe shape of plays. Relaxing each such constraint usually corresponds to the\nintroduction of a new language operation, a feature of game semantics commonly\nknown as the `Abramsky Cube'. In this presentation we relax all such\ncombinatorial constraints, resulting in the most general game model, in which\nall the other game models live. This is perhaps the simplest set up in which to\nunderstand game semantics, so it should serve as a portal to the other, more\ncomplex, game models in the literature. It might also be interesting in its own\nright, as an extremal instance of the game-semantic paradigm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:22:58 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Ghica", "Dan R.", ""]]}, {"id": "1908.04381", "submitter": "Jeffrey M. Dudek", "authors": "Jeffrey M. Dudek, Leonardo Due\\~nas-Osorio, Moshe Y. Vardi", "title": "Efficient Contraction of Large Tensor Networks for Weighted Model\n  Counting through Graph Decompositions", "comments": "Submitted to AIJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained counting is a fundamental problem in artificial intelligence. A\npromising new algebraic approach to constrained counting makes use of tensor\nnetworks, following a reduction from constrained counting to the problem of\ntensor-network contraction. Contracting a tensor network efficiently requires\ndetermining an efficient order to contract the tensors inside the network,\nwhich is itself a difficult problem.\n  In this work, we apply graph decompositions to find contraction orders for\ntensor networks. We prove that finding an efficient contraction order for a\ntensor network is equivalent to the well-known problem of finding an optimal\ncarving decomposition. Thus memory-optimal contraction orders for planar tensor\nnetworks can be found in cubic time. We show that tree decompositions can be\nused both to find carving decompositions and to factor tensor networks with\nhigh-rank, structured tensors.\n  We implement these algorithms on top of state-of-the-art solvers for tree\ndecompositions and show empirically that the resulting weighted model counter\nis quite effective and useful as part of a portfolio of counters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 21:01:49 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 23:39:11 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Dudek", "Jeffrey M.", ""], ["Due\u00f1as-Osorio", "Leonardo", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1908.04700", "submitter": "Emile Van Krieken", "authors": "Emile van Krieken, Erman Acar, Frank van Harmelen", "title": "Semi-Supervised Learning using Differentiable Reasoning", "comments": null, "journal-ref": "IFCoLog Journal of Logic and its Applications 6 (2019) 633-653", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Differentiable Reasoning (DR), a novel semi-supervised learning\ntechnique which uses relational background knowledge to benefit from unlabeled\ndata. We apply it to the Semantic Image Interpretation (SII) task and show that\nbackground knowledge provides significant improvement. We find that there is a\nstrong but interesting imbalance between the contributions of updates from\nModus Ponens (MP) and its logical equivalent Modus Tollens (MT) to the learning\nprocess, suggesting that our approach is very sensitive to a phenomenon called\nthe Raven Paradox. We propose a solution to overcome this situation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 15:21:37 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["van Krieken", "Emile", ""], ["Acar", "Erman", ""], ["van Harmelen", "Frank", ""]]}, {"id": "1908.04921", "submitter": "EPTCS", "authors": "L\\^e Th\\`anh D\\~ung Nguyen (LIPN, Universit\\'e Paris 13)", "title": "On the Elementary Affine Lambda-Calculus with and Without Fixed Points", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478", "journal-ref": "EPTCS 298, 2019, pp. 15-29", "doi": "10.4204/EPTCS.298.2", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elementary affine lambda-calculus was introduced as a polyvalent setting\nfor implicit computational complexity, allowing for characterizations of\npolynomial time and hyperexponential time predicates. But these results rely on\ntype fixpoints (a.k.a. recursive types), and it was unknown whether this\nfeature of the type system was really necessary. We give a positive answer by\nshowing that without type fixpoints, we get a characterization of regular\nlanguages instead of polynomial time. The proof uses the semantic evaluation\nmethod. We also propose an aesthetic improvement on the characterization of the\nfunction classes FP and k-FEXPTIME in the presence of recursive types.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:57:27 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Nguyen", "L\u00ea Th\u00e0nh D\u0169ng", "", "LIPN, Universit\u00e9 Paris 13"]]}, {"id": "1908.04922", "submitter": "EPTCS", "authors": "Paulin Jacob\\'e de Naurois (CNRS/universit\\'e paris13)", "title": "Pointers in Recursion: Exploring the Tropics", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478", "journal-ref": "EPTCS 298, 2019, pp. 31-45", "doi": "10.4204/EPTCS.298.3", "report-no": null, "categories": "cs.CC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We translate the usual class of partial/primitive recursive functions to a\npointer recursion framework, accessing actual input values via a pointer\nreading unit-cost function. These pointer recursive functions classes are\nproven equivalent to the usual partial/primitive recursive functions.\nComplexity-wise, this framework captures in a streamlined way most of the\nrelevant sub-polynomial classes. Pointer recursion with the safe/normal tiering\ndiscipline of Bellantoni and Cook corresponds to polylogtime computation. We\nintroduce a new, non-size increasing tiering discipline, called tropical\ntiering. Tropical tiering and pointer recursion, used with some of the most\ncommon recursion schemes, capture the classes logspace, logspace/polylogtime,\nptime, and NC. Finally, in a fashion reminiscent of the safe recursive\nfunctions, tropical tiering is expressed directly in the syntax of the function\nalgebras, yielding the tropical recursive function algebras.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:57:44 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["de Naurois", "Paulin Jacob\u00e9", "", "CNRS/universit\u00e9 paris13"]]}, {"id": "1908.04923", "submitter": "EPTCS", "authors": "Bruce M. Kapron (University of Victoria), Florian Steinberg (INRIA\n  Saclay)", "title": "Type-two Iteration with Bounded Query Revision", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478", "journal-ref": "EPTCS 298, 2019, pp. 61-73", "doi": "10.4204/EPTCS.298.5", "report-no": null, "categories": "cs.CC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent results of Kapron and Steinberg (LICS 2018) we introduce\nnew forms of iteration on length in the setting of applied lambda-calculi for\nhigher-type poly-time computability. In particular, in a type-two setting, we\nconsider functionals which capture iteration on input length which bound\ninteraction with the type-one input parameter, by restricting to a constant\neither the number of times the function parameter may return a value of\nincreasing size, or the number of times the function parameter may be applied\nto an argument of increasing size. We prove that for any constant bound, the\niterators obtained are equivalent, with respect to lambda-definability over\ntype-one poly-time functions, to the recursor of Cook and Urquhart which\ncaptures Cobham's notion of limited recursion on notation in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:58:18 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Kapron", "Bruce M.", "", "University of Victoria"], ["Steinberg", "Florian", "", "INRIA\n  Saclay"]]}, {"id": "1908.05106", "submitter": "Maximilian Weininger", "authors": "Pranav Ashok and Krishnendu Chatterjee and Jan Kretinsky and\n  Maximilian Weininger and Tobias Winkler", "title": "Approximating Values of Generalized-Reachability Stochastic Games", "comments": null, "journal-ref": null, "doi": "10.1145/3373718.3394761", "report-no": null, "categories": "cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple stochastic games are turn-based 2.5-player games with a reachability\nobjective. The basic question asks whether one player can ensure reaching a\ngiven target with at least a given probability. A natural extension is games\nwith a conjunction of such conditions as objective. Despite a plethora of\nrecent results on the analysis of systems with multiple objectives, the\ndecidability of this basic problem remains open. In this paper, we present an\nalgorithm approximating the Pareto frontier of the achievable values to a given\nprecision. Moreover, it is an anytime algorithm, meaning it can be stopped at\nany time returning the current approximation and its error bound.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 13:00:49 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 08:43:47 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ashok", "Pranav", ""], ["Chatterjee", "Krishnendu", ""], ["Kretinsky", "Jan", ""], ["Weininger", "Maximilian", ""], ["Winkler", "Tobias", ""]]}, {"id": "1908.05145", "submitter": "Krishna Balajirao Manoorkar", "authors": "Sabine Frittella, Krishna Manoorkar, Alessandra Palmigiano, Apostolos\n  Tzimoulis, Nachoem M. Wijnberg", "title": "Toward a Dempster-Shafer theory of concepts", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijar.2020.05.004", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize the basic notions and results of Dempster-Shafer\ntheory from predicates to formal concepts. Results include the representation\nof conceptual belief functions as inner measures of suitable probability\nfunctions, and a Dempster-Shafer rule of combination on belief functions on\nformal concepts.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 14:27:00 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 15:19:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Frittella", "Sabine", ""], ["Manoorkar", "Krishna", ""], ["Palmigiano", "Alessandra", ""], ["Tzimoulis", "Apostolos", ""], ["Wijnberg", "Nachoem M.", ""]]}, {"id": "1908.05268", "submitter": "Sandra Kiefer", "authors": "Sandra Kiefer, Daniel Neuen", "title": "The Power of the Weisfeiler-Leman Algorithm to Decompose Graphs", "comments": "30 pages, 4 figures, full version of a paper accepted at MFCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weisfeiler-Leman procedure is a widely-used approach for graph\nisomorphism testing that works by iteratively computing an\nisomorphism-invariant coloring of vertex tuples. Meanwhile, a fundamental tool\nin structural graph theory, which is often exploited in approaches to tackle\nthe graph isomorphism problem, is the decomposition into 2- and 3-connected\ncomponents.\n  We prove that the 2-dimensional Weisfeiler-Leman algorithm implicitly\ncomputes the decomposition of a graph into its 3-connected components. Thus,\nthe dimension of the algorithm needed to distinguish two given graphs is at\nmost the dimension required to distinguish the corresponding decompositions\ninto 3-connected components (assuming it is at least 2).\n  This result implies that for k >= 2, the k-dimensional algorithm\ndistinguishes k-separators, i.e., k-tuples of vertices that separate the graph,\nfrom other vertex k-tuples. As a byproduct, we also obtain insights about the\nconnectivity of constituent graphs of association schemes.\n  In an application of the results, we show the new upper bound of k on the\nWeisfeiler-Leman dimension of graphs of treewidth at most k. Using a\nconstruction by Cai, F\\\"urer, and Immerman, we also provide a new lower bound\nthat is asymptotically tight up to a factor of 2.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 17:49:29 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Kiefer", "Sandra", ""], ["Neuen", "Daniel", ""]]}, {"id": "1908.05402", "submitter": "Chao Wang", "authors": "Meng Wu, Jingbo Wang, Jyotirmoy Deshmukh, Chao Wang", "title": "Shield Synthesis for Real: Enforcing Safety in Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems are often safety-critical in that violations of safety\nproperties may lead to catastrophes. We propose a method to enforce the safety\nof systems with real-valued signals by synthesizing a runtime enforcer called\nthe shield. Whenever the system violates a property, the shield, composed with\nthe system, makes correction instantaneously to ensure that no erroneous output\nis generated by the combined system. While techniques for synthesizing Boolean\nshields are well understood, they do not handle real-valued signals ubiquitous\nin cyber-physical systems, meaning corrections may be either unrealizable or\ninefficient to compute in the real domain. We solve the realizability and\nefficiency problems by statically analyzing the compatibility of predicates\ndefined over real-valued signals, and using the analysis result to constrain a\ntwo-player safety game used to synthesize the shield. We have implemented the\nmethod and demonstrated its effectiveness and efficiency on a variety of\napplications, including an automotive powertrain control system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 03:28:16 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wu", "Meng", ""], ["Wang", "Jingbo", ""], ["Deshmukh", "Jyotirmoy", ""], ["Wang", "Chao", ""]]}, {"id": "1908.05528", "submitter": "Apostolos Tzimoulis", "authors": "Giuseppe Greco and Fei Liang and Michael Moortgat and Alessandra\n  Palmigiano and Apostolos Tzimoulis", "title": "Vector spaces as Kripke frames", "comments": "Fixed list of authors in metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the compositional distributional approach in computational\nlinguistics has opened the way for an integration of the \\emph{lexical} aspects\nof meaning into Lambek's type-logical grammar program. This approach is based\non the observation that a sound semantics for the associative, commutative and\nunital Lambek calculus can be based on vector spaces by interpreting fusion as\nthe tensor product of vector spaces.\n  In this paper, we build on this observation and extend it to a `vector space\nsemantics' for the \\emph{general} Lambek calculus, based on \\emph{algebras over\na field} $\\mathbb{K}$ (or $\\mathbb{K}$-algebras), i.e. vector spaces endowed\nwith a bilinear binary product. Such structures are well known in algebraic\ngeometry and algebraic topology, since they are important instances of Lie\nalgebras and Hopf algebras. Applying results and insights from duality and\nrepresentation theory for the algebraic semantics of nonclassical logics, we\nregard $\\mathbb{K}$-algebras as `Kripke frames' the complex algebras of which\nare complete residuated lattices.\n  This perspective makes it possible to establish a systematic connection\nbetween vector space semantics and the standard Routley-Meyer semantics of\n(modal) substructural logics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:26:44 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 15:16:04 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 18:09:06 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 11:30:37 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Greco", "Giuseppe", ""], ["Liang", "Fei", ""], ["Moortgat", "Michael", ""], ["Palmigiano", "Alessandra", ""], ["Tzimoulis", "Apostolos", ""]]}, {"id": "1908.05535", "submitter": "Brandon Bohrer", "authors": "Brandon Bohrer and Andr\\'e Platzer", "title": "Toward Structured Proofs for Dynamic Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Kaisar, a structured interactive proof language for differential\ndynamic logic (dL), for safety-critical cyber-physical systems (CPS). The\ndefining feature of Kaisar is *nominal terms*, which simplify CPS proofs by\nmaking the frequently needed historical references to past program states\nfirst-class. To support nominals, we extend the notion of structured proof with\na first-class notion of *structured symbolic execution* of CPS models. We\nimplement Kaisar in the theorem prover KeYmaera X and reproduce an example on\nthe safe operation of a parachute and a case study on ground robot control. We\nshow how nominals simplify common CPS reasoning tasks when combined with other\nfeatures of structured proof. We develop an extensive metatheory for Kaisar. In\naddition to soundness and completeness, we show a formal specification for\nKaisar's nominals and relate Kaisar to a nominal variant of dL.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:46:13 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Bohrer", "Brandon", ""], ["Platzer", "Andr\u00e9", ""]]}, {"id": "1908.05677", "submitter": "Sam Sanders", "authors": "Sam Sanders", "title": "Lifting countable to uncountable mathematics", "comments": "24 pages. To appear in: 'Information and Computation' Special Issue\n  WoLLIC19 post-proceedings. Same \"Preliminaries\" section as e.g.\n  arXiv:1905.04058, arXiv:1908.05676", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turing's famous 'machine' framework provides an intuitively clear conception\nof 'computing with real numbers'. A recursive counterexample to a theorem shows\nthat the theorem does not hold when restricted to computable objects. These\ncounterexamples are often crucial in establishing reversals in the Reverse\nMathematics program. All the previous is essentially limited to a language that\ncan only express countable mathematics directly. The aim of this paper is to\nshow that reversals and recursive counterexamples, countable in nature as they\nmight be, directly yield new and interesting results about uncountable\nmathematics with little-to-no modification. We shall treat the following\ntopics/theorems: the monotone convergence theorem/Specker sequences, compact\nand closed sets in metric spaces, the Rado selection lemma, the ordering and\nalgebraic closures of fields, and ideals of rings. The higher-order\ngeneralisation of sequence is of course provided by nets (aka Moore-Smith\nsequences ).\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 12:50:20 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 13:09:50 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 08:00:22 GMT"}, {"version": "v4", "created": "Sun, 21 Jun 2020 19:39:10 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sanders", "Sam", ""]]}, {"id": "1908.05737", "submitter": "EPTCS", "authors": "Francesco Olivieri (Data61, CSIRO (Australia)), Guido Governatori\n  (Data61, CSIRO (Australia)), Claudio Tomazzoli (Department of Computer\n  Science, University of Verona), Matteo Cristani (Department of Computer\n  Science, University of Verona)", "title": "Applications of Linear Defeasible Logic: combining resource consumption\n  and exceptions to energy management and business processes", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478. arXiv admin note:\n  substantial text overlap with arXiv:1809.03656", "journal-ref": "EPTCS 298, 2019, pp. 1-14", "doi": "10.4204/EPTCS.298.1", "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Logic and Defeasible Logic have been adopted to formalise different\nfeatures of knowledge representation: consumption of resources, and non\nmonotonic reasoning in particular to represent exceptions. Recently, a\nframework to combine sub-structural features, corresponding to the consumption\nof resources, with defeasibility aspects to handle potentially conflicting\ninformation, has been discussed in literature, by some of the authors. Two\napplications emerged that are very relevant: energy management and business\nprocess management. We illustrate a set of guide lines to determine how to\napply linear defeasible logic to those contexts.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:57:11 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Olivieri", "Francesco", "", "Data61, CSIRO"], ["Governatori", "Guido", "", "Data61, CSIRO"], ["Tomazzoli", "Claudio", "", "Department of Computer\n  Science, University of Verona"], ["Cristani", "Matteo", "", "Department of Computer\n  Science, University of Verona"]]}, {"id": "1908.05799", "submitter": "Arshavir Ter-Gabrielyan", "authors": "Arshavir Ter-Gabrielyan and Alexander J. Summers and Peter M\\\"uller", "title": "Modular Verification of Heap Reachability Properties in Separation Logic", "comments": "OOPSLA-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The correctness of many algorithms and data structures depends on\nreachability properties, that is, on the existence of chains of references\nbetween objects in the heap. Reasoning about reachability is difficult for two\nmain reasons. First, any heap modification may affect an unbounded number of\nreference chains, which complicates modular verification, in particular,\nframing. Second, general graph reachability is not supported by SMT solvers,\nwhich impedes automatic verification. In this paper, we present a modular\nspecification and verification technique for reachability properties in\nseparation logic. For each method, we specify reachability only locally within\nthe fragment of the heap on which the method operates. A novel form of\nreachability framing for relatively convex subheaps allows one to extend\nreachability properties from the heap fragment of a callee to the larger\nfragment of its caller, enabling precise procedure-modular reasoning. Our\ntechnique supports practically important heap structures, namely acyclic graphs\nwith a bounded outdegree as well as (potentially cyclic) graphs with at most\none path (modulo cycles) between each pair of nodes. The integration into\nseparation logic allows us to reason about reachability and other properties in\na uniform way, to verify concurrent programs, and to automate our technique via\nexisting separation logic verifiers. We demonstrate that our verification\ntechnique is amenable to SMT-based verification by encoding a number of\nbenchmark examples into the Viper verification infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 00:18:43 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Ter-Gabrielyan", "Arshavir", ""], ["Summers", "Alexander J.", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "1908.05839", "submitter": "Jana Dunfield", "authors": "Jana Dunfield and Neel Krishnaswami", "title": "Bidirectional Typing", "comments": "37 pages; submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional typing combines two modes of typing: type checking, which\nchecks that a program satisfies a known type, and type synthesis, which\ndetermines a type from the program. Using checking enables bidirectional typing\nto support features for which inference is undecidable; using synthesis enables\nbidirectional typing to avoid the large annotation burden of explicitly typed\nlanguages. In addition, bidirectional typing improves error locality. We\nhighlight the design principles that underlie bidirectional type systems,\nsurvey the development of bidirectional typing from the prehistoric period\nbefore Pierce and Turner's local type inference to the present day, and provide\nguidance for future investigations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 04:22:23 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 18:26:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dunfield", "Jana", ""], ["Krishnaswami", "Neel", ""]]}, {"id": "1908.05964", "submitter": "Christian M\\\"uller", "authors": "Helmut Seidl, Christian M\\\"uller, Bernd Finkbeiner", "title": "How to Win First-Order Safety Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order (FO) transition systems have recently attracted attention for the\nverification of parametric systems such as network protocols, software-defined\nnetworks or multi-agent workflows like conference management systems. Desirable\nproperties of these systems such as functional correctness or noninterference\nhave conveniently been formulated as safety properties. In order to\nautomatically synthesize strategies that enforce safety or noninterference, we\ngeneralize FO transition systems to FO safety games. We prove that the\nexistence of a winning strategy of safety player in finite games is in fact,\nequivalent to second-order quantifier elimination. For the important case of FO\ngames with monadic predicates only, we provide a complete classification into\ndecidable and undecidable cases. For games with non-monadic predicates, we\nconcentrate on universal first-order invariants, since these are sufficient to\nexpress a large class of noninterference properties. Based on general\ntechniques for second-order quantifier elimination, we provide abstraction and\nrefinement techniques in order to synthesize FO strategies that enforce safety.\nWe demonstrate the usefulness of our approach by inferring nontrivial FO\nspecifications in a leader election protocol as well as for paper assignment in\na conference mangagement system to exclude unappreciated disclosure of reports.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 13:19:13 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 03:39:28 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Seidl", "Helmut", ""], ["M\u00fcller", "Christian", ""], ["Finkbeiner", "Bernd", ""]]}, {"id": "1908.05979", "submitter": "Chuangjie Xu", "authors": "Chuangjie Xu", "title": "A Gentzen-style monadic translation of G\\\"odel's System T", "comments": "17 pages. Changes: (1) remove the restriction of satisfying the monad\n  laws in the definition of nuclei, (2) add a unified theorem of logical\n  relation. This paper will appear in FSCD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a syntactic translation of Goedel's System T parametrized by a\nweak notion of a monad, and prove a corresponding fundamental theorem of\nlogical relation. Our translation structurally corresponds to Gentzen's\nnegative translation of classical logic. By instantiating the monad and the\nlogical relation, we reveal the well-known properties and structures of\nT-definable functionals including majorizability, continuity and bar recursion.\nOur development has been formalized in the Agda proof assistant.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 14:04:35 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 15:45:49 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Xu", "Chuangjie", ""]]}, {"id": "1908.06177", "submitter": "Koustuv Sinha", "authors": "Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L.\n  Hamilton", "title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text", "comments": "Accepted at EMNLP 2019, 9 page content + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of natural language understanding (NLU) systems has been\ntroubled by results highlighting the failure of these models to generalize in a\nsystematic and robust way. In this work, we introduce a diagnostic benchmark\nsuite, named CLUTRR, to clarify some key issues related to the robustness and\nsystematicity of NLU systems. Motivated by classic work on inductive logic\nprogramming, CLUTRR requires that an NLU system infer kinship relations between\ncharacters in short stories. Successful performance on this task requires both\nextracting relationships between entities, as well as inferring the logical\nrules governing these relationships. CLUTRR allows us to precisely measure a\nmodel's ability for systematic generalization by evaluating on held-out\ncombinations of logical rules, and it allows us to evaluate a model's\nrobustness by adding curated noise facts. Our empirical results highlight a\nsubstantial performance gap between state-of-the-art NLU models (e.g., BERT and\nMAC) and a graph neural network model that works directly with symbolic\ninputs---with the graph-based model exhibiting both stronger generalization and\ngreater robustness.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 21:12:15 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 00:14:56 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Sinha", "Koustuv", ""], ["Sodhani", "Shagun", ""], ["Dong", "Jin", ""], ["Pineau", "Joelle", ""], ["Hamilton", "William L.", ""]]}, {"id": "1908.06275", "submitter": "S. Akshay", "authors": "S. Akshay, Jatin Arora, Supratik Chakraborty, S. Krishna, Divya\n  Raghunathan and Shetal Shah", "title": "Knowledge Compilation for Boolean Functional Synthesis", "comments": "Full version of conference paper accepted at FMCAD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a Boolean formula F(X,Y), where X is a vector of outputs and Y is a\nvector of inputs, the Boolean functional synthesis problem requires us to\ncompute a Skolem function vector G(Y)for X such that F(G(Y),Y) holds whenever\n\\exists X F(X,Y) holds. In this paper, we investigate the relation between the\nrepresentation of the specification F(X,Y) and the complexity of synthesis. We\nintroduce a new normal form for Boolean formulas, called SynNNF, that\nguarantees polynomial-time synthesis and also polynomial-time existential\nquantification for some order of quantification of variables. We show that\nseveral normal forms studied in the knowledge compilation literature are\nsubsumed by SynNNF, although SynNNFcan be super-polynomially more succinct than\nthem. Motivated by these results, we propose an algorithm to convert a\nspecification in CNF to SynNNF, with the intent of solving the Boolean\nfunctional synthesis problem. Experiments with a prototype implementation show\nthat this approach solves several benchmarks beyond the reach of\nstate-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 09:42:00 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Akshay", "S.", ""], ["Arora", "Jatin", ""], ["Chakraborty", "Supratik", ""], ["Krishna", "S.", ""], ["Raghunathan", "Divya", ""], ["Shah", "Shetal", ""]]}, {"id": "1908.06479", "submitter": "Paulo Oliva", "authors": "Rob Arthan and Paulo Oliva", "title": "Studying Algebraic Structures using Prover9 and Mace4", "comments": "21 pages, to appear as Chapter 5 in \"Proof Technology in Mathematics\n  Research and Teaching\", Mathematics Education in the Digital Era 14, edited\n  by G. Hanna et al. (eds.), published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we present a case study, drawn from our research work, on the\napplication of a fully automated theorem prover together with an automatic\ncounter-example generator in the investigation of a class of algebraic\nstructures. We will see that these tools, when combined with human insight and\ntraditional algebraic methods, help us to explore the problem space quickly and\neffectively. The counter-example generator rapidly rules out many false\nconjectures, while the theorem prover is often much more efficient than a human\nbeing at verifying algebraic identities. The specific tools in our case study\nare Prover9 and Mace4; the algebraic structures are generalisations of Heyting\nalgebras known as hoops. We will see how this approach helped us to discover\nnew theorems and to find new or improved proofs of known results. We also make\nsome suggestions for how one might deploy these tools to supplement a more\nconventional approach to teaching algebra.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 15:24:27 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Arthan", "Rob", ""], ["Oliva", "Paulo", ""]]}, {"id": "1908.06510", "submitter": "Kirstin Peters", "authors": "Kirstin Peters, Christoph Wagner and Uwe Nestmann", "title": "Taming Concurrency for Verification Using Multiparty Session Types\n  (Technical Report)", "comments": "This technical report provides proofs and additional materials for a\n  paper (with the same title) at ICTAC'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The additional complexity caused by concurrently communicating processes in\ndistributed systems render the verification of such systems into a very hard\nproblem. Multiparty session types were developed to govern communication and\nconcurrency in distributed systems. As such, they provide an efficient\nverification method w.r.t. properties about communication and concurrency, like\ncommunication safety or progress. However, they do not support the analysis of\nproperties that require the consideration of concrete runs or concrete values\nof variables.\n  We sequentialise well-typed systems of processes guided by the structure of\ntheir global type to obtain interaction-free abstractions thereof. Without\ninteraction, concurrency in the system is reduced to sequential and completely\nindependent parallel compositions. In such abstractions, the verification of\nproperties such as e.g. data-based termination that are not covered by\nmultiparty session types, but rely on concrete runs or values of variables,\nbecomes significantly more efficient.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 20:24:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Peters", "Kirstin", ""], ["Wagner", "Christoph", ""], ["Nestmann", "Uwe", ""]]}, {"id": "1908.06550", "submitter": "Rob van Glabbeek", "authors": "Wan Fokkink, Rob van Glabbeek and Bas Luttik", "title": "Divide and Congruence III: From Decomposition of Modal Formulas to\n  Preservation of Stability and Divergence", "comments": "An extended abstract of this paper appeared in Proc. CONCUR'17", "journal-ref": null, "doi": "10.1016/j.ic.2019.104435", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In two earlier papers we derived congruence formats with regard to transition\nsystem specifications for weak semantics on the basis of a decomposition method\nfor modal formulas. The idea is that a congruence format for a semantics must\nensure that the formulas in the modal characterisation of this semantics are\nalways decomposed into formulas that are again in this modal characterisation.\nThe stability and divergence requirements that are imposed on many of the known\nweak semantics have so far been outside the realm of this method. Stability\nrefers to the absence of a $\\tau$-transition. We show, using the decomposition\nmethod, how congruence formats can be relaxed for weak semantics that are\nstability-respecting. This relaxation for instance brings the priority operator\nwithin the range of the stability-respecting branching bisimulation format.\nDivergence, which refers to the presence of an infinite sequence of\n$\\tau$-transitions, escapes the inductive decomposition method. We circumvent\nthis problem by proving that a congruence format for a stability-respecting\nweak semantics is also a congruence format for its divergence-preserving\ncounterpart.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 01:21:17 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Fokkink", "Wan", ""], ["van Glabbeek", "Rob", ""], ["Luttik", "Bas", ""]]}, {"id": "1908.06601", "submitter": "Mike Ji", "authors": "Mike H. Ji", "title": "Implicit Recursive Characteristics of STOP", "comments": "5 pages. A proof that STOP itself is a recursive process.\n  STOP$_{\\alpha X} = \\mu$ X. nil $\\rightarrow$ X", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important notations of Communicating Sequential Process(CSP) are the\nprocess and the prefix (event)$\\rightarrow$(process) operator. While we can\nformally apply the $\\rightarrow$ operator to define a live process's behavior,\nthe STOP process, which usually resulted from deadlock, starving or livelock,\nis lack of formal description, defined by most literatures as \"doing nothing\nbut halt\". In this paper, we argue that the STOP process should not be\nconsidered as a black box, it should follow the prefix $\\rightarrow$ schema and\nthe same inference rules so that a unified and consistent process algebra model\ncan be established. In order to achieve this goal, we introduce a special event\ncalled \"nil\" that any process can take. This nil event will do nothing\nmeaningful and leave nothing on a process's observable record. With the nil\nevent and its well-defined rules, we can successfully use the $\\rightarrow$\noperator to formally describe a process's complete behavior in its whole life\ncircle. More interestingly, we can use prefix $\\rightarrow$ and nil event to\nfully describe the STOP process's internal behavior and conclude that the\nSTOP's formal equation can be given as simple as STOP$_{\\alpha X} = \\mu$ X. nil\n$\\rightarrow$ X.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 05:43:13 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 03:22:01 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Ji", "Mike H.", ""]]}, {"id": "1908.06633", "submitter": "\\'Etienne Andr\\'e", "authors": "\\'Etienne Andr\\'e, Didier Lime and Mathias Ramparison", "title": "On the expressive power of invariants in parametric timed automata", "comments": "This is the author version of the manuscript of the same name\n  published in the proceedings of the 24th International Conference on\n  Engineering of Complex Computer Systems (ICECCS 2019). This work is partially\n  supported by the ANR national research program PACS (ANR-14-CE28-0002) and by\n  ERATO HASUO Metamathematics for Systems Design Project (No. JPMJER1603), JST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The verification of systems combining hard timing constraints with\nconcurrency is challenging. This challenge becomes even harder when some timing\nconstants are missing or unknown. Parametric timed formalisms, such as\nparametric timed automata (PTAs), tackle the synthesis of such timing constants\n(seen as parameters) for which a property holds. Such formalisms are highly\nexpressive, but also undecidable, and few decidable subclasses were proposed.\nWe propose here a syntactic restriction on PTAs consisting in removing guards\n(constraints on transitions) to keep only invariants (constraints on\nlocations). While this restriction preserves the expressiveness of PTAs (and\ntherefore their undecidability), an additional restriction on the type of\nconstraints allows to not only prove decidability, but also to perform the\nexact synthesis of parameter valuations satisfying reachability. This\nformalism, that seems trivial at first sight as it benefits from the\ndecidability of the reachability problem with a better complexity than Timed\nAutomata (TAs), suffers from the undecidability of the whole TCTL logic that\nTAs, on the contrary enjoy. We believe our formalism allows for an interesting\ntrade-off between decidability and practical expressiveness and is therefore\npromising. We show its applicability in a small case study.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 08:06:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Andr\u00e9", "\u00c9tienne", ""], ["Lime", "Didier", ""], ["Ramparison", "Mathias", ""]]}, {"id": "1908.06684", "submitter": "Thorsten Wissmann", "authors": "Eric Goubault, Samuel Mimram", "title": "Directed Homotopy in Non-Positively Curved Spaces", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 3 (July 13,\n  2020) lmcs:6634", "doi": "10.23638/LMCS-16(3:4)2020", "report-no": null, "categories": "cs.LO cs.DC math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A semantics of concurrent programs can be given using precubical sets, in\norder to study (higher) commutations between the actions, thus encoding the\n\"geometry\" of the space of possible executions of the program. Here, we study\nthe particular case of programs using only mutexes, which are the most widely\nused synchronization primitive. We show that in this case, the resulting\nprograms have non-positive curvature, a notion that we introduce and study here\nfor precubical sets, and can be thought of as an algebraic analogue of the\nwell-known one for metric spaces. Using this it, as well as categorical\nrewriting techniques, we are then able to show that directed and non-directed\nhomotopy coincide for directed paths in these precubical sets. Finally, we\nstudy the geometric realization of precubical sets in metric spaces, to show\nthat our conditions on precubical sets actually coincide with those for metric\nspaces. Since the category of metric spaces is not cocomplete, we are lead to\nwork with generalized metric spaces and study some of their properties.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 10:32:53 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 18:10:49 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 07:33:38 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 10:09:58 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Goubault", "Eric", ""], ["Mimram", "Samuel", ""]]}, {"id": "1908.06723", "submitter": "EPTCS", "authors": "Alexei Lisitsa (The University of Liverpool), Andrei Nemytykh (Program\n  Systems Institute of RAS)", "title": "Proceedings Seventh International Workshop on Verification and Program\n  Transformation", "comments": null, "journal-ref": "EPTCS 299, 2019", "doi": "10.4204/EPTCS.299", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains a final and revised selection of papers presented at the\nSeventh International Workshop on Verification and Program Transformation (VPT\n2019), which took place in Genova, Italy, on April 2nd, 2019, affiliated with\nProgramming 2019.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 05:53:26 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Lisitsa", "Alexei", "", "The University of Liverpool"], ["Nemytykh", "Andrei", "", "Program\n  Systems Institute of RAS"]]}, {"id": "1908.06757", "submitter": "arXiv Admin", "authors": "Karthik Ganesan, Srinivasa Shashank Nuthakki", "title": "Boosting the Bounds of Symbolic QED for Effective Pre-Silicon\n  Verification of Processor Cores", "comments": "arXiv admin note: withdrawn by arXiv administrators due to incomplete\n  author list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to ensure functional correctness and hardware trust\nduring pre-silicon verification face severe limitations. In this work, we\nsystematically leverage two key ideas: 1) Symbolic Quick Error Detection\n(Symbolic QED or SQED), a recent bug detection and localization technique using\nBounded Model Checking (BMC); and 2) Symbolic starting states, to present a\nmethod that: i) Effectively detects both \"difficult\" logic bugs and Hardware\nTrojans, even with long activation sequences where traditional BMC techniques\nfail; and ii) Does not need skilled manual guidance for writing testbenches,\nwriting design-specific assertions, or debugging spurious counter-examples.\nUsing open-source RISC-V cores, we demonstrate the following: 1. Quick (<5\nminutes for an in-order scalar core and <2.5 hours for an out-of-order\nsuperscalar core) detection of 100% of hundreds of logic bug and hardware\nTrojan scenarios from commercial chips and research literature, and 97.9% of\n\"extremal\" bugs (randomly-generated bugs requiring ~100,000 activation\ninstructions taken from random test programs). 2. Quick (~1 minute) detection\nof several previously unknown bugs in open-source RISC-V designs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:26:58 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 00:02:36 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 17:01:55 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 14:22:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ganesan", "Karthik", ""], ["Nuthakki", "Srinivasa Shashank", ""]]}, {"id": "1908.07021", "submitter": "Tobias Fritz", "authors": "Tobias Fritz", "title": "A synthetic approach to Markov kernels, conditional independence and\n  theorems on sufficient statistics", "comments": "98 pages. v6: fixed error in Section 7. v7: incorporates referee's\n  comments. v8: minor correction", "journal-ref": "Adv. Math. 370, 107239 (2020)", "doi": "10.1016/j.aim.2020.107239", "report-no": null, "categories": "math.ST cs.LO math.CT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Markov categories as a framework for synthetic probability and\nstatistics, following work of Golubtsov as well as Cho and Jacobs. This means\nthat we treat the following concepts in purely abstract categorical terms:\nconditioning and disintegration; various versions of conditional independence\nand its standard properties; conditional products; almost surely; sufficient\nstatistics; versions of theorems on sufficient statistics due to\nFisher--Neyman, Basu, and Bahadur.\n  Besides the conceptual clarity offered by our categorical setup, its main\nadvantage is that it provides a uniform treatment of various types of\nprobability theory, including discrete probability theory, measure-theoretic\nprobability with general measurable spaces, Gaussian probability, stochastic\nprocesses of either of these kinds, and many others.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 18:54:09 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 02:57:58 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 17:25:47 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 22:37:03 GMT"}, {"version": "v5", "created": "Mon, 24 Feb 2020 01:17:05 GMT"}, {"version": "v6", "created": "Tue, 31 Mar 2020 16:16:30 GMT"}, {"version": "v7", "created": "Tue, 28 Apr 2020 17:14:02 GMT"}, {"version": "v8", "created": "Sun, 31 May 2020 18:29:36 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Fritz", "Tobias", ""]]}, {"id": "1908.07188", "submitter": "EPTCS", "authors": "Emanuele De Angelis (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Fabio Fioravanti (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Alberto Pettorossi (DICII, University of Roma Tor\n  Vergata, Italy), Maurizio Proietti (CNR-IASI, Rome, Italy)", "title": "Lemma Generation for Horn Clause Satisfiability: A Preliminary Study", "comments": "In Proceedings VPT 2019, arXiv:1908.06723", "journal-ref": "EPTCS 299, 2019, pp. 4-18", "doi": "10.4204/EPTCS.299.4", "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the verification of imperative, functional, and logic\nprograms can be reduced to the satisfiability of constrained Horn clauses\n(CHCs), and this satisfiability check can be performed by using CHC solvers,\nsuch as Eldarica and Z3. These solvers perform well when they act on simple\nconstraint theories, such as Linear Integer Arithmetic and the theory of\nBooleans, but their efficacy is very much reduced when the clauses refer to\nconstraints on inductively defined structures, such as lists or trees.\nRecently, we have presented a transformation technique for eliminating those\ninductively defined data structures, and hence avoiding the need for\nincorporating induction principles into CHC solvers. However, this technique\nmay fail when the transformation requires the use of lemmata whose generation\nneeds ingenuity. In this paper we show, through an example, how during the\nprocess of transforming CHCs for eliminating inductively defined structures one\ncan introduce suitable predicates, called difference predicates, whose\ndefinitions correspond to the lemmata to be introduced. Through a second\nexample, we show that, whenever difference predicates cannot be introduced, we\ncan introduce, instead, auxiliary queries which also correspond to lemmata, and\nthe proof of these lemmata can be done by showing the satisfiability of those\nqueries.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 06:35:19 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["De Angelis", "Emanuele", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Fioravanti", "Fabio", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Pettorossi", "Alberto", "", "DICII, University of Roma Tor\n  Vergata, Italy"], ["Proietti", "Maurizio", "", "CNR-IASI, Rome, Italy"]]}, {"id": "1908.07189", "submitter": "EPTCS", "authors": "John P. Gallagher (Roskilde University and IMDEA Software Institute)", "title": "Polyvariant Program Specialisation with Property-based Abstraction", "comments": "In Proceedings VPT 2019, arXiv:1908.06723", "journal-ref": "EPTCS 299, 2019, pp. 34-48", "doi": "10.4204/EPTCS.299.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that property-based abstraction, an established\ntechnique originating in software model checking, is a flexible method of\ncontrolling polyvariance in program specialisation in a standard online\nspecialisation algorithm. Specialisation is a program transformation that\ntransforms a program with respect to given constraints that restrict its\nbehaviour. Polyvariant specialisation refers to the generation of two or more\nspecialised versions of the same program code. The same program point can be\nreached more than once during a computation, with different constraints\napplying in each case, and polyvariant specialisation allows different\nspecialisations to be realised. A property-based abstraction uses a finite set\nof properties to define a finite set of abstract versions of predicates,\nensuring that only a finite number of specialised versions is generated. The\nparticular choice of properties is critical for polyvariance; too few versions\ncan result in insufficient specialisation, while too many can result in an\nincrease of code size with no corresponding efficiency gains. Using examples,\nwe show the flexibility of specialisation with property-based abstraction and\ndiscuss its application in control flow refinement, verification, termination\nanalysis and dimension-based specialisation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 06:36:01 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Gallagher", "John P.", "", "Roskilde University and IMDEA Software Institute"]]}, {"id": "1908.07239", "submitter": "Tony Tan", "authors": "Yanger Ma, Tony Tan", "title": "A simple combinatorial proof for small model property of two-variable\n  logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present another proof for the well-known {\\em small model property} of\ntwo-variable logic. As far as we know, existing proofs of this property rely\nheavily on model theoretic concepts. In contrast, ours is purely combinatorial\nand uses only a very simple counting argument, which we find rather intuitive\nand elegant.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 09:31:55 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 03:42:52 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ma", "Yanger", ""], ["Tan", "Tony", ""]]}, {"id": "1908.07282", "submitter": "Alain Finkel", "authors": "Alain Finkel, M. Praveen", "title": "Verification of Flat FIFO Systems", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (October\n  14, 2020) lmcs:6839", "doi": "10.23638/LMCS-16(4:4)2020", "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The decidability and complexity of reachability problems and model-checking\nfor flat counter machines have been explored in detail. However, only few\nresults are known for flat (lossy) FIFO machines, only in some particular cases\n(a single loop or a single bounded expression). We prove, by establishing\nreductions between properties, and by reducing SAT to a subset of these\nproperties that many verification problems like reachability, non-termination,\nunboundedness are NP-complete for flat FIFO machines, generalizing similar\nexisting results for flat counter machines. We also show that reachability is\nNP-complete for flat lossy FIFO machines and for flat front-lossy FIFO\nmachines. We construct a trace-flattable system of many counter machines\ncommunicating via rendez-vous that is bisimilar to a given flat FIFO machine,\nwhich allows to model-check the original flat FIFO machine. Our results lay the\ntheoretical foundations and open the way to build a verification tool for\n(general) FIFO machines based on analysis of flat sub-machines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 11:44:36 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 06:03:21 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 18:09:28 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 13:54:03 GMT"}, {"version": "v5", "created": "Mon, 12 Oct 2020 13:04:52 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Finkel", "Alain", ""], ["Praveen", "M.", ""]]}, {"id": "1908.08132", "submitter": "Jonathan Rawski", "authors": "Jonathan Rawski", "title": "Tensor Product Representations of Subregular Formal Languages", "comments": "to appear in Proceedings of IJCAI 2019 workshop on Neural-Symbolic\n  Learning and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a geometric characterization of subclasses of the regular\nlanguages. We use finite model theory to characterize objects like strings and\ntrees as relational structures. Logical statements meeting certain criteria\nover these models define subregular classes of languages. The semantics of such\nstatements can be compiled into tensor structures, using multilinear maps as\nfunction application for evaluation. This method is applied to consider two\nproperly subregular languages over different string models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 22:37:22 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Rawski", "Jonathan", ""]]}, {"id": "1908.08213", "submitter": "EPTCS", "authors": "Jorge A. P\\'erez (University of Groningen), Jurriaan Rot (UCL and\n  Radboud University)", "title": "Proceedings Combined 26th International Workshop on Expressiveness in\n  Concurrency and 16th Workshop on Structural Operational Semantics", "comments": null, "journal-ref": "EPTCS 300, 2019", "doi": "10.4204/EPTCS.300", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of EXPRESS/SOS 2019: the Combined 26th\nInternational Workshop on Expressiveness in Concurrency and the 16th Workshop\non Structural Operational Semantics, which was held on August 26, 2019, in\nAmsterdam (The Netherlands), as an affiliated workshop of CONCUR 2019, the 30th\nInternational Conference on Concurrency Theory.\n  The EXPRESS/SOS workshop series aims at bringing together researchers\ninterested in the formal semantics of systems and programming concepts, and in\nthe expressiveness of computational models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 06:06:57 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["P\u00e9rez", "Jorge A.", "", "University of Groningen"], ["Rot", "Jurriaan", "", "UCL and\n  Radboud University"]]}, {"id": "1908.08406", "submitter": "Marcos Cramer", "authors": "Marcos Cramer and Leendert van der Torre", "title": "SCF2 -- an Argumentation Semantics for Rational Human Judgments on\n  Argument Acceptability: Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In abstract argumentation theory, many argumentation semantics have been\nproposed for evaluating argumentation frameworks. This paper is based on the\nfollowing research question: Which semantics corresponds well to what humans\nconsider a rational judgment on the acceptability of arguments? There are two\nsystematic ways to approach this research question: A normative perspective is\nprovided by the principle-based approach, in which semantics are evaluated\nbased on their satisfaction of various normatively desirable principles. A\ndescriptive perspective is provided by the empirical approach, in which\ncognitive studies are conducted to determine which semantics best predicts\nhuman judgments about arguments. In this paper, we combine both approaches to\nmotivate a new argumentation semantics called SCF2. For this purpose, we\nintroduce and motivate two new principles and show that no semantics from the\nliterature satisfies both of them. We define SCF2 and prove that it satisfies\nboth new principles. Furthermore, we discuss findings of a recent empirical\ncognitive study that provide additional support to SCF2.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 14:27:28 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Cramer", "Marcos", ""], ["van der Torre", "Leendert", ""]]}, {"id": "1908.08633", "submitter": "EPTCS", "authors": "Kirstin Peters (TU Berlin/TU Darmstadt)", "title": "Comparing Process Calculi Using Encodings", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213", "journal-ref": "EPTCS 300, 2019, pp. 19-38", "doi": "10.4204/EPTCS.300.2", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encodings or the proof of their absence are the main way to compare process\ncalculi. To analyse the quality of encodings and to rule out trivial or\nmeaningless encodings, they are augmented with encodability criteria. There\nexists a bunch of different criteria and different variants of criteria in\norder to reason in different settings. This leads to incomparable results.\nMoreover, it is not always clear whether the criteria used to obtain a result\nin a particular setting do indeed fit to this setting. This paper provides a\nshort survey on often used encodability criteria, general frameworks that try\nto provide a unified notion of the quality of an encoding, and methods to\nanalyse and compare encodability criteria.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 01:54:49 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Peters", "Kirstin", "", "TU Berlin/TU Darmstadt"]]}, {"id": "1908.08634", "submitter": "EPTCS", "authors": "Frank Valencia (CNRS-LIX, Ecole Polytechnique de Paris and Univ.\n  Javeriana Cali.)", "title": "Semantic Structures for Spatially-Distributed Multi-Agent Systems", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213. This is an invited\n  contribution to EXPRESS/SOS 2019 based on my invited talk", "journal-ref": "EPTCS 300, 2019, pp. 39-53", "doi": "10.4204/EPTCS.300.3", "report-no": null, "categories": "cs.MA cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial constraint systems (scs) are semantic structures for reasoning about\nspatial and epistemic information in concurrent systems. They have been used to\nreason about beliefs, lies, and group epistemic behaviour inspired by social\nnetworks. They have also been used for proving new results about modal logics\nand giving semantics to process calculi. In this paper we will discuss the\ntheory and main results about scs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 01:55:26 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Valencia", "Frank", "", "CNRS-LIX, Ecole Polytechnique de Paris and Univ.\n  Javeriana Cali."]]}, {"id": "1908.08635", "submitter": "EPTCS", "authors": "Rob van Glabbeek (Data61, CSIRO)", "title": "On the Meaning of Transition System Specifications", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213", "journal-ref": "EPTCS 300, 2019, pp. 69-85", "doi": "10.4204/EPTCS.300.5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transition System Specifications provide programming and specification\nlanguages with a semantics. They provide the meaning of a closed term as a\nprocess graph: a state in a labelled transition system. At the same time they\nprovide the meaning of an n-ary operator, or more generally an open term with n\nfree variables, as an n-ary operation on process graphs. The classical way of\ndoing this, the closed-term semantics, reduces the meaning of an open term to\nthe meaning of its closed instantiations. It makes the meaning of an operator\ndependent on the context in which it is employed. Here I propose an alternative\nprocess graph semantics of TSSs that does not suffer from this drawback.\nSemantic equivalences on process graphs can be lifted to open terms conform\neither the closed-term or the process graph semantics. For pure TSSs the latter\nis more discriminating. I consider five sanity requirements on the semantics of\nprogramming and specification languages equipped with a recursion construct:\ncompositionality, applied to n-ary operators, recursion and variables,\ninvariance under $\\alpha$-conversion, and the recursive definition principle,\nsaying that the meaning of a recursive call should be a solution of the\ncorresponding recursion equations. I establish that the satisfaction of four of\nthese requirements under the closed-term semantics of a TSS implies their\nsatisfaction under the process graph semantics.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 01:56:37 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["van Glabbeek", "Rob", "", "Data61, CSIRO"]]}, {"id": "1908.08636", "submitter": "EPTCS", "authors": "Daniele Gorla (Dip. Informatica, Sapienza Univ. di Roma), Ivano Salvo\n  (Dip. Informatica, Sapienza Univ. di Roma), Adolfo Piperno (Dip. Informatica,\n  Sapienza Univ. di Roma)", "title": "Conflict vs Causality in Event Structures", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213", "journal-ref": "EPTCS 300, 2019, pp. 86-101", "doi": "10.4204/EPTCS.300.6", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event structures are one of the best known models for concurrency. Many\nvariants of the basic model and many possible notions of equivalence for them\nhave been devised in the literature. In this paper, we study how the spectrum\nof equivalences for Labelled Prime Event Structures built by Van Glabbeek and\nGoltz changes if we consider two simplified notions of event structures: the\nfirst is obtained by removing the causality relation (Coherence Spaces) and the\nsecond by removing the conflict relation (Elementary Event Structures). As\nexpected, in both cases the spectrum turns out to be simplified, since some\nnotions of equivalence coincide in the simplified settings; actually, we prove\nthat removing causality simplifies the spectrum considerably more than removing\nconflict. Furthermore, while the labeling of events and their cardinality play\nno role when removing causality, both the labeling function and the cardinality\nof the event set dramatically influence the spectrum of equivalences in the\nconflict-free setting.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 01:57:08 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Gorla", "Daniele", "", "Dip. Informatica, Sapienza Univ. di Roma"], ["Salvo", "Ivano", "", "Dip. Informatica, Sapienza Univ. di Roma"], ["Piperno", "Adolfo", "", "Dip. Informatica,\n  Sapienza Univ. di Roma"]]}, {"id": "1908.08639", "submitter": "EPTCS", "authors": "Giselle Reis (Carnegie Mellon University), Haniel Barbosa (The\n  University of Iowa)", "title": "Proceedings Sixth Workshop on Proof eXchange for Theorem Proving", "comments": null, "journal-ref": "EPTCS 301, 2019", "doi": "10.4204/EPTCS.301", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume of EPTCS contains the proceedings of the Sixth Workshop on Proof\nExchange for Theorem Proving (PxTP 2019), held on 26 August 2019 as part of the\nCADE-27 conference in Natal, Brazil. The PxTP workshop series brings together\nresearchers working on various aspects of communication, integration, and\ncooperation between reasoning systems and formalisms, with a special focus on\nproofs. The progress in computer-aided reasoning, both automated and\ninteractive, during the past decades, made it possible to build deduction tools\nthat are increasingly more applicable to a wider range of problems and are able\nto tackle larger problems progressively faster. In recent years, cooperation\nbetween such tools in larger systems has demonstrated the potential to reduce\nthe amount of manual intervention. Cooperation between reasoning systems relies\non availability of theoretical formalisms and practical tools to exchange\nproblems, proofs, and models. The PxTP workshop series strives to encourage\nsuch cooperation by inviting contributions on all aspects of cooperation\nbetween reasoning tools, whether automatic or interactive.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 02:10:45 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Reis", "Giselle", "", "Carnegie Mellon University"], ["Barbosa", "Haniel", "", "The\n  University of Iowa"]]}, {"id": "1908.09068", "submitter": "Alex Horn", "authors": "Alex Horn and Ali Kheradmand and Mukul R. Prasad", "title": "A Precise and Expressive Lattice-theoretical Framework for Efficient\n  Network Verification", "comments": "ICNP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network verification promises to detect errors, such as black holes and\nforwarding loops, by logically analyzing the control or data plane. To do so\nefficiently, the state-of-the-art (e.g., Veriflow) partitions packet headers\nwith identical forwarding behavior into the same packet equivalence class\n(PEC).\n  Recently, Yang and Lam showed how to construct the minimal set of PECs,\ncalled atomic predicates. Their construction uses Binary Decision Diagrams\n(BDDs). However, BDDs have been shown to incur significant overhead per packet\nheader bit, performing poorly when analyzing large-scale data centers. The\noverhead of atomic predicates prompted ddNF to devise a specialized data\nstructure of Ternary Bit Vectors (TBV) instead.\n  However, TBVs are strictly less expressive than BDDs. Moreover, unlike atomic\npredicates, ddNF's set of PECs is not minimal. We show that ddNF's\nnon-minimality is due to empty PECs. In addition, empty PECs are shown to\ntrigger wrong analysis results. This reveals an inherent tension between\nprecision, expressiveness and performance in formal network verification.\n  Our paper resolves this tension through a new lattice-theoretical\nPEC-construction algorithm, #PEC, that advances the field as follows: (i) #PEC\ncan encode more kinds of forwarding rules (e.g., ip-tables) than ddNF and\nVeriflow, (ii) #PEC verifies a wider class of errors (e.g., shadowed rules)\nthan ddNF, and (iii) on a broad range of real-world datasets, #PEC is 10X\nfaster than atomic predicates. By achieving precision, expressiveness and\nperformance, this paper answers a longstanding quest that has spanned three\ngenerations of formal network analysis techniques.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 01:31:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Horn", "Alex", ""], ["Kheradmand", "Ali", ""], ["Prasad", "Mukul R.", ""]]}, {"id": "1908.09123", "submitter": "Gabriel Scherer", "authors": "Pierre-\\'Evariste Dagand, Lionel Rieg, Gabriel Scherer", "title": "Dependent Pearl: Normalization by realizability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For those of us who generally live in the world of syntax, semantic proof\ntechniques such as reducibility, realizability or logical relations seem\nsomewhat magical despite -- or perhaps due to -- their seemingly unreasonable\neffectiveness. Why do they work? At which point in the proof is \"the real work\"\ndone?\n  Hoping to build a programming intuition of these proofs, we implement a\nnormalization argument for the simply-typed lambda-calculus with sums: instead\nof a proof, it is described as a program in a dependently-typed meta-language.\n  The semantic technique we set out to study is Krivine's classical\nrealizability, which amounts to a proof-relevant presentation of reducibility\narguments -- unary logical relations. Reducibility assigns a predicate to each\ntype, realizability assigns a set of realizers, which are abstract machines\nthat extend lambda-terms with a first-class notion of contexts. Normalization\nis a direct consequence of an adequacy theorem or \"fundamental lemma\", which\nstates that any well-typed term translates to a realizer of its type.\n  We show that the adequacy theorem, when written as a dependent program,\ncorresponds to an evaluation procedure. In particular, a weak normalization\nproof precisely computes a series of reduction from the input term to a normal\nform. Interestingly, the choices that we make when we define the reducibility\npredicates -- truth and falsity witnesses for each connective -- determine the\nevaluation order of the proof, with each datatype constructor behaving in a\nlazy or strict fashion.\n  While most of the ideas in this presentation are folklore among specialists,\nour dependently-typed functional program provides an accessible presentation to\na wider audience. In particular, our work provides a gentle introduction to\nabstract machine calculi which have recently been used as an effective research\nvehicle.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 10:53:27 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 12:56:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dagand", "Pierre-\u00c9variste", ""], ["Rieg", "Lionel", ""], ["Scherer", "Gabriel", ""]]}, {"id": "1908.09302", "submitter": "EPTCS", "authors": "Davide Ancona (DIBRIS, University of Genova, Italy), Gordon Pace\n  (Department of Computer Science, University of Malta)", "title": "Proceedings of the Second Workshop on Verification of Objects at RunTime\n  EXecution", "comments": null, "journal-ref": "EPTCS 302, 2019", "doi": "10.4204/EPTCS.302", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the post-proceedings of the second Workshop on\nVerification of Objects at RunTime EXecution (VORTEX 2018) that was held in\nAmsterdam, co-located with the European Conference on Object-Oriented\nProgramming (ECOOP 2018) and the ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis (ISSTA 2018).\n  Runtime verification is an approach to software verification which is\nconcerned with monitoring and analysis of software and hardware system\nexecutions. Recently, it has gained more consensus as an effective and\npromising approach to ensure software reliability, bridging a gap between\nformal verification, and conventional testing; monitoring a system during\nruntime execution offers additional opportunities for addressing error\nrecovery, self-adaptation, and other issues that go beyond software\nreliability. The goal of VORTEX is to bring together researchers working on\nruntime verification for topics covering either theoretical, or practical\naspects, or, preferably, both, with emphasis on object-oriented languages, and\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 11:23:42 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ancona", "Davide", "", "DIBRIS, University of Genova, Italy"], ["Pace", "Gordon", "", "Department of Computer Science, University of Malta"]]}, {"id": "1908.09477", "submitter": "EPTCS", "authors": "Eunice Palmeira (Federal Institute of Alagoas), Fred Freitas (Federal\n  University of Pernambuco), Jens Otten (University of Oslo)", "title": "Converting ALC Connection Proofs into ALC Sequents", "comments": "In Proceedings PxTP 2019, arXiv:1908.08639. Thanks to CAPES:\n  Coordination for the Improvement of Higher Level Personnel", "journal-ref": "EPTCS 301, 2019, pp. 3-17", "doi": "10.4204/EPTCS.301.3", "report-no": null, "categories": "cs.SC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection method has earned good reputation in the area of automated\ntheorem proving, due to its simplicity, efficiency and rational use of memory.\nThis method has been applied recently in automatic provers that reason over\nontologies written in the description logic ALC. However, proofs generated by\nconnection calculi are difficult to understand. Proof readability is largely\nlost by the transformations to disjunctive normal form applied over the\nformulae to be proven. Such a proof model, albeit efficient, prevents inference\nsystems based on it from effectively providing justifications and/or\ndescriptions of the steps used in inferences. To address this problem, in this\npaper we propose a method for converting matricial proofs generated by the ALC\nconnection method to ALC sequent proofs, which are much easier to understand,\nand whose translation to natural language is more straightforward. We also\ndescribe a calculus that accepts the input formula in a non-clausal ALC format,\nwhat simplifies the translation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:38:33 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Palmeira", "Eunice", "", "Federal Institute of Alagoas"], ["Freitas", "Fred", "", "Federal\n  University of Pernambuco"], ["Otten", "Jens", "", "University of Oslo"]]}, {"id": "1908.09478", "submitter": "EPTCS", "authors": "Burak Ekici (University of Innsbruck), Arjun Viswanathan (University\n  of Iowa), Yoni Zohar (Stanford University), Clark Barrett (Stanford\n  University), Cesare Tinelli (University of Iowa)", "title": "Verifying Bit-vector Invertibility Conditions in Coq (Extended Abstract)", "comments": "In Proceedings PxTP 2019, arXiv:1908.08639. Presented as an extended\n  abstract", "journal-ref": "EPTCS 301, 2019, pp. 18-26", "doi": "10.4204/EPTCS.301.4", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a part of an ongoing effort to prove the correctness of\ninvertibility conditions for the theory of fixed-width bit-vectors, which are\nused to solve quantified bit-vector formulas in the Satisfiability Modulo\nTheories (SMT) solver CVC4. While many of these were proved in a completely\nautomatic fashion for any bit-width, some were only proved for bit-widths up to\n65, even though they are being used to solve formulas over arbitrary\nbit-widths. In this paper we describe our initial efforts in proving a subset\nof these invertibility conditions in the Coq proof assistant. We describe the\nCoq library that we use, as well as the extensions that we introduced to it.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:38:56 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ekici", "Burak", "", "University of Innsbruck"], ["Viswanathan", "Arjun", "", "University\n  of Iowa"], ["Zohar", "Yoni", "", "Stanford University"], ["Barrett", "Clark", "", "Stanford\n  University"], ["Tinelli", "Cesare", "", "University of Iowa"]]}, {"id": "1908.09479", "submitter": "EPTCS", "authors": "Mohamed Yacine El Haddad (CNRS), Guillaume Burel (ENSIIE),\n  Fr\\'ed\\'eric Blanqui (Inria)", "title": "EKSTRAKTO A tool to reconstruct Dedukti proofs from TSTP files (extended\n  abstract)", "comments": "In Proceedings PxTP 2019, arXiv:1908.08639", "journal-ref": "EPTCS 301, 2019, pp. 27-35", "doi": "10.4204/EPTCS.301.5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof assistants often call automated theorem provers to prove subgoals.\nHowever, each prover has its own proof calculus and the proof traces that it\nproduces often lack many details to build a complete proof. Hence these traces\nare hard to check and reuse in proof assistants. Dedukti is a proof checker\nwhose proofs can be translated to various proof assistants: Coq, HOL, Lean,\nMatita, PVS. We implemented a tool that extracts TPTP subproblems from a TSTP\nfile and reconstructs complete proofs in Dedukti using automated provers able\nto generate Dedukti proofs like ZenonModulo or ArchSAT. This tool is generic:\nit assumes nothing about the proof calculus of the prover producing the trace,\nand it can use different provers to produce the Dedukti proof. We applied our\ntool on traces produced by automated theorem provers on the CNF problems of the\nTPTP library and we were able to reconstruct a proof for a large proportion of\nthem, significantly increasing the number of Dedukti proofs that could be\nobtained for those problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:39:13 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Haddad", "Mohamed Yacine El", "", "CNRS"], ["Burel", "Guillaume", "", "ENSIIE"], ["Blanqui", "Fr\u00e9d\u00e9ric", "", "Inria"]]}, {"id": "1908.09480", "submitter": "EPTCS", "authors": "Mathias Fleury (Max Planck Institut for Informatics), Hans-J\\\"org\n  Schurr (University of Lorraine, CNRS, Inria, and LORIA)", "title": "Reconstructing veriT Proofs in Isabelle/HOL", "comments": "In Proceedings PxTP 2019, arXiv:1908.08639", "journal-ref": "EPTCS 301, 2019, pp. 36-50", "doi": "10.4204/EPTCS.301.6", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated theorem provers are now commonly used within interactive theorem\nprovers to discharge an increasingly large number of proof obligations. To\nmaintain the trustworthiness of a proof, the automatically found proof must be\nverified inside the proof assistant. We present here a reconstruction procedure\nin the proof assistant Isabelle/HOL for proofs generated by the satisfiability\nmodulo theories solver veriT which is part of the smt tactic. We describe in\ndetail the architecture of our improved reconstruction method and the\nchallenges we faced in designing it. Our experiments show that the\nveriT-powered smt tactic is regularly suggested by Sledgehammer as the fastest\nmethod to automatically solve proof goals.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:39:28 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Fleury", "Mathias", "", "Max Planck Institut for Informatics"], ["Schurr", "Hans-J\u00f6rg", "", "University of Lorraine, CNRS, Inria, and LORIA"]]}, {"id": "1908.09481", "submitter": "EPTCS", "authors": "Fadil Kallat (Technical University of Dortmund, Germany), Tristan\n  Sch\\\"afer (Technical University of Dortmund, Germany), Anna Vasileva\n  (Technical University of Dortmund, Germany)", "title": "CLS-SMT: Bringing Together Combinatory Logic Synthesis and\n  Satisfiability Modulo Theories", "comments": "In Proceedings PxTP 2019, arXiv:1908.08639", "journal-ref": "EPTCS 301, 2019, pp. 51-65", "doi": "10.4204/EPTCS.301.7", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach that aims to combine the usage of satisfiability\nmodulo theories (SMT) solvers with the Combinatory Logic Synthesizer (CL)S\nframework. (CL)S is a tool for the automatic composition of software components\nfrom a user-specified repository. The framework yields a tree grammar that\ncontains all composed terms that comply with a target type. Type specifications\nfor (CL)S are based on combinatory logic with intersection types. Our approach\ntranslates the tree grammar into SMT functions, which allows the consideration\nof additional domain-specific constraints. We demonstrate the usefulness of our\napproach in several experiments.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:39:48 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kallat", "Fadil", "", "Technical University of Dortmund, Germany"], ["Sch\u00e4fer", "Tristan", "", "Technical University of Dortmund, Germany"], ["Vasileva", "Anna", "", "Technical University of Dortmund, Germany"]]}, {"id": "1908.09658", "submitter": "Rasmus Kr{\\ae}mmer Rendsvig", "authors": "Andr\\'es Occhipinti Liberman and Rasmus K. Rendsvig", "title": "Dynamic Term-Modal Logic for Epistemic Social Network Dynamics (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LO cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logics for social networks have been studied in recent literature. This paper\npresents a framework based on *dynamic term-modal logic* (DTML), a quantified\nvariant of dynamic epistemic logic (DEL). In contrast with DEL where it is\ncommonly known to whom agent names refer, DTML can represent dynamics with\nuncertainty about agent identity. We exemplify dynamics where such uncertainty\nand de re/de dicto distinctions are key to social network epistemics.\nTechnically, we show that DTML semantics can represent a popular class of\nhybrid logic epistemic social network models. We also show that DTML can encode\npreviously discussed dynamics for which finding a complete logic was left open.\nAs complete reduction axioms systems exist for DTML, this yields a complete\nsystem for the dynamics in question.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 13:08:49 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Liberman", "Andr\u00e9s Occhipinti", ""], ["Rendsvig", "Rasmus K.", ""]]}, {"id": "1908.09868", "submitter": "Razvan Diaconescu", "authors": "R\\u{a}zvan Diaconescu", "title": "Introducing H, an institution-based formal specification and\n  verification language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a short survey on the development of the formal specification and\nverification language H with emphasis on the scientific part. H is a modern\nhighly expressive language solidly based upon advanced mathematical theories\nsuch as the internalisation of Kripke semantics within institution theory.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 18:21:52 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Diaconescu", "R\u0103zvan", ""]]}, {"id": "1908.10203", "submitter": "Sebastian Krings", "authors": "Sebastian Krings, Joshua Schmidt, Patrick Skowronek, Jannik Dunkelau,\n  Dierk Ehmke", "title": "Towards Constraint Logic Programming over Strings for Test Data\n  Generation", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to properly test software, test data of a certain quality is needed.\nHowever, useful test data is often unavailable: Existing or hand-crafted data\nmight not be diverse enough to enable desired test cases. Furthermore, using\nproduction data might be prohibited due to security or privacy concerns or\nother regulations. At the same time, existing tools for test data generation\nare often limited. In this paper, we evaluate to what extent constraint logic\nprogramming can be used to generate test data, focussing on strings in\nparticular. To do so, we introduce a prototypical CLP solver over string\nconstraints. As case studies, we use it to generate IBAN numbers and calender\ndates.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 14:01:04 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Krings", "Sebastian", ""], ["Schmidt", "Joshua", ""], ["Skowronek", "Patrick", ""], ["Dunkelau", "Jannik", ""], ["Ehmke", "Dierk", ""]]}, {"id": "1908.10405", "submitter": "Marcelo Finger", "authors": "Marcelo Finger", "title": "Extending Description Logic EL++ with Linear Constraints on the\n  Probability of Axioms", "comments": "An earlier version of this work has appeared at Franz Baader's\n  festschrift. Here we detail the column generation method and present a\n  detailed example", "journal-ref": "In Lecture Notes in Computer Science 11560, pp. 286--300. Springer\n  (2019)", "doi": "10.1007/978-3-030-22102-7", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main reasons to employ a description logic such as EL or EL++ is\nthe fact that it has efficient, polynomial-time algorithmic properties such as\ndeciding consistency and inferring subsumption. However, simply by adding\nnegation of concepts to it, we obtain the expressivity of description logics\nwhose decision procedure is {ExpTime}-complete. Similar complexity explosion\noccurs if we add probability assignments on concepts. To lower the resulting\ncomplexity, we instead concentrate on assigning probabilities to Axioms (GCIs).\nWe show that the consistency detection problem for such a probabilistic\ndescription logic is NP-complete, and present a linear algebraic deterministic\nalgorithm to solve it, using the column generation technique. We also examine\nand provide algorithms for the probabilistic extension problem, which consists\nof inferring the minimum and maximum probabilities for a new axiom, given a\nconsistent probabilistic knowledge base.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:38:44 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Finger", "Marcelo", ""]]}, {"id": "1908.10416", "submitter": "Youkichi Hosoi", "authors": "Youkichi Hosoi, Naoki Kobayashi, Takeshi Tsukada", "title": "A Type-Based HFL Model Checking Algorithm", "comments": "A longer version of APLAS 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order modal fixpoint logic (HFL) is a higher-order extension of the\nmodal mu-calculus, and strictly more expressive than the modal mu-calculus. It\nhas recently been shown that various program verification problems can\nnaturally be reduced to HFL model checking: the problem of whether a given\nfinite state system satisfies a given HFL formula. In this paper, we propose a\nnovel algorithm for HFL model checking: it is the first practical algorithm in\nthat it runs fast for typical inputs, despite the hyper-exponential worst-case\ncomplexity of the HFL model checking problem. Our algorithm is based on\nKobayashi et al.'s type-based characterization of HFL model checking, and was\ninspired by a saturation-based algorithm for HORS model checking, another\nhigher-order extension of model checking. We prove the correctness of the\nalgorithm and report on an implementation and experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 19:14:27 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Hosoi", "Youkichi", ""], ["Kobayashi", "Naoki", ""], ["Tsukada", "Takeshi", ""]]}, {"id": "1908.11132", "submitter": "Marcos Cramer", "authors": "Marcos Cramer, Zohreh Baniasadi, Pieter Van Hertum", "title": "Technical report of \"The Knowledge Base Paradigm Applied to Delegation\n  Revocation\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ownership-based access control frameworks with the possibility of\ndelegating permissions and administrative rights, delegation chains will form.\nThere are different ways to treat delegation chains when revoking rights, which\ngive rise to different revocation schemes. In this paper, we investigate the\nproblem of delegation revocation from the perspective of the knowledge base\nparadigm. A knowledge base is a formal specification of domain knowledge in a\nrich formal language. Multiple forms of inference can be applied to this formal\nspecification in order to solve various problems and tasks that arise in the\ndomain. In other words, the paradigm proposes a strict separation of concerns\nbetween information and problem solving. The knowledge base that we use in this\npaper specifies the effects of the various revocation schemes. By applying\ndifferent inferences to this knowledge base, we can solve the following tasks:\nto determine the state of the system after a certain delegation or revocation;\nto interactively simulate the progression of the system state through time; to\ndetermine whether a user has a certain permission or administrative right given\na certain state of the system; to verify invariants of the system; and to\ndetermine which revocation schemes give rise to a certain specified set of\ndesired outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 10:00:35 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Cramer", "Marcos", ""], ["Baniasadi", "Zohreh", ""], ["Van Hertum", "Pieter", ""]]}, {"id": "1908.11137", "submitter": "Christoph Wernhard", "authors": "Christoph Wernhard", "title": "PIE -- Proving, Interpolating and Eliminating on the Basis of\n  First-Order Logic", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PIE is a Prolog-embedded environment for automated reasoning on the basis of\nfirst-order logic. It includes a versatile formula macro system and supports\nthe creation of documents that intersperse macro definitions, reasoner\ninvocations and LaTeX-formatted natural language text. Invocation of various\nreasoners is supported: External provers as well as sub-systems of PIE, which\ninclude preprocessors, a Prolog-based first-order prover, methods for Craig\ninterpolation and methods for second-order quantifier elimination.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 10:17:35 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Wernhard", "Christoph", ""]]}, {"id": "1908.11169", "submitter": "EPTCS", "authors": "Tom Hirschowitz (Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS,\n  LAMA, 73000 Chamb\\'ery, France)", "title": "Cellular Monads from Positive GSOS Specifications", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213", "journal-ref": "EPTCS 300, 2019, pp. 1-18", "doi": "10.4204/EPTCS.300.1", "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a leisurely introduction to our abstract framework for operational\nsemantics based on cellular monads on transition categories. Furthermore, we\nrelate it for the first time to an existing format, by showing that all\nPositive GSOS specifications generate cellular monads whose free algebras are\nall compositional. As a consequence, we recover the known result that\nbisimilarity is a congruence in the generated labelled transition system.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 12:11:23 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Hirschowitz", "Tom", "", "Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS,\n  LAMA, 73000 Chamb\u00e9ry, France"]]}, {"id": "1908.11289", "submitter": "Claudia Faggian", "authors": "Beniamino Accattoli, Claudia Faggian, and Giulio Guerrieri", "title": "Factorization and Normalization, Essentially", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lambda-calculi come with no fixed evaluation strategy. Different strategies\nmay then be considered, and it is important that they satisfy some abstract\nrewriting property, such as factorization or normalization theorems.\n  In this paper we provide simple proof techniques for these theorems. Our\nstarting point is a revisitation of Takahashi's technique to prove\nfactorization for head reduction. Our technique is both simpler and more\npowerful, as it works in cases where Takahishi's does not. We then pair\nfactorization with two other abstract properties, defining \\emph{essential\nsystems}, and show that normalization follows. Concretely, we apply the\ntechnique to four case studies, two classic ones, head and the\nleftmost-outermost reductions, and two less classic ones, non-deterministic\nweak call-by-value and least-level reductions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:20:14 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 18:40:48 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Faggian", "Claudia", ""], ["Guerrieri", "Giulio", ""]]}, {"id": "1908.11341", "submitter": "Sergei O. Kuznetsov", "authors": "Sergei O. Kuznetsov", "title": "Ordered Sets for Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book dwells on mathematical and algorithmic issues of data analysis\nbased on generality order of descriptions and respective precision. To speak of\nthese topics correctly, we have to go some way getting acquainted with the\nimportant notions of relation and order theory. On the one hand, data often\nhave a complex structure with natural order on it. On the other hand, many\nsymbolic methods of data analysis and machine learning allow to compare the\nobtained classifiers w.r.t. their generality, which is also an order relation.\nEfficient algorithms are very important in data analysis, especially when one\ndeals with big data, so scalability is a real issue. That is why we analyze the\ncomputational complexity of algorithms and problems of data analysis. We start\nfrom the basic definitions and facts of algorithmic complexity theory and\nanalyze the complexity of various tools of data analysis we consider. The tools\nand methods of data analysis, like computing taxonomies, groups of similar\nobjects (concepts and n-clusters), dependencies in data, classification, etc.,\nare illustrated with applications in particular subject domains, from\nchemoinformatics to text mining and natural language processing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:01:13 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Kuznetsov", "Sergei O.", ""]]}, {"id": "1908.11342", "submitter": "Alex Shkotin", "authors": "Alex Shkotin", "title": "Quantifiers metamorphoses. Generalizations, variations, algorithmic\n  semantics", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article contains ideas and their elaboration for quantifiers, which\nappeared after checking in practice the experimental language of the formal\nknowledge representation YAFOLL [1]: - looking at for_all and exists\nquantifiers as operators clarifying two trivial properties of a function: the\nconstancy of result value and presence of a value in the result; -It turned out\nthat the quantifier term can be written in the lambda calculus technique, i.e.\nas definition; -quantifier of quantity # is introduced into the language, as\nneeded in practice and does not cause logical and algorithmic problems on\nfinite structures; - the quantifier of the sum is mentioned because it is a\nquantifier of the language; -algorithmic semantics is written for for_all and\nexists quantifiers as an introduction to the topic.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 10:24:16 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Shkotin", "Alex", ""]]}, {"id": "1908.11343", "submitter": "Michael Schaper", "authors": "Martin Avanzini, Michael Schaper and Georg Moser", "title": "Modular Runtime Complexity Analysis of Probabilistic While Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the average case runtime complexity analysis of a\nprototypical imperative language endowed with primitives for sampling and\nprobabilistic choice. Taking inspiration from known approaches from to the\nmodular resource analysis of non-probabilistic programs, we investigate how a\nmodular runtime analysis is obtained for probabilistic programs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:38:36 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Avanzini", "Martin", ""], ["Schaper", "Michael", ""], ["Moser", "Georg", ""]]}, {"id": "1908.11345", "submitter": "Radu Iosif", "authors": "Marius Bozga and Radu Iosif and Joseph Sifakis", "title": "Local Reasoning about Parametric and Reconfigurable Component-based\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a logical framework for the specification and verification of\ncomponent-based systems, in which finitely many component instances are active,\nbut the bound on their number is not known. Besides specifying and verifying\nparametric systems, we consider the aspect of dynamic reconfiguration, in which\ncomponents can migrate at runtime on a physical map, whose shape and size may\nchange. We describe such parametric and reconfigurable architectures using\nresource logics, close in spirit to Separation Logic, used to reason about\ndynamic pointer structures. These logics support the principle of local\nreasoning, which is the key for writing modular specifications and building\nscalable verification algorithms, that deal with large industrial-size systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 13:06:09 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Bozga", "Marius", ""], ["Iosif", "Radu", ""], ["Sifakis", "Joseph", ""]]}, {"id": "1908.11353", "submitter": "Paolo Pistone", "authors": "Paolo Pistone, Luca Tranchini, Mattia Petrolo", "title": "The naturality of natural deduction (II). Some remarks on atomic\n  polymorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper (of which this is a prosecution) we investigated the\nextraction of proof-theoretic properties of natural deduction derivations from\ntheir impredicative translation into System F. Our key idea was to introduce an\nextended equational theory for System F codifying at a syntactic level some\nproperties found in parametric models. In a recent series of papers a different\napproach to extract proof-theoretic properties of natural deduction derivations\nwas proposed by defining predicative variants of the usual translation,\nembedding intuitionistic propositional logic into the atomic fragment of System\nF. In this paper we show that this approach finds a general explanation within\nour equational study of second-order natural deduction, and a clear semantic\njustification provided by parametricity.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 17:03:43 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 16:14:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Pistone", "Paolo", ""], ["Tranchini", "Luca", ""], ["Petrolo", "Mattia", ""]]}, {"id": "1908.11360", "submitter": "Tim Lyon", "authors": "Tim Lyon and Kees van Berkel", "title": "Automating Agential Reasoning: Proof-Calculi and Syntactic Decidability\n  for STIT Logics", "comments": "Included version of the paper \"Automating Agential Reasoning:\n  Proof-Calculi and Syntactic Decidability for STIT Logics\", accepted to the\n  22nd International Conference on Principles and Practice of Multi-Agent\n  Systems (PRIMA 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-33792-6_13", "report-no": null, "categories": "cs.LO cs.AI cs.MA math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides proof-search algorithms and automated counter-model\nextraction for a class of STIT logics. With this, we answer an open problem\nconcerning syntactic decision procedures and cut-free calculi for STIT logics.\nA new class of cut-free complete labelled sequent calculi G3LdmL^m_n, for\nmulti-agent STIT with at most n-many choices, is introduced. We refine the\ncalculi G3LdmL^m_n through the use of propagation rules and demonstrate the\nadmissibility of their structural rules, resulting in auxiliary calculi\nLdm^m_nL. In the single-agent case, we show that the refined calculi Ldm^m_nL\nderive theorems within a restricted class of (forestlike) sequents, allowing us\nto provide proof-search algorithms that decide single-agent STIT logics. We\nprove that the proof-search algorithms are correct and terminate.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 17:33:37 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 13:48:33 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 17:16:02 GMT"}, {"version": "v4", "created": "Sat, 15 Feb 2020 08:29:26 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lyon", "Tim", ""], ["van Berkel", "Kees", ""]]}, {"id": "1908.11642", "submitter": "Johannes Doleschal", "authors": "Johannes Doleschal and Benny Kimelfeld and Wim Martens and Liat\n  Peterfreund", "title": "Weight Annotation in Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of document spanners abstracts the task of information\nextraction from text as a function that maps every document (a string) into a\nrelation over the document's spans (intervals identified by their start and end\nindices). For instance, the regular spanners are the closure under the\nRelational Algebra (RA) of the regular expressions with capture variables, and\nthe expressive power of the regular spanners is precisely captured by the class\nof VSet-automata - a restricted class of transducers that mark the endpoints of\nselected spans.\n  In this work, we embark on the investigation of document spanners that can\nannotate extractions with auxiliary information such as confidence, support,\nand confidentiality measures. To this end, we adopt the abstraction of\nprovenance semirings by Green et al., where tuples of a relation are annotated\nwith the elements of a commutative semiring, and where the annotation\npropagates through the (positive) RA operators via the semiring operators.\nHence, the proposed spanner extension, referred to as an annotator, maps every\nstring into an annotated relation over the spans. As a specific instantiation,\nwe explore weighted VSet-automata that, similarly to weighted automata and\ntransducers, attach semiring elements to transitions. We investigate key\naspects of expressiveness, such as the closure under the positive RA, and key\naspects of computational complexity, such as the enumeration of annotated\nanswers and their ranked enumeration in the case of numeric semirings. For a\nnumber of these problems, fundamental properties of the underlying semiring,\nsuch as positivity, are crucial for establishing tractability.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:39:21 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 13:46:59 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 12:11:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Doleschal", "Johannes", ""], ["Kimelfeld", "Benny", ""], ["Martens", "Wim", ""], ["Peterfreund", "Liat", ""]]}, {"id": "1908.11769", "submitter": "\\'Oscar Mart\\'in", "authors": "\\'Oscar Mart\\'in, Alberto Verdejo, Narciso Mart\\'i-Oliet", "title": "Compositional specification in rewriting logic", "comments": "Changes in this revision: Removed last sentence from abstract (on the\n  paper being considered for publication). Added note before abstract (on the\n  paper being published)", "journal-ref": "Theory and Practice of Logic Programming, 20(1), 44-98, 2020", "doi": "10.1017/S1471068419000425", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rewriting logic is naturally concurrent: several subterms of the state term\ncan be rewritten simultaneously. But state terms are global, which makes\ncompositionality difficult to achieve. Compositionality here means being able\nto decompose a complex system into its functional components and code each as\nan isolated and encapsulated system. Our goal is to help bringing\ncompositionality to system specification in rewriting logic. The base of our\nproposal is the operation that we call synchronous composition. We discuss the\nmotivations and implications of our proposal, formalize it for rewriting logic\nand also for transition structures, to be used as semantics, and show the power\nof our approach with some examples. This paper is under consideration in Theory\nand Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 14:49:36 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 16:45:42 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 12:17:57 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Mart\u00edn", "\u00d3scar", ""], ["Verdejo", "Alberto", ""], ["Mart\u00ed-Oliet", "Narciso", ""]]}]