[{"id": "1907.00087", "submitter": "Yoni Zohar", "authors": "Alex Ozdemir, Aina Niemetz, Mathias Preiner, Yoni Zohar, and Clark\n  Barrett", "title": "DRAT-based Bit-Vector Proofs in CVC4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art Satisfiability Modulo Theories (SMT) solvers for the\ntheory of fixed-size bit-vectors employ an approach called bit-blasting, where\na given formula is translated into a Boolean satisfiability (SAT) problem and\ndelegated to a SAT solver. Consequently, producing bit-vector proofs in an SMT\nsolver requires incorporating SAT proofs into its proof infrastructure. In this\npaper, we describe three approaches for integrating DRAT proofs generated by an\noff-the-shelf SAT solver into the proof infrastructure of the SMT solver CVC4\nand explore their strengths and weaknesses. We implemented all three approaches\nusing cryptominisat as the SAT back-end for its bit-blasting engine and\nevaluated performance in terms of proof-production and proof-checking.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 21:29:07 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 19:06:34 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ozdemir", "Alex", ""], ["Niemetz", "Aina", ""], ["Preiner", "Mathias", ""], ["Zohar", "Yoni", ""], ["Barrett", "Clark", ""]]}, {"id": "1907.00202", "submitter": "Robert Egrot", "authors": "Rob Egrot", "title": "Recursive axiomatisations from separation properties", "comments": "Version 3 includes extended exposition and some rewriting for clarity", "journal-ref": null, "doi": "10.1017/jsl.2021.19", "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a fragment of monadic infinitary second-order logic corresponding\nto an abstract separation property. We use this to define the concept of a\nseparation subclass. We use model theoretic techniques and games to show that\nseparation subclasses whose axiomatisations are recursively enumerable in our\nsecond-order fragment can also be recursively axiomatised in their original\nfirst-order language. We pin down the expressive power of this formalism with\nrespect to first-order logic, and investigate some questions relating to\ndecidability and computational complexity. As applications of these results, by\nshowing that certain classes can be straightforwardly defined as separation\nsubclasses, we obtain first-order axiomatisability results for these classes.\nIn particular we apply this technique to graph colourings and a class of\npartial algebras arising from separation logic.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 13:18:39 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 10:35:51 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 13:59:08 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Egrot", "Rob", ""]]}, {"id": "1907.00239", "submitter": "Dmitriy Zhuk", "authors": "Dmitriy Zhuk and Barnaby Martin", "title": "QCSP monsters and the demise of the Chen Conjecture", "comments": "with minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a surprising classification for the computational complexity of the\nQuantified Constraint Satisfaction Problem over a constraint language $\\Gamma$,\nQCSP$(\\Gamma)$, where $\\Gamma$ is a finite language over $3$ elements which\ncontains all constants. In particular, such problems are either in P,\nNP-complete, co-NP-complete or PSpace-complete. Our classification refutes the\nhitherto widely-believed Chen Conjecture.\n  Additionally, we show that already on a 4-element domain there exists a\nconstraint language $\\Gamma$ such that QCSP$(\\Gamma)$ is DP-complete (from\nBoolean Hierarchy), and on a 10-element domain there exists a constraint\nlanguage giving the complexity class $\\Theta_{2}^{P}$.\n  Meanwhile, we prove the Chen Conjecture for finite conservative languages\n$\\Gamma$. If the polymorphism clone of $\\Gamma$ has the polynomially generated\npowers (PGP) property then QCSP$(\\Gamma)$ is in NP. Otherwise, the polymorphism\nclone of $\\Gamma$ has the exponentially generated powers (EGP) property and\nQCSP$(\\Gamma)$ is PSpace-complete.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 17:13:13 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 20:41:38 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 07:35:11 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhuk", "Dmitriy", ""], ["Martin", "Barnaby", ""]]}, {"id": "1907.00298", "submitter": "Adithya Murali", "authors": "Umang Mathur, Adithya Murali, Paul Krogmeier, P. Madhusudan, Mahesh\n  Viswanathan", "title": "Deciding Memory Safety for Single-Pass Heap-Manipulating Programs", "comments": "StreamVerif tool for automata-based verification of uninterpreted\n  programs can be found at https://github.com/umangm/streamverif", "journal-ref": "Proceedings of the ACM on Programming Languages Vol. 4, Issue\n  POPL, Article 35 (December 2019)", "doi": "10.1145/3371103", "report-no": null, "categories": "cs.PL cs.FL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the decidability of automatic program verification for\nprograms that manipulate heaps, and in particular, decision procedures for\nproving memory safety for them. We extend recent work that identified a\ndecidable subclass of uninterpreted programs to a class of alias-aware programs\nthat can update maps. We apply this theory to develop verification algorithms\nfor memory safety--- determining if a heap-manipulating program that allocates\nand frees memory locations and manipulates heap pointers does not dereference\nan unallocated memory location. We show that this problem is decidable when the\ninitial allocated heap forms a forest data-structure and when programs are\nstreaming-coherent, which intuitively restricts programs to make a single pass\nover a data-structure. Our experimental evaluation on a set of library routines\nthat manipulate forest data-structures shows that common single-pass algorithms\non data-structures often fall in the decidable class, and that our decision\nprocedure is efficient in verifying them.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 23:53:22 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 12:34:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mathur", "Umang", ""], ["Murali", "Adithya", ""], ["Krogmeier", "Paul", ""], ["Madhusudan", "P.", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1907.00359", "submitter": "Krishna Balajirao Manoorkar", "authors": "Willem Conradie, Sabine Frittella, Krishna Manoorkar, Sajad Nazari,\n  Alessandra Palmigiano, Apostolos Tzimoulis, Nachoem M. Wijnberg", "title": "Rough concepts", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2020.05.074", "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper proposes a novel way to unify Rough Set Theory and Formal\nConcept Analysis. Our method stems from results and insights developed in the\nalgebraic theory of modal logic, and is based on the idea that Pawlak's\noriginal approximation spaces can be seen as special instances of enriched\nformal contexts, i.e. relational structures based on formal contexts from\nFormal Concept Analysis.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 10:35:20 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 10:28:48 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Conradie", "Willem", ""], ["Frittella", "Sabine", ""], ["Manoorkar", "Krishna", ""], ["Nazari", "Sajad", ""], ["Palmigiano", "Alessandra", ""], ["Tzimoulis", "Apostolos", ""], ["Wijnberg", "Nachoem M.", ""]]}, {"id": "1907.00467", "submitter": "L\\^e Th\\`anh D\\~ung Nguy\\^en", "authors": "L\\^e Th\\`anh D\\~ung Nguy\\^en", "title": "Typed lambda-calculi and superclasses of regular functions", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to use Church encodings in typed lambda-calculi as the basis for\nan automata-theoretic counterpart of implicit computational complexity, in the\nsame way that monadic second-order logic provides a counterpart to descriptive\ncomplexity. Specifically, we look at transductions i.e. string-to-string (or\ntree-to-tree) functions - in particular those with superlinear growth, such as\npolyregular functions, HDT0L transductions and S\\'enizergues's \"k-computable\nmappings\".\n  Our first results towards this aim consist showing the inclusion of some\ntransduction classes in some classes defined by lambda-calculi. In particular,\nthis sheds light on a basic open question on the expressivity of the simply\ntyped lambda-calculus. We also encode regular functions (and, by changing the\ntype of programs considered, we get a larger subclass of polyregular functions)\nin the elementary affine lambda-calculus, a variant of linear logic originally\ndesigned for implicit computational complexity.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 21:24:59 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Nguy\u00ean", "L\u00ea Th\u00e0nh D\u0169ng", ""]]}, {"id": "1907.00537", "submitter": "\\'Etienne Andr\\'e", "authors": "\\'Etienne Andr\\'e and Jun Sun", "title": "Parametric Timed Model Checking for Guaranteeing Timed Opacity", "comments": "This is the author (and extended) version of the manuscript of the\n  same name published in the proceedings of ATVA 2019. This work is partially\n  supported by the ANR national research program PACS (ANR-14-CE28-0002), the\n  ANR-NRF research program (ProMiS) and by ERATO HASUO Metamathematics for\n  Systems Design Project (No. JPMJER1603), JST", "journal-ref": "Proceedings of the 17th International Symposium on Automated\n  Technology for Verification and Analysis (ATVA 2019), Springer LNCS 11781,\n  pages 115-130, 2019", "doi": "10.1007/978-3-030-31784-3_7", "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information leakage can have dramatic consequences on systems security. Among\nharmful information leaks, the timing information leakage is the ability for an\nattacker to deduce internal information depending on the system execution time.\nWe address the following problem: given a timed system, synthesize the\nexecution times for which one cannot deduce whether the system performed some\nsecret behavior. We solve this problem in the setting of timed automata (TAs).\nWe first provide a general solution, and then extend the problem to parametric\nTAs, by synthesizing internal timings making the TA secure. We study\ndecidability, devise algorithms, and show that our method can also apply to\nprogram analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 04:27:31 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 06:46:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Andr\u00e9", "\u00c9tienne", ""], ["Sun", "Jun", ""]]}, {"id": "1907.00540", "submitter": "Mohammad Rahman", "authors": "Mohammad Ashiqur Rahman, Md Hasan Shahriar, Ehab Al-Shaer, Quanyan Zhu", "title": "A Formal Approach for Efficient Navigation Management of Hybrid Electric\n  Vehicles on Long Trips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.ET cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Plug-in Hybrid Electric Vehicles (PHEVs) are gaining popularity due to their\neconomic efficiency as well as their contribution to green management. PHEVs\nallow the driver to use electric power exclusively for driving and then switch\nto gasoline as needed. The more gasoline a vehicle uses, the higher cost is\nrequired for the trip. However, a PHEV cannot last for a long period on stored\nelectricity without being recharged. Thus, it needs frequent recharging\ncompared to traditional gasoline-powered vehicles. Moreover, the battery\nrecharging time is usually long, which leads to longer delays on a trip.\nTherefore, it is necessary to provide a flexible navigation management scheme\nalong with an efficient recharging schedule, which allows the driver to choose\nan optimal route based on the fuel-cost and time-to-destination constraints. In\nthis paper, we present a formal model to solve this PHEV navigation management\nproblem. The model is solved to provide a driver with a comprehensive routing\nplan including the potential recharging and refueling points that satisfy the\ngiven requirements, particularly the maximum fuel cost and the maximum trip\ntime. In addition, we propose a price-based navigation control technique to\nachieve better load balance for the traffic system. Evaluation results show\nthat the proposed formal models can be solved efficiently even with large road\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 04:37:06 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Rahman", "Mohammad Ashiqur", ""], ["Shahriar", "Md Hasan", ""], ["Al-Shaer", "Ehab", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1907.00555", "submitter": "\\'Etienne Andr\\'e", "authors": "\\'Etienne Andr\\'e, Micha{\\l} Knapik, Didier Lime, Wojciech Penczek and\n  Laure Petrucci", "title": "Parametric Verification: An Introduction", "comments": "This is the author version of the manuscript of the same name\n  published in the Transactions on Petri Nets and Other Models of Concurrency\n  (ToPNoC). This work is partially supported by the ANR national research\n  program PACS (ANR-14-CE28-0002)", "journal-ref": "Transactions on Petri Nets and Other Models of Concurrency, volume\n  14, pages 64-100, November 2019", "doi": "10.1007/978-3-662-60651-3_3", "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper constitutes a short introduction to parametric verification of\nconcurrent systems. It originates from two 1-day tutorial sessions held at the\nPetri nets conferences in Toru\\'n (2016) and Zaragoza (2017). The paper\npresents not only the basic formal concepts tackled in the video version, but\nalso an extensive literature to provide the reader with further references\ncovering the area.\n  We first introduce motivation behind parametric verification in general, and\nthen focus on different models and approaches, for verifying several kinds of\nsystems. They include Parametric Timed Automata, for modelling real-time\nsystems, where the timing constraints are not necessarily known a priori.\nSimilarly, Parametric Interval Markov Chains allow for modelling systems where\nprobabilities of events occurrences are intervals with parametric bounds.\nParametric Petri Nets allow for compact representation of systems, and cope\nwith different types of parameters. Finally, Action Synthesis aims at enabling\nor disabling actions in a concurrent system to guarantee some of its\nproperties. Some tools implementing these approaches were used during hands-on\nsessions at the tutorial. The corresponding practicals are freely available on\nthe Web.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 05:31:52 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Andr\u00e9", "\u00c9tienne", ""], ["Knapik", "Micha\u0142", ""], ["Lime", "Didier", ""], ["Penczek", "Wojciech", ""], ["Petrucci", "Laure", ""]]}, {"id": "1907.00658", "submitter": "Saeed Salehi", "authors": "Saeed Salehi", "title": "On the Notions of Rudimentarity, Primitive Recursivity and\n  Representability of Functions and Relations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is quite well-known from Kurt Godel's (1931) ground-breaking result on the\nIncompleteness Theorem that rudimentary relations (i.e., those definable by\nbounded formulae) are primitive recursive, and that primitive recursive\nfunctions are representable in sufficiently strong arithmetical theories. It is\nalso known, though perhaps not as well-known as the former one, that some\nprimitive recursive relations are not rudimentary. We present a simple and\nelementary proof of this fact in the first part of the paper. In the second\npart, we review some possible notions of representability of functions studied\nin the literature, and give a new proof of the equivalence of the weak\nrepresentability with the (strong) representability of functions in\nsufficiently strong arithmetical theories. Our results shed some new light on\nthe notions of rudimentary, primitive recursive, and representable functions\nand relations, and clarify, hopefully, some misunderstandings and confusing\nerrors in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 11:13:10 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 12:39:43 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 09:15:08 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Salehi", "Saeed", ""]]}, {"id": "1907.00713", "submitter": "Robert Sison", "authors": "Robert Sison (Data61, CSIRO and UNSW Sydney), Toby Murray (University\n  of Melbourne)", "title": "Verifying that a compiler preserves concurrent value-dependent\n  information-flow security", "comments": "To appear in the 10th International Conference on Interactive Theorem\n  Proving (ITP 2019). Extended version with appendix. For supplement material,\n  see https://covern.org/itp19.html", "journal-ref": null, "doi": "10.4230/LIPIcs.ITP.2019.27", "report-no": null, "categories": "cs.LO cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is common to prove by reasoning over source code that programs do not leak\nsensitive data. But doing so leaves a gap between reasoning and reality that\ncan only be filled by accounting for the behaviour of the compiler. This task\nis complicated when programs enforce value-dependent information-flow security\nproperties (in which classification of locations can vary depending on values\nin other locations) and complicated further when programs exploit\nshared-variable concurrency.\n  Prior work has formally defined a notion of concurrency-aware refinement for\npreserving value-dependent security properties. However, that notion is\nconsiderably more complex than standard refinement definitions typically\napplied in the verification of semantics preservation by compilers. To date it\nremains unclear whether it can be applied to a realistic compiler, because\nthere exist no general decomposition principles for separating it into smaller,\nmore familiar, proof obligations.\n  In this work, we provide such a decomposition principle, which we show can\nalmost halve the complexity of proving secure refinement. Further, we\ndemonstrate its applicability to secure compilation, by proving in Isabelle/HOL\nthe preservation of value-dependent security by a proof-of-concept compiler\nfrom an imperative While language to a generic RISC-style assembly language,\nfor programs with shared-memory concurrency mediated by locking primitives.\nFinally, we execute our compiler in Isabelle on a While language model of the\nCross Domain Desktop Compositor, demonstrating to our knowledge the first use\nof a compiler verification result to carry an information-flow security\nproperty down to the assembly-level model of a non-trivial concurrent program.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:34:15 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sison", "Robert", "", "Data61, CSIRO and UNSW Sydney"], ["Murray", "Toby", "", "University\n  of Melbourne"]]}, {"id": "1907.00829", "submitter": "Jesko Hecking-Harbusch", "authors": "Raven Beutner, Bernd Finkbeiner, Jesko Hecking-Harbusch", "title": "Translating Asynchronous Games for Distributed Synthesis (Full Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed synthesis, we generate a set of process implementations that,\ntogether, accomplish an objective against all possible behaviors of the\nenvironment. A lot of recent work has focussed on systems with causal memory,\ni.e., sets of asynchronous processes that exchange their causal histories upon\nsynchronization. Decidability results for this problem have been stated either\nin terms of control games, which extend Zielonka's asynchronous automata by\npartitioning the actions into controllable and uncontrollable, or in terms of\nPetri games, which extend Petri nets by partitioning the tokens into system and\nenvironment players. The precise connection between these two models was so\nfar, however, an open question. In this paper, we provide the first formal\nconnection between control games and Petri games. We establish the equivalence\nof the two game models based on weak bisimulations between their strategies.\nFor both directions, we show that a game of one type can be translated into an\nequivalent game of the other type. We provide exponential upper and lower\nbounds for the translations. Our translations make it possible to transfer and\ncombine decidability results between the two types of games. Exemplarily, we\ntranslate decidability in acyclic communication architectures, originally\nobtained for control games, to Petri games, and decidability in single-process\nsystems, originally obtained for Petri games, to control games.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:42:47 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 10:57:17 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Beutner", "Raven", ""], ["Finkbeiner", "Bernd", ""], ["Hecking-Harbusch", "Jesko", ""]]}, {"id": "1907.00929", "submitter": "Marijn Heule", "authors": "Marijn J.H. Heule", "title": "Trimming Graphs Using Clausal Proof Optimization", "comments": "arXiv admin note: text overlap with arXiv:1805.12181", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to gradually compute a smaller and smaller unsatisfiable\ncore of a propositional formula by minimizing proofs of unsatisfiability. The\ngoal is to compute a minimal unsatisfiable core that is relatively small\ncompared to other minimal unsatisfiable cores of the same formula. We try to\nachieve this goal by postponing deletion of arbitrary clauses from the formula\nas long as possible---in contrast to existing minimal unsatisfiable core\nalgorithms. We applied this method to reduce the smallest known unit-distance\ngraph with chromatic number 5 from 553 vertices and 2720 edges to 529 vertices\nand 2670 edges.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:06:14 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 13:59:43 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Heule", "Marijn J. H.", ""]]}, {"id": "1907.01076", "submitter": "Florian Zuleger", "authors": "Florian Zuleger", "title": "The Polynomial Complexity of Vector Addition Systems with States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector addition systems are an important model in theoretical computer\nscience and have been used in a variety of areas. In this paper, we consider\nvector addition systems with states over a parameterized initial configuration.\nFor these systems, we are interested in the standard notion of computational\ncomplexity, i.e., we want to understand the length of the longest trace for a\nfixed vector addition system with states depending on the size of the initial\nconfiguration. We show that the asymptotic complexity of a given vector\naddition system with states is either $\\Theta(N^k)$ for some computable integer\n$k$, where $N$ is the size of the initial configuration, or at least\nexponential. We further show that $k$ can be computed in polynomial time in the\nsize of the considered vector addition system. Finally, we show that $1 \\le k\n\\le 2^n$, where $n$ is the dimension of the considered vector addition system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 21:09:19 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 15:00:00 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 19:34:26 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zuleger", "Florian", ""]]}, {"id": "1907.01192", "submitter": "Jingchao Chen", "authors": "Jingchao Chen", "title": "Core First Unit Propagation", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unit propagation (which is called also Boolean Constraint Propagation) has\nbeen an important component of every modern CDCL SAT solver since the CDCL\nsolver was developed. In general, unit propagation is implemented by scanning\nsequentially every clause over a linear watch-list. This paper presents a new\nunit propagation technique called core first unit propagation. The main idea is\nto prefer core clauses over non-core ones during unit propagation, trying to\ngenerate a shorter learnt clause. Here, the core clause is defined as one with\nliteral block distance less than or equal to 7. Empirical results show that\ncore first unit propagation improves the performance of the winner of the SAT\nCompetition 2018, MapleLCMDistChronoBT.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 06:46:13 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Chen", "Jingchao", ""]]}, {"id": "1907.01214", "submitter": "Paul Gastin", "authors": "Paul Gastin and Amaldev Manuel and R. Govind", "title": "Logics for Reversible Regular Languages and Semigroups with Involution", "comments": "Accepted for DLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MSO and FO logics with predicates `between' and `neighbour' that\ncharacterise various fragments of the class of regular languages that are\nclosed under the reverse operation. The standard connections that exist between\nMSO and FO logics and varieties of finite semigroups extend to this setting\nwith semigroups extended with an involution. The case is different for FO with\nneighbour relation where we show that one needs additional equations to\ncharacterise the class.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 07:54:10 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Gastin", "Paul", ""], ["Manuel", "Amaldev", ""], ["Govind", "R.", ""]]}, {"id": "1907.01240", "submitter": "Patrick Totzke", "authors": "Lorenzo Clemente and Piotr Hofman and Patrick Totzke", "title": "Timed Basic Parallel Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Timed basic parallel processes (TBPP) extend communication-free Petri nets\n(aka. BPP or commutative context-free grammars) by a global notion of time.\nTBPP can be seen as an extension of timed automata (TA) with context-free\nbranching rules, and as such may be used to model networks of independent timed\nautomata with process creation.\n  We show that the coverability and reachability problems (with unary encoded\ntarget multiplicities) are PSPACE-complete and EXPTIME-complete, respectively.\nFor the special case of 1-clock TBPP, both are NP-complete and hence not more\ncomplex than for untimed BPP. This contrasts with known\nsuper-Ackermannian-completeness and undecidability results for general timed\nPetri nets.\n  As a result of independent interest, and basis for our NP upper bounds, we\nshow that the reachability relation of 1-clock TA can be expressed by a formula\nof polynomial size in the existential fragment of linear arithmetic, which\nimproves on recent results from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:52:36 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 10:20:40 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Clemente", "Lorenzo", ""], ["Hofman", "Piotr", ""], ["Totzke", "Patrick", ""]]}, {"id": "1907.01270", "submitter": "Bj\\\"orn Lellmann", "authors": "Rajeev Gor\\'e and Bj\\\"orn Lellmann", "title": "Syntactic cut-elimination and backward proof-search for tense logic via\n  linear nested sequents (Extended version)", "comments": "Extended version of the paper accepted at TABLEAUX2019, containing an\n  additional technical appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a linear nested sequent calculus for the basic normal tense logic Kt.\nWe show that the calculus enables backwards proof-search, counter-model\nconstruction and syntactic cut-elimination. Linear nested sequents thus provide\nthe minimal amount of nesting necessary to provide an adequate proof-theory for\nmodal logics containing converse. As a bonus, this yields a cut-free calculus\nfor symmetric modal logic KB.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:01:57 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Gor\u00e9", "Rajeev", ""], ["Lellmann", "Bj\u00f6rn", ""]]}, {"id": "1907.01283", "submitter": "Mario Carneiro", "authors": "Mario Carneiro", "title": "Specifying verified x86 software from scratch", "comments": "4 pages, submitted to SpISA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple framework for specifying and proving facts about the\ninput/output behavior of ELF binary files on the x86-64 architecture. A strong\nemphasis has been placed on simplicity at all levels: the specification says\nonly what it needs to about the target executable, the specification is\nperformed inside a simple logic (equivalent to first-order Peano Arithmetic),\nand the verification language and proof checker are custom-designed to have\nonly what is necessary to perform efficient general purpose verification. This\nforms a part of the Metamath Zero project, to build a minimal verifier that is\ncapable of verifying its own binary. In this paper, we will present the\nspecification of the dynamic semantics of x86 machine code, together with\nenough information about Linux system calls to perform simple IO.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:26:10 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Carneiro", "Mario", ""]]}, {"id": "1907.01318", "submitter": "Jorge A. P\\'erez", "authors": "Lu\\'is Caires, Jorge A. P\\'erez, Frank Pfenning, Bernardo Toninho", "title": "Domain-Aware Session Types (Extended Version)", "comments": "Extended version of a CONCUR 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a generalization of existing Curry-Howard interpretations of\n(binary) session types by relying on an extension of linear logic with features\nfrom hybrid logic, in particular modal worlds that indicate domains. These\nworlds govern domain migration, subject to a parametric accessibility relation\nfamiliar from the Kripke semantics of modal logic. The result is an expressive\nnew typed process framework for domain-aware, message-passing concurrency. Its\nlogical foundations ensure that well-typed processes enjoy session fidelity,\nglobal progress, and termination. Typing also ensures that processes only\ncommunicate with accessible domains and so respect the accessibility relation.\nRemarkably, our domain-aware framework can specify scenarios in which domain\ninformation is available only at runtime; flexible accessibility relations can\nbe cleanly defined and statically enforced. As a specific application, we\nintroduce domain-aware multiparty session types, in which global protocols can\nexpress arbitrarily nested sub-protocols via domain migration. We develop a\nprecise analysis of these multiparty protocols by reduction to our binary\ndomain-aware framework: complex domain-aware protocols can be reasoned about at\nthe right level of abstraction, ensuring also the principled transfer of key\ncorrectness properties from the binary to the multiparty setting.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 12:21:14 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Caires", "Lu\u00eds", ""], ["P\u00e9rez", "Jorge A.", ""], ["Pfenning", "Frank", ""], ["Toninho", "Bernardo", ""]]}, {"id": "1907.01449", "submitter": "Robert Y. Lewis", "authors": "Sander R. Dahmen, Johannes H\\\"olzl, Robert Y. Lewis", "title": "Formalizing the Solution to the Cap Set Problem", "comments": "To appear in proceedings of Interactive Theorem Proving (ITP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2016, Ellenberg and Gijswijt established a new upper bound on the size of\nsubsets of $\\mathbb{F}^n_q$ with no three-term arithmetic progression. This\nproblem has received much mathematical attention, particularly in the case $q =\n3$, where it is commonly known as the \\emph{cap set problem}. Ellenberg and\nGijswijt's proof was published in the \\emph{Annals of Mathematics} and is\nnoteworthy for its clever use of elementary methods. This paper describes a\nformalization of this proof in the Lean proof assistant, including both the\ngeneral result in $\\mathbb{F}^n_q$ and concrete values for the case $q = 3$. We\nfaithfully follow the pen and paper argument to construct the bound. Our work\nshows that (some) modern mathematics is within the range of proof assistants.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:29:31 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Dahmen", "Sander R.", ""], ["H\u00f6lzl", "Johannes", ""], ["Lewis", "Robert Y.", ""]]}, {"id": "1907.01721", "submitter": "\\'Etienne Andr\\'e", "authors": "\\'Etienne Andr\\'e", "title": "What's decidable about parametric timed automata?", "comments": "This is the author version of the manuscript of the same name\n  published in the International Journal on Software Tools for Technology\n  Transfer (STTT), April 2019, Volume 21, Issue 2, pp 203-219. This work is\n  partially supported by the ANR national research program PACS\n  (ANR-14-CE28-0002)", "journal-ref": "International Journal on Software Tools for Technology Transfer\n  (STTT), April 2019, Volume 21, Issue 2, pp 203-219", "doi": "10.1007/s10009-017-0467-0", "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric timed automata (PTAs) are a powerful formalism to reason, simulate\nand formally verify critical real-time systems. After 25 years of research on\nPTAs, it is now well-understood that any non-trivial problem studied is\nundecidable for general PTAs. We provide here a survey of decision and\ncomputation problems for PTAs. On the one hand, bounding time, bounding the\nnumber of parameters or the domain of the parameters does not (in general) lead\nto any decidability. On the other hand, restricting the number of clocks, the\nuse of clocks (compared or not with the parameters), and the use of parameters\n(e.g. used only as upper or lower bounds) leads to decidability of some\nproblems. We also put emphasis on open problems. We also discuss formalisms\nclose to parametric timed automata (such as parametric hybrid automata or\nparametric interrupt timed automata), and we study tools dedicated to PTAs and\ntheir extensions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 03:26:33 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Andr\u00e9", "\u00c9tienne", ""]]}, {"id": "1907.01768", "submitter": "Giorgio Bacci", "authors": "Giorgio Bacci and Giovanni Bacci and Kim G. Larsen and Radu Mardare\n  and Qiyi Tang and Franck van Breugel", "title": "Computing Probabilistic Bisimilarity Distances for Probabilistic\n  Automata", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 1 (February\n  2, 2021) lmcs:7147", "doi": null, "report-no": null, "categories": "cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The probabilistic bisimilarity distance of Deng et al. has been proposed as a\nrobust quantitative generalization of Segala and Lynch's probabilistic\nbisimilarity for probabilistic automata. In this paper, we present a\ncharacterization of the bisimilarity distance as the solution of a simple\nstochastic game. The characterization gives us an algorithm to compute the\ndistances by applying Condon's simple policy iteration on these games. The\ncorrectness of Condon's approach, however, relies on the assumption that the\ngames are stopping. Our games may be non-stopping in general, yet we are able\nto prove termination for this extended class of games. Already other algorithms\nhave been proposed in the literature to compute these distances, with\ncomplexity in $\\textbf{UP} \\cap \\textbf{coUP}$ and \\textbf{PPAD}. Despite the\ntheoretical relevance, these algorithms are inefficient in practice. To the\nbest of our knowledge, our algorithm is the first practical solution.\n  The characterization of the probabilistic bisimilarity distance mentioned\nabove crucially uses a dual presentation of the Hausdorff distance due to\nM\\'emoli. As an additional contribution, in this paper we show that M\\'emoli's\nresult can be used also to prove that the bisimilarity distance bounds the\ndifference in the maximal (or minimal) probability of two states to satisfying\narbitrary $\\omega$-regular properties, expressed, eg., as LTL formulas.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 07:04:26 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 08:48:03 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 17:06:09 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 06:48:30 GMT"}, {"version": "v5", "created": "Wed, 27 May 2020 10:06:32 GMT"}, {"version": "v6", "created": "Mon, 14 Sep 2020 12:46:36 GMT"}, {"version": "v7", "created": "Mon, 21 Sep 2020 07:49:59 GMT"}, {"version": "v8", "created": "Mon, 1 Feb 2021 14:27:26 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bacci", "Giorgio", ""], ["Bacci", "Giovanni", ""], ["Larsen", "Kim G.", ""], ["Mardare", "Radu", ""], ["Tang", "Qiyi", ""], ["van Breugel", "Franck", ""]]}, {"id": "1907.02133", "submitter": "\\'Etienne Andr\\'e", "authors": "\\'Etienne Andr\\'e, Paolo Arcaini, Angelo Gargantini and Marco\n  Radavelli", "title": "Repairing Timed Automata Clock Guards through Abstraction and Testing", "comments": "This is the author (and slightly extended) version of the manuscript\n  of the same name published in the proceedings of the 13th International\n  Conference on Tests and Proofs (TAP 2019). This version contains some\n  additional explanations and all proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timed automata (TAs) are a widely used formalism to specify systems having\ntemporal requirements. However, exactly specifying the system may be difficult,\nas the user may not know the exact clock constraints triggering state\ntransitions. In this work, we assume the user already specified a TA, and (s)he\nwants to validate it against an oracle that can be queried for acceptance.\nUnder the assumption that the user only wrote wrong guard transitions (i.e.,\nthe structure of the TA is correct), the search space for the correct TA can be\nrepresented by a Parametric Timed Automaton (PTA), i.e., a TA in which some\nconstants are parametrized. The paper presents a process that i) abstracts the\ninitial (faulty) TA tainit in a PTA pta; ii) generates some test data (i.e.,\ntimed traces) from pta; iii) assesses the correct evaluation of the traces with\nthe oracle; iv) uses the IMITATOR tool for synthesizing some constraints phi on\nthe parameters of pta; v) instantiate from phi a TA tarep as final repaired\nmodel. Experiments show that the approach is successfully able to partially\nrepair the initial design of the user.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 08:45:25 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Andr\u00e9", "\u00c9tienne", ""], ["Arcaini", "Paolo", ""], ["Gargantini", "Angelo", ""], ["Radavelli", "Marco", ""]]}, {"id": "1907.02170", "submitter": "Duligur Ibeling", "authors": "Duligur Ibeling, Thomas Icard", "title": "On Open-Universe Causal Reasoning", "comments": "UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend two kinds of causal models, structural equation models and\nsimulation models, to infinite variable spaces. This enables a semantics for\nconditionals founded on a calculus of intervention, and axiomatization of\ncausal reasoning for rich, expressive generative models -- including those in\nwhich a causal representation exists only implicitly -- in an open-universe\nsetting. Further, we show that under suitable restrictions the two kinds of\nmodels are equivalent, perhaps surprisingly as their axiomatizations differ\nsubstantially in the general case. We give a series of complete axiomatizations\nin which the open-universe nature of the setting is seen to be essential.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 00:31:20 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:08:14 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ibeling", "Duligur", ""], ["Icard", "Thomas", ""]]}, {"id": "1907.02192", "submitter": "Ramy Shahin", "authors": "Ramy Shahin, Marsha Chechik, Rick Salay", "title": "Lifting Datalog-Based Analyses to Software Product Lines", "comments": "FSE'19 paper", "journal-ref": null, "doi": "10.1145/3338906.3338928", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying program analyses to Software Product Lines (SPLs) has been a\nfundamental research problem at the intersection of Product Line Engineering\nand software analysis. Different attempts have been made to \"lift\" particular\nproduct-level analyses to run on the entire product line. In this paper, we\ntackle the class of Datalog-based analyses (e.g., pointer and taint analyses),\nstudy the theoretical aspects of lifting Datalog inference, and implement a\nlifted inference algorithm inside the Souffl\\'e Datalog engine. We evaluate our\nimplementation on a set of benchmark product lines. We show significant savings\nin processing time and fact database size (billions of times faster on one of\nthe benchmarks) compared to brute-force analysis of each product individually.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 02:31:13 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 00:01:30 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Shahin", "Ramy", ""], ["Chechik", "Marsha", ""], ["Salay", "Rick", ""]]}, {"id": "1907.02296", "submitter": "B Srivathsan", "authors": "R. Govind, Fr\\'ed\\'eric Herbreteau, B. Srivathsan, Igor Walukiewicz", "title": "Revisiting local time semantics for networks of timed automata", "comments": "A shorter version appears in proceedings of CONCUR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate a zone based approach for the reachability problem in timed\nautomata. The challenge is to alleviate the size explosion of the search space\nwhen considering networks of timed automata working in parallel. In the timed\nsetting this explosion is particularly visible as even different interleavings\nof local actions of processes may lead to different zones. Salah et al. in 2006\nhave shown that the union of all these different zones is also a zone. This\nobservation was used in an algorithm which from time to time detects and\naggregates these zones into a single zone.\n  We show that such aggregated zones can be calculated more efficiently using\nthe local time semantics and the related notion of local zones proposed by\nBengtsson et al. in 1998. Next, we point out a flaw in the existing method to\nensure termination of the local zone graph computation. We fix this with a new\nalgorithm that builds the local zone graph and uses abstraction techniques over\n(standard) zones for termination. We evaluate our algorithm on standard\nexamples. On various examples, we observe an order of magnitude decrease in the\nsearch space. On the other examples, the algorithm performs like the standard\nzone algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 09:37:59 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Govind", "R.", ""], ["Herbreteau", "Fr\u00e9d\u00e9ric", ""], ["Srivathsan", "B.", ""], ["Walukiewicz", "Igor", ""]]}, {"id": "1907.02509", "submitter": "Alexey Ignatiev", "authors": "Alexey Ignatiev, Nina Narodytska, Joao Marques-Silva", "title": "On Validating, Repairing and Refining Heuristic ML Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a fast-growing interest in computing explanations\nfor Machine Learning (ML) models predictions. For non-interpretable ML models,\nthe most commonly used approaches for computing explanations are heuristic in\nnature. In contrast, recent work proposed rigorous approaches for computing\nexplanations, which hold for a given ML model and prediction over the entire\ninstance space. This paper extends earlier work to the case of boosted trees\nand assesses the quality of explanations obtained with state-of-the-art\nheuristic approaches. On most of the datasets considered, and for the vast\nmajority of instances, the explanations obtained with heuristic approaches are\nshown to be inadequate when the entire instance space is (implicitly)\nconsidered.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:45:11 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Ignatiev", "Alexey", ""], ["Narodytska", "Nina", ""], ["Marques-Silva", "Joao", ""]]}, {"id": "1907.02594", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima", "title": "Domain-Specific Language to Encode Induction Heuristics", "comments": "Accepted at ICFP2019-SRC (International Conference on Functional\n  Programming Student Research Competition). Our draft, \"LiFtEr: Language to\n  Encode Induction Heuristics for Isabelle/HOL\" (arXiv:1906.08084), gives\n  further details of this domain specific language. arXiv admin note:\n  substantial text overlap with arXiv:1906.08084", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof assistants, such as Isabelle/HOL, offer tools to facilitate inductive\ntheorem proving. Isabelle experts know how to use these tools effectively;\nhowever, they did not have a systematic way to encode their expertise. To\naddress this problem, we present our domain-specific language, LiFtEr. LiFtEr\nallows experienced Isabelle users to encode their induction heuristics in a\nstyle independent of any problem domain. LiFtEr's interpreter mechanically\nchecks if a given application of induction tool matches the heuristics\nspecified by experienced users, thus systematically transferring experienced\nusers' expertise to new Isabelle users.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 19:57:43 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Nagashima", "Yutaka", ""]]}, {"id": "1907.02668", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "Operational Semantics of Games", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:1611.09035, arXiv:1810.00868", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce operational semantics into games. And based on the operational\nsemantics, we establish a full algebra of games, including basic algebra of\ngames, algebra of concurrent games, recursion and abstraction. The algebra can\nbe used widely to reason on the behaviors of systems (not only computational\nsystems) with game theory supported.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 04:01:13 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 04:36:24 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1907.02769", "submitter": "Paul-Elliot Angl\\`es D'Auriac", "authors": "Paul-Elliot Angl\\`es d'Auriac and Takayuki Kihara", "title": "A comparison of various analytic choice principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate computability theoretic and descriptive set theoretic contents\nof various kinds of analytic choice principles by performing detailed analysis\nof the Medvedev lattice of $\\Sigma^1_1$-closed sets. Among others, we solve an\nopen problem on the Weihrauch degree of the parallelization of the\n$\\Sigma^1_1$-choice principle on the integers. Harrington's unpublished result\non a jump hierarchy along a pseudo-well-ordering plays a key role in solving\nthe problem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 11:02:39 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["d'Auriac", "Paul-Elliot Angl\u00e8s", ""], ["Kihara", "Takayuki", ""]]}, {"id": "1907.02836", "submitter": "Lawrence Paulson", "authors": "Lawrence C. Paulson, Tobias Nipkow and Makarius Wenzel", "title": "From LCF to Isabelle/HOL", "comments": "25 pages. Accepted to Formal Aspects of Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive theorem provers have developed dramatically over the past four\ndecades, from primitive beginnings to today's powerful systems. Here, we focus\non Isabelle/HOL and its distinctive strengths. They include automatic proof\nsearch, borrowing techniques from the world of first order theorem proving, but\nalso the automatic search for counterexamples. They include a highly readable\nstructured language of proofs and a unique interactive development environment\nfor editing live proof documents. Everything rests on the foundation conceived\nby Robin Milner for Edinburgh LCF: a proof kernel, using abstract types to\nensure soundness and eliminate the need to store proofs. Compared with the\nresearch prototypes of the 1970s, Isabelle is a practical and versatile tool.\nIt is used by system designers, mathematicians and many others.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:02:54 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 13:13:15 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Paulson", "Lawrence C.", ""], ["Nipkow", "Tobias", ""], ["Wenzel", "Makarius", ""]]}, {"id": "1907.02860", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "Truly Concurrent Bisimilarities are Game Equivalent", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design games for truly concurrent bisimilarities, including strongly truly\nconcurrent bisimilarities and branching truly concurrent bisimilarities, such\nas pomset bisimilarities, step bisimilarities, history-preserving\nbisimilarities and hereditary history-preserving bisimilarities.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 05:56:54 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1907.02881", "submitter": "Simon Lunel", "authors": "Simon Lunel and Stefan Mitsch and Benoit Boyer and Jean-Pierre Talpin", "title": "Parallel Composition and Modular Verification of Computer Controlled\n  Systems in Differential Dynamic Logic", "comments": "Long version of an article accepted to the conference FM'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Controlled Systems (CCS) are a subclass of hybrid systems where the\nperiodic relation of control components to time is paramount. Since they\nadditionally are at the heart of many safety-critical devices, it is of primary\nimportance to correctly model such systems and to ensure they function\ncorrectly according to safety requirements. Differential dynamic logic\n$d\\mathcal{L}$ is a powerful logic to model hybrid systems and to prove their\ncorrectness. We contribute a component-based modeling and reasoning framework\nto $d\\mathcal{L}$ that separates models into components with timing guarantees,\nsuch as reactivity of controllers and controllability of continuous dynamics.\nComponents operate in parallel, with coarse-grained interleaving, periodic\nexecution and communication. We present techniques to automate system safety\nproofs from isolated, modular, and possibly mechanized proofs of component\nproperties parameterized with timing characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:08:39 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:50:36 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Lunel", "Simon", ""], ["Mitsch", "Stefan", ""], ["Boyer", "Benoit", ""], ["Talpin", "Jean-Pierre", ""]]}, {"id": "1907.03265", "submitter": "Tim Lyon", "authors": "Kees van Berkel and Tim Lyon", "title": "A Neutral Temporal Deontic STIT Logic", "comments": "Appended version of the paper \"A Neutral Temporal Deontic STIT\n  logic\", accepted to the 7th International Conference on Logic, Rationality\n  and Interaction (LORI 2019)", "journal-ref": "Logic, Rationality, and Interaction (2019) 340-354", "doi": "10.1007/978-3-662-60292-8_25", "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we answer a long standing request for temporal embeddings of\ndeontic STIT logics by introducing the multi-agent STIT logic TDS. The logic is\nbased upon atemporal utilitarian STIT logic. Yet, the logic presented here will\nbe neutral: instead of committing ourselves to utilitarian theories, we prove\nthe logic TDS sound and complete with respect to relational frames not\nemploying any utilitarian function. We demonstrate how these neutral frames can\nbe transformed into utilitarian temporal frames, while preserving validity.\nLast, we discuss problems that arise from employing binary utility functions in\na temporal setting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 10:42:35 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 07:48:01 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 13:50:48 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2019 12:19:30 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["van Berkel", "Kees", ""], ["Lyon", "Tim", ""]]}, {"id": "1907.03481", "submitter": "Paolo Pistone", "authors": "Paolo Pistone, Luca Tranchini", "title": "The Yoneda Reduction of Polymorphic Types (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore a family of type isomorphisms in System F whose\nvalidity corresponds, semantically, to some form of the Yoneda isomorphism from\ncategory theory. These isomorphisms hold under theories of equivalence stronger\nthan beta-eta-equivalence, like those induced by parametricity and\ndinaturality. We show that the Yoneda type isomorphisms yield a rewriting over\ntypes, that we call Yoneda reduction, which can be used to eliminate\nquantifiers from a polymorphic type, replacing them with a combination of\nmonomorphic type constructors. We establish some sufficient conditions under\nwhich quantifiers can be fully eliminated from a polymorphic type, and we show\nsome application of these conditions to count the inhabitants of a type and to\ncompute program equivalence in some fragments of System F.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 09:49:17 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 00:55:27 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 17:28:48 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Pistone", "Paolo", ""], ["Tranchini", "Luca", ""]]}, {"id": "1907.03523", "submitter": "EPTCS", "authors": "Emanuele De Angelis (University \"G. d'Annunzio\" of Chieti-Pescara,\n  Italy), Grigory Fedyukovich (Princeton University, USA), Nikos Tzevelekos\n  (Queen Mary University of London, UK), Mattias Ulbrich (Karlsruhe Institute\n  of Technology, Germany)", "title": "Proceedings of the Sixth Workshop on Horn Clauses for Verification and\n  Synthesis and Third Workshop on Program Equivalence and Relational Reasoning", "comments": null, "journal-ref": "EPTCS 296, 2019", "doi": "10.4204/EPTCS.296", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the joint post-proceedings of the 3rd Workshop on\nProgram Equivalence and Relational Reasoning (PERR) and the 6th Workshop on\nHorn Clauses for Verification and Synthesis (HCVS), which took place in Prague,\nCzech Republic on 6th and 7th April, respectively, as affiliated workshops of\nETAPS.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:33:52 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["De Angelis", "Emanuele", "", "University \"G. d'Annunzio\" of Chieti-Pescara,\n  Italy"], ["Fedyukovich", "Grigory", "", "Princeton University, USA"], ["Tzevelekos", "Nikos", "", "Queen Mary University of London, UK"], ["Ulbrich", "Mattias", "", "Karlsruhe Institute\n  of Technology, Germany"]]}, {"id": "1907.03533", "submitter": "Rasoul Ramezanian", "authors": "Rasoul Ramezanian", "title": "A Formal Axiomatization of Computation", "comments": "13 page. arXiv admin note: substantial text overlap with\n  arXiv:1906.09873, arXiv:1205.5994", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an axiomatization for the notion of computation. Based on the\nidea of Brouwer choice sequences, we construct a model, denoted by $E$, which\nsatisfies our axioms and $E \\models \\mathrm{ P \\neq NP}$. In other words,\nregarding \"effective computability\" in Brouwer intuitionism viewpoint, we show\n$\\mathrm{ P \\neq NP}$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 04:52:45 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 07:47:41 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ramezanian", "Rasoul", ""]]}, {"id": "1907.03564", "submitter": "Muhammad Syifa'ul Mufid", "authors": "Muhammad Syifa'ul Mufid, Dieky Adzkiya, Alessandro Abate", "title": "Bounded Model Checking of Max-Plus Linear Systems via Predicate\n  Abstractions", "comments": "19 pages, accepted in FORMATS 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the abstraction of max-plus linear (MPL) systems via\npredicates. Predicates are automatically selected from system matrix, as well\nas from the specifications under consideration. We focus on verifying\ntime-difference specifications, which encompass the relation between successive\nevents in MPL systems. We implement a bounded model checking (BMC) procedure\nover a predicate abstraction of the given MPL system, to verify the\nsatisfaction of time-difference specifications. Our predicate abstractions are\nexperimentally shown to improve on existing MPL abstractions algorithms.\nFurthermore, with focus on the BMC algorithm, we can provide an explicit upper\nbound on the completeness threshold by means of the transient and the cyclicity\nof the underlying MPL system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 12:41:22 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Mufid", "Muhammad Syifa'ul", ""], ["Adzkiya", "Dieky", ""], ["Abate", "Alessandro", ""]]}, {"id": "1907.03631", "submitter": "Federico Aschieri", "authors": "Federico Aschieri and Francesco A. Genco", "title": "$\\unicode{8523}$ means Parallel: Multiplicative Linear Logic Proofs as\n  Concurrent Functional Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along the lines of the Abramsky ``Proofs-as-Processes'' program, we present\nan interpretation of multiplicative linear logic as typing system for\nconcurrent functional programming. In particular, we study a linear\nmultiple-conclusion natural deduction system and show it is isomorphic to a\nsimple and natural extension of $\\lambda$-calculus with parallelism and\ncommunication primitives, called $\\lambda_{\\unicode{8523}}$. We shall prove\nthat $\\lambda_{\\unicode{8523}}$ satisfies all the desirable properties for a\ntyped programming language: subject reduction, progress, strong normalization\nand confluence.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 14:08:53 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Aschieri", "Federico", ""], ["Genco", "Francesco A.", ""]]}, {"id": "1907.03858", "submitter": "Lev Gordeev", "authors": "Lev Gordeev", "title": "Proof compression and NP versus PSPACE. Part 2", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We upgrade [1] to a complete proof of the conjecture NP = PSPACE.\n  [1]: L. Gordeev, E. H. Haeusler, Proof Compression and NP Versus PSPACE,\nStudia Logica (107) (1): 55-83 (2019)\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:32:53 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 05:32:04 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Gordeev", "Lev", ""]]}, {"id": "1907.03928", "submitter": "Chenyi  Zhang", "authors": "Chenyi Zhang and Jun Pang", "title": "Characterising Probabilistic Alternating Simulation for Concurrent Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic game structures combine both nondeterminism and stochasticity,\nwhere players repeatedly take actions simultaneously to move to the next state\nof the concurrent game. Probabilistic alternating simulation is an important\ntool to compare the behaviour of different probabilistic game structures. In\nthis paper, we present a sound and complete modal characterisation of this\nsimulation relation by proposing a new logic based on probabilistic\ndistributions. The logic enables a player to enforce a property in the next\nstate or distribution. Its extension with fixpoints, which also characterises\nthe simulation relation, can express a lot of interesting properties in\npractical applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 01:12:25 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Zhang", "Chenyi", ""], ["Pang", "Jun", ""]]}, {"id": "1907.03997", "submitter": "EPTCS", "authors": "Qi Zhou (Georgia Institute of Technology), David Heath (Georgia\n  Institute of Technology), William Harris (Galois Inc.)", "title": "Relational Verification via Invariant-Guided Synchronization", "comments": "In Proceedings HCVS/PERR 2019, arXiv:1907.03523", "journal-ref": "EPTCS 296, 2019, pp. 28-41", "doi": "10.4204/EPTCS.296.6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational properties describe relationships that hold over multiple\nexecutions of one or more programs, such as functional equivalence.\nConventional approaches for automatically verifying such properties typically\nrely on syntax-based, heuristic strategies for finding synchronization points\namong the input programs. These synchronization points are then annotated with\nappropriate relational invariants to complete the proof. However, when\nsuboptimal synchronization points are chosen the required invariants can be\ncomplicated or even inexpressible in the target theory.\n  In this work, we propose a novel approach to verifying relational properties.\nThis approach searches for synchronization points and synthesizes relational\ninvariants simultaneously. Specifically, the approach uses synthesized\ninvariants as a guide for finding proper synchronization points that lead to a\ncomplete proof. We implemented our approach as a tool named PEQUOD, which\ntargets Java Virtual Machine (JVM) bytecode. We evaluated PEQUOD by using it to\nsolve verification challenges drawn from the from the research literature and\nby verifying properties of student-submitted solutions to online challenge\nproblems. The results show that PEQUOD solve verification problems that cannot\nbe addressed by current techniques.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:02:04 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Zhou", "Qi", "", "Georgia Institute of Technology"], ["Heath", "David", "", "Georgia\n  Institute of Technology"], ["Harris", "William", "", "Galois Inc."]]}, {"id": "1907.03998", "submitter": "EPTCS", "authors": "Daniel Dietsch (University of Freiburg), Matthias Heizmann (University\n  of Freiburg), Jochen Hoenicke (University of Freiburg), Alexander Nutz\n  (University of Freiburg), Andreas Podelski (University of Freiburg)", "title": "Ultimate TreeAutomizer (CHC-COMP Tool Description)", "comments": "In Proceedings HCVS/PERR 2019, arXiv:1907.03523", "journal-ref": "EPTCS 296, 2019, pp. 42-47", "doi": "10.4204/EPTCS.296.7", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Ultimate TreeAutomizer, a solver for satisfiability of sets of\nconstrained Horn clauses. Constrained Horn clauses (CHC) are a fragment of\nfirst order logic with attractive properties in terms of expressiveness and\naccessibility to algorithmic solving. Ultimate TreeAutomizer is based on the\ntechniques of trace abstraction, tree automata and tree interpolation. This\npaper serves as a tool description for TreeAutomizer in CHC-COMP 2019.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:02:28 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Dietsch", "Daniel", "", "University of Freiburg"], ["Heizmann", "Matthias", "", "University\n  of Freiburg"], ["Hoenicke", "Jochen", "", "University of Freiburg"], ["Nutz", "Alexander", "", "University of Freiburg"], ["Podelski", "Andreas", "", "University of Freiburg"]]}, {"id": "1907.03999", "submitter": "EPTCS", "authors": "Emanuele De Angelis (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Fabio Fioravanti (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Alberto Pettorossi (DICII, University of Roma Tor\n  Vergata, Italy), Maurizio Proietti (CNR-IASI, Rome, Italy)", "title": "Proving Properties of Sorting Programs: A Case Study in Horn Clause\n  Verification", "comments": "In Proceedings HCVS/PERR 2019, arXiv:1907.03523", "journal-ref": "EPTCS 296, 2019, pp. 48-75", "doi": "10.4204/EPTCS.296.8", "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proof of a program property can be reduced to the proof of satisfiability\nof a set of constrained Horn clauses (CHCs) which can be automatically\ngenerated from the program and the property. In this paper we have conducted a\ncase study in Horn clause verification by considering several sorting programs\nwith the aim of exploring the effectiveness of a transformation technique which\nallows us to eliminate inductive data structures such as lists or trees. If\nthis technique is successful, we derive a set of CHCs with constraints over the\nintegers and booleans only, and the satisfiability check can often be performed\nin an effective way by using state-of-the-art CHC solvers, such as Eldarica or\nZ3. In this case study we have also illustrated the usefulness of a companion\ntechnique based on the introduction of the so-called difference predicates,\nwhose definitions correspond to lemmata required during the verification. We\nhave considered functional programs which implement the following kinds of\nsorting algorithms acting on lists of integers: (i) linearly recursive sorting\nalgorithms, such as insertion sort and selection sort, and (ii) non-linearly\nrecursive sorting algorithms, such as quicksort and mergesort, and we have\nconsidered the following properties: (i) the partial correctness properties,\nthat is, the orderedness of the output lists, and the equality of the input and\noutput lists when viewed as multisets, and (ii) some arithmetic properties,\nsuch as the equality of the sum of the elements before and after sorting.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:02:42 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["De Angelis", "Emanuele", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Fioravanti", "Fabio", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Pettorossi", "Alberto", "", "DICII, University of Roma Tor\n  Vergata, Italy"], ["Proietti", "Maurizio", "", "CNR-IASI, Rome, Italy"]]}, {"id": "1907.04005", "submitter": "Ian Hayes", "authors": "Larissa A. Meinicke and Ian J. Hayes", "title": "Handling localisation in rely/guarantee concurrency: An algebraic\n  approach", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rely/guarantee approach of Jones extends Hoare logic with rely and\nguarantee conditions in order to allow compositional reasoning about\nshared-variable concurrent programs. This paper focuses on localisation in the\ncontext of rely/guarantee concurrency in order to support local variables.\nBecause we allow the body of a local variable block to contain component\nprocesses that run in parallel, the approach needs to allow variables local to\na block to become shared variables of its component parallel processes. To\nsupport the mechanisation of the rely/guarantee approach, we have developed a\nsynchronous concurrent refinement algebra. Its foundation consists of a small\nset of primitive commands plus a small set of primitive operators from which\nall remaining constructs are defined. To support local variables we add a\nprimitive localisation operator to our algebra that is used to define local\nvariable blocks. From this we can prove properties of localisation, including\nits interaction with rely and guarantee conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:27:44 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Meinicke", "Larissa A.", ""], ["Hayes", "Ian J.", ""]]}, {"id": "1907.04036", "submitter": "Luca Reggio", "authors": "Mai Gehrke, Tom\\'a\\v{s} Jakl and Luca Reggio", "title": "A duality theoretic view on limits of finite structures", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A systematic theory of structural limits for finite models has been developed\nby Nesetril and Ossona de Mendez. It is based on the insight that the\ncollection of finite structures can be embedded, via a map they call the Stone\npairing, in a space of measures, where the desired limits can be computed. We\nshow that a closely related but finer grained space of measures arises --- via\nStone-Priestley duality and the notion of types from model theory --- by\nenriching the expressive power of first-order logic with certain\n``probabilistic operators''. We provide a sound and complete calculus for this\nextended logic and expose the functorial nature of this construction.\n  The consequences are two-fold. On the one hand, we identify the logical gist\nof the theory of structural limits. On the other hand, our construction shows\nthat the duality-theoretic variant of the Stone pairing captures the adding of\na layer of quantifiers, thus making a strong link to recent work on semiring\nquantifiers in logic on words. In the process, we identify the model theoretic\nnotion of types as the unifying concept behind this link. These results\ncontribute to bridging the strands of logic in computer science which focus on\nsemantics and on more algorithmic and complexity related areas, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:10:37 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 08:37:57 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 16:20:37 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Gehrke", "Mai", ""], ["Jakl", "Tom\u00e1\u0161", ""], ["Reggio", "Luca", ""]]}, {"id": "1907.04065", "submitter": "Kurt Mehlhorn", "authors": "Mohammad Abdulaziz and Kurt Mehlhorn and Tobias Nipkow", "title": "Trustworthy Graph Algorithms", "comments": "to appear in MFCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the LEDA project was to build an easy-to-use and extendable\nlibrary of correct and efficient data structures, graph algorithms and\ngeometric algorithms. We report on the use of formal program verification to\nachieve an even higher level of trustworthiness. Specifically, we report on an\nongoing and largely finished verification of the blossom-shrinking algorithm\nfor maximum cardinality matching.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 10:06:30 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Abdulaziz", "Mohammad", ""], ["Mehlhorn", "Kurt", ""], ["Nipkow", "Tobias", ""]]}, {"id": "1907.04134", "submitter": "David Wonnacott", "authors": "David G. Wonnacott and Peter-Michael Osera", "title": "A Bridge Anchored on Both Sides: Formal Deduction in Introductory CS,\n  and Code Proofs in Discrete Math", "comments": "36 pages, including references; \"experiments\" section to be discussed\n  at ICER 2019 work-in-progress session; prior material currently under review", "journal-ref": null, "doi": null, "report-no": "HC-CS-TR 2019-01", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a sharp disconnect between the programming and mathematical portions\nof the standard undergraduate computer science curriculum, leading to student\nmisunderstanding about how the two are related. We propose connecting the\nsubjects early in the curriculum---specifically, in CS1 and the introductory\ndiscrete mathematics course---by using formal reasoning about programs as a\nbridge between them.\n  This article reports on Haverford and Grinnell College's experience in\nconstructing the end points of this bridge between CS1 and discrete\nmathematics. Haverford's long-standing \"3-2-1\" curriculum introduces code\nreasoning in conjunction with introductory programming concepts, and Grinnell's\ndiscrete mathematics introduces code reasoning as a motivation for logic and\nformal deduction. Both courses present code reasoning in a style based on\nsymbolic code execution techniques from the programming language community, but\ntuned to address the particulars of each course.\n  These courses rely primarily on traditional means of proof authoring with\npen-and-paper. This is unsatisfactory for students who receive no feedback\nuntil grading on their work and instructors who must shoulder the burden of\ninterpreting students' proofs and giving useful feedback. To this end, we also\ndescribe the current state of Orca, an in-development proof assistant for\nundergraduate education that we are developing to address these issues in our\ncourses.\n  Finally, in teaching our courses, we have discovered a number of educational\nresearch questions about the effectiveness of code reasoning in bridging the\ngap between programming and mathematics, and the ability of tools like \\orca to\nsupport this pedagogy. We pose these research questions as next steps to\nformalize our initial experiences in our courses with the hope of eventually\ngeneralizing our approaches for wider adoption.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 15:33:35 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Wonnacott", "David G.", ""], ["Osera", "Peter-Michael", ""]]}, {"id": "1907.04211", "submitter": "Genaro J. Martinez", "authors": "Sergio J. Martinez, Ivan M. Mendoza, Genaro J. Martinez, Shigeru\n  Ninagawa", "title": "Universal One-Dimensional Cellular Automata Derived for Turing Machines\n  and its Dynamical Behaviour", "comments": "18 pages, 8 tables, 3 figures.\n  https://www.oldcitypublishing.com/journals/ijuc-home/ijuc-issue-contents/ijuc-volume-14-number-2-2019/ijuc-14-2-p-121-138/", "journal-ref": "International Journal of Unconventional Computing 14(2) pages\n  121-138, 2019", "doi": null, "report-no": null, "categories": "nlin.CG cs.CL cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universality in cellular automata theory is a central problem studied and\ndeveloped from their origins by John von Neumann. In this paper, we present an\nalgorithm where any Turing machine can be converted to one-dimensional cellular\nautomaton with a 2-linear time and display its spatial dynamics. Three\nparticular Turing machines are converted in three universal one-dimensional\ncellular automata, they are: binary sum, rule 110 and a universal reversible\nTuring machine.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:12:13 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Martinez", "Sergio J.", ""], ["Mendoza", "Ivan M.", ""], ["Martinez", "Genaro J.", ""], ["Ninagawa", "Shigeru", ""]]}, {"id": "1907.04243", "submitter": "Antoine Genitrini", "authors": "Olivier Bodini, Matthieu Dien, Antoine Genitrini and Fr\\'ed\\'eric\n  Peschanski", "title": "The Combinatorics of Barrier Synchronization", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-21571-2_21", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the notion of synchronization from the point of view\nof combinatorics. As a first step, we address the quantitative problem of\ncounting the number of executions of simple processes interacting with\nsynchronization barriers. We elaborate a systematic decomposition of processes\nthat produces a symbolic integral formula to solve the problem. Based on this\nprocedure, we develop a generic algorithm to generate process executions\nuniformly at random. For some interesting sub-classes of processes we propose\nvery efficient counting and random sampling algorithms. All these algorithms\nhave one important characteristic in common: they work on the control graph of\nprocesses and thus do not require the explicit construction of the state-space.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:27:46 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Bodini", "Olivier", ""], ["Dien", "Matthieu", ""], ["Genitrini", "Antoine", ""], ["Peschanski", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1907.04262", "submitter": "\\'Akos Hajdu", "authors": "\\'Akos Hajdu and Dejan Jovanovi\\'c", "title": "solc-verify: A Modular Verifier for Solidity Smart Contracts", "comments": "Authors' manuscript. Published in S. Chakraborty and J. A. Navas\n  (Eds.): VSTTE 2019, LNCS 12031, 2020. The final publication is available at\n  Springer via https://doi.org/10.1007/978-3-030-41600-3_11", "journal-ref": null, "doi": "10.1007/978-3-030-41600-3_11", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present solc-verify, a source-level verification tool for Ethereum smart\ncontracts. Solc-verify takes smart contracts written in Solidity and discharges\nverification conditions using modular program analysis and SMT solvers. Built\non top of the Solidity compiler, solc-verify reasons at the level of the\ncontract source code, as opposed to the more common approaches that operate at\nthe level of Ethereum bytecode. This enables solc-verify to effectively reason\nabout high-level contract properties while modeling low-level language\nsemantics precisely. The contract properties, such as contract invariants, loop\ninvariants, and function pre- and post-conditions, can be provided as\nannotations in the code by the developer. This enables automated, yet\nuser-friendly formal verification for smart contracts. We demonstrate\nsolc-verify by examining real-world examples where our tool can effectively\nfind bugs and prove correctness of non-trivial properties with minimal user\neffort.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 15:58:08 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 13:43:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hajdu", "\u00c1kos", ""], ["Jovanovi\u0107", "Dejan", ""]]}, {"id": "1907.04319", "submitter": "EPTCS", "authors": "Ozan Kahramano\\u{g}ullar{\\i} (University of Trento, Department of\n  Mathematics)", "title": "On Quantitative Comparison of Chemical Reaction Network Models", "comments": "In Proceedings HCVS/PERR 2019, arXiv:1907.03523", "journal-ref": "EPTCS 296, 2019, pp. 14-27", "doi": "10.4204/EPTCS.296.5", "report-no": null, "categories": "q-bio.MN cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical reaction networks (CRNs) provide a convenient language for modelling\na broad variety of biological systems. These models are commonly studied with\nrespect to the time series they generate in deterministic or stochastic\nsimulations. Their dynamic behaviours are then analysed, often by using\ndeterministic methods based on differential equations with a focus on the\nsteady states. Here, we propose a method for comparing CRNs with respect to\ntheir behaviour in stochastic simulations. Our method is based on using the\nflux graphs that are delivered by stochastic simulations as abstract\nrepresentations of their dynamic behaviour. This allows us to compare the\nbehaviour of any two CRNs for any time interval, and define a notion of\nequivalence on them that overlaps with graph isomorphism at the lowest level of\nrepresentation. The similarity between the compared CRNs can be quantified in\nterms of their distance. The results can then be used to refine the models or\nto replace a larger model with a smaller one that produces the same behaviour\nor vice versa.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:01:27 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Kahramano\u011fullar\u0131", "Ozan", "", "University of Trento, Department of\n  Mathematics"]]}, {"id": "1907.04358", "submitter": "Shruthi Chari", "authors": "Shruthi Chari, Miao Qi, Nkcheniyere N. Agu, Oshani Seneviratne, James\n  P. McCusker, Kristin P. Bennett, Amar K. Das, Deborah L. McGuinness", "title": "Making Study Populations Visible through Knowledge Graphs", "comments": "16 pages, 4 figures, 1 table, accepted to the ISWC 2019 Resources\n  Track (https://iswc2019.semanticweb.org/call-for-resources-track-papers/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO q-bio.PE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Treatment recommendations within Clinical Practice Guidelines (CPGs) are\nlargely based on findings from clinical trials and case studies, referred to\nhere as research studies, that are often based on highly selective clinical\npopulations, referred to here as study cohorts. When medical practitioners\napply CPG recommendations, they need to understand how well their patient\npopulation matches the characteristics of those in the study cohort, and thus\nare confronted with the challenges of locating the study cohort information and\nmaking an analytic comparison. To address these challenges, we develop an\nontology-enabled prototype system, which exposes the population descriptions in\nresearch studies in a declarative manner, with the ultimate goal of allowing\nmedical practitioners to better understand the applicability and\ngeneralizability of treatment recommendations. We build a Study Cohort Ontology\n(SCO) to encode the vocabulary of study population descriptions, that are often\nreported in the first table in the published work, thus they are often referred\nto as Table 1. We leverage the well-used Semanticscience Integrated Ontology\n(SIO) for defining property associations between classes. Further, we model the\nkey components of Table 1s, i.e., collections of study subjects, subject\ncharacteristics, and statistical measures in RDF knowledge graphs. We design\nscenarios for medical practitioners to perform population analysis, and\ngenerate cohort similarity visualizations to determine the applicability of a\nstudy population to the clinical population of interest. Our semantic approach\nto make study populations visible, by standardized representations of Table 1s,\nallows users to quickly derive clinically relevant inferences about study\npopulations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:27:55 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Chari", "Shruthi", ""], ["Qi", "Miao", ""], ["Agu", "Nkcheniyere N.", ""], ["Seneviratne", "Oshani", ""], ["McCusker", "James P.", ""], ["Bennett", "Kristin P.", ""], ["Das", "Amar K.", ""], ["McGuinness", "Deborah L.", ""]]}, {"id": "1907.04383", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek, Venkatesan Guruswami, Marcin Wrochna, and Stanislav\n  \\v{Z}ivn\\'y", "title": "The Power of the Combined Basic LP and Affine Relaxation for Promise\n  CSPs", "comments": "17 pages, to appear in SICOMP", "journal-ref": "SIAM Journal on Computing 49(6) (2020) 1232-1248", "doi": "10.1137/20M1312745", "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of constraint satisfaction problems (CSP), promise CSPs are an\nexciting new direction of study. In a promise CSP, each constraint comes in two\nforms: \"strict\" and \"weak,\" and in the associated decision problem one must\ndistinguish between being able to satisfy all the strict constraints versus not\nbeing able to satisfy all the weak constraints. The most commonly cited example\nof a promise CSP is the approximate graph coloring problem--which has recently\nseen exciting progress [BKO19, WZ20] benefiting from a systematic algebraic\napproach to promise CSPs based on \"polymorphisms,\" operations that map tuples\nin the strict form of each constraint to tuples in the corresponding weak form.\n  In this work, we present a simple algorithm which in polynomial time solves\nthe decision problem for all promise CSPs that admit infinitely many symmetric\npolymorphisms, which are invariant under arbitrary coordinate permutations.\nThis generalizes previous work of the first two authors [BG19]. We also extend\nthis algorithm to a more general class of block-symmetric polymorphisms. As a\ncorollary, this single algorithm solves all polynomial-time tractable Boolean\nCSPs simultaneously. These results give a new perspective on Schaefer's classic\ndichotomy theorem and shed further light on how symmetries of polymorphisms\nenable algorithms. Finally, we show that block symmetric polymorphisms are not\nonly sufficient but also necessary for this algorithm to work, thus\nestablishing its precise power\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:54:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 21:35:32 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 14:35:12 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Guruswami", "Venkatesan", ""], ["Wrochna", "Marcin", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1907.04408", "submitter": "Curtis Bright", "authors": "Curtis Bright, Ilias Kotsireas, Vijay Ganesh", "title": "SAT Solvers and Computer Algebra Systems: A Powerful Combination for\n  Mathematics", "comments": "To appear in Proceedings of the 29th International Conference on\n  Computer Science and Software Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.SC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, many distinct lines of research aimed at\nautomating mathematics have been developed, including computer algebra systems\n(CASs) for mathematical modelling, automated theorem provers for first-order\nlogic, SAT/SMT solvers aimed at program verification, and higher-order proof\nassistants for checking mathematical proofs. More recently, some of these lines\nof research have started to converge in complementary ways. One success story\nis the combination of SAT solvers and CASs (SAT+CAS) aimed at resolving\nmathematical conjectures.\n  Many conjectures in pure and applied mathematics are not amenable to\ntraditional proof methods. Instead, they are best addressed via computational\nmethods that involve very large combinatorial search spaces. SAT solvers are\npowerful methods to search through such large combinatorial\nspaces---consequently, many problems from a variety of mathematical domains\nhave been reduced to SAT in an attempt to resolve them. However, solvers\ntraditionally lack deep repositories of mathematical domain knowledge that can\nbe crucial to pruning such large search spaces. By contrast, CASs are deep\nrepositories of mathematical knowledge but lack efficient general search\ncapabilities. By combining the search power of SAT with the deep mathematical\nknowledge in CASs we can solve many problems in mathematics that no other known\nmethods seem capable of solving.\n  We demonstrate the success of the SAT+CAS paradigm by highlighting many\nconjectures that have been disproven, verified, or partially verified using our\ntool MathCheck. These successes indicate that the paradigm is positioned to\nbecome a standard method for solving problems requiring both a significant\namount of search and deep mathematical reasoning. For example, the SAT+CAS\nparadigm has recently been used by Heule, Kauers, and Seidl to find many new\nalgorithms for $3\\times3$ matrix multiplication.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:49:14 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 17:24:42 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Bright", "Curtis", ""], ["Kotsireas", "Ilias", ""], ["Ganesh", "Vijay", ""]]}, {"id": "1907.04521", "submitter": "Ivan Latkin", "authors": "Ivan V. Latkin", "title": "The complexity of the first-order theory of pure equality", "comments": "40 pages, 19 references bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We will find a lower bound on the recognition complexity of the theories that\nare nontrivial relative to some equivalence relation (this relation may be\nequality), namely, each of these theories is consistent with the formula, whose\nsense is that there exist two non-equivalent elements. However, at first, we\nwill obtain a lower bound on the computational complexity for the first-order\ntheory of Boolean algebra that has only two elements. For this purpose, we will\ncode the long-continued deterministic Turing machine computations by the\nrelatively short-length quantified Boolean formulae; the modified Stockmeyer\nand Meyer method will appreciably be used for this simulation. Then, we will\ntransform the modeling formulae of the theory of this Boolean algebra to the\nsimulation ones of the first-order theory of the only equivalence relation in\npolynomial time. Since the computational complexity of these theories is not\npolynomial, we obtain that the class $\\mathbf{P}$ is a proper subclass of\n$\\mathbf{PSPACE}$ (Polynomial Time is a proper subset of Polynomial Space).\n  Keywords: Computational complexity, the theory of equality, the coding of\ncomputations, simulation by means formulae, polynomial time, polynomial space,\nlower complexity bound\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 05:56:10 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 13:07:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Latkin", "Ivan V.", ""]]}, {"id": "1907.04592", "submitter": "Alexey Potapov", "authors": "Alexey Potapov, Anatoly Belikov, Vitaly Bogdanov, Alexander Scherbatiy", "title": "Differentiable Probabilistic Logic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic logic reasoning is a central component of such cognitive\narchitectures as OpenCog. However, as an integrative architecture, OpenCog\nfacilitates cognitive synergy via hybridization of different inference methods.\nIn this paper, we introduce a differentiable version of Probabilistic Logic\nnetworks, which rules operate over tensor truth values in such a way that a\nchain of reasoning steps constructs a computation graph over tensors that\naccepts truth values of premises from the knowledge base as input and produces\ntruth values of conclusions as output. This allows for both learning truth\nvalues of premises and formulas for rules (specified in a form with trainable\nweights) by backpropagation combining subsymbolic optimization and symbolic\nreasoning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 09:44:10 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Potapov", "Alexey", ""], ["Belikov", "Anatoly", ""], ["Bogdanov", "Vitaly", ""], ["Scherbatiy", "Alexander", ""]]}, {"id": "1907.04987", "submitter": "Curtis Bright", "authors": "Curtis Bright, Dragomir \\v{Z}. {\\DJ}okovi\\'c, Ilias Kotsireas, Vijay\n  Ganesh", "title": "The SAT+CAS Method for Combinatorial Search with Applications to Best\n  Matrices", "comments": "To appear in Annals of Mathematics and Artificial Intelligence", "journal-ref": null, "doi": "10.1007/s10472-019-09681-3", "report-no": null, "categories": "cs.LO cs.SC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide an overview of the SAT+CAS method that combines\nsatisfiability checkers (SAT solvers) and computer algebra systems (CAS) to\nresolve combinatorial conjectures, and present new results vis-\\`a-vis best\nmatrices. The SAT+CAS method is a variant of the\nDavis$\\unicode{8211}$Putnam$\\unicode{8211}$Logemann$\\unicode{8211}$Loveland\n$\\operatorname{DPLL}(T)$ architecture, where the $T$ solver is replaced by a\nCAS. We describe how the SAT+CAS method has been previously used to resolve\nmany open problems from graph theory, combinatorial design theory, and number\ntheory, showing that the method has broad applications across a variety of\nfields. Additionally, we apply the method to construct the largest best\nmatrices yet known and present new skew Hadamard matrices constructed from best\nmatrices. We show the best matrix conjecture (that best matrices exist in all\norders of the form $r^2+r+1$) which was previously known to hold for $r\\leq6$\nalso holds for $r=7$. We also confirmed the results of the exhaustive searches\nthat have been previously completed for $r\\leq6$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 03:46:54 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 04:12:30 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Bright", "Curtis", ""], ["\u0110okovi\u0107", "Dragomir \u017d.", ""], ["Kotsireas", "Ilias", ""], ["Ganesh", "Vijay", ""]]}, {"id": "1907.05000", "submitter": "Vu Hoang Nguyen Phan", "authors": "Jeffrey M. Dudek, Vu H. N. Phan, Moshe Y. Vardi", "title": "ADDMC: Weighted Model Counting with Algebraic Decision Diagrams", "comments": "Presented at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to compute exact literal-weighted model counts of\nBoolean formulas in Conjunctive Normal Form. Our algorithm employs dynamic\nprogramming and uses Algebraic Decision Diagrams as the primary data structure.\nWe implement this technique in ADDMC, a new model counter. We empirically\nevaluate various heuristics that can be used with ADDMC. We then compare ADDMC\nto state-of-the-art exact weighted model counters (Cachet, c2d, d4, and\nminiC2D) on 1914 standard model counting benchmarks and show that ADDMC\nsignificantly improves the virtual best solver.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 05:21:10 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 05:42:12 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Dudek", "Jeffrey M.", ""], ["Phan", "Vu H. N.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1907.05029", "submitter": "Natalia Moang\\u{a}", "authors": "Ioana Leu\\c{s}tean, Natalia Moang\\u{a} and Traian Florin\n  \\c{S}erb\\u{a}nu\\c{t}\\u{a}", "title": "From Hybrid Modal Logic to Matching Logic and Back", "comments": "In Proceedings FROM 2019, arXiv:1909.00584", "journal-ref": "EPTCS 303, 2019, pp. 16-31", "doi": "10.4204/EPTCS.303.2", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on our previous work on hybrid polyadic modal logic we identify\nmodal logic equivalents for Matching Logic, a logic for program specification\nand verification. This provides a rigorous way to transfer results between the\ntwo approaches, which should benefit both systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 07:34:10 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 13:06:13 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Leu\u015ftean", "Ioana", ""], ["Moang\u0103", "Natalia", ""], ["\u015eerb\u0103nu\u0163\u0103", "Traian Florin", ""]]}, {"id": "1907.05045", "submitter": "Bernhard Scholz", "authors": "David Zhao, Pavle Subotic, Bernhard Scholz", "title": "Provenance for Large-scale Datalog", "comments": "28 pages, 18 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programming languages such as Datalog have become popular as Domain\nSpecific Languages (DSLs) for solving large-scale, real-world problems, in\nparticular, static program analysis and network analysis. The logic\nspecifications which model analysis problems, process millions of tuples of\ndata and contain hundreds of highly recursive rules. As a result, they are\nnotoriously difficult to debug. While the database community has proposed\nseveral data-provenance techniques that address the Declarative Debugging\nChallenge for Databases, in the cases of analysis problems, these\nstate-of-the-art techniques do not scale.\n  In this paper, we introduce a novel bottom-up Datalog evaluation strategy for\ndebugging: our provenance evaluation strategy relies on a new provenance\nlattice that includes proof annotations, and a new fixed-point semantics for\nsemi-naive evaluation. A debugging query mechanism allows arbitrary provenance\nqueries, constructing partial proof trees of tuples with minimal height. We\nintegrate our technique into Souffle, a Datalog engine that synthesizes C++\ncode, and achieve high performance by using specialized parallel data\nstructures. Experiments are conducted with DOOP/DaCapo, producing proof\nannotations for tens of millions of output tuples. We show that our method has\na runtime overhead of 1.27x on average while being more flexible than existing\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:33:13 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Zhao", "David", ""], ["Subotic", "Pavle", ""], ["Scholz", "Bernhard", ""]]}, {"id": "1907.05070", "submitter": "Corto Mascle", "authors": "Corto Mascle and Martin Zimmermann", "title": "The Keys to Decidable HyperLTL Satisfiability: Small Models or Very\n  Simple Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HyperLTL, the extension of Linear Temporal Logic by trace quantifiers, is a\nuniform framework for expressing information flow policies by relating multiple\ntraces of a security-critical system. HyperLTL has been successfully applied to\nexpress fundamental security policies like noninterference and observational\ndeterminism, but has also found applications beyond security, e.g., distributed\nprotocols and coding theory. However, HyperLTL satisfiability is undecidable as\nsoon as there are existential quantifiers in the scope of a universal one. To\novercome this severe limitation to applicability, we investigate here\nrestricted variants of the satisfiability problem to pinpoint the decidability\nborder.\n  First, we restrict the space of admissible models and show decidability when\nrestricting the search space to models of bounded size or to finitely\nrepresentable ones. Second, we consider formulas with restricted nesting of\ntemporal operators and show that nesting depth one yields decidability for a\nslightly larger class of quantifier prefixes. We provide tight complexity\nbounds in almost all cases.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 09:22:58 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 16:20:24 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 22:23:41 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Mascle", "Corto", ""], ["Zimmermann", "Martin", ""]]}, {"id": "1907.05121", "submitter": "Daniele Gorla", "authors": "Michele Boreale, Daniele Gorla", "title": "Approximate Model Counting, Sparse XOR Constraints and Minimum Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of counting the number of models of a given Boolean formula has\nnumerous applications, including computing the leakage of deterministic\nprograms in Quantitative Information Flow. Model counting is a hard,\n#P-complete problem. For this reason, many approximate counters have been\ndeveloped in the last decade, offering formal guarantees of confidence and\naccuracy. A popular approach is based on the idea of using random XOR\nconstraints to, roughly, successively halving the solution set until no model\nis left: this is checked by invocations to a SAT solver. The effectiveness of\nthis procedure hinges on the ability of the SAT solver to deal with XOR\nconstraints, which in turn crucially depends on the length of such constraints.\nWe study to what extent one can employ sparse, hence short, constraints,\nkeeping guarantees of correctness. We show that the resulting bounds are\nclosely related to the geometry of the set of models, in particular to the\nminimum Hamming distance between models. We evaluate our theoretical results on\na few concrete formulae. Based on our findings, we finally discuss possible\ndirections for improvements of the current state of the art in approximate\nmodel counting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 11:38:44 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Boreale", "Michele", ""], ["Gorla", "Daniele", ""]]}, {"id": "1907.05244", "submitter": "Catalin Hritcu", "authors": "Kenji Maillard, Catalin Hritcu, Exequiel Rivas, Antoine Van Muylder", "title": "The Next 700 Relational Program Logics", "comments": "POPL 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the first framework for defining relational program logics for\narbitrary monadic effects. The framework is embedded within a relational\ndependent type theory and is highly expressive. At the semantic level, we\nprovide an algebraic presentation of relational specifications as a class of\nrelative monads, and link computations and specifications by introducing\nrelational effect observations, which map pairs of monadic computations to\nrelational specifications in a way that respects the algebraic structure. For\nan arbitrary relational effect observation, we generically define the core of a\nsound relational program logic, and explain how to complete it to a\nfull-fledged logic for the monadic effect at hand. We show that this generic\nframework can be used to define relational program logics for effects as\ndiverse as state, input-output, nondeterminism, and discrete probabilities. We,\nmoreover, show that by instantiating our framework with state and unbounded\niteration we can embed a variant of Benton's Relational Hoare Logic, and also\nsketch how to reconstruct Relational Hoare Type Theory. Finally, we identify\nand overcome conceptual challenges that prevented previous relational program\nlogics from properly dealing with control effects, and are the first to provide\na relational program logic for exceptions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 14:39:27 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 14:20:45 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 10:13:13 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Maillard", "Kenji", ""], ["Hritcu", "Catalin", ""], ["Rivas", "Exequiel", ""], ["Van Muylder", "Antoine", ""]]}, {"id": "1907.05706", "submitter": "Riccardo Treglia", "authors": "Ugo de'Liguoro and Riccardo Treglia", "title": "Intersection Types for the Computational lambda-Calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study polymorphic type assignment systems for untyped lambda-calculi with\neffects, based on Moggi's monadic approach. Moving from the abstract definition\nof monads, we introduce a version of the call-by-value computational\nlambda-calculus based on Wadler's variant with unit and bind combinators, and\nwithout let. We define a notion of reduction for the calculus and prove it\nconfluent, and also we relate our calculus to the original work by Moggi\nshowing that his untyped metalanguage can be interpreted and simulated in our\ncalculus. We then introduce an intersection type system inspired to Barendregt,\nCoppo and Dezani system for ordinary untyped lambda-calculus, establishing type\ninvariance under conversion, and provide models of the calculus via inverse\nlimit and filter model constructions and relate them. We prove soundness and\ncompleteness of the type system, together with subject reduction and expansion\nproperties. Finally, we introduce a notion of convergence, which is precisely\nrelated to reduction, and characterize convergent terms via their types.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 12:46:24 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 11:47:48 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 13:27:37 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["de'Liguoro", "Ugo", ""], ["Treglia", "Riccardo", ""]]}, {"id": "1907.05878", "submitter": "Adithya Murali", "authors": "Adithya Murali and P. Madhusudan", "title": "Augmenting Neural Nets with Symbolic Synthesis: Applications to Few-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose symbolic learning as extensions to standard inductive learning\nmodels such as neural nets as a means to solve few shot learning problems. We\ndevice a class of visual discrimination puzzles that calls for recognizing\nobjects and object relationships as well learning higher-level concepts from\nvery few images. We propose a two-phase learning framework that combines models\nlearned from large data sets using neural nets and symbolic first-order logic\nformulas learned from a few shot learning instance. We develop first-order\nlogic synthesis techniques for discriminating images by using symbolic search\nand logic constraint solvers. By augmenting neural nets with them, we develop\nand evaluate a tool that can solve few shot visual discrimination puzzles with\ninterpretable concepts.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 17:50:31 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Murali", "Adithya", ""], ["Madhusudan", "P.", ""]]}, {"id": "1907.05920", "submitter": "Steffen Smolka", "authors": "Steffen Smolka, Nate Foster, Justin Hsu, Tobias Kapp\\'e, Dexter Kozen,\n  and Alexandra Silva", "title": "Guarded Kleene Algebra with Tests: Verification of Uninterpreted\n  Programs in Nearly Linear Time", "comments": "Extended version with appendix", "journal-ref": "POPL 2020", "doi": "10.1145/3371129", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guarded Kleene Algebra with Tests (GKAT) is a variation on Kleene Algebra\nwith Tests (KAT) that arises by restricting the union ($+$) and iteration ($*$)\noperations from KAT to predicate-guarded versions. We develop the (co)algebraic\ntheory of GKAT and show how it can be efficiently used to reason about\nimperative programs. In contrast to KAT, whose equational theory is\nPSPACE-complete, we show that the equational theory of GKAT is (almost) linear\ntime. We also provide a full Kleene theorem and prove completeness for an\nanalogue of Salomaa's axiomatization of Kleene Algebra.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 18:54:04 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 09:21:41 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 08:06:44 GMT"}, {"version": "v4", "created": "Fri, 13 Dec 2019 17:45:10 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Smolka", "Steffen", ""], ["Foster", "Nate", ""], ["Hsu", "Justin", ""], ["Kapp\u00e9", "Tobias", ""], ["Kozen", "Dexter", ""], ["Silva", "Alexandra", ""]]}, {"id": "1907.05995", "submitter": "Yusuke Kawamoto", "authors": "Yusuke Kawamoto", "title": "Statistical Epistemic Logic", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-31175-9_20", "report-no": null, "categories": "cs.LO cs.FL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a modal logic for describing statistical knowledge, which we\ncall statistical epistemic logic. We propose a Kripke model dealing with\nprobability distributions and stochastic assignments, and show a stochastic\nsemantics for the logic. To our knowledge, this is the first semantics for\nmodal logic that can express the statistical knowledge dependent on\nnon-deterministic inputs and the statistical significance of observed results.\nBy using statistical epistemic logic, we express a notion of statistical\nsecrecy with a confidence level. We also show that this logic is useful to\nformalize statistical hypothesis testing and differential privacy in a simple\nand abstract manner.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 01:42:56 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Kawamoto", "Yusuke", ""]]}, {"id": "1907.06057", "submitter": "Giulio Guerrieri", "authors": "Beniamino Accattoli, Andrea Condoluci, Giulio Guerrieri, Claudio\n  Sacerdoti Coen", "title": "Crumbling Abstract Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the lambda-calculus with a construct for sharing, such as let\nexpressions, enables a special representation of terms: iterated applications\nare decomposed by introducing sharing points in between any two of them,\nreducing to the case where applications have only values as immediate subterms.\n  This work studies how such a crumbled representation of terms impacts on the\ndesign and the efficiency of abstract machines for call-by-value evaluation.\nAbout the design, it removes the need for data structures encoding the\nevaluation context, such as the applicative stack and the dump, that get\nencoded in the environment. About efficiency, we show that there is no\nslowdown, clarifying in particular a point raised by Kennedy, about the\npotential inefficiency of such a representation.\n  Moreover, we prove that everything smoothly scales up to the delicate case of\nopen terms, needed to implement proof assistants. Along the way, we also point\nout that continuation-passing style transformations--that may be alternatives\nto our representation--do not scale up to the open case.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 11:46:02 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Condoluci", "Andrea", ""], ["Guerrieri", "Giulio", ""], ["Coen", "Claudio Sacerdoti", ""]]}, {"id": "1907.06101", "submitter": "Andrea Condoluci", "authors": "Andrea Condoluci, Beniamino Accattoli, Claudio Sacerdoti Coen", "title": "Sharing Equality is Linear", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\lambda$-calculus is a handy formalism to specify the evaluation of\nhigher-order programs. It is not very handy, however, when one interprets the\nspecification as an execution mechanism, because terms can grow exponentially\nwith the number of $\\beta$-steps. This is why implementations of functional\nlanguages and proof assistants always rely on some form of sharing of subterms.\nThese frameworks however do not only evaluate $\\lambda$-terms, they also have\nto compare them for equality. In presence of sharing, one is actually\ninterested in the equality---or more precisely $\\alpha$-conversion---of the\nunderlying unshared $\\lambda$-terms. The literature contains algorithms for\nsuch a sharing equality, that are polynomial in the sizes of the shared terms.\nThis paper improves the bounds in the literature by presenting the first linear\ntime algorithm. As others before us, we are inspired by Paterson and Wegman's\nalgorithm for first-order unification, itself based on representing terms with\nsharing as DAGs, and sharing equality as bisimulation of DAGs.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 15:52:06 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Condoluci", "Andrea", ""], ["Accattoli", "Beniamino", ""], ["Coen", "Claudio Sacerdoti", ""]]}, {"id": "1907.06264", "submitter": "Fabio Mogavero Ph.D.", "authors": "Massimo Benerecetti and Daniele Dell'Erba and Fabio Mogavero", "title": "Solving Mean-Payoff Games via Quasi Dominions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for the solution of mean-payoff games that\nmerges together two seemingly unrelated concepts introduced in the context of\nparity games, small progress measures and quasi dominions. We show that the\nintegration of the two notions can be highly beneficial and significantly\nspeeds up convergence to the problem solution. Experiments show that the\nresulting algorithm performs orders of magnitude better than the\nasymptotically-best solution algorithm currently known, without sacrificing on\nthe worst-case complexity.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 19:11:51 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Benerecetti", "Massimo", ""], ["Dell'Erba", "Daniele", ""], ["Mogavero", "Fabio", ""]]}, {"id": "1907.06541", "submitter": "Erisa Karafili", "authors": "Jo\\~ao Rasga, Cristina Sernadas, Erisa Karafili, Luca Vigan\\`o", "title": "Time-Stamped Claim Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CR math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this paper is to define a logic for reasoning about\ndistributed time-stamped claims. Such a logic is interesting for theoretical\nreasons, i.e., as a logic per se, but also because it has a number of practical\napplications, in particular when one needs to reason about a huge amount of\npieces of evidence collected from different sources, where some of the pieces\nof evidence may be contradictory and some sources are considered to be more\ntrustworthy than others. We introduce the Time-Stamped Claim Logic including a\nsound and complete sequent calculus that allows one to reduce the size of the\ncollected set of evidence and removes inconsistencies, i.e., the logic ensures\nthat the result is consistent with respect to the trust relations considered.\nIn order to show how Time-Stamped Claim Logic can be used in practice, we\nconsider a concrete cyber-attribution case study.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 15:10:29 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 17:14:24 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Rasga", "Jo\u00e3o", ""], ["Sernadas", "Cristina", ""], ["Karafili", "Erisa", ""], ["Vigan\u00f2", "Luca", ""]]}, {"id": "1907.07020", "submitter": "Daniel Hausmann", "authors": "Daniel Hausmann and Lutz Schr\\\"oder", "title": "Quasipolynomial Computation of Nested Fixpoints", "comments": "extended version of conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the winning region of a parity game with $n$ nodes and\n$k$ priorities can be computed as a $k$-nested fixpoint of a suitable function;\nstraightforward computation of this nested fixpoint requires\n$\\mathcal{O}(n^{\\frac{k}{2}})$ iterations of the function. Calude et al.'s\nrecent quasipolynomial-time parity game solving algorithm essentially shows how\nto compute the same fixpoint in only quasipolynomially many iterations by\nreducing parity games to quasipolynomially sized safety games. Universal graphs\nhave been used to modularize this transformation of parity games to equivalent\nsafety games that are obtained by combining the original game with a universal\ngraph. We show that this approach naturally generalizes to the computation of\nsolutions of systems of \\emph{any} fixpoint equations over finite lattices;\nhence, the solution of fixpoint equation systems can be computed by\nquasipolynomially many iterations of the equations. We present applications to\nmodal fixpoint logics and games beyond relational semantics. For instance, the\nmodel checking problems for the energy $\\mu$-calculus, finite latticed\n$\\mu$-calculi, and the graded and the (two-valued) probabilistic $\\mu$-calculus\n-- with numbers coded in binary -- can be solved via nested fixpoints of\nfunctions that differ substantially from the function for parity games but\nstill can be computed in quasipolynomial time; our result hence implies that\nmodel checking for these $\\mu$-calculi is in QP. Moreover, we improve the\nexponent in known exponential bounds on satisfiability checking.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:12:11 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 00:42:01 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 10:50:38 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 12:00:50 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 21:33:31 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hausmann", "Daniel", ""], ["Schr\u00f6der", "Lutz", ""]]}, {"id": "1907.07042", "submitter": "Paolo Baldan", "authors": "Paolo Baldan and Alessandra Raffaet\\`a", "title": "Minimisation of Event Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event structures are fundamental models in concurrency theory, providing a\nrepresentation of events in computation and of their relations, notably\nconcurrency, conflict and causality. In this paper we present a theory of\nminimisation for event structures. Working in a class of event structures that\ngeneralises many stable event structure models in the literature (e.g., prime,\nasymmetric, flow and bundle event structures), we study a notion of\nbehaviour-preserving quotient, referred to as a folding, taking (hereditary)\nhistory preserving bisimilarity as a reference behavioural equivalence. We show\nthat for any event structure a folding producing a uniquely determined minimal\nquotient always exists. We observe that each event structure can be seen as the\nfolding of a prime event structure, and that all foldings between general event\nstructures arise from foldings of (suitably defined) corresponding prime event\nstructures. This gives a special relevance to foldings in the class of prime\nevent structures, which are studied in detail. We identify folding conditions\nfor prime and asymmetric event structures, and show that also prime event\nstructures always admit a unique minimal quotient (while this is not the case\nfor various other event structure models).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:48:54 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Baldan", "Paolo", ""], ["Raffaet\u00e0", "Alessandra", ""]]}, {"id": "1907.07283", "submitter": "Vikraman Choudhury", "authors": "Vikraman Choudhury and Neel Krishnaswami", "title": "Recovering Purity with Comonads and Capabilities", "comments": "Extended version of the ICFP 2020 paper", "journal-ref": null, "doi": "10.1145/3408993", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we take a pervasively effectful (in the style of ML) typed\nlambda calculus, and show how to extend it to permit capturing pure expressions\nwith types. Our key observation is that, just as the pure simply-typed lambda\ncalculus can be extended to support effects with a monadic type discipline, an\nimpure typed lambda calculus can be extended to support purity with a comonadic\ntype discipline. We establish the correctness of our type system via a simple\ndenotational model, which we call the capability space model. Our model\nformalizes the intuition common to systems programmers that the ability to\nperform effects should be controlled via access to a permission or capability,\nand that a program is capability-safe if it performs no effects that it does\nnot have a runtime capability for. We then identify the axiomatic categorical\nstructure that the capability space model validates, and use these axioms to\ngive a categorical semantics for our comonadic type system. We then give an\nequational theory (substitution and the call-by-value $\\beta$ and $\\eta$ laws)\nfor the imperative lambda calculus, and show its soundness relative to this\nsemantics. Finally, we give a translation of the pure simply-typed lambda\ncalculus into our comonadic imperative calculus, and show that any two terms\nwhich are $\\beta\\eta$-equal in the STLC are equal in the equational theory of\nthe comonadic calculus, establishing that pure programs can be mapped in an\nequation-preserving way into our imperative calculus.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 23:06:31 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 18:22:23 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 23:33:54 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Choudhury", "Vikraman", ""], ["Krishnaswami", "Neel", ""]]}, {"id": "1907.07368", "submitter": "Andreas Fellner", "authors": "Andreas Fellner, Mitra Tabaei Befrouei, Georg Weissenbacher", "title": "Mutation Testing with Hyperproperties", "comments": "To appear at the international conference of software engineering and\n  formal methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for model-based mutation-driven test case generation.\nMutants are generated by making small syntactical modifications to the model or\nsource code of the system under test. A test case kills a mutant if the\nbehavior of the mutant deviates from the original system when running the test.\nIn this work, we use hyperproperties-which allow to express relations between\nmultiple executions-to formalize different notions of killing for both\ndeterministic as well as non-deterministic models. The resulting\nhyperproperties are universal in the sense that they apply to arbitrary\nreactive models and mutants. Moreover, an off-the-shelf model checking tool for\nhyperproperties can be used to generate test cases. We evaluate our approach on\na number of models expressed in two different modeling languages by generating\ntests using a state-of-the-art mutation testing tool.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 07:43:50 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Fellner", "Andreas", ""], ["Befrouei", "Mitra Tabaei", ""], ["Weissenbacher", "Georg", ""]]}, {"id": "1907.07501", "submitter": "Andrew Pitts", "authors": "Andrew M. Pitts", "title": "Typal Heterogeneous Equality Types", "comments": "13 pages", "journal-ref": "ACM Trans. Comput. Logic 21, 3, Article 25 (March 2020), 10 pages", "doi": "10.1145/3379447", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The usual homogeneous form of equality type in Martin-L\\\"of Type Theory\ncontains identifications between elements of the same type. By contrast, the\nheterogeneous form of equality contains identifications between elements of\npossibly different types. This paper introduces a simple set of axioms for such\ntypes. The axioms are equivalent to the combination of systematic elimination\nrules for both forms of equality, albeit with typal (also known as\n\"propositional\") computation properties, together with Streicher's Axiom K, or\nequivalently, the principle of uniqueness of identity proofs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:20:16 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 17:27:29 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Pitts", "Andrew M.", ""]]}, {"id": "1907.07559", "submitter": "Lawrence Paulson", "authors": "Lawrence C. Paulson", "title": "Inductive Analysis of the Internet Protocol TLS", "comments": "19 pages", "journal-ref": "ACM Transactions on Information and System Security (TISSEC);\n  Volume 2 Issue 3, Aug. 1999, Pages 332-351", "doi": "10.1145/322510.322530", "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet browsers use security protocols to protect sensitive messages. An\ninductive analysis of TLS (a descendant of SSL 3.0) has been performed using\nthe theorem prover Isabelle. Proofs are based on higher-order logic and make no\nassumptions concerning beliefs or finiteness. All the obvious security goals\ncan be proved; session resumption appears to be secure even if old session keys\nhave been compromised. The proofs suggest minor changes to simplify the\nanalysis. TLS, even at an abstract level, is much more complicated than most\nprotocols that researchers have verified. Session keys are negotiated rather\nthan distributed, and the protocol has many optional parts. Nevertheless, the\nresources needed to verify TLS are modest: six man-weeks of effort and three\nminutes of processor time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:01:00 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Paulson", "Lawrence C.", ""]]}, {"id": "1907.07562", "submitter": "Nicolai Kraus", "authors": "Ambrus Kaposi, Andr\\'as Kov\\'acs, Nicolai Kraus", "title": "Shallow Embedding of Type Theory is Morally Correct", "comments": "36 pages, to be presented at the 13th International Conference on\n  Mathematics of Program Construction (MPC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple ways to formalise the metatheory of type theory. For some\npurposes, it is enough to consider specific models of a type theory, but\nsometimes it is necessary to refer to the syntax, for example in proofs of\ncanonicity and normalisation. One option is to embed the syntax deeply, by\nusing inductive definitions in a proof assistant. However, in this case the\nhandling of definitional equalities becomes technically challenging.\nAlternatively, we can reuse conversion checking in the metatheory by shallowly\nembedding the object theory. In this paper, we consider the standard model of a\ntype theoretic object theory in Agda. This model has the property that all of\nits equalities hold definitionally, and we can use it as a shallow embedding by\nbuilding expressions from the components of this model. However, if we are to\nreason soundly about the syntax with this setup, we must ensure that\ndistinguishable syntactic constructs do not become provably equal when\nshallowly embedded. First, we prove that shallow embedding is injective up to\ndefinitional equality, by modelling the embedding as a syntactic translation\ntargeting the metatheory. Second, we use an implementation hiding trick to\ndisallow illegal propositional equality proofs and constructions which do not\ncome from the syntax. We showcase our technique with very short formalisations\nof canonicity and parametricity for Martin-L\\\"of type theory. Our technique\nonly requires features which are available in all major proof assistants based\non dependent type theory.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:03:59 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kaposi", "Ambrus", ""], ["Kov\u00e1cs", "Andr\u00e1s", ""], ["Kraus", "Nicolai", ""]]}, {"id": "1907.07591", "submitter": "Lawrence Paulson", "authors": "Lawrence C. Paulson", "title": "Defining Functions on Equivalence Classes", "comments": "18 pages", "journal-ref": "ACM Trans. on Computational Logic 7 4 (2006), 658-675", "doi": "10.1145/1183278.1183280", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quotient construction defines an abstract type from a concrete type, using\nan equivalence relation to identify elements of the concrete type that are to\nbe regarded as indistinguishable. The elements of a quotient type are\n\\emph{equivalence classes}: sets of equivalent concrete values. Simple\ntechniques are presented for defining and reasoning about quotient\nconstructions, based on a general lemma library concerning functions that\noperate on equivalence classes. The techniques are applied to a definition of\nthe integers from the natural numbers, and then to the definition of a\nrecursive datatype satisfying equational constraints.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:38:36 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Paulson", "Lawrence C.", ""]]}, {"id": "1907.07761", "submitter": "Malte Schmitz", "authors": "Martin Leucker, C\\'esar S\\'anchez, Torben Scheffel, Malte Schmitz,\n  Daniel Thoma", "title": "Runtime Verification For Timed Event Streams With Partial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime Verification (RV) studies how to analyze execution traces of a system\nunder observation. Stream Runtime Verification (SRV) applies stream\ntransformations to obtain information from observed traces. Incomplete traces\nwith information missing in gaps pose a common challenge when applying RV and\nSRV techniques to real-world systems as RV approaches typically require the\ncomplete trace without missing parts. This paper presents a solution to perform\nSRV on incomplete traces based on abstraction. We use TeSSLa as specification\nlanguage for non-synchronized timed event streams and define abstract event\nstreams representing the set of all possible traces that could have occurred\nduring gaps in the input trace. We show how to translate a TeSSLa specification\nto its abstract counterpart that can propagate gaps through the transformation\nof the input streams and thus generate sound outputs even if the input streams\ncontain gaps and events with imprecise values. The solution has been\nimplemented as a set of macros for the original TeSSLa and an empirical\nevaluation shows the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:27:44 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Leucker", "Martin", ""], ["S\u00e1nchez", "C\u00e9sar", ""], ["Scheffel", "Torben", ""], ["Schmitz", "Malte", ""], ["Thoma", "Daniel", ""]]}, {"id": "1907.07767", "submitter": "Andrey Nechesov", "authors": "Andrey Nechesov", "title": "Delta -- new logic programming language and Delta-methodology for\n  p-computable programs on Turing Complete Languages", "comments": "Delta language is a new direction in the development of the theory of\n  semantic programming. Delta-methodology it's new way for p-computable\n  programs on Turing complete languages. Sobolev Institute of Mathematics.\n  Novosibirsk. Paper 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In paper describes the new logic programming language Delta, which have a\nmany good properties. Delta-programs is p-computable, verifiable and can\ntranslation on other languages. Also we describe the Delta-methodology for\nconstructing p-computable programs in high-level languages such as PHP, Java,\nJavaScript, C++, Pascal, Delphi, Python, Solidity and other. We would like to\nespecially note the use of the Delta methodology for creating Smart Contracts\nand for Internet of things. We change the concept of the formula and define\nD-formulas(or Delta programs) are special list-formulas. Then we define the\nexecution of a program how is the process of checking truth D-formula on a\ndynamic model. Main idea our paper consider program how list-formula from\nanother formulas on dynamic models. And we created by iterations new\nDelta-programs use simple base formulas for this. Also we entered a dynamic\nmodels how models where we save final values of variables when check formula on\nthis model.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 12:18:50 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Nechesov", "Andrey", ""]]}, {"id": "1907.07815", "submitter": "Rupert H\\\"olzl", "authors": "Rupert H\\\"olzl and Christopher P. Porter", "title": "Degrees of Randomized Computability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey we discuss work of Levin and V'yugin on collections of\nsequences that are non-negligible in the sense that they can be computed by a\nprobabilistic algorithm with positive probability. More precisely, Levin and\nV'yugin introduced an ordering on collections of sequences that are closed\nunder Turing equivalence. Roughly speaking, given two such collections\n$\\mathcal{A}$ and $\\mathcal{B}$, $\\mathcal{A}$ is below $\\mathcal{B}$ in this\nordering if $\\mathcal{A}\\setminus\\mathcal{B}$ is negligible. The degree\nstructure associated with this ordering, the Levin-V'yugin degrees (or\nLV-degrees), can be shown to be a Boolean algebra, and in fact a measure\nalgebra.\n  We demonstrate the interactions of this work with recent results in\ncomputability theory and algorithmic randomness: First, we recall the\ndefinition of the Levin-V'yugin algebra and identify connections between its\nproperties and classical properties from computability theory. In particular,\nwe apply results on the interactions between notions of randomness and Turing\nreducibility to establish new facts about specific LV-degrees, such as the\nLV-degree of the collection of 1-generic sequences, that of the collection of\nsequences of hyperimmune degree, and those collections corresponding to various\nnotions of effective randomness. Next, we provide a detailed explanation of a\ncomplex technique developed by V'yugin that allows the construction of\nsemi-measures into which computability-theoretic properties can be encoded. We\nprovide two examples of the use of this technique by explicating a result of\nV'yugin's about the LV-degree of the collection of Martin-L\\\"of random\nsequences and extending the result to the LV-degree of the collection of\nsequences of DNC degree.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:50:01 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 10:24:04 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 11:49:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["H\u00f6lzl", "Rupert", ""], ["Porter", "Christopher P.", ""]]}, {"id": "1907.07885", "submitter": "Abhishek Kr Singh", "authors": "Suneel Sarswat and Abhishek Kr Singh", "title": "Formal verification of trading in financial markets", "comments": "Preprint of 12 pages in lipicsv2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.FL cs.GT cs.SC q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a formal framework for analyzing trades in financial markets. An\nexchange is where multiple buyers and sellers participate to trade. These days,\nall big exchanges use computer algorithms that implement double sided auctions\nto match buy and sell requests and these algorithms must abide by certain\nregulatory guidelines. For example, market regulators enforce that a matching\nproduced by exchanges should be \\emph{fair}, \\emph{uniform} and\n\\emph{individual rational}. To verify these properties of trades, we first\nformally define these notions in a theorem prover and then give formal proofs\nof relevant results on matchings. Finally, we use this framework to verify\nproperties of two important classes of double sided auctions. All the\ndefinitions and results presented in this paper are completely formalised in\nthe Coq proof assistant without adding any additional axioms to it.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 05:50:29 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sarswat", "Suneel", ""], ["Singh", "Abhishek Kr", ""]]}, {"id": "1907.07974", "submitter": "Pedro Ribeiro", "authors": "Pedro Ribeiro, James Baxter, Ana Cavalcanti", "title": "Priorities in tock-CSP", "comments": "9 pages, submitted to Information Processing Letters, July 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $tock$-CSP encoding embeds a rich and flexible approach to modelling\ndiscrete timed behaviours in CSP where the event $tock$ is interpreted to mark\nthe passage of time. The model checker FDR provides tailored support for\n$tock$-CSP, including a prioritisation operator that has typically been used to\nensure maximal progress, where time only advances after internal activity has\nstabilised. Prioritisation may also be used on its own right as a modelling\nconstruct. Its operational semantics, however, is only congruent over the most\ndiscriminating semantic model of CSP: the finite-linear model. To enable sound\nand compositional reasoning in a $tock$-CSP setting, we calculate a\ndenotational definition for prioritisation. For that we establish a Galois\nconnection between a specialisation of the finite-linear model, with $tock$ and\n$\\checkmark$, that signals termination, as special events, and\n$\\checkmark$-$tock$-CSP, a model for $tock$-CSP that captures termination,\ndeadlines, and is adequate for reasoning about timed refinement. Our results\nare mechanised using Isabelle/HOL.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 10:38:19 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Ribeiro", "Pedro", ""], ["Baxter", "James", ""], ["Cavalcanti", "Ana", ""]]}, {"id": "1907.08251", "submitter": "Chaoqiang Deng", "authors": "Chaoqiang Deng, Patrick Cousot", "title": "Responsibility Analysis by Abstract Interpretation", "comments": "This is the extended version (33 pages) of a paper to be appeared in\n  the Static Analysis Symposium (SAS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a behavior of interest in the program, statically determining the\ncorresponding responsible entity is a task of critical importance, especially\nin program security. Classical static analysis techniques (e.g. dependency\nanalysis, taint analysis, slicing, etc.) assist programmers in narrowing down\nthe scope of responsibility, but none of them can explicitly identify the\nresponsible entity. Meanwhile, the causality analysis is generally not\npertinent for analyzing programs, and the structural equations model (SEM) of\nactual causality misses some information inherent in programs, making its\nanalysis on programs imprecise. In this paper, a novel definition of\nresponsibility based on the abstraction of event trace semantics is proposed,\nwhich can be applied in program security and other scientific fields. Briefly\nspeaking, an entity ER is responsible for behavior B, if and only if ER is free\nto choose its input value, and such a choice is the first one that ensures the\noccurrence of B in the forthcoming execution. Compared to current analysis\nmethods, the responsibility analysis is more precise. In addition, our\ndefinition of responsibility takes into account the cognizance of the observer,\nwhich, to the best of our knowledge, is a new innovative idea in program\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 19:03:38 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Deng", "Chaoqiang", ""], ["Cousot", "Patrick", ""]]}, {"id": "1907.08257", "submitter": "Joel Ouaknine", "authors": "Nathana\\\"el Fijalkow, Engel Lefaucheux, Pierre Ohlmann, Jo\\\"el\n  Ouaknine, Amaury Pouly, James Worrell", "title": "On the Monniaux Problem in Abstract Interpretation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32304-2_9", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Monniaux Problem in abstract interpretation asks, roughly speaking,\nwhether the following question is decidable: given a program $P$, a safety\n(\\emph{e.g.}, non-reachability) specification $\\varphi$, and an abstract domain\nof invariants $\\mathcal{D}$, does there exist an inductive invariant $I$ in\n$\\mathcal{D}$ guaranteeing that program $P$ meets its specification $\\varphi$.\nThe Monniaux Problem is of course parameterised by the classes of programs and\ninvariant domains that one considers. In this paper, we show that the Monniaux\nProblem is undecidable for unguarded affine programs and semilinear invariants\n(unions of polyhedra). Moreover, we show that decidability is recovered in the\nimportant special case of simple linear loops.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 19:20:19 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Fijalkow", "Nathana\u00ebl", ""], ["Lefaucheux", "Engel", ""], ["Ohlmann", "Pierre", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Pouly", "Amaury", ""], ["Worrell", "James", ""]]}, {"id": "1907.08335", "submitter": "EPTCS", "authors": "Lawrence S. Moss", "title": "Proceedings Seventeenth Conference on Theoretical Aspects of Rationality\n  and Knowledge", "comments": null, "journal-ref": "EPTCS 297, 2019", "doi": "10.4204/EPTCS.297", "report-no": null, "categories": "cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the proceedings of the Seventeenth conference on Theoretical Aspects\nof Rationality and Knowledge, 17-19 July 2019, Institut de Recherche en\nInformatique de Toulouse (IRIT), Toulouse University Toulouse, France. The\nmission of the TARK conferences is to bring together researchers from a wide\nvariety of fields, including Artificial Intelligence, Cryptography, Distributed\nComputing, Economics and Game Theory, Linguistics, Philosophy, and Psychology,\nin order to further our understanding of interdisciplinary issues involving\nreasoning about rationality and knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 01:35:47 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Moss", "Lawrence S.", ""]]}, {"id": "1907.08336", "submitter": "Marcel Jackson G", "authors": "Marcel Jackson and Tim Stokes", "title": "Override and update", "comments": null, "journal-ref": "Journal of Pure and Applied Algebra 225 (2021)", "doi": "10.1016/j.jpaa.2020.106532", "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Override and update are natural constructions for combining partial\nfunctions, which arise in various program specification contexts. We use an\nunexpected connection with combinatorial geometry to provide a complete finite\nsystem of equational axioms for the first order theory of the override and\nupdate constructions on partial functions, resolving the main unsolved problem\nin the area.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 01:43:48 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 21:54:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Jackson", "Marcel", ""], ["Stokes", "Tim", ""]]}, {"id": "1907.08368", "submitter": "Karol P\\k{a}k", "authors": "Chad E. Brown, Karol P\\k{a}k", "title": "A Tale of Two Set Theories", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-23250-4_4", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the relationship between two versions of Tarski-Grothendieck set\ntheory: the first-order set theory of Mizar and the higher-order set theory of\nEgal. We show how certain higher-order terms and propositions in Egal have\nequivalent first-order presentations. We then prove Tarski's Axiom A (an axiom\nin Mizar) in Egal and construct a Grothendieck Universe operator (a primitive\nwith axioms in Egal) in Mizar.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 08:21:49 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Brown", "Chad E.", ""], ["P\u0105k", "Karol", ""]]}, {"id": "1907.08470", "submitter": "Erich Gr\\\"adel", "authors": "Erich Gr\\\"adel and Val Tannen", "title": "Provenance Analysis for Logic and Games", "comments": null, "journal-ref": "Moscow J. Comb. Number Th. 9 (2020) 203-228", "doi": "10.2140/moscow.2020.9.203", "report-no": null, "categories": "cs.LO cs.DB math.LO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A model checking computation checks whether a given logical sentence is true\nin a given finite structure. Provenance analysis abstracts from such a\ncomputation mathematical information on how the result depends on the atomic\ndata that describe the structure. In database theory, provenance analysis by\ninterpretations in commutative semirings has been rather succesful for positive\nquery languages (such a unions of conjunctive queries, positive relational\nalgebra, or datalog). However, it did not really offer an adequate treatment of\nnegation or missing information.\n  Here we propose a new approach for the provenance analysis of logics with\nnegation, such as first-order logic and fixed-point logics. It is closely\nrelated to a provenance analysis of the associated model-checking games, and\nbased on new semirings of dual-indeterminate polynomials or dual-indeterminate\nformal power series. These are obtained by taking quotients of traditional\nprovenance semirings by congruences that are generated by products of positive\nand negative provenance tokens. Beyond the use for model-checking problems in\nlogics, provenance analysis of games is of independent interest. Provenance\nvalues in games provide detailed information about the number and properties of\nthe strategies of the players, far beyond the question whether or not a player\nhas a winning strategy from a given position.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 11:43:41 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 12:06:00 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Gr\u00e4del", "Erich", ""], ["Tannen", "Val", ""]]}, {"id": "1907.08491", "submitter": "Jip Spel", "authors": "Jip Spel, Sebastian Junges, Joost-Pieter Katoen", "title": "Are Parametric Markov Chains Monotonic?", "comments": "Extended version of ATVA 2019 paper with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple algorithm to check whether reachability\nprobabilities in parametric Markov chains are monotonic in (some of) the\nparameters. The idea is to construct - only using the graph structure of the\nMarkov chain and local transition probabilities - a pre-order on the states.\nOur algorithm cheaply checks a sufficient condition for monotonicity.\nExperiments show that monotonicity in several benchmarks is automatically\ndetected, and monotonicity can speed up parameter synthesis up to orders of\nmagnitude faster than a symbolic baseline.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 12:52:39 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Spel", "Jip", ""], ["Junges", "Sebastian", ""], ["Katoen", "Joost-Pieter", ""]]}, {"id": "1907.08820", "submitter": "Pablo Barenbaum", "authors": "Pablo Barenbaum, Gonzalo Ciruelos", "title": "Factoring Derivation Spaces via Intersection Types (Extended Version)", "comments": null, "journal-ref": "Lecture Notes in Computer Science 11275, Springer 2018", "doi": "10.1007/978-3-030-02768-1\\_2", "report-no": null, "categories": "cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In typical non-idempotent intersection type systems, proof normalization is\nnot confluent. In this paper we introduce a confluent non-idempotent\nintersection type system for the lambda-calculus. Typing derivations are\npresented using proof term syntax. The system enjoys good properties: subject\nreduction, strong normalization, and a very regular theory of residuals. A\ncorrespondence with the lambda-calculus is established by simulation theorems.\nThe machinery of non-idempotent intersection types allows us to track the usage\nof resources required to obtain an answer. In particular, it induces a notion\nof garbage: a computation is garbage if it does not contribute to obtaining an\nanswer. Using these notions, we show that the derivation space of a lambda-term\nmay be factorized using a variant of the Grothendieck construction for\nsemilattices. This means, in particular, that any derivation in the\nlambda-calculus can be uniquely written as a garbage-free prefix followed by\ngarbage.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 14:57:40 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Barenbaum", "Pablo", ""], ["Ciruelos", "Gonzalo", ""]]}, {"id": "1907.08834", "submitter": "James Cheney", "authors": "S\\'andor Bartha and James Cheney", "title": "Towards meta-interpretive learning of programming language semantics", "comments": "ILP 2019, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new application for inductive logic programming: learning the\nsemantics of programming languages from example evaluations. In this short\npaper, we explored a simplified task in this domain using the Metagol\nmeta-interpretive learning system. We highlighted the challenging aspects of\nthis scenario, including abstracting over function symbols, nonterminating\nexamples, and learning non-observed predicates, and proposed extensions to\nMetagol helpful for overcoming these challenges, which may prove useful in\nother domains.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 16:39:06 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bartha", "S\u00e1ndor", ""], ["Cheney", "James", ""]]}, {"id": "1907.09097", "submitter": "EPTCS", "authors": "Krzysztof R. Apt (Centrum Wiskunde & Informatica, Amsterdam, The\n  Netherlands and University of Warsaw, Warsaw, Poland), Dominik Wojtczak\n  (University of Liverpool, Liverpool, UK)", "title": "Open Problems in a Logic of Gossips", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 1-18", "doi": "10.4204/EPTCS.297.1", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gossip protocols are programs used in a setting in which each agent holds a\nsecret and the aim is to reach a situation in which all agents know all\nsecrets. Such protocols rely on a point-to-point or group communication.\nDistributed epistemic gossip protocols use epistemic formulas in the component\nprograms for the agents. The advantage of the use of epistemic logic is that\nthe resulting protocols are very concise and amenable for a simple\nverification.\n  Recently, we introduced a natural modal logic that allows one to express\ndistributed epistemic gossip protocols and to reason about their correctness.\nWe proved that the resulting protocols are implementable and that all aspects\nof their correctness, including termination, are decidable. To establish these\nresults we showed that both the definition of semantics and of truth of the\nunderlying logic are decidable. We also showed that the analogous results hold\nfor an extension of this logic with the 'common knowledge' operator.\n  However, several, often deceptively simple, questions about this logic and\nthe corresponding gossip protocols remain open. The purpose of this paper is to\nlist and elucidate these questions and provide for them an appropriate\nbackground information in the form of partial of related results.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:12:30 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Apt", "Krzysztof R.", "", "Centrum Wiskunde & Informatica, Amsterdam, The\n  Netherlands and University of Warsaw, Warsaw, Poland"], ["Wojtczak", "Dominik", "", "University of Liverpool, Liverpool, UK"]]}, {"id": "1907.09098", "submitter": "EPTCS", "authors": "Adam Bjorndahl (Carnegie Mellon University), Ayb\\\"uke \\\"Ozg\\\"un (ILLC,\n  University of Amsterdam & Arch\\'e, University of St. Andrews)", "title": "Uncertainty About Evidence", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 68-81", "doi": "10.4204/EPTCS.297.5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a logical framework for reasoning about knowledge and evidence in\nwhich the agent may be uncertain about how to interpret their evidence. Rather\nthan representing an evidential state as a fixed subset of the state space, our\nmodels allow the set of possible worlds that a piece of evidence corresponds to\nto vary from one possible world to another, and therefore itself be the subject\nof uncertainty. Such structures can be viewed as (epistemically motivated)\ngeneralizations of topological spaces. In this context, there arises a natural\ndistinction between what is actually entailed by the evidence and what the\nagent knows is entailed by the evidence -- with the latter, in general, being\nmuch weaker. We provide a sound and complete axiomatization of the\ncorresponding bi-modal logic of knowledge and evidence entailment, and\ninvestigate some natural extensions of this core system, including the addition\nof a belief modality and its interaction with evidence interpretation and\nentailment, and the addition of a \"knowability\" modality interpreted via a\n(generalized) interior operator.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:13:41 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bjorndahl", "Adam", "", "Carnegie Mellon University"], ["\u00d6zg\u00fcn", "Ayb\u00fcke", "", "ILLC,\n  University of Amsterdam & Arch\u00e9, University of St. Andrews"]]}, {"id": "1907.09099", "submitter": "EPTCS", "authors": "Giacomo Bonanno (University of California Davis)", "title": "Credible Information, Allowable Information and Belief Revision --\n  Extended Abstract", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 82-90", "doi": "10.4204/EPTCS.297.6", "report-no": null, "categories": "cs.LO cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an earlier paper [Rational choice and AGM belief revision, Artificial\nIntelligence, 2009] a correspondence was established between the choice\nstructures of revealed-preference theory (developed in economics) and the\nsyntactic belief revision functions of the AGM theory (developed in philosophy\nand computer science). In this paper we extend the re-interpretation of (a\ngeneralized notion of) choice structure in terms of belief revision by adding:\n(1) the possibility that an item of \"information\" might be discarded as not\ncredible (thus dropping the AGM success axiom) and (2) the possibility that an\nitem of information, while not accepted as fully credible, may still be \"taken\nseriously\" (we call such items of information \"allowable\"). We establish a\ncorrespondence between generalized choice structures (GCS) and AGM belief\nrevision; furthermore, we provide a syntactic analysis of the proposed notion\nof belief revision, which we call filtered belief revision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:14:00 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bonanno", "Giacomo", "", "University of California Davis"]]}, {"id": "1907.09101", "submitter": "EPTCS", "authors": "Yifeng Ding (University of California, Berkeley), Wesley H. Holliday\n  (University of California, Berkeley), Cedegao Zhang (University of\n  California, Berkeley)", "title": "When Do Introspection Axioms Matter for Multi-Agent Epistemic Reasoning?", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 121-139", "doi": "10.4204/EPTCS.297.9", "report-no": null, "categories": "cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early literature on epistemic logic in philosophy focused on reasoning\nabout the knowledge or belief of a single agent, especially on controversies\nabout \"introspection axioms\" such as the 4 and 5 axioms. By contrast, the later\nliterature on epistemic logic in computer science and game theory has focused\non multi-agent epistemic reasoning, with the single-agent 4 and 5 axioms\nlargely taken for granted. In the relevant multi-agent scenarios, it is often\nimportant to reason about what agent A believes about what agent B believes\nabout what agent A believes; but it is rarely important to reason just about\nwhat agent A believes about what agent A believes. This raises the question of\nthe extent to which single-agent introspection axioms actually matter for\nmulti-agent epistemic reasoning. In this paper, we formalize and answer this\nquestion. To formalize the question, we first define a set of multi-agent\nformulas that we call agent-alternating formulas, including formulas like Box_a\nBox_b Box_a p but not formulas like Box_a Box_a p. We then prove, for the case\nof belief, that if one starts with multi-agent K or KD, then adding both the 4\nand 5 axioms (or adding the B axiom) does not allow the derivation of any new\nagent-alternating formulas -- in this sense, introspection axioms do not\nmatter. By contrast, we show that such conservativity results fail for\nknowledge and multi-agent KT, though they hold with respect to a smaller class\nof agent-nonrepeating formulas.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:14:46 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Ding", "Yifeng", "", "University of California, Berkeley"], ["Holliday", "Wesley H.", "", "University of California, Berkeley"], ["Zhang", "Cedegao", "", "University of\n  California, Berkeley"]]}, {"id": "1907.09102", "submitter": "EPTCS", "authors": "Adam Dominiak (Virginia Tech), Burkhard Schipper (University of\n  California, Davis)", "title": "Common Belief in Choquet Rationality and Ambiguity Attitudes -- Extended\n  Abstract", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 140-154", "doi": "10.4204/EPTCS.297.10", "report-no": null, "categories": "cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider finite games in strategic form with Choquet expected utility.\nUsing the notion of (unambiguously) believed, we define Choquet\nrationalizability and characterize it by Choquet rationality and common beliefs\nin Choquet rationality in the universal capacity type space in a purely\nmeasurable setting. We also show that Choquet rationalizability is equivalent\nto iterative elimination of strictly dominated actions (not in the original\ngame but) in an extended game. This allows for computation of Choquet\nrationalizable actions without the need to first compute Choquet integrals.\nChoquet expected utility allows us to investigate common belief in ambiguity\nlove/aversion. We show that ambiguity love/aversion leads to smaller/larger\nChoquet rationalizable sets of action profiles.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:15:07 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Dominiak", "Adam", "", "Virginia Tech"], ["Schipper", "Burkhard", "", "University of\n  California, Davis"]]}, {"id": "1907.09103", "submitter": "EPTCS", "authors": "Nourhan Ehab, Haythem O. Ismail", "title": "A Unified Algebraic Framework for Non-Monotonicity", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 155-174", "doi": "10.4204/EPTCS.297.11", "report-no": null, "categories": "cs.LO cs.AI cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous research effort has been dedicated over the years to thoroughly\ninvestigate non-monotonic reasoning. With the abundance of non-monotonic\nlogical formalisms, a unified theory that enables comparing the different\napproaches is much called for. In this paper, we present an algebraic graded\nlogic we refer to as LogAG capable of encompassing a wide variety of\nnon-monotonic formalisms. We build on Lin and Shoham's argument systems first\ndeveloped to formalize non-monotonic commonsense reasoning. We show how to\nencode argument systems as LogAG theories, and prove that LogAG captures the\nnotion of belief spaces in argument systems. Since argument systems capture\ndefault logic, autoepistemic logic, the principle of negation as failure, and\ncircumscription, our results show that LogAG captures the before-mentioned\nnon-monotonic logical formalisms as well. Previous results show that LogAG\nsubsumes possibilistic logic and any non-monotonic inference relation\nsatisfying Makinson's rationality postulates. In this way, LogAG provides a\npowerful unified framework for non-monotonicity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:15:28 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Ehab", "Nourhan", ""], ["Ismail", "Haythem O.", ""]]}, {"id": "1907.09104", "submitter": "EPTCS", "authors": "Satoshi Fukuda (Department of Decision Sciences and IGIER, Bocconi\n  University)", "title": "On the Consistency among Prior, Posteriors, and Information Sets\n  (Extended Abstract)", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 189-205", "doi": "10.4204/EPTCS.297.13", "report-no": null, "categories": "cs.GT cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies implications of the consistency conditions among prior,\nposteriors, and information sets on introspective properties of qualitative\nbelief induced from information sets. The main result reformulates the\nconsistency conditions as: (i) the information sets, without any assumption,\nalmost surely form a partition; and (ii) the posterior at a state is equal to\nthe Bayes conditional probability given the corresponding information set.\nImplications are as follows. First, each posterior is uniquely determined.\nSecond, qualitative belief reduces to fully introspective knowledge in a\n``standard'' environment. Thus, a care must be taken when one studies\nnon-veridical belief or non-introspective knowledge. Third, an information\npartition compatible with the consistency conditions is uniquely determined by\nthe posteriors. Fourth, qualitative and probability-one beliefs satisfy truth\naxiom almost surely. The paper also sheds light on how the additivity of the\nposteriors yields negative introspective properties of beliefs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:16:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fukuda", "Satoshi", "", "Department of Decision Sciences and IGIER, Bocconi\n  University"]]}, {"id": "1907.09105", "submitter": "EPTCS", "authors": "Malvin Gattinger (Bernoulli Institute, University of Groningen),\n  Yanjing Wang (Department of Philosophy, Peking University)", "title": "How to Agree without Understanding Each Other: Public Announcement Logic\n  with Boolean Definitions", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 206-220", "doi": "10.4204/EPTCS.297.14", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard epistemic logic, knowing that p is the same as knowing that p is\ntrue, but it does not say anything about understanding p or knowing its\nmeaning. In this paper, we present a conservative extension of Public\nAnnouncement Logic (PAL) in which agents have knowledge or belief about both\nthe truth values and the meanings of propositions. We give a complete\naxiomatization of PAL with Boolean Definitions and discuss various examples. An\nagent may understand a proposition without knowing its truth value or the other\nway round. Moreover, multiple agents can agree on something without agreeing on\nits meaning and vice versa.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:16:20 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Gattinger", "Malvin", "", "Bernoulli Institute, University of Groningen"], ["Wang", "Yanjing", "", "Department of Philosophy, Peking University"]]}, {"id": "1907.09106", "submitter": "EPTCS", "authors": "Joseph Y. Halpern (Cornell University), Rafael Pass (Cornell\n  University)", "title": "A Conceptually Well-Founded Characterization of Iterated Admissibility\n  Using an \"All I Know\" Operator", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 221-232", "doi": "10.4204/EPTCS.297.15", "report-no": null, "categories": "cs.GT cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brandenburger, Friedenberg, and Keisler provide an epistemic characterization\nof iterated admissibility (IA), also known as iterated deletion of weakly\ndominated strategies, where uncertainty is represented using LPSs\n(lexicographic probability sequences). Their characterization holds in a rich\nstructure called a complete structure, where all types are possible. In earlier\nwork, we gave a characterization of iterated admissibility using an \"all I\nknow\" operator, that captures the intuition that \"all the agent knows\" is that\nagents satisfy the appropriate rationality assumptions. That characterization\ndid not need complete structures and used probability structures, not LPSs.\nHowever, that characterization did not deal with Samuelson's conceptual concern\nregarding IA, namely, that at higher levels, players do not consider possible\nstrategies that were used to justify their choice of strategy at lower levels.\nIn this paper, we give a characterization of IA using the all I know operator\nthat does deal with Samuelson's concern. However, it uses LPSs. We then show\nhow to modify the characterization using notions of \"approximate belief\" and\n\"approximately all I know\" so as to deal with Samuelson's concern while still\nworking with probability structures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:16:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Halpern", "Joseph Y.", "", "Cornell University"], ["Pass", "Rafael", "", "Cornell\n  University"]]}, {"id": "1907.09112", "submitter": "EPTCS", "authors": "Roman Kuznets (TU Wien), Laurent Prosperi (ENS Paris-Saclay), Ulrich\n  Schmid (TU Wien), Krisztina Fruzsa (TU Wien)", "title": "Causality and Epistemic Reasoning in Byzantine Multi-Agent Systems", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 293-312", "doi": "10.4204/EPTCS.297.19", "report-no": null, "categories": "cs.MA cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality is an important concept both for proving impossibility results and\nfor synthesizing efficient protocols in distributed computing. For asynchronous\nagents communicating over unreliable channels, causality is well studied and\nunderstood. This understanding, however, relies heavily on the assumption that\nagents themselves are correct and reliable. We provide the first epistemic\nanalysis of causality in the presence of byzantine agents, i.e., agents that\ncan deviate from their protocol and, thus, cannot be relied upon. Using our new\nframework for epistemic reasoning in fault-tolerant multi-agent systems, we\ndetermine the byzantine analog of the causal cone and describe a communication\nstructure, which we call a multipede, necessary for verifying preconditions for\nactions in this setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:18:05 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Kuznets", "Roman", "", "TU Wien"], ["Prosperi", "Laurent", "", "ENS Paris-Saclay"], ["Schmid", "Ulrich", "", "TU Wien"], ["Fruzsa", "Krisztina", "", "TU Wien"]]}, {"id": "1907.09113", "submitter": "EPTCS", "authors": "Grzegorz Lisowski (University of Warwick), Sylvie Doutre (University\n  of Toulouse), Umberto Grandi (University of Toulouse)", "title": "Aggregation in Value-Based Argumentation Frameworks", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 313-331", "doi": "10.4204/EPTCS.297.20", "report-no": null, "categories": "cs.MA cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value-based argumentation enhances a classical abstract argumentation graph -\nin which arguments are modelled as nodes connected by directed arrows called\nattacks - with labels on arguments, called values, and an ordering on values,\ncalled audience, to provide a more fine-grained justification of the attack\nrelation. With more than one agent facing such an argumentation problem, agents\nmay differ in their ranking of values. When needing to reach a collective view,\nsuch agents face a dilemma between two equally justifiable approaches:\naggregating their views at the level of values, or aggregating their attack\nrelations, remaining therefore at the level of the graphs. We explore the\nstrenghts and limitations of both approaches, employing techniques from\npreference aggregation and graph aggregation, and propose a third possibility\naggregating rankings extracted from given attack relations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:18:29 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lisowski", "Grzegorz", "", "University of Warwick"], ["Doutre", "Sylvie", "", "University\n  of Toulouse"], ["Grandi", "Umberto", "", "University of Toulouse"]]}, {"id": "1907.09114", "submitter": "EPTCS", "authors": "Emiliano Lorini (IRIT-CNRS, Toulouse University, France)", "title": "Exploiting Belief Bases for Building Rich Epistemic Structures", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 332-353", "doi": "10.4204/EPTCS.297.21", "report-no": null, "categories": "cs.GT cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a semantics for epistemic logic exploiting a belief base\nabstraction. Differently from existing Kripke-style semantics for epistemic\nlogic in which the notions of possible world and epistemic alternative are\nprimitive, in the proposed semantics they are non-primitive but are defined\nfrom the concept of belief base. We show that this semantics allows us to\ndefine the universal epistemic model in a simpler and more compact way than\nexisting inductive constructions of it. We provide (i) a number of semantic\nequivalence results for both the basic epistemic language with \"individual\nbelief\" operators and its extension by the notion of \"only believing\", and (ii)\na lower bound complexity result for epistemic logic model checking relative to\nthe universal epistemic model.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:18:48 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lorini", "Emiliano", "", "IRIT-CNRS, Toulouse University, France"]]}, {"id": "1907.09124", "submitter": "EPTCS", "authors": "Pablo F. Castro, Valentin Cassano, Raul Fervari, Carlos Areces", "title": "An Algebraic Approach for Action Based Default Reasoning", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 91-105", "doi": "10.4204/EPTCS.297.7", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, we assume that an action is permitted simply because it is not\nexplicitly forbidden; or, similarly, that an action is forbidden simply because\nit is not explicitly permitted. This kind of assumptions appear, e.g., in\nautonomous computing systems where decisions must be taken in the presence of\nan incomplete set of norms regulating a particular scenario. Combining default\nand deontic reasoning over actions allows us to formally reason about such\nassumptions. With this in mind, we propose a logical formalism for default\nreasoning over a deontic action logic. The novelty of our approach is twofold.\nFirst, our formalism for default reasoning deals with actions and action\noperators, and it is based on the deontic action logic originally proposed by\nSegerberg. Second, inspired by Segerberg's approach, we use tools coming from\nthe theory of Boolean Algebra. These tools allow us to extend Segerberg's\nalgebraic completeness result to the setting of Default Logics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:57:15 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Castro", "Pablo F.", ""], ["Cassano", "Valentin", ""], ["Fervari", "Raul", ""], ["Areces", "Carlos", ""]]}, {"id": "1907.09212", "submitter": "Giovambattista Ianni", "authors": "Francesco Calimeri, Giovambattista Ianni, Francesco Pacenza, Simona\n  Perri and Jessica Zangari", "title": "Incremental Answer Set Programming with Overgrounding", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 957-973", "doi": "10.1017/S1471068419000292", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated executions of reasoning tasks for varying inputs are necessary in\nmany applicative settings, such as stream reasoning. In this context, we\npropose an incremental grounding approach for the answer set semantics. We\nfocus on the possibility of generating incrementally larger ground logic\nprograms equivalent to a given non-ground one; so called overgrounded programs\ncan be reused in combination with deliberately many different sets of inputs.\nUpdating overgrounded programs requires a small effort, thus making the\ninstantiation of logic programs considerably faster when grounding is repeated\non a series of inputs similar to each other. Notably, the proposed approach\nworks \"under the hood\", relieving designers of logic programs from controlling\ntechnical aspects of grounding engines and answer set systems. In this work we\npresent the theoretical basis of the proposed incremental grounding technique,\nwe illustrate the consequent repeated evaluation strategy and report about our\nexperiments. This paper is under consideration in Theory and Practice of Logic\nProgramming (TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 10:07:47 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Calimeri", "Francesco", ""], ["Ianni", "Giovambattista", ""], ["Pacenza", "Francesco", ""], ["Perri", "Simona", ""], ["Zangari", "Jessica", ""]]}, {"id": "1907.09247", "submitter": "Jorge Fandinno", "authors": "Jorge Fandinno", "title": "Founded (Auto)Epistemic Equilibrium Logic Satisfies Epistemic Splitting", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 671-687", "doi": "10.1017/S1471068419000127", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent line of research, two familiar concepts from logic programming\nsemantics (unfounded sets and splitting) were extrapolated to the case of\nepistemic logic programs. The property of epistemic splitting provides a\nnatural and modular way to understand programs without epistemic cycles but,\nsurprisingly, was only fulfilled by Gelfond's original semantics (G91), among\nthe many proposals in the literature. On the other hand, G91 may suffer from a\nkind of self-supported, unfounded derivations when epistemic cycles come into\nplay. Recently, the absence of these derivations was also formalised as a\nproperty of epistemic semantics called foundedness. Moreover, a first semantics\nproved to satisfy foundedness was also proposed, the so-called Founded\nAutoepistemic Equilibrium Logic (FAEEL). In this paper, we prove that FAEEL\nalso satisfies the epistemic splitting property something that, together with\nfoundedness, was not fulfilled by any other approach up to date. To prove this\nresult, we provide an alternative characterisation of FAEEL as a combination of\nG91 with a simpler logic we called Founded Epistemic Equilibrium Logic (FEEL),\nwhich is somehow an extrapolation of the stable model semantics to the modal\nlogic S5. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:48:15 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 15:33:26 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Fandinno", "Jorge", ""]]}, {"id": "1907.09402", "submitter": "Carmine Dodaro", "authors": "Giovanni Amendola, Carmine Dodaro, Marco Maratea", "title": "Abstract Solvers for Computing Cautious Consequences of ASP programs", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  20 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 740-756", "doi": "10.1017/S1471068419000164", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract solvers are a method to formally analyze algorithms that have been\nprofitably used for describing, comparing and composing solving techniques in\nvarious fields such as Propositional Satisfiability (SAT), Quantified SAT,\nSatisfiability Modulo Theories, Answer Set Programming (ASP), and Constraint\nASP. In this paper, we design, implement and test novel abstract solutions for\ncautious reasoning tasks in ASP. We show how to improve the current abstract\nsolvers for cautious reasoning in ASP with new techniques borrowed from\nbackbone computation in SAT, in order to design new solving algorithms. By\ndoing so, we also formally show that the algorithms for solving cautious\nreasoning tasks in ASP are strongly related to those for computing backbones of\nBoolean formulas. We implement some of the new solutions in the ASP solver WASP\nand show that their performance are comparable to state-of-the-art solutions on\nthe benchmark problems from the past ASP Competitions. Under consideration for\nacceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:25:11 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Amendola", "Giovanni", ""], ["Dodaro", "Carmine", ""], ["Maratea", "Marco", ""]]}, {"id": "1907.09426", "submitter": "Francesco Ricca", "authors": "Giovanni Amendola, Francesco Ricca", "title": "Paracoherent Answer Set Semantics meets Argumentation Frameworks", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 688-704", "doi": "10.1017/S1471068419000139", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, abstract argumentation has met with great success in AI,\nsince it has served to capture several non-monotonic logics for AI. Relations\nbetween argumentation framework (AF) semantics and logic programming ones are\ninvestigating more and more. In particular, great attention has been given to\nthe well-known stable extensions of an AF, that are closely related to the\nanswer sets of a logic program. However, if a framework admits a small\nincoherent part, no stable extension can be provided. To overcome this\nshortcoming, two semantics generalizing stable extensions have been studied,\nnamely semi-stable and stage. In this paper, we show that another perspective\nis possible on incoherent AFs, called paracoherent extensions, as they have a\ncounterpart in paracoherent answer set semantics. We compare this perspective\nwith semi-stable and stage semantics, by showing that computational costs\nremain unchanged, and moreover an interesting symmetric behaviour is\nmaintained. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:53:33 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Amendola", "Giovanni", ""], ["Ricca", "Francesco", ""]]}, {"id": "1907.09472", "submitter": "EPTCS", "authors": "Alexandru Baltag, Soroush Rafiee Rad, Sonja Smets", "title": "Learning Probabilities: Towards a Logic of Statistical Learning", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 35-49", "doi": "10.4204/EPTCS.297.3", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for forming beliefs and learning about unknown\nprobabilities (such as the probability of picking a red marble from a bag with\nan unknown distribution of coloured marbles). The most widespread model for\nsuch situations of 'radical uncertainty' is in terms of imprecise\nprobabilities, i.e. representing the agent's knowledge as a set of probability\nmeasures. We add to this model a plausibility map, associating to each measure\na plausibility number, as a way to go beyond what is known with certainty and\nrepresent the agent's beliefs about probability. There are a number of standard\nexamples: Shannon Entropy, Centre of Mass etc. We then consider learning of two\ntypes of information: (1) learning by repeated sampling from the unknown\ndistribution (e.g. picking marbles from the bag); and (2) learning higher-order\ninformation about the distribution (in the shape of linear inequalities, e.g.\nwe are told there are more red marbles than green marbles). The first changes\nonly the plausibility map (via a 'plausibilistic' version of Bayes' Rule), but\nleaves the given set of measures unchanged; the second shrinks the set of\nmeasures, without changing their plausibility. Beliefs are defined as in Belief\nRevision Theory, in terms of truth in the most plausible worlds. But our belief\nchange does not comply with standard AGM axioms, since the revision induced by\n(1) is of a non-AGM type. This is essential, as it allows our agents to learn\nthe true probability: we prove that the beliefs obtained by repeated sampling\nconverge almost surely to the correct belief (in the true probability). We end\nby sketching the contours of a dynamic doxastic logic for statistical learning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:13:04 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Baltag", "Alexandru", ""], ["Rad", "Soroush Rafiee", ""], ["Smets", "Sonja", ""]]}, {"id": "1907.09548", "submitter": "Jo\\~ao Alc\\^antara", "authors": "Jo\\~ao Alc\\^antara and Samy S\\'a and Juan Acosta-Guadarrama", "title": "On the Equivalence Between Abstract Dialectical Frameworks and Logic\n  Programs", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract Dialectical Frameworks (ADFs) are argumentation frameworks where\neach node is associated with an acceptance condition. This allows us to model\ndifferent types of dependencies as supports and attacks. Previous studies\nprovided a translation from Normal Logic Programs (NLPs) to ADFs and proved the\nstable models semantics for a normal logic program has an equivalent semantics\nto that of the corresponding ADF. However, these studies failed in identifying\na semantics for ADFs equivalent to a three-valued semantics (as partial stable\nmodels and well-founded models) for NLPs. In this work, we focus on a fragment\nof ADFs, called Attacking Dialectical Frameworks (ADF$^+$s), and provide a\ntranslation from NLPs to ADF$^+$s robust enough to guarantee the equivalence\nbetween partial stable models, well-founded models, regular models, stable\nmodels semantics for NLPs and respectively complete models, grounded models,\npreferred models, stable models for ADFs. In addition, we define a new\nsemantics for ADF$^+$s, called L-stable, and show it is equivalent to the\nL-stable semantics for NLPs. This paper is under consideration for acceptance\nin TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:54:20 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Alc\u00e2ntara", "Jo\u00e3o", ""], ["S\u00e1", "Samy", ""], ["Acosta-Guadarrama", "Juan", ""]]}, {"id": "1907.09559", "submitter": "Giovanni Amendola", "authors": "Giovanni Amendola, Francesco Ricca, Mirek Truszczynski", "title": "Beyond NP: Quantifying over Answer Sets", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 705-721", "doi": "10.1017/S1471068419000140", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a logic programming paradigm featuring a\npurely declarative language with comparatively high modeling capabilities.\nIndeed, ASP can model problems in NP in a compact and elegant way. However,\nmodeling problems beyond NP with ASP is known to be complicated, on the one\nhand, and limited to problems in {\\Sigma}^P_2 on the other. Inspired by the way\nQuantified Boolean Formulas extend SAT formulas to model problems beyond NP, we\npropose an extension of ASP that introduces quantifiers over stable models of\nprograms. We name the new language ASP with Quantifiers (ASP(Q)). In the paper\nwe identify computational properties of ASP(Q); we highlight its modeling\ncapabilities by reporting natural encodings of several complex problems with\napplications in artificial intelligence and number theory; and we compare\nASP(Q) with related languages. Arguably, ASP(Q) allows one to model problems in\nthe Polynomial Hierarchy in a direct way, providing an elegant expansion of ASP\nbeyond the class NP. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:25:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Amendola", "Giovanni", ""], ["Ricca", "Francesco", ""], ["Truszczynski", "Mirek", ""]]}, {"id": "1907.09560", "submitter": "Carmine Dodaro", "authors": "Giovanni Amendola, Carmine Dodaro, Francesco Ricca", "title": "Better Paracoherent Answer Sets with Less Resources", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  15 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 757-772", "doi": "10.1017/S1471068419000176", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a well-established formalism for logic\nprogramming. Problem solving in ASP requires to write an ASP program whose\nanswers sets correspond to solutions. Albeit the non-existence of answer sets\nfor some ASP programs can be considered as a modeling feature, it turns out to\nbe a weakness in many other cases, and especially for query answering.\nParacoherent answer set semantics extend the classical semantics of ASP to draw\nmeaningful conclusions also from incoherent programs, with the result of\nincreasing the range of applications of ASP. State of the art implementations\nof paracoherent ASP adopt the semi-equilibrium semantics, but cannot be lifted\nstraightforwardly to compute efficiently the (better) split semi-equilibrium\nsemantics that discards undesirable semi-equilibrium models. In this paper an\nefficient evaluation technique for computing a split semi-equilibrium model is\npresented. An experiment on hard benchmarks shows that better paracoherent\nanswer sets can be computed consuming less computational resources than\nexisting methods. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:27:43 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Amendola", "Giovanni", ""], ["Dodaro", "Carmine", ""], ["Ricca", "Francesco", ""]]}, {"id": "1907.09603", "submitter": "Francisco Eiras", "authors": "Francisco Eiras, Morteza Lahijanian, Marta Kwiatkowska", "title": "Correct-by-Construction Advanced Driver Assistance Systems based on a\n  Cognitive Architecture", "comments": "Proceedings at IEEE CAVS 2019", "journal-ref": null, "doi": "10.1109/CAVS.2019.8887768", "report-no": null, "categories": "cs.RO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into safety in autonomous and semi-autonomous vehicles has, so far,\nlargely been focused on testing and validation through simulation. Due to the\nfact that failure of these autonomous systems is potentially life-endangering,\nformal methods arise as a complementary approach. This paper studies the\napplication of formal methods to the verification of a human driver model built\nusing the cognitive architecture ACT-R, and to the design of\ncorrect-by-construction Advanced Driver Assistance Systems (ADAS). The novelty\nlies in the integration of ACT-R in the formal analysis and an abstraction\ntechnique that enables finite representation of a large dimensional, continuous\nsystem in the form of a Markov process. The situation considered is a\nmulti-lane highway driving scenario and the interactions that arise. The\nefficacy of the method is illustrated in two case studies with various driving\nconditions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 22:07:14 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Eiras", "Francisco", ""], ["Lahijanian", "Morteza", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "1907.09634", "submitter": "Yuichi Komorida", "authors": "Yuichi Komorida, Shin-ya Katsumata, Nick Hu, Bartek Klin, Ichiro Hasuo", "title": "Codensity Games for Bisimilarity", "comments": "13 pages + 3 page appendix, to appear in Proceedings of the 34th\n  Annual ACM/IEEE Symposium on Logic in Computer Science (LICS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bisimilarity as an equivalence notion of systems has been central to process\ntheory. Due to the recent rise of interest in quantitative systems\n(probabilistic, weighted, hybrid, etc.), bisimilarity has been extended in\nvarious ways: notably, bisimulation metric between probabilistic systems. An\nimportant feature of bisimilarity is its game-theoretic characterization, where\nSpoiler and Duplicator play against each other; extension of bisimilarity games\nto quantitative settings has been actively pursued too. In this paper, we\npresent a general framework that uniformly describes game characterizations of\nbisimilarity-like notions. Our framework is formalized categorically using\nfibrations and coalgebras. In particular, our characterization of bisimilarity\nin terms of fibrational predicate transformers allows us to derive codensity\nbisimilarity games: a general categorical game characterization of\nbisimilarity. Our framework covers known bisimilarity-like notions (such as\nbisimulation metric) as well as new ones (including what we call bisimulation\ntopology).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 23:48:19 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Komorida", "Yuichi", ""], ["Katsumata", "Shin-ya", ""], ["Hu", "Nick", ""], ["Klin", "Bartek", ""], ["Hasuo", "Ichiro", ""]]}, {"id": "1907.09820", "submitter": "Angelos Charalambidis", "authors": "Angelos Charalambidis, Christos Nomikos, Panos Rondogiannis", "title": "The Expressive Power of Higher-Order Datalog", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  24 pages, LaTeX", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 925-940", "doi": "10.1017/S1471068419000279", "report-no": null, "categories": "cs.PL cs.CC cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical result in descriptive complexity theory states that Datalog\nexpresses exactly the class of polynomially computable queries on ordered\ndatabases. In this paper we extend this result to the case of higher-order\nDatalog. In particular, we demonstrate that on ordered databases, for all\n$k\\geq2$, $k$-order Datalog captures $(k-1)$-EXPTIME. This result suggests that\nhigher-order extensions of Datalog possess superior expressive power and they\nare worthwhile of further investigation both in theory and in practice. This\npaper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 11:21:49 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 14:03:23 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Charalambidis", "Angelos", ""], ["Nomikos", "Christos", ""], ["Rondogiannis", "Panos", ""]]}, {"id": "1907.09867", "submitter": "Stefania  Costantini", "authors": "Stefania Costantini", "title": "About epistemic negation and world views in Epistemic Logic Programs", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 790-807", "doi": "10.1017/S147106841900019X", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider Epistemic Logic Programs, which extend Answer Set\nProgramming (ASP) with \"epistemic operators\" and \"epistemic negation\", and a\nrecent approach to the semantics of such programs in terms of World Views. We\npropose some observations on the existence and number of world views. We show\nhow to exploit an extended ASP semantics in order to: (i) provide a\ncharacterization of world views, different from existing ones; (ii) query world\nviews and query the whole set of world views.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:29:10 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 21:09:05 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Costantini", "Stefania", ""]]}, {"id": "1907.09920", "submitter": "Peter Munk", "authors": "Simon Greiner and Peter Munk and Arne Nordmann", "title": "Proof of Compositionality of CFT Correctness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper Compositionality of Component Fault Trees, we present a\ndiscussion of the compositionality of correctness of component fault trees. In\nthis technical report, we present the formal proof of the central theorem of\nthe aforementioned publication.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:47:04 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Greiner", "Simon", ""], ["Munk", "Peter", ""], ["Nordmann", "Arne", ""]]}, {"id": "1907.09927", "submitter": "Antonin Delpeuch", "authors": "Antonin Delpeuch", "title": "The word problem for double categories", "comments": "18 pages", "journal-ref": "Theory and Applications of Categories, Vol. 35, 2020, No. 1, pp\n  1-18", "doi": null, "report-no": null, "categories": "math.CT cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We solve the word problem for free double categories without equations\nbetween generators by translating it to the word problem for 2-categories. This\nyields a quadratic algorithm deciding the equality of diagrams in a free double\ncategory. The translation is of interest in its own right since and can for\ninstance be used to reason about double categories with the language of\n2-categories, sidestepping the pinwheel problem. It also shows that although\ndouble categories are formally more general than 2-categories, they are not\nactually more expressive, explaining the rarity of applications of this notion.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:55:48 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 09:39:16 GMT"}, {"version": "v3", "created": "Sun, 18 Aug 2019 11:47:00 GMT"}, {"version": "v4", "created": "Thu, 2 Jan 2020 20:10:53 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Delpeuch", "Antonin", ""]]}, {"id": "1907.10096", "submitter": "Enrique Martin-Martin", "authors": "Elvira Albert, Miquel Bofill, Cristina Borralleras, Enrique\n  Martin-Martin, Albert Rubio", "title": "Resource Analysis driven by (Conditional) Termination Proofs", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 722-739", "doi": "10.1017/S1471068419000152", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When programs feature a complex control flow, existing techniques for\nresource analysis produce cost relation systems (CRS) whose cost functions\nretain the complex flow of the program and, consequently, might not be solvable\ninto closed-form upper bounds. This paper presents a novel approach to resource\nanalysis that is driven by the result of a termination analysis. The\nfundamental idea is that the termination proof encapsulates the flows of the\nprogram which are relevant for the cost computation so that, by driving the\ngeneration of the CRS using the termination proof, we produce a\nlinearly-bounded CRS (LB-CRS). A LB-CRS is composed of cost functions that are\nguaranteed to be locally bounded by linear ranking functions and thus greatly\nsimplify the process of CRS solving. We have built a new resource analysis\ntool, named MaxCore, that is guided by the VeryMax termination analyzer and\nuses CoFloCo and PUBS as CRS solvers. Our experimental results on the set of\nbenchmarks from the Complexity and Termination Competition 2019 for C Integer\nprograms show that MaxCore outperforms all other resource analysis tools. Under\nconsideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:34:36 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Albert", "Elvira", ""], ["Bofill", "Miquel", ""], ["Borralleras", "Cristina", ""], ["Martin-Martin", "Enrique", ""], ["Rubio", "Albert", ""]]}, {"id": "1907.10175", "submitter": "Haniel Barbosa", "authors": "Andrew Reynolds, Haniel Barbosa, Andres N\\\"otzli, Clark Barrett,\n  Cesare Tinelli", "title": "CVC4SY for SyGuS-COMP 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CVC4Sy is a syntax-guided synthesis (SyGuS) solver based on bounded term\nenumeration and, for restricted fragments, quantifier elimination. The\nenumerative strategies are based on encoding term enumeration as an extension\nof the quantifier-free theory of algebraic datatypes and on a highly optimized\nbrute-force algorithm. The quantifier elimination strategy extracts solutions\nfrom unsatisfiability proofs of the negated form of synthesis conjectures. It\nuses recent counterexample-guided techniques for quantifier instantiation that\nmake finding such proofs practically feasible. CVC4Sy implements these\nstrategies by extending the satisfiability modulo theories (SMT) solver CVC4.\nThe strategy to be applied on a given problem is chosen heuristically based on\nthe problem's structure. This document gives an overview of these techniques\nand their implementation in the SyGuS Solver CVC4Sy, an entry for SyGuS-Comp\n2019.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 23:21:15 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Reynolds", "Andrew", ""], ["Barbosa", "Haniel", ""], ["N\u00f6tzli", "Andres", ""], ["Barrett", "Clark", ""], ["Tinelli", "Cesare", ""]]}, {"id": "1907.10278", "submitter": "Ariyam Das", "authors": "Ariyam Das and Carlo Zaniolo", "title": "A Case for Stale Synchronous Distributed Model for Declarative Recursive\n  Computation", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of traditional graph and data mining algorithms can be\nconcisely expressed in Datalog, and other Logic-based languages, once\naggregates are allowed in recursion. In fact, for most BigData algorithms, the\ndifficult semantic issues raised by the use of non-monotonic aggregates in\nrecursion are solved by Pre-Mappability (PreM), a property that assures that\nfor a program with aggregates in recursion there is an equivalent\naggregate-stratified program. In this paper we show that, by bringing together\nthe formal abstract semantics of stratified programs with the efficient\noperational one of unstratified programs, PreM can also facilitate and improve\ntheir parallel execution. We prove that PreM-optimized lock-free and\ndecomposable parallel semi-naive evaluations produce the same results as the\nsingle executor programs. Therefore, PreM can be assimilated into the\ndata-parallel computation plans of different distributed systems, irrespective\nof whether these follow bulk synchronous parallel (BSP) or asynchronous\ncomputing models. In addition, we show that non-linear recursive queries can be\nevaluated using a hybrid stale synchronous parallel (SSP) model on distributed\nenvironments. After providing a formal correctness proof for the recursive\nquery evaluation with PreM under this relaxed synchronization model, we present\nexperimental evidence of its benefits. This paper is under consideration for\nacceptance in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:35:18 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Das", "Ariyam", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1907.10327", "submitter": "Yusuke Kawamoto", "authors": "Yusuke Kawamoto", "title": "Towards Logical Specification of Statistical Machine Learning", "comments": "SEFM'19 conference paper (full version with errors corrected)", "journal-ref": null, "doi": "10.1007/978-3-030-30446-1_16", "report-no": null, "categories": "cs.LO cs.AI cs.CR cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a logical approach to formalizing statistical properties of\nmachine learning. Specifically, we propose a formal model for statistical\nclassification based on a Kripke model, and formalize various notions of\nclassification performance, robustness, and fairness of classifiers by using\nepistemic logic. Then we show some relationships among properties of\nclassifiers and those between classification performance and robustness, which\nsuggests robustness-related properties that have not been formalized in the\nliterature as far as we know. To formalize fairness properties, we define a\nnotion of counterfactual knowledge and show techniques to formalize conditional\nindistinguishability by using counterfactual epistemic operators. As far as we\nknow, this is the first work that uses logical formulas to express statistical\nproperties of machine learning, and that provides epistemic (resp.\ncounterfactually epistemic) views on robustness (resp. fairness) of\nclassifiers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:33:07 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 14:30:40 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kawamoto", "Yusuke", ""]]}, {"id": "1907.10333", "submitter": "Gonzague Yernaux", "authors": "Gonzague Yernaux and Wim Vanhoof", "title": "Anti-unification in Constraint Logic Programming", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 773-789", "doi": "10.1017/S1471068419000188", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anti-unification refers to the process of generalizing two (or more) goals\ninto a single, more general, goal that captures some of the structure that is\ncommon to all initial goals. In general one is typically interested in\ncomputing what is often called a most specific generalization, that is a\ngeneralization that captures a maximal amount of shared structure. In this work\nwe address the problem of anti-unification in CLP, where goals can be seen as\nunordered sets of atoms and/or constraints. We show that while the concept of a\nmost specific generalization can easily be defined in this context, computing\nit becomes an NP-complete problem. We subsequently introduce a generalization\nalgorithm that computes a well-defined abstraction whose computation can be\nbound to a polynomial execution time. Initial experiments show that even a\nnaive implementation of our algorithm produces acceptable generalizations in an\nefficient way. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:55:27 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yernaux", "Gonzague", ""], ["Vanhoof", "Wim", ""]]}, {"id": "1907.10381", "submitter": "EPTCS", "authors": "Frank M. V. Feys (Delft University of Technology), Helle Hvid Hansen\n  (Delft University of Technology)", "title": "Arrow's Theorem Through a Fixpoint Argument", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 175-188", "doi": "10.4204/EPTCS.297.12", "report-no": null, "categories": "econ.TH cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof of Arrow's theorem from social choice theory that uses a\nfixpoint argument. Specifically, we use Banach's result on the existence of a\nfixpoint of a contractive map defined on a complete metric space. Conceptually,\nour approach shows that dictatorships can be seen as fixpoints of a certain\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:15:46 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Feys", "Frank M. V.", "", "Delft University of Technology"], ["Hansen", "Helle Hvid", "", "Delft University of Technology"]]}, {"id": "1907.10386", "submitter": "Brett McLean", "authors": "Brett McLean", "title": "Free Kleene algebras with domain", "comments": "22 pages. Some proofs expanded", "journal-ref": "Journal of Logical and Algebraic Methods in Programming, Volume\n  117 (December 2020) 100606", "doi": "10.1016/j.jlamp.2020.100606", "report-no": null, "categories": "math.LO cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First we identify the free algebras of the class of algebras of binary\nrelations equipped with the composition and domain operations. Elements of the\nfree algebras are pointed labelled finite rooted trees. Then we extend to the\nanalogous case when the signature includes all the Kleene algebra with domain\noperations; that is, we add union and reflexive transitive closure to the\nsignature. In this second case, elements of the free algebras are 'regular'\nsets of the trees of the first case. As a corollary, the axioms of domain\nsemirings provide a finite quasiequational axiomatisation of the equational\ntheory of algebras of binary relations for the intermediate signature of\ncomposition, union, and domain. Next we note that our regular sets of trees are\nnot closed under complement, but prove that they are closed under intersection.\nFinally, we prove that under relational semantics the equational validities of\nKleene algebras with domain form a decidable set.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:29:49 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 15:07:10 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["McLean", "Brett", ""]]}, {"id": "1907.10389", "submitter": "Markus Hecher", "authors": "Mario Alviano, Carmine Dodaro, Johannes K. Fichte, Markus Hecher,\n  Tobias Philipp, Jakob Rath", "title": "Inconsistency Proofs for ASP: The ASP-DRUPE Format", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) solvers are highly-tuned and complex procedures\nthat implicitly solve the consistency problem, i.e., deciding whether a logic\nprogram admits an answer set. Verifying whether a claimed answer set is\nformally a correct answer set of the program can be decided in polynomial time\nfor (normal) programs. However, it is far from immediate to verify whether a\nprogram that is claimed to be inconsistent, indeed does not admit any answer\nsets. In this paper, we address this problem and develop the new proof format\nASP-DRUPE for propositional, disjunctive logic programs, including weight and\nchoice rules. ASP-DRUPE is based on the Reverse Unit Propagation (RUP) format\ndesigned for Boolean satisfiability. We establish correctness of ASP-DRUPE and\ndiscuss how to integrate it into modern ASP solvers. Later, we provide an\nimplementation of ASP-DRUPE into the wasp solver for normal logic programs.\nThis work is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:32:37 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Alviano", "Mario", ""], ["Dodaro", "Carmine", ""], ["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Philipp", "Tobias", ""], ["Rath", "Jakob", ""]]}, {"id": "1907.10469", "submitter": "Bernardo Cuteri", "authors": "Bernardo Cuteri, Carmine Dodaro, Francesco Ricca, Peter Sch\\\"uller", "title": "Partial Compilation of ASP Programs", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages, 6 figures", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 857-873", "doi": "10.1017/S1471068419000231", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a well-known declarative formalism in logic\nprogramming. Efficient implementations made it possible to apply ASP in many\nscenarios, ranging from deductive databases applications to the solution of\nhard combinatorial problems. State-of-the-art ASP systems are based on the\ntraditional ground\\&solve approach and are general-purpose implementations,\ni.e., they are essentially built once for any kind of input program. In this\npaper, we propose an extended architecture for ASP systems, in which parts of\nthe input program are compiled into an ad-hoc evaluation algorithm (i.e., we\nobtain a specific binary for a given program), and might not be subject to the\ngrounding step. To this end, we identify a condition that allows the\ncompilation of a sub-program, and present the related partial compilation\ntechnique. Importantly, we have implemented the new approach on top of a\nwell-known ASP solver and conducted an experimental analysis on\npublicly-available benchmarks. Results show that our compilation-based approach\nimproves on the state of the art in various scenarios, including cases in which\nthe input program is stratified or the grounding blow-up makes the evaluation\nunpractical with traditional ASP systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 14:42:57 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Cuteri", "Bernardo", ""], ["Dodaro", "Carmine", ""], ["Ricca", "Francesco", ""], ["Sch\u00fcller", "Peter", ""]]}, {"id": "1907.10492", "submitter": "EPTCS", "authors": "Francesco Belardinelli (Department of Computing, Imperial College\n  London, UK and Laboratoire IBISC, University of Evry, France), Umberto Grandi\n  (IRIT, University of Toulouse, France)", "title": "Social Choice Methods for Database Aggregation", "comments": "In Proceedings TARK 2019, arXiv:1907.08335. arXiv admin note:\n  substantial text overlap with arXiv:1802.08586", "journal-ref": "EPTCS 297, 2019, pp. 50-67", "doi": "10.4204/EPTCS.297.4", "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge can be represented compactly in multiple ways, from a set of\npropositional formulas, to a Kripke model, to a database. In this paper we\nstudy the aggregation of information coming from multiple sources, each source\nsubmitting a database modelled as a first-order relational structure. In the\npresence of integrity constraints, we identify classes of aggregators that\nrespect them in the aggregated database, provided these are satisfied in all\nindividual databases. We also characterise languages for first-order queries on\nwhich the answer to a query on the aggregated database coincides with the\naggregation of the answers to the query obtained on each individual database.\nThis contribution is meant to be a first step on the application of techniques\nfrom social choice theory to knowledge representation in databases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:13:22 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Belardinelli", "Francesco", "", "Department of Computing, Imperial College\n  London, UK and Laboratoire IBISC, University of Evry, France"], ["Grandi", "Umberto", "", "IRIT, University of Toulouse, France"]]}, {"id": "1907.10674", "submitter": "Danil Annenkov", "authors": "Danil Annenkov, Jakob Botsch Nielsen, Bas Spitters", "title": "ConCert: A Smart Contract Certification Framework in Coq", "comments": "Extended the related work section. Significantly extended sections on\n  translation and semantics. Added more examples and details about the\n  formalisation. Commented of unquote and the trusted computing base. Commented\n  on adequacy", "journal-ref": "CPP 2020: Proceedings of the 9th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, January 2020, Pages 215-228", "doi": "10.1145/3372885.3373829", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new way of embedding functional languages into the Coq proof\nassistant by using meta-programming. This allows us to develop the meta-theory\nof the language using the deep embedding and provides a convenient way for\nreasoning about concrete programs using the shallow embedding. We connect the\ndeep and the shallow embeddings by a soundness theorem. As an instance of our\napproach, we develop an embedding of a core smart contract language into Coq\nand verify several important properties of a crowdfunding contract based on a\nprevious formalisation of smart contract execution in blockchains.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 19:19:29 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 07:19:01 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 10:00:02 GMT"}, {"version": "v4", "created": "Fri, 20 Dec 2019 17:59:41 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Annenkov", "Danil", ""], ["Nielsen", "Jakob Botsch", ""], ["Spitters", "Bas", ""]]}, {"id": "1907.10708", "submitter": "Justin Hsu", "authors": "Gilles Barthe, Justin Hsu, Kevin Liao", "title": "A Probabilistic Separation Logic", "comments": null, "journal-ref": null, "doi": "10.1145/3371123", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic independence is a useful concept for describing the result of\nrandom sampling---a basic operation in all probabilistic languages---and for\nreasoning about groups of random variables. Nevertheless, existing verification\nmethods handle independence poorly, if at all. We propose a probabilistic\nseparation logic PSL, where separation models probabilistic independence. We\nfirst give a new, probabilistic model of the logic of bunched implications\n(BI). We then build a program logic based on these assertions, and prove\nsoundness of the proof system. We demonstrate our logic by verifying\ninformation-theoretic security of cryptographic constructions for several\nwell-known tasks, including private information retrieval, oblivious transfer,\nsecure multi-party addition, and simple oblivious RAM. Our proofs reason purely\nin terms of high-level properties, like independence and uniformity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:39:30 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 19:38:23 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 19:23:36 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 21:40:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Barthe", "Gilles", ""], ["Hsu", "Justin", ""], ["Liao", "Kevin", ""]]}, {"id": "1907.10914", "submitter": "Fernando S\\'aenz-P\\'erez", "authors": "Fernando S\\'aenz-P\\'erez", "title": "Applying Constraint Logic Programming to SQL Semantic Analysis", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 808-825", "doi": "10.1017/S1471068419000206", "report-no": null, "categories": "cs.DB cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the use of Constraint Logic Programming (CLP) to model\nSQL queries in a data-independent abstract layer by focusing on some semantic\nproperties for signalling possible errors in such queries. First, we define a\ntranslation from SQL to Datalog, and from Datalog to CLP, so that solving this\nCLP program will give information about inconsistency, tautology, and possible\nsimplifications. We use different constraint domains which are mapped to SQL\ntypes, and propose them to cooperate for improving accuracy. Our approach\nleverages a deductive system that includes SQL and Datalog, and we present an\nimplementation in this system which is currently being tested in classroom,\nshowing its advantages and differences with respect to other approaches, as\nwell as some performance data. This paper is under consideration for acceptance\nin TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:19:30 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", ""]]}, {"id": "1907.10919", "submitter": "Julia Sapi\\~na", "authors": "Mar\\'ia Alpuente, Demis Ballis, Santiago Escobar, Julia Sapi\\~na", "title": "Symbolic Analysis of Maude Theories with Narval", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent functional languages that are endowed with symbolic reasoning\ncapabilities such as Maude offer a high-level, elegant, and efficient approach\nto programming and analyzing complex, highly nondeterministic software systems.\nMaude's symbolic capabilities are based on equational unification and narrowing\nin rewrite theories, and provide Maude with advanced logic programming\ncapabilities such as unification modulo user-definable equational theories and\nsymbolic reachability analysis in rewrite theories. Intricate computing\nproblems may be effectively and naturally solved in Maude thanks to the synergy\nof these recently developed symbolic capabilities and classical Maude features,\nsuch as: (i) rich type structures with sorts (types), subsorts, and\noverloading; (ii) equational rewriting modulo various combinations of axioms\nsuch as associativity, commutativity, and identity; and (iii) classical\nreachability analysis in rewrite theories. However, the combination of all of\nthese features may hinder the understanding of Maude symbolic computations for\nnon-experienced developers. The purpose of this article is to describe how\nprogramming and analysis of Maude rewrite theories can be made easier by\nproviding a sophisticated graphical tool called Narval that supports the\nfine-grained inspection of Maude symbolic computations. This paper is under\nconsideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:27:07 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Alpuente", "Mar\u00eda", ""], ["Ballis", "Demis", ""], ["Escobar", "Santiago", ""], ["Sapi\u00f1a", "Julia", ""]]}, {"id": "1907.10925", "submitter": "Michael Morak", "authors": "Wolfgang Faber, Michael Morak, and Stefan Woltran", "title": "On Uniform Equivalence of Epistemic Logic Programs", "comments": "Accepted for publication and presentation at the 35th International\n  Conference of Logic Programming, ICLP 2019, in Las Cruces, New Mexico, USA", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 826-840", "doi": "10.1017/S1471068419000218", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Epistemic Logic Programs (ELPs) extend Answer Set Programming (ASP) with\nepistemic negation and have received renewed interest in recent years. This led\nto the development of new research and efficient solving systems for ELPs. In\npractice, ELPs are often written in a modular way, where each module interacts\nwith other modules by accepting sets of facts as input, and passing on sets of\nfacts as output. An interesting question then presents itself: under which\nconditions can such a module be replaced by another one without changing the\noutcome, for any set of input facts? This problem is known as uniform\nequivalence, and has been studied extensively for ASP. For ELPs, however, such\nan investigation is, as of yet, missing. In this paper, we therefore propose a\ncharacterization of uniform equivalence that can be directly applied to the\nlanguage of state-of-the-art ELP solvers. We also investigate the computational\ncomplexity of deciding uniform equivalence for two ELPs, and show that it is on\nthe third level of the polynomial hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:34:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Faber", "Wolfgang", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1907.11007", "submitter": "Efthimis Tsilionis", "authors": "Efthimis Tsilionis, Nikolaos Koutroumanis, Panagiotis Nikitopoulos,\n  Christos Doulkeridis and Alexander Artikis", "title": "Online Event Recognition from Moving Vehicles: Application Paper", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 841-856", "doi": "10.1017/S147106841900022X", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for online composite event recognition over streaming\npositions of commercial vehicles. Our system employs a data enrichment module,\naugmenting the mobility data with external information, such as weather data\nand proximity to points of interest. In addition, the composite event\nrecognition module, based on a highly optimised logic programming\nimplementation of the Event Calculus, consumes the enriched data and identifies\nactivities that are beneficial in fleet management applications. We evaluate\nour system on large, real-world data from commercial vehicles, and illustrate\nits efficiency. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 12:30:08 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Tsilionis", "Efthimis", ""], ["Koutroumanis", "Nikolaos", ""], ["Nikitopoulos", "Panagiotis", ""], ["Doulkeridis", "Christos", ""], ["Artikis", "Alexander", ""]]}, {"id": "1907.11061", "submitter": "Manuel Gieseking", "authors": "Bernd Finkbeiner, Manuel Gieseking, Jesko Hecking-Harbusch,\n  Ernst-R\\\"udiger Olderog", "title": "Model Checking Data Flows in Concurrent Network Updates (Full Version)", "comments": "42 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model checking approach for the verification of data flow\ncorrectness in networks during concurrent updates of the network configuration.\nThis verification problem is of great importance for software-defined\nnetworking (SDN), where errors can lead to packet loss, black holes, and\nsecurity violations. Our approach is based on a specification of temporal\nproperties of individual data flows, such as the requirement that the flow is\nfree of cycles. We check whether these properties are simultaneously satisfied\nfor all active data flows while the network configuration is updated. To\nrepresent the behavior of the concurrent network controllers and the resulting\nevolutions of the configurations, we introduce an extension of Petri nets with\na transit relation, which characterizes the data flow caused by each transition\nof the Petri net. For safe Petri nets with transits, we reduce the verification\nof temporal flow properties to a circuit model checking problem that can be\nsolved with effective verification techniques like IC3, interpolation, and\nbounded model checking. We report on encouraging experiments with a prototype\nimplementation based on the hardware model checker ABC.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:59:52 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 14:44:23 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Finkbeiner", "Bernd", ""], ["Gieseking", "Manuel", ""], ["Hecking-Harbusch", "Jesko", ""], ["Olderog", "Ernst-R\u00fcdiger", ""]]}, {"id": "1907.11184", "submitter": "Prithviraj Sen", "authors": "Yiwei Yang, Eser Kandogan, Yunyao Li, Walter S. Lasecki, and\n  Prithviraj Sen", "title": "HEIDL: Learning Linguistic Expressions with Deep Learning and\n  Human-in-the-Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the role of humans is increasingly recognized in machine learning\ncommunity, representation of and interaction with models in current\nhuman-in-the-loop machine learning (HITL-ML) approaches are too low-level and\nfar-removed from human's conceptual models. We demonstrate HEIDL, a prototype\nHITL-ML system that exposes the machine-learned model through high-level,\nexplainable linguistic expressions formed of predicates representing semantic\nstructure of text. In HEIDL, human's role is elevated from simply evaluating\nmodel predictions to interpreting and even updating the model logic directly by\nenabling interaction with rule predicates themselves. Raising the currency of\ninteraction to such semantic levels calls for new interaction paradigms between\nhumans and machines that result in improved productivity for text analytics\nmodel development process. Moreover, by involving humans in the process, the\nhuman-machine co-created models generalize better to unseen data as domain\nexperts are able to instill their expertise by extrapolating from what has been\nlearned by automated algorithms from few labelled data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 16:45:06 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Yang", "Yiwei", ""], ["Kandogan", "Eser", ""], ["Li", "Yunyao", ""], ["Lasecki", "Walter S.", ""], ["Sen", "Prithviraj", ""]]}, {"id": "1907.11321", "submitter": "Mark-Oliver Stehr", "authors": "Mark-Oliver Stehr, Minyoung Kim, Carolyn L. Talcott, Merrill Knapp,\n  Akos Vertes", "title": "Probabilistic Approximate Logic and its Implementation in the Logical\n  Imagination Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the rapidly increasing number of applications of machine learning\nin various domains, a principled and systematic approach to the incorporation\nof domain knowledge in the engineering process is still lacking and ad hoc\nsolutions that are difficult to validate are still the norm in practice, which\nis of growing concern not only in mission-critical applications.\n  In this note, we introduce Probabilistic Approximate Logic (PALO) as a logic\nbased on the notion of mean approximate probability to overcome conceptual and\ncomputational difficulties inherent to strictly probabilistic logics. The logic\nis approximate in several dimensions. Logical independence assumptions are used\nto obtain approximate probabilities, but by averaging over many instances of\nformulas a useful estimate of mean probability with known confidence can\nusually be obtained. To enable efficient computational inference, the logic has\na continuous semantics that reflects only a subset of the structural properties\nof classical logic, but this imprecision can be partly compensated by richer\ntheories obtained by classical inference or other means. Computational\ninference, which refers to the construction of models and validation of logical\nproperties, is based on Stochastic Gradient Descent (SGD) and Markov Chain\nMonte Carlo (MCMC) techniques and hence another dimension where approximations\nare involved.\n  We also present the Logical Imagination Engine (LIME), a prototypical\nimplementation of PALO based on TensorFlow. Albeit not limited to the\nbiological domain, we illustrate its operation in a quite substantial\nbioinformatics machine learning application concerned with network synthesis\nand analysis in a recent DARPA project.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 22:13:24 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Stehr", "Mark-Oliver", ""], ["Kim", "Minyoung", ""], ["Talcott", "Carolyn L.", ""], ["Knapp", "Merrill", ""], ["Vertes", "Akos", ""]]}, {"id": "1907.11467", "submitter": "Jorge Fandinno", "authors": "Felicidad Aguado, Pedro Cabalar, Jorge Fandinno, David Pearce,\n  Gilberto Perez and Concepcion Vidal", "title": "Revisiting Explicit Negation in Answer Set Programming", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 908-924", "doi": "10.1017/S1471068419000267", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common feature in Answer Set Programming is the use of a second negation,\nstronger than default negation and sometimes called explicit, strong or\nclassical negation. This explicit negation is normally used in front of atoms,\nrather than allowing its use as a regular operator. In this paper we consider\nthe arbitrary combination of explicit negation with nested expressions, as\nthose defined by Lifschitz, Tang and Turner. We extend the concept of reduct\nfor this new syntax and then prove that it can be captured by an extension of\nEquilibrium Logic with this second negation. We study some properties of this\nvariant and compare to the already known combination of Equilibrium Logic with\nNelson's strong negation. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 10:19:47 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Aguado", "Felicidad", ""], ["Cabalar", "Pedro", ""], ["Fandinno", "Jorge", ""], ["Pearce", "David", ""], ["Perez", "Gilberto", ""], ["Vidal", "Concepcion", ""]]}, {"id": "1907.11468", "submitter": "Francesco Giannini", "authors": "Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco\n  Maggini and Marco Gori", "title": "T-Norms Driven Loss Functions for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural-symbolic approaches have recently gained popularity to inject prior\nknowledge into a learner without requiring it to induce this knowledge from\ndata. These approaches can potentially learn competitive solutions with a\nsignificant reduction of the amount of supervised data. A large class of\nneural-symbolic approaches is based on First-Order Logic to represent prior\nknowledge, relaxed to a differentiable form using fuzzy logic. This paper shows\nthat the loss function expressing these neural-symbolic learning tasks can be\nunambiguously determined given the selection of a t-norm generator. When\nrestricted to supervised learning, the presented theoretical apparatus provides\na clean justification to the popular cross-entropy loss, which has been shown\nto provide faster convergence and to reduce the vanishing gradient problem in\nvery deep structures. However, the proposed learning formulation extends the\nadvantages of the cross-entropy loss to the general knowledge that can be\nrepresented by a neural-symbolic method. Therefore, the methodology allows the\ndevelopment of a novel class of loss functions, which are shown in the\nexperimental results to lead to faster convergence rates than the approaches\npreviously proposed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 10:22:16 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 09:18:21 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 10:27:46 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 16:49:12 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Marra", "Giuseppe", ""], ["Giannini", "Francesco", ""], ["Diligenti", "Michelangelo", ""], ["Maggini", "Marco", ""], ["Gori", "Marco", ""]]}, {"id": "1907.11501", "submitter": "Christoph Benzm\\\"uller", "authors": "Alexander Steen and Christoph Benzm\\\"uller", "title": "Extensional Higher-Order Paramodulation in Leo-III", "comments": "34 pages, 7 Figures, 1 Table; submitted article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.SC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leo-III is an automated theorem prover for extensional type theory with\nHenkin semantics and choice. Reasoning with primitive equality is enabled by\nadapting paramodulation-based proof search to higher-order logic. The prover\nmay cooperate with multiple external specialist reasoning systems such as\nfirst-order provers and SMT solvers. Leo-III is compatible with the TPTP/TSTP\nframework for input formats, reporting results and proofs, and standardized\ncommunication between reasoning systems, enabling e.g. proof reconstruction\nfrom within proof assistants such as Isabelle/HOL. Leo-III supports reasoning\nin polymorphic first-order and higher-order logic, in all normal quantified\nmodal logics, as well as in different deontic logics. Its development had\ninitiated the ongoing extension of the TPTP infrastructure to reasoning within\nnon-classical logics.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:58:08 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 06:05:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Steen", "Alexander", ""], ["Benzm\u00fcller", "Christoph", ""]]}, {"id": "1907.11838", "submitter": "Paul Tarau", "authors": "Paul Tarau", "title": "Modality Definition Synthesis for Epistemic Intuitionistic Logic via a\n  Theorem Prover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a Prolog theorem prover for an Intuitionistic Epistemic Logic by\nstarting from the sequent calculus {\\bf G4IP} that we extend with operator\ndefinitions providing an embedding in intuitionistic propositional logic ({\\bf\nIPC}). With help of a candidate definition formula generator, we discover\nepistemic operators for which axioms and theorems of Artemov and Protopopescu's\n{\\em Intuitionistic Epistemic Logic} ({\\bf IEL}) hold and formulas expected to\nbe non-theorems fail. We compare the embedding of {\\bf IEL} in {\\bf IPC} with a\nsimilarly discovered successful embedding of Dosen's double negation modality,\njudged inadequate as an epistemic operator. Finally, we discuss the failure of\nthe {\\em necessitation rule} for an otherwise successful {\\bf S4} embedding and\nshare our thoughts about the intuitions explaining these differences between\nepistemic and alethic modalities in the context of the\nBrouwer-Heyting-Kolmogorov semantics of intuitionistic reasoning and knowledge\nacquisition. Keywords: epistemic intuitionistic logic, propositional\nintuitionistic logic, Prolog-based theorem provers, automatic synthesis of\nlogic systems, definition formula generation algorithms, embedding of modal\nlogics into intuitionistic logic.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:47:05 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 21:17:06 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Tarau", "Paul", ""]]}, {"id": "1907.11981", "submitter": "Curtis Bright", "authors": "Curtis Bright, Ilias Kotsireas, Albert Heinle, Vijay Ganesh", "title": "Complex Golay Pairs up to Length 28: A Search via Computer Algebra and\n  Programmatic SAT", "comments": "Extended version of arXiv:1805.05488, to appear in the Journal of\n  Symbolic Computation", "journal-ref": null, "doi": "10.1016/j.jsc.2019.10.013", "report-no": null, "categories": "cs.SC cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use techniques from the fields of computer algebra and satisfiability\nchecking to develop a new algorithm to search for complex Golay pairs. We\nimplement this algorithm and use it to perform a complete search for complex\nGolay pairs of lengths up to 28. In doing so, we find that complex Golay pairs\nexist in the lengths 24 and 26 but do not exist in the lengths 23, 25, 27, and\n28. This independently verifies work done by F. Fiedler in 2013 and confirms\nthe 2002 conjecture of Craigen, Holzmann, and Kharaghani that complex Golay\npairs of length 23 don't exist. Our algorithm is based on the recently proposed\nSAT+CAS paradigm of combining SAT solvers with computer algebra systems to\nefficiently search large spaces specified by both algebraic and logical\nconstraints. The algorithm has two stages: first, a fine-tuned computer program\nuses functionality from computer algebra systems and numerical libraries to\nconstruct a list containing every sequence which could appear as the first\nsequence in a complex Golay pair up to equivalence. Second, a programmatic SAT\nsolver constructs every sequence (if any) that pair off with the sequences\nconstructed in the first stage to form a complex Golay pair. This extends work\noriginally presented at the International Symposium on Symbolic and Algebraic\nComputation (ISSAC) in 2018; we discuss and implement several improvements to\nour algorithm that enabled us to improve the efficiency of the search and\nincrease the maximum length we search from length 25 to 28.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 21:21:34 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Bright", "Curtis", ""], ["Kotsireas", "Ilias", ""], ["Heinle", "Albert", ""], ["Ganesh", "Vijay", ""]]}, {"id": "1907.12139", "submitter": "Vladimir Lifschitz", "authors": "Amelia Harrison and Vladimir Lifschitz", "title": "Relating Two Dialects of Answer Set Programming", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  20 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 1006-1020", "doi": "10.1017/S1471068419000322", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input language of the answer set solver clingo is based on the definition\nof a stable model proposed by Paolo Ferraris. The semantics of the ASP-Core\nlanguage, developed by the ASP Standardization Working Group, uses the approach\nto stable models due to Wolfgang Faber, Nicola Leone, and Gerald Pfeifer. The\ntwo languages are based on different versions of the stable model semantics,\nand the ASP-Core document requires, \"for the sake of an uncontroversial\nsemantics,\" that programs avoid the use of recursion through aggregates. In\nthis paper we prove that the absence of recursion through aggregates does\nindeed guarantee the equivalence between the two versions of the stable model\nsemantics, and show how that requirement can be relaxed without violating the\nequivalence property. The paper is under consideration for publication in\nTheory and Practice of Logic Programming.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 20:49:19 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Harrison", "Amelia", ""], ["Lifschitz", "Vladimir", ""]]}, {"id": "1907.12156", "submitter": "Ankit Kumar Shukla", "authors": "Oliver Kullmann, Ankit Shukla", "title": "Introducing Autarkies for DQCNF", "comments": "5 pages", "journal-ref": "International Workshop on Quantified Boolean Formulas and\n  Beyond-2019", "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autarkies for SAT can be used for theoretical studies, pre-processing and\ninprocessing. They generalise satisfying assignments by allowing to leave some\nclauses \"untouched\" (no variable assigned). We introduce the natural\ngeneralisation to DQCNF (dependency-quantified boolean CNF), with the\nperspective of SAT translations for special cases. Finding an autarky for DQCNF\nis as hard as finding a satisfying assignment. Fortunately there are (many)\nnatural autarky-systems, which allow restricting the range of autarkies to a\nmore feasible domain, while still maintaining the good general properties of\narbitrary autarkies. We discuss what seems the most fundamental autarky\nsystems, and how the related reductions can be found by SAT solvers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:26:40 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Kullmann", "Oliver", ""], ["Shukla", "Ankit", ""]]}, {"id": "1907.12157", "submitter": "Tobias Meggendorfer", "authors": "Jan K\\v{r}et\\'insk\\'y, Alexander Manta, Tobias Meggendorfer", "title": "Semantic Labelling and Learning for Parity Game Solving in LTL Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose \"semantic labelling\" as a novel ingredient for solving games in\nthe context of LTL synthesis. It exploits recent advances in the automata-based\napproach, yielding more information for each state of the generated parity game\nthan the game graph can capture. We utilize this extra information to improve\nstandard approaches as follows. (i) Compared to strategy improvement (SI) with\nrandom initial strategy, a more informed initialization often yields a winning\nstrategy directly without any computation. (ii) This initialization makes SI\nalso yield smaller solutions. (iii) While Q-learning on the game graph turns\nout not too efficient, Q-learning with the semantic information becomes\ncompetitive to SI. Since already the simplest heuristics achieve significant\nimprovements the experimental results demonstrate the utility of semantic\nlabelling. This extra information opens the door to more advanced learning\napproaches both for initialization and improvement of strategies.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 09:41:22 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["K\u0159et\u00ednsk\u00fd", "Jan", ""], ["Manta", "Alexander", ""], ["Meggendorfer", "Tobias", ""]]}, {"id": "1907.12321", "submitter": "Malvin Gattinger", "authors": "Hans van Ditmarsch, Malvin Gattinger, Louwe B. Kuijer, Pere Pardo", "title": "Strengthening Gossip Protocols using Protocol-Dependent Knowledge", "comments": null, "journal-ref": "Journal of Applied Logics - IfCoLog Journal of Logics and their\n  Applications, Volume 6, Number 1 (2019)", "doi": null, "report-no": null, "categories": "cs.LO cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed dynamic gossip is a generalization of the classic telephone\nproblem in which agents communicate to share secrets, with the additional twist\nthat also telephone numbers are exchanged to determine who can call whom.\nRecent work focused on the success conditions of simple protocols such as\n\"Learn New Secrets\" (LNS) wherein an agent a may only call another agent b if a\ndoes not know b's secret. A protocol execution is successful if all agents get\nto know all secrets. On partial networks these protocols sometimes fail because\nthey ignore information available to the agents that would allow for better\ncoordination. We study how epistemic protocols for dynamic gossip can be\nstrengthened, using epistemic logic as a simple protocol language with a new\noperator for protocol-dependent knowledge. We provide definitions of different\nstrengthenings and show that they perform better than LNS, but we also prove\nthat there is no strengthening of LNS that always terminates successfully.\nTogether, this gives us a better picture of when and how epistemic coordination\ncan help in the dynamic gossip problem in particular and distributed systems in\ngeneral.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 10:27:58 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["van Ditmarsch", "Hans", ""], ["Gattinger", "Malvin", ""], ["Kuijer", "Louwe B.", ""], ["Pardo", "Pere", ""]]}, {"id": "1907.12495", "submitter": "Jessica Zangari", "authors": "Alessio Fiorentino, Nicola Leone, Marco Manna, Simona Perri, Jessica\n  Zangari", "title": "Precomputing Datalog evaluation plans in large-scale scenarios", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 1073-1089", "doi": "10.1017/S147106841900036X", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the more and more growing demand for semantic Web services over large\ndatabases, an efficient evaluation of Datalog queries is arousing a renewed\ninterest among researchers and industry experts. In this scenario, to reduce\nmemory consumption and possibly optimize execution times, the paper proposes\nnovel techniques to determine an optimal indexing schema for the underlying\ndatabase together with suitable body-orderings for the Datalog rules. The new\napproach is compared with the standard execution plans implemented in DLV over\nwidely used ontological benchmarks. The results confirm that the memory usage\ncan be significantly reduced without paying any cost in efficiency. This paper\nis under consideration in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:52:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Fiorentino", "Alessio", ""], ["Leone", "Nicola", ""], ["Manna", "Marco", ""], ["Perri", "Simona", ""], ["Zangari", "Jessica", ""]]}, {"id": "1907.12636", "submitter": "Iwona Skalna", "authors": "Grzegorz Wiaderek and Iwona Skalna", "title": "Generating theorem proving procedures from axioms of Truncated Predicate\n  Calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a~novel approach to the problem of automated theorem proving.\nPolynomial cost procedures that recognise sentences belonging to a theory are\ngenerated on a basis of a set of axioms of the so-called Truncated Predicate\nCalculus being a~subset of standard predicate calculus. Several exemplary\nproblems are included to show the performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 20:51:10 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Wiaderek", "Grzegorz", ""], ["Skalna", "Iwona", ""]]}, {"id": "1907.12933", "submitter": "Lucas Carvalho Cordeiro", "authors": "Luiz H. Sena, Iury V. Bessa, Mikhail R. Gadelha, Lucas C. Cordeiro,\n  and Edjard Mota", "title": "Incremental Bounded Model Checking of Artificial Neural Networks in CUDA", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural networks (ANNs) are powerful computing systems employed for\nvarious applications due to their versatility to generalize and to respond to\nunexpected inputs/patterns. However, implementations of ANNs for\nsafety-critical systems might lead to failures, which are hardly predicted in\nthe design phase since ANNs are highly parallel and their parameters are hardly\ninterpretable. Here we develop and evaluate a novel symbolic software\nverification framework based on incremental bounded model checking (BMC) to\ncheck for adversarial cases and coverage methods in multi-layer perceptron\n(MLP). In particular, we further develop the efficient SMT-based\nContext-Bounded Model Checker for Graphical Processing Units (ESBMC-GPU) in\norder to ensure the reliability of certain safety properties in which\nsafety-critical systems can fail and make incorrect decisions, thereby leading\nto unwanted material damage or even put lives in danger. This paper marks the\nfirst symbolic verification framework to reason over ANNs implemented in CUDA.\nOur experimental results show that our approach implemented in ESBMC-GPU can\nsuccessfully verify safety properties and covering methods in ANNs and\ncorrectly generate 28 adversarial cases in MLPs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:50:34 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Sena", "Luiz H.", ""], ["Bessa", "Iury V.", ""], ["Gadelha", "Mikhail R.", ""], ["Cordeiro", "Lucas C.", ""], ["Mota", "Edjard", ""]]}, {"id": "1907.13115", "submitter": "Tom\\'a\\v{s} Masopust", "authors": "Tom\\'a\\v{s} Masopust and Markus Kr\\\"otzsch", "title": "Partially Ordered Automata and Piecewise Testability", "comments": "arXiv admin note: text overlap with arXiv:1704.07860", "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 2 (May 11,\n  2021) lmcs:7475", "doi": null, "report-no": null, "categories": "cs.LO cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partially ordered automata are automata where the transition relation induces\na partial order on states. The expressive power of partially ordered automata\nis closely related to the expressivity of fragments of first-order logic on\nfinite words or, equivalently, to the language classes of the levels of the\nStraubing-Th\\'erien hierarchy. Several fragments (levels) have been intensively\ninvestigated under various names. For instance, the fragment of first-order\nformulae with a single existential block of quantifiers in prenex normal form\nis known as piecewise testable languages or $J$-trivial languages. These\nlanguages are characterized by confluent partially ordered DFAs or by complete,\nconfluent, and self-loop-deterministic partially ordered NFAs (ptNFAs for\nshort). In this paper, we study the complexity of basic questions for several\ntypes of partially ordered automata on finite words; namely, the questions of\ninclusion, equivalence, and ($k$-)piecewise testability. The lower-bound\ncomplexity boils down to the complexity of universality. The universality\nproblem asks whether a system recognizes all words over its alphabet. For\nptNFAs, the complexity of universality decreases if the alphabet is fixed, but\nit is open if the alphabet may grow with the number of states. We show that\ndeciding universality for general ptNFAs is as hard as for general NFAs. Our\nproof is a novel and nontrivial extension of our recent construction for\nself-loop-deterministic partially ordered NFAs, a model strictly more\nexpressive than ptNFAs. We provide a comprehensive picture of the complexities\nof the problems of inclusion, equivalence, and ($k$-)piecewise testability for\nthe considered types of automata.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 10:50:26 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 18:03:19 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 16:33:55 GMT"}, {"version": "v4", "created": "Sun, 5 Jul 2020 20:42:56 GMT"}, {"version": "v5", "created": "Fri, 16 Apr 2021 20:04:03 GMT"}, {"version": "v6", "created": "Wed, 21 Apr 2021 20:17:12 GMT"}, {"version": "v7", "created": "Mon, 10 May 2021 13:46:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Masopust", "Tom\u00e1\u0161", ""], ["Kr\u00f6tzsch", "Markus", ""]]}, {"id": "1907.13227", "submitter": "Paul Downen", "authors": "Paul Downen, Zena M. Ariola", "title": "Compiling With Classical Connectives", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 3 (August\n  28, 2020) lmcs:6740", "doi": "10.23638/LMCS-16(3:13)2020", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of polarity in computation has revealed that an \"ideal\" programming\nlanguage combines both call-by-value and call-by-name evaluation; the two\ncalling conventions are each ideal for half the types in a programming\nlanguage. But this binary choice leaves out call-by-need which is used in\npractice to implement lazy-by-default languages like Haskell. We show how the\nnotion of polarity can be extended beyond the value/name dichotomy to include\ncall-by-need by adding a mechanism for sharing which is enough to compile a\nHaskell-like functional language with user-defined types. The key to capturing\nsharing in this mixed-evaluation setting is to generalize the usual notion of\npolarity \"shifts:\" rather than just two shifts (between positive and negative)\nwe have a family of four dual shifts.\n  We expand on this idea of logical duality -- \"and\" is dual to \"or;\" proof is\ndual to refutation -- for the purpose of compiling a variety of types. Based on\na general notion of data and codata, we show how classical connectives can be\nused to encode a wide range of built-in and user-defined types. In contrast\nwith an intuitionistic logic corresponding to pure functional programming,\nthese classical connectives bring more of the pleasant symmetries of classical\nlogic to the computationally-relevant, constructive setting. In particular, an\ninvolutive pair of negations bridges the gulf between the wide-spread notions\nof parametric polymorphism and abstract data types in programming languages. To\ncomplete the study of duality in compilation, we also consider the dual to\ncall-by-need evaluation, which shares the computation within the control flow\nof a program instead of computation within the information flow.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:31:06 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 22:36:52 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 20:06:47 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 17:58:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Downen", "Paul", ""], ["Ariola", "Zena M.", ""]]}, {"id": "1907.13263", "submitter": "Pedro Lopez-Garcia", "authors": "Ignacio Casso, Jose F. Morales, Pedro Lopez-Garcia, Manuel V.\n  Hermenegildo", "title": "Computing Abstract Distances in Logic Programs", "comments": "21 pages, 8 figures; submitted to ICLP'19, accepted as technical\n  communication", "journal-ref": null, "doi": null, "report-no": "CLIP-2/2019.0", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract interpretation is a well-established technique for performing static\nanalyses of logic programs. However, choosing the abstract domain, widening,\nfixpoint, etc. that provides the best precision-cost trade-off remains an open\nproblem. This is in a good part because of the challenges involved in measuring\nand comparing the precision of different analyses. We propose a new approach\nfor measuring such precision, based on defining distances in abstract domains\nand extending them to distances between whole analyses of a given program, thus\nallowing comparing precision across different analyses. We survey and extend\nexisting proposals for distances and metrics in lattices or abstract domains,\nand we propose metrics for some common domains used in logic program analysis,\nas well as extensions of those metrics to the space of whole program analysis.\nWe implement those metrics within the CiaoPP framework and apply them to\nmeasure the precision of different analyses over both benchmarks and a\nrealistic program.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 23:58:04 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Casso", "Ignacio", ""], ["Morales", "Jose F.", ""], ["Lopez-Garcia", "Pedro", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1907.13329", "submitter": "Rob van Glabbeek", "authors": "Rob van Glabbeek, Peter H\\\"ofner and Michael Markl", "title": "A Process Algebra for Link Layer Protocols", "comments": null, "journal-ref": "Proc. 28th European Symposium on Programming, ESOP'19, (L. Caires,\n  ed.), LNCS 11423, Springer, 2019, pp. 668-693", "doi": "10.1007/978-3-030-17184-1_24", "report-no": null, "categories": "cs.NI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a process algebra for link layer protocols, featuring a unique\nmechanism for modelling frame collisions. We also formalise suitable liveness\nproperties for link layer protocols specified in this framework. To show\napplicability we model and analyse two versions of the Carrier-Sense Multiple\nAccess with Collision Avoidance (CSMA/CA) protocol. Our analysis confirms the\nhidden station problem for the version without virtual carrier sensing.\nHowever, we show that the version with virtual carrier sensing not only\novercomes this problem, but also the exposed station problem with probability\n1. Yet the protocol cannot guarantee packet delivery, not even with probability\n1.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 06:51:20 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["van Glabbeek", "Rob", ""], ["H\u00f6fner", "Peter", ""], ["Markl", "Michael", ""]]}, {"id": "1907.13348", "submitter": "Rob van Glabbeek", "authors": "Rob van Glabbeek", "title": "Reward Testing Equivalences for Processes", "comments": "This paper is dedicated to Rocco De Nicola, on the occasion of his\n  65th birthday. Rocco's work has been a source of inspiration to my own", "journal-ref": "In: Models, Languages, and Tools for Concurrent and Distributed\n  Programming, Essays Dedicated to Rocco De Nicola on the Occasion of His 65th\n  Birthday, LNCS 11665, Springer, 2019, pp. 45-70", "doi": "10.1007/978-3-030-21485-2_5", "report-no": null, "categories": "cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  May and must testing were introduced by De Nicola and Hennessy to define\nsemantic equivalences on processes. May-testing equivalence exactly captures\nsafety properties, and must-testing equivalence liveness properties. This paper\nproposes reward testing and shows that the resulting semantic equivalence also\ncaptures conditional liveness properties. It is strictly finer than both the\nmay- and must-testing equivalence.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:37:27 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["van Glabbeek", "Rob", ""]]}, {"id": "1907.13447", "submitter": "\\'Etienne Andr\\'e", "authors": "\\'Etienne Andr\\'e, Jawher Jerray and Sahar Mhiri", "title": "Time4sys2imi: A tool to formalize real-time system models under\n  uncertainty", "comments": "This is the author (and extended) version of the manuscript of the\n  same name published in the proceedings of ICTAC 2019. This work is supported\n  by the ASTREI project funded by the Paris \\^Ile-de-France Region, with the\n  additional support of the ANR national research program PACS\n  (ANR-14-CE28-0002) and ERATO HASUO Metamathematics for Systems Design Project\n  (No. JPMJER1603), JST", "journal-ref": "Springer LNCS, pages 113-123, 2019", "doi": "10.1007/978-3-030-32505-3_7", "report-no": null, "categories": "cs.SE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time4sys is a formalism developed by Thales, realizing a graphical\nspecification for real-time systems. However, this formalism does not allow to\nperform formal analyses for real-time systems. So a translation of this tool to\na formalism equipped with a formal semantics is needed. We present here\nTime4sys2imi, a tool translating Time4sys models into parametric timed automata\nin the input language of IMITATOR. This translation allows not only to check\nthe schedulability of real-time systems, but also to infer some timing\nconstraints (e.g., deadlines, offsets) guaranteeing schedulability. We\nsuccessfully applied Time4sys2imi to various examples.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:19:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Andr\u00e9", "\u00c9tienne", ""], ["Jerray", "Jawher", ""], ["Mhiri", "Sahar", ""]]}]